I"µ<h3><img class="alignnone size-full wp-image-8150" src="/assets/2017/03/storage_article_008.jpg" alt="Shipping containers" width="1200" height="400" /></h3>
<h3>Overview</h3>
<p>This articleÂ is a collaboration between Daniel Messer (Technical Marketing ManagerÂ Storage @RedHat) and Keith Tenzer (Solutions Architect @RedHat).</p>
<ul>
<li><a href="https://keithtenzer.com/2017/03/07/storage-for-containers-overview-part-i/">Storage for Containers Overview â€“ Part I</a></li>
<li><a href="https://keithtenzer.com/2017/03/24/storage-for-containers-using-gluster-part-ii/">Storage for Containers using Gluster â€“ Part II</a></li>
<li><a href="https://keithtenzer.com/2017/03/29/storage-for-containers-using-container-native-storage-part-iii/">Storage for Containers using Container Native Storage â€“ Part III</a></li>
<li><a href="https://keithtenzer.com/2017/04/07/storage-for-containers-using-ceph-rbd-part-iv/">Storage for Containers using CephÂ â€“ Part IV</a></li>
<li><a href="https://keithtenzer.com/2017/04/05/storage-for-containers-using-netapp-ontap-nas-part-v/">Storage for Containers using NetApp ONTAP NAS â€“ Part V</a></li>
<li><a href="https://keithtenzer.com/2017/04/05/storage-for-containers-using-netapp-solidfire-part-vi/">Storage for Containers using NetApp SolidFire â€“ Part VI</a></li>
</ul>
<h3>Gluster as Container-Ready Storage (CRS)</h3>
<p>In this article we will look at one of the first options of storage for containers and how to deploy it. Support for GlusterFS has been in Kubernetes and OpenShift for some time. GlusterFS is a good fit because it is available across all deployment options: bare-metal, virtual, on-premise and public cloud. The recent addition of GlusterFS running in a container will be discussed later inÂ this series.</p>
<p>GlusterFS is a distributed filesystem at heart with a native protocol (GlusterFS) and various other protocols (NFS, SMB,...). For integration with OpenShift, nodes will use the native protocol via FUSE to mount GlusterFS volumes on the nodeÂ itself and then have them bind-mount'ed into the target containers. OpenShift/Kubernetes has a native provisioner that implements requesting, releasing and (un-)mounting GlusterFS volumes.</p>
<p><!--more--></p>
<h3>CRS Overview</h3>
<p>On the storage side there is an additional component managing the cluster at the request of OpenShift/Kubernetes called "heketi". This is effectively a REST API for GlusterFS and also ships a CLI version. In the following steps we will deploy heketi among 3 GlusterFS nodes, use it to deploy a GlusterFS storage pool, connect it to OpenShift and use it to provision storage for containers via PersistentVolumeClaims.Â In total we will deploy 4 Virtual Machines. One for OpenShift (lab setup) and three for GlusterFS.</p>
<p>Note: your system should have at least a quad-core CPU, 16GB RAM and 20 GB of free disk space.</p>
<h3>Deploying OpenShift</h3>
<p>At first you will need an OpenShift deployment. An All-in-One deploymentÂ in a VM is sufficient, instructions can be found inÂ <a href="http://keithtenzer.com/2017/03/13/openshift-enterprise-3-4-all-in-one-lab-environment/">the "OpenShift Enterprise 3.4 all-in-one Lab Environment" article.</a></p>
<p>Make sure your OpenShift VM can resolve external domain names. Edit /etc/dnsmasq.conf and add the following line to use Google DNS:</p>
<pre>server=8.8.8.8</pre>
<p>Restart it:</p>
<pre># systemctl restart dnsmasq
# ping -c1 google.com</pre>
<h3>DeployingÂ Gluster</h3>
<p>For GlusterFS at least 3 VMs are required with the following specs:</p>
<ul>
<li>RHEL 7.3</li>
<li>2 CPUs</li>
<li>2 GB RAM</li>
<li>30 GB disk for OS</li>
<li>10 GB disk for GlusterFS bricks</li>
</ul>
<p>It is necessary to provide local name resolution for the 3 VMs and the OpenShift VM via a common /etc/hosts file.</p>
<p>For example (feel free to adjust the domain and host names to your environment):</p>
<pre># cat /etc/hosts
127.0.0.1      localhost localhost.localdomain localhost4 localhost4.localdomain4
::1            localhost localhost.localdomain localhost6 localhost6.localdomain6
172.16.99.144  ocp-master.lab ocp-master
172.16.128.7   crs-node1.lab crs-node1
172.16.128.8   crs-node2.lab crs-node2
172.16.128.9   crs-node3.lab crs-node3</pre>
<p><span style="text-decoration:underline;"><strong>Execute the following steps on all 3 GlusterFS VMs:</strong></span></p>
<pre># subscription-manager repos --disable="*"
# subscription-manager repos --enable=rhel-7-server-rpms</pre>
<p>If you have a GlusterFS subscription you can use it and enable theÂ rh-gluster-3-for-rhel-7-server-rpms repository.</p>
<p>If you don't you can use the unsupported GlusterFS community repositories for testing via EPEL:</p>
<pre># yum -y install http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
# rpm --import http://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-7</pre>
<p>Create a file named glusterfs-3.10.repo in /etc/yum.repos.d/</p>
<pre>[glusterfs-3.10]
name=glusterfs-3.10
description="GlusterFS 3.10 Community Version"
baseurl=https://buildlogs.centos.org/centos/7/storage/x86_64/gluster-3.10/
gpgcheck=0
enabled=1</pre>
<p>Verify the repository is active:</p>
<pre># yum repolist</pre>
<p>You should now be able to install GlusterFS</p>
<pre># yum -y install glusterfs-server</pre>
<p>A couple of basic TCP ports need to be opened for GlusterFS peers to communicate and provide storage to OpenShift:</p>
<pre># firewall-cmd --add-port=24007-24008/tcp --add-port=49152-49664/tcp --add-port=2222/tcp
# firewall-cmd --runtime-to-permanent</pre>
<p>Now we areÂ ready to startÂ the GlusterFS daemon:</p>
<pre># systemctl enable glusterd
# systemctl start glusterd</pre>
<p>That's it. GlusterFS is up and running. The rest of the configuration will be done via heketi.</p>
<p><span style="text-decoration:underline;"><strong>Install heketi on one of the GlusterFS VMs:</strong></span></p>
<pre>[root@crs-node1 ~]# yum -y install heketi heketi-client</pre>
<hr />

<h3><span style="color:#ff0000;">Update forÂ for EPEL users</span></h3>
<p>If you don't have a Red Hat Gluster Storage subscription you will get heketi from EPEL. At the time of writing this is version 3.0.0-1.el7 from October 2016 which is not working with OpenShift 3.4. You will need to update to a more current version:</p>
<pre>[root@crs-node1 ~]# yum -y install wget
[root@crs-node1 ~]# wget https://github.com/heketi/heketi/releases/download/v4.0.0/heketi-v4.0.0.linux.amd64.tar.gz
[root@crs-node1 ~]# tar -xzf heketi-v4.0.0.linux.amd64.tar.gz
[root@crs-node1 ~]# systemctl stop heketi
[root@crs-node1 ~]# cp heketi/heketi* /usr/bin/
[root@crs-node1 ~]# chown heketi:heketi /usr/bin/heketi*</pre>
<p>Create a file in /etc/systemd/system/heketi.service for the updated syntax of the v4 heketi binary:</p>
<pre>[Unit]
Description=Heketi Server
</pre>
:ET