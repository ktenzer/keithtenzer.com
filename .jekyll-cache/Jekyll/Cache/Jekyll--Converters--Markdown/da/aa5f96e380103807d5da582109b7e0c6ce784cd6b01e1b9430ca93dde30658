I"	,<p>Welcome to part two of the three-part series on creating storage services in OpenStack on NetApp. In this post we will look at how to configure NetApp storage in a OpenStack environment. In part one of the series we looked at how to install and configure OpenStack. In part three we will look at how to configure storage services in OpenStack.<!--more--></p>
<ul>
<li><a href="http://keithtenzer.com/2014/12/05/creating-storage-services-in-openstack-on-netapp-part-i-of-iii/">Building Storage Services in OpenStack on NetApp - Part I of III</a></li>
<li><a href="//keithtenzer.com/2014/12/10/building-storage-services-in-openstack-on-netapp-part-iii-of-iii/">Building Storage Services in OpenStack on NetApp - Part III of III</a></li>
</ul>
<h3>Overview</h3>
<p>NetApp has the most unified storage combined with the best feature set in the industry. NetApp provides the only storage operating system which allows you to manage your data on-premise, off-premise, in private clouds, public clouds or even hybrid clouds. NetApp is a perfect fit for OpenStack as a storage platform. In this post we will explore why, discuss options and show you how to properly configure your NetApp storage system for OpenStack.</p>
<p>Below is a diagram illustrating the broad-reaching data management capabilities of Data ONTAP:</p>
<p><a href="https://keithtenzer.files.wordpress.com/2014/12/ontap_architecture1.jpg"><img class="  wp-image-190 aligncenter" src="/assets/2014/12/ontap_architecture1.jpg?w=300" alt="ontap_architecture" width="716" height="296" /></a></p>
<h3>Storage Considerations</h3>
<p>There are currently two OpenStack services which support storage: Cinder (block) and Swift (object). Manilla (file services) is an incubation project and slated for the kilo release later in 2015. For the purpose of this post I am going to focus on Cinder.</p>
<h4>E-Series vs FAS</h4>
<p>NetApp offers two product lines for storage the E-Series and FAS lines. I would not recommend choosing between E-Series or FAS as they have different uses in terms of OpenStack. E-Series is best used for Swift as a backend to object storage. It is very fast and has a great price to performance value. Swift is an object store and abstracts storage management. You want a fast, resilient, highly available storage system and that is the E-Series. The E-Series storage also has a nice feature called "Dynamic Disk Pools" which implements CRUSH (erasure coding) and it reduces storage footprint from 3x to just 1.25x for Swift in OpenStack environments.</p>
<p>FAS is the clear choice when it comes to Cinder storage where you want data management features and capabilities exposed in OpenStack.</p>
<h4>ONTAP 7mode vs Clustered Data ONTAP</h4>
<p>In the FAS world there are also two choices. Personally I don't think it is much of a choice. Clustered Data ONTAP is the evolution of FAS storage. It is a scale-out architecture and scales in three dimensions: operations, performance and capacity. It is by far the most complete storage system ever built and certainly most capable storage system on the market today. Clustered Data ONTAP virtualizes storage into what are called Storage Virtual Machines (SVMs) which enable secure multi-tenancy. If we think about cloud computing platforms, multi-tenancy is the basis and a raw ingredient. Clustered Data ONTAP provides world-class data management, scalability in multiple dimensions, unified protocols (ISCSI, NFS, CIFS, and FC) and non-disruptive always-on operations in the same storage system. The only reason not to go with Clustered Data ONTAP is that you already have 7mode and don't have budget, otherwise it is no-brainer. NetApp of course supports 7mode with its OpenStack driver so this is an option for those that can't go with Clustered Data ONTAP.</p>
<h3>Storage Protocol</h3>
<p>There are two options for connecting storage to Cinder: ISCSI or NFS. At this time Fiber Chanel (FC) is not supported and if we consider cloud computing architectures, FC does not really have the horizontal scalability required. Let's face it we don't need a second segregated island for data traffic, IP networks are proven and scale, FC is simply out of this picture.</p>
<p>Cinder exposes block devices to virtual machines or instances but it can reside on both NFS or ISCSI storage. I would recommend NFS as NFS storage is much easier to manage than ISCSI. However if you are a SAN only shop or coming from a SAN environment, ISCSI might be a nice way to bridge into OpenStack and not have to make changes to how hosts connect to their storage. You can even use a combination of both. You could use NFS for creating Cinder volumes where instances reside and ISCSI for host mount points. Again I would recommend NFS for everything and that is what I am going to cover but I wanted to at least explain both options.</p>
<h3>Configuring Clustered Data ONTAP</h3>
<p>We have made the following storage decisions for our OpenStack Cinder configuration:</p>
<ul>
<li>FAS (storage platform)</li>
<li>Clustered Data ONTAP (storage operating system)</li>
<li>NFS (protocol)</li>
</ul>
<h4>Storage Virtual Machines</h4>
<p>Configure two storage virtual machines. One will be used to provide storage to Cinder volumes and the other for DR. At a minimum you will need two IPs for each SVM. One for the management LIF and the other the data LIF. Ensure that both SVMs data LIFs are reachable from the OpenStack host.</p>
<p>Create SVMs</p>
<ul>
<li>
<pre>vserver create -vserver &lt;svm&gt; -subtype default -rootvolume &lt;rootvol&gt; -aggregate &lt;aggr&gt; -rootvolume-security-style unix</pre>
</li>
</ul>
<p>Create management LIFs</p>
<ul>
<li>
<pre>net int create -vserver &lt;svm&gt; -lif data -role data -data-protocol nfs -home-node steve-02 -home-port e0a -address &lt;ip&gt; -netmask &lt;netmask&gt; -firewall-policy mgmt</pre>
</li>
</ul>
<p>Create Data LIFs</p>
<ul>
<li>
<pre>net int create -vserver &lt;svm&gt; -lif data -role data -data-protocol nfs -home-node steve-02 -home-port e0a -address &lt;ip&gt; -netmask &lt;netmask&gt; -firewall-policy data</pre>
</li>
</ul>
<p>Ensure NFS services are running on both SVMs</p>
<ul>
<li>
<pre>nfs status -vserver</pre>
</li>
</ul>
<p>If NFS is not running you can start NFS as follows:</p>
<ul>
<li>
<pre>nfs on -vserver</pre>
</li>
<li>
<pre>nfs start -vserver</pre>
</li>
</ul>
<h4>Create Export policy</h4>
<p>In order to export NetApp volumes to OpenStack via NFS we need an export policy. Create an export policy as follows:</p>
<ul>
<li>
<pre>export-policy create -vserver &lt;svm&gt; -policyname openstack</pre>
</li>
</ul>
<ul>
<li>
<pre>export-policy rule create -vserver &lt;svm&gt; -policyname openstack -clientmatch &lt;IP or CIDR&gt; -rorule any -rwrule any</pre>
</li>
</ul>
<p>Add export policy to SVM root volume otherwise OpenStack host wont have write permission</p>
<ul>
<li>
<pre>volume modify -vserver &lt;svm&gt; -volume &lt;rootvol&gt; -policy openstack</pre>
</li>
</ul>
<h4>Create NetApp Volumes</h4>
<p>NetApp volumes are containers for storing files or LUNs. In the case of OpenStack a NetApp volume is a container for Cinder volumes which is a container for block devices. We will create three volumes for Cinder and each volume will define a different storage service e.g. gold, silver and bronze. In addition we will create a fourth volume for storing glance images. The beauty here is that the storage admin can define storage services based on containers and expose them up through Cinder for consumption in OpenStack.</p>
<p>Clustered Data ONTAP allows us to mix SSDs and Spinning Disk in the same storage system to enable building services with different performance characteristics. One can create aggregates which contain NetApp volumes using SSDs, Spinning Disk or even a mix (hybrid aggregates). In addition Clustered Data ONTAP provides QoS at the IOP or bandwidth level which we can use to further define or even restrict our storage services.</p>
<h5>Gold Volume</h5>
<p>The gold volume will be our most expensive storage service but will have the best performance and availability. We will create a volume on the best performing aggregate and in addition mirror that volume to another SVM.</p>
<ul>
<li>
<pre>volume create -vserver &lt;svm&gt; -volume openstack_gold -aggregate &lt;aggr&gt; -size 500g -state online -type RW -policy openstack -snapshot-policy none -space-guarantee volume</pre>
</li>
</ul>
<ul>
<li>
<pre>volume mount -vserver &lt;svm&gt; -volume openstack_gold -junction-path /openstack_gold</pre>
</li>
</ul>
<ul>
<li>
<pre>snapmirror initialize -source-path &lt;svm&gt;:openstack_gold -destination-path &lt;svm dr&gt;:openstack_gold_dr</pre>
</li>
</ul>
<h5>Silver Volume</h5>
<p>The silver volume will have a good price to performance but not include DR. We will create volume from a best performing aggregate and enable compression for space savings.</p>
<ul>
<li>
<pre>volume create -vserver &lt;svm&gt; -volume openstack_silver -aggregate &lt;aggr&gt; -size 500g -state online -type RW -policy openstack -snapshot-policy none -space-guarantee volume</pre>
</li>
</ul>
<ul>
<li>
<pre>volume mount -vserver &lt;svm&gt; -volume openstack_silver -junction-path /openstack_silver</pre>
</li>
</ul>
<ul>
<li>
<pre>volume efficiency on -vserver &lt;svm&gt; -volume openstack_silver</pre>
</li>
</ul>
<ul>
<li>
<pre>volume efficiency modify -vserver &lt;svm&gt; -volume openstack_silver -compression true -inline-compression true</pre>
</li>
</ul>
<h5>Bronze Volume</h5>
<p>The bronze volume will have lower performing storage and be the most cost-effective service. We will create volume from lower performing aggregate and enable dedup for space savings. We will not enable DR.</p>
<ul>
<li>
<pre>volume create -vserver &lt;svm&gt; -volume openstack_bronze -aggregate &lt;aggr&gt; -size 500g -state online -type RW -policy openstack -snapshot-policy none -space-guarantee volume</pre>
</li>
</ul>
<ul>
<li>
<pre>volume mount -vserver &lt;svm&gt; -volume openstack_bronze -junction-path /openstack_bronze</pre>
</li>
</ul>
<ul>
<li>
<pre>volume efficiency on -vserver &lt;svm&gt; -volume openstack_bronze</pre>
</li>
</ul>
<ul>
<li>
<pre>volume efficiency policy create -vserver &lt;svm&gt; -policy bronze_dedupe -schedule  -duration - -enabled true</pre>
</li>
</ul>
<h5>Glance Volume</h5>
<p>Finally we will create a volume for storing images in Glance. This volume does not have high performance requirements but we will certainly enable dedup to greatly reduce the storage footprint.</p>
<ul>
<li>
<pre>volume create -vserver &lt;svm&gt; -volume openstack_glance -aggregate &lt;aggr&gt; -size 500g -state online -type RW -policy openstack -snapshot-policy none -space-guarantee volume</pre>
</li>
</ul>
<ul>
<li>
<pre>volume mount -vserver &lt;svm&gt; -volume openstack_glance -junction-path /openstack_glance</pre>
</li>
</ul>
<ul>
<li>
<pre>volume efficiency on -vserver &lt;svm&gt; -volume openstack_bronze</pre>
</li>
</ul>
<ul>
<li>
<pre>volume efficiency policy create -vserver &lt;svm&gt; -policy bronze_dedupe -schedule  -duration - -enabled true</pre>
</li>
</ul>
<p>The next post in the series will cover configuring the above NetApp volumes in OpenStack as storage services.</p>
<p>(c) 2014 Keith Tenzer</p>
:ET