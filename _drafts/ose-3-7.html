---
layout: post
title: OSE 3.7
date: 
type: post
parent_id: '0'
published: false
password: ''
status: draft
categories: []
tags: []
meta: {}
author:
  login: ktenzer1
  email: keith.tenzer@gmail.com
  display_name: ktenzer
  first_name: ''
  last_name: ''
permalink: "/"
---
<h2><img class="alignnone size-full wp-image-11475" src="{{ site.baseurl }}/assets/logotype_rh_openshiftcontainerplatform_wlogo_cmyk_black.jpg" alt="Logotype_RH_OpenShiftContainerPlatform_wLogo_CMYK_Black" width="5612" height="1444" /></h2>
<h2>Overview</h2>
<p>OpenShift Container Platform 3.6 went GA on August 9, 2017. You can read more about the release and new features <a href="https://blog.openshift.com/announcing-the-openshift-container-platform-3-6-ga/">here</a>. In this article we will setup a standard non-HA environment that is perfect for PoCs or labs. Before we begin, let's explain OpenShift for those that may be starting their OpenShift journey today. OpenShift is a complete container application build + run-time platform built on Kubernetes (Container Orchestration) and Docker (Container Packaging Format). Organizations looking to adopt containerization for their applications need of course a lot more than just technology, (Kubernetes and Docker), they need a real platform. OpenShift provides a service catalog for containerized applications, <a href="https://access.redhat.com/containers/#/product/RedHatOpenshiftContainerPlatform">huge selection of already certified application runtimes + xPaaS services</a>, a method for building containerized applications (source to image), centralized application logging, metrics, autoscaling, application deployments (Blue-Green, A/B, Canary, Rolling), integrated Jenkins CI/CD pipelines, integrated docker registry, load balancing / routes to containerized apps, multi-tenant SDN, security features (SELinux, secrets, security context), management tooling supporting multiple OpenShift environments (CloudForms), persistent storage (built-in Container Native Storage), automated deployment tooling based on Ansible and much, much more. OpenShift is a platform that runs on any infrastructure, from bare-metal to virtualization to public cloud (Amazon, Google, Microsoft), providing portability across cloud infrastructure for containerized applications. All of these things together is what truly enables organizations to move to DevOps, increase application release cycles, speed up innovation cycles, scale efficiently, gain independence from infrastructure providers and deliver new capabilities faster with more reliability to their customers.</p>
<p><!--more--></p>
<h2>OpenShift Environment</h2>
<p>OpenShift has a few different architectures (Single Master, Multiple Master Integrated ETCD or Multiple Master Separate ETCD).</p>
<p><strong>Single Master - </strong>This is a non-HA configuration. First there is only a single master which also runs ETCD and a single infrastructure node. The infrastructure node also runs OpenShift services such as docker registry, router, metrics and logging. This is only intended for lab or PoC environments.</p>
<p><strong>Multiple Master Integrated ETCD - </strong>This is an HA configuration with three masters and two or more infrastructure nodes. OpenShift control plane as well as critical services are all in active/active HA configuration. ETCD service is running on the three masters.</p>
<p><strong>Multiple Master Separate ETCD</strong> - This is an HA configuration with three masters and two or more infrastructure nodes. OpenShift control plane as well as critical services are all in active/active HA configuration. ETCD service is running on three separate servers. Since ETCD maintains state for OpenShift cluster it is critical the service is not only available but provides fast responses and updates. In some environments due to number of objects or size, it is preferred to separate ETCD. This is of course an architectural decision that requires discussion and planning.</p>
<p>Here we have chosen to go with Single Master. Note: when referring to regions and zones, these are just labels that allow us to isolate nodes to different applications / projects. They are not dependent on public cloud regions.</p>
<p><img class="alignnone size-full wp-image-11304" src="{{ site.baseurl }}/assets/ocp_arch1.png" alt="ocp_arch" width="882" height="628" /></p>
<p>API and management traffic go to the master server. In a multiple master configuration a load balancer would balance traffic across the masters. Application traffic instead goes directly to the infrastructure node. In case of HA configuration,  multiple infrastructure nodes and of course a load balancer in front would be configured. Application domains are in this case directed toward the single infrastructure node in DNS. In case of multiple infrastructure nodes, DNS should point application domains to load balancer. DNS round-robin is not a good option as it will continue to send traffic to infrastructure nodes, even if unavailable.</p>
<p>In this configuration we will create two regions and two zones. OpenShift through labeling lets you tailor the architecture to meet your infrastructure and Affinity / Anti-Affinity requirements. Infrastructure and applications are isolated to specific nodes using the region. Within the application region (Primary) we separate applications between zones test and production. Using the multi-tenant SDN included in OpenShift, traffic between applications in test and production will be restricted or not allowed.</p>
<p>Application persistence is provided by container native storage. Normally we would define special nodes for providing storage, in this case however the application nodes will fill that need. Container Native Storage works by running glusterfs (software-defined scale-out storage system) inside a container on each node. Container Native Storage requires a minimum of three nodes. The glusterfs container consumes local disks, available on node and builds a cluster-wide highly redundant storage system spanning all the nodes. OpenShift provides a storage class for consuming the storage dynamically. Developers can simply ask for storage by size (GB) and is is provided on the fly, dynamically assuming of course quotas aren't overwritten. In this environment storage is also automatically reclaimed based on policies defined by the Developer.</p>
<h2>Configure OpenShift Nodes</h2>
<p>In OpenShift there are several types of nodes (master, infra, app, storage and etcd). Each type of node has different CPU, memory and storage requirements. The minimum requirements are 8GB of memory for a node but you can get away with far less (if you disable checking in configuration). Since this is a lab setup we will do just that.</p>
<p>For storage we will either use 40GB or 30GB root disk, 15GB for Docker and 100GB for Container Native Storage. Everything is being provided by the virtualization layer, in this case Red Hat Virtualization powered by KVM. Again this could be bare-metal, any virtualization platform or any public cloud provider.</p>
<p><strong>Master Nodes</strong></p>
<p><img class="alignnone size-full wp-image-11371" src="{{ site.baseurl }}/assets/ocp_cpu_master.png" alt="ocp_cpu_master" width="496" height="106" /></p>
<p><img class="alignnone size-full wp-image-11310" src="{{ site.baseurl }}/assets/ocp_disks_master.png" alt="ocp_disks_master" width="997" height="186" /></p>
<p><strong>Infra Nodes</strong></p>
<p><img class="alignnone size-full wp-image-11374" src="{{ site.baseurl }}/assets/ocp_cpu_infra.png" alt="ocp_cpu_infra" width="502" height="111" /></p>
<p><img class="alignnone size-full wp-image-11308" src="{{ site.baseurl }}/assets/ocp_disks_infra.png" alt="ocp_disks_infra" width="1009" height="191" /></p>
<p><strong>App Nodes</strong></p>
<p><img class="alignnone size-full wp-image-11376" src="{{ site.baseurl }}/assets/ocp_cpu_app.png" alt="ocp_cpu_app" width="499" height="107" /></p>
<p><img class="alignnone size-full wp-image-11307" src="{{ site.baseurl }}/assets/ocp_disks_app.png" alt="ocp_disks_app" width="1014" height="219" /></p>
<h2>Prepare OpenShift Nodes</h2>
<p>Once the infrastructure is complete, a Container Operating System needs to be installed. The Operating System is as important as ever since containers are simply processes that run on Linux. Containers do not provide good isolation like VMs so security features in Operating System like SELinux, Kernel Capabilities, Container Signing, Secured Registries, TLS communications, Secrets and Security Context's are incredibly important. Thankfully you get it all out-of-the-box with OpenShift.</p>
<p>The choice of Operating Systems comes down to RHEL or Atomic. Atomic is a RHEL kernel but is image based (no RPMs), everything that is installed or runs in Atomic must run as container. RHEL allows more flexibility to install other required software but the recommendation is definitely Atomic for OpenShift, especially if additional software is not required (monitoring, etc).</p>
<p><strong>[All Nodes]</strong></p>
<p><strong>Register Repositories</strong></p>
<pre># subscription-manager register</pre>
<pre># subscription-manager attach --pool=&lt;pool_id&gt;</pre>
<pre># subscription-manager repos --disable="*"</pre>
<pre># subscription-manager repos \
    --enable="rhel-7-server-rpms" \
    --enable="rhel-7-server-extras-rpms" \
    --enable="rhel-7-server-ose-3.7-rpms" \
    --enable="rhel-7-fast-datapath-rpms"</pre>
<p><strong>Install Required Packages</strong></p>
<pre># yum install -y wget git net-tools bind-utils iptables-services bridge-utils bash-completion kexec-tools sos psacct</pre>
<p><strong>Update Operating System</strong></p>
<pre># yum update -y</pre>
<p><strong>Install OpenShift Utilities</strong></p>
<pre># yum install -y atomic-openshift-utils</pre>
<p><strong>Configure Docker</strong></p>
<pre># yum install -y docker-1.12.6</pre>
<pre># vi /etc/sysconfig/docker
OPTIONS='--insecure-registry=172.30.0.0/16 --selinux-enabled --log-driver=journald'</pre>
<p>Since we are not using Docker Hub (god save you if you are doing that) and the OpenShift Docker registry is internal, we can safely allow insecure registry. Make sure SELinux is enabled, this is a big deal for securing your containers. The log driver should be journald.</p>
<p>[root@ose37 ~]# lsblk<br />
NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT<br />
sr0 11:0 1 1024M 0 rom<br />
vda 252:0 0 100G 0 disk<br />
├─vda1 252:1 0 500M 0 part /boot<br />
└─vda2 252:2 0 99.5G 0 part<br />
├─rhel-root 253:0 0 91.6G 0 lvm /<br />
└─rhel-swap 253:1 0 7.9G 0 lvm [SWAP]<br />
vdb 252:16 0 20G 0 disk</p>
<pre>cat &lt;&lt;EOF &gt; /etc/sysconfig/docker-storage-setup
DEVS=/dev/vdb
VG=docker-vg
EOF</pre>
<p>Here we will use the 15GB disk for docker storage. You can use the 'lsblk' command to easily see the device numbers of your disks. Make sure the disk has no partitions.</p>
<pre># docker-storage-setup</pre>
<pre># systemctl enable docker</pre>
<pre> # systemctl start docker</pre>
<p><strong>Name Resolution</strong></p>
<p>Here we have two options, either use DNS or hosts file. Obviously for a real environment you are going with DNS but some people are lazy.</p>
<p>If you are using DNS, create a wildcard pointing from application domain (apps.lab.com) to the infrastructure node. You can simply create an 'A' record in your named zone (*.apps.lab).</p>
<pre>vi /etc/hosts
192.168.122.60 ose37.lab.com ose37</pre>
<p><strong>[On Laptop]</strong></p>
<p>In case you are using hosts file, make sure your laptop can resolve the OpenShift master and Hawkular (metrics) as well as Kibana (logging) need to be pointed at infra node. Additionally if you create any applications in OpenShift and expose them via routes, those host names need to go into hosts file and resolve to infra node. Hopefully you see now why a wildcard DNS is recommended.</p>
<p>Example of my Laptop /etc/hosts.</p>
<pre># vi /etc/hosts
192.168.122.60 ose37.lab.com ose37
192.168.122.60 hawkular-metrics.apps.lab.com hawkular-metrics
192.168.122.60 kibana.apps.lab.com kibana</pre>
<h2>Configure OpenShift</h2>
<p>As mentioned, OpenShift uses Ansible as it's life-cyle management and deployment tool. There are playbooks for installing, upgrading and adding or removing various components (Metrics, Logging, Storage, etc). Normally a Bastion host would be deployed for Ansible. In this case we install Ansible on the master.</p>
<p><strong>[On Master]</strong></p>
<p><strong>Configure SSH Authorization</strong></p>
<pre># ssh-keygen</pre>
<pre># ssh-copy-id ose37.lab.com</pre>
<p><strong>Configure Deployment Options</strong></p>
<p>The Ansible playbooks require an inventory file that defines the hosts and also configuration options for OpenShift via group vars.</p>
<pre># vi /etc/ansible/hosts</pre>
<pre># Create an OSEv3 group that contains the masters and nodes groups
[OSEv3:children]
masters
nodes
nfs

# Set variables common for all OSEv3 hosts
[OSEv3:vars]
# SSH user, this user should allow ssh based auth without requiring a password
ansible_ssh_user=root

# OpenShift Deployment, enterprise of course!
openshift_deployment_type=openshift-enterprise

# Set Domain for Apps
openshift_master_default_subdomain=apps.lab

# Enable htpasswd authentication; defaults to DenyAllPasswordIdentityProvider
openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]

# Set networking to multi-tenant
os_sdn_network_plugin_name='redhat/openshift-ovs-multitenant'

# Disable disk and memory checks
openshift_disable_check=disk_availability,memory_availability

# Deploy router
openshift_hosted_manage_router=true
openshift_router_selector='region=infra'

# Deploy Registry
openshift_hosted_manage_registry=true
openshift_registry_selector='region=infra'
openshift_hosted_registry_storage_kind=nfs
openshift_hosted_registry_storage_access_modes=['ReadWriteMany']
openshift_hosted_registry_storage_nfs_directory=/exports
openshift_hosted_registry_storage_nfs_options='*(rw,root_squash)'
openshift_hosted_registry_storage_volume_name=registry
openshift_hosted_registry_storage_volume_size=10Gi

# Deploy logging
openshift_logging_install_logging=true
openshift_logging_storage_kind=nfs
openshift_logging_storage_access_modes=['ReadWriteOnce']
openshift_logging_storage_nfs_directory=/exports
openshift_logging_storage_nfs_options='*(rw,root_squash)'
openshift_logging_storage_volume_name=logging
openshift_logging_storage_volume_size=10Gi

# Deploy Metrics
openshift_metrics_install_metrics=true
openshift_metrics_storage_kind=nfs
openshift_metrics_storage_access_modes=['ReadWriteOnce']
openshift_metrics_storage_nfs_directory=/exports
openshift_metrics_storage_nfs_options='*(rw,root_squash)'
openshift_metrics_storage_volume_name=metrics
openshift_metrics_storage_volume_size=10Gi

# Deploy Service Catalog
openshift_enable_service_catalog=true

# ETCD Storage
openshift_hosted_etcd_storage_kind=nfs
openshift_hosted_etcd_storage_nfs_options="*(rw,root_squash,sync,no_wdelay)"
openshift_hosted_etcd_storage_nfs_directory=/exports/etcd 
openshift_hosted_etcd_storage_volume_name=etcd-vol2 
openshift_hosted_etcd_storage_access_modes=["ReadWriteOnce"]
openshift_hosted_etcd_storage_volume_size=1G
openshift_hosted_etcd_storage_labels={'storage': 'etcd'}

[nfs]
ose37.lab.com

# host group for masters
[masters]
ose37.lab.com

# host group for nodes, includes region info
[nodes]
ose37.lab.com openshift_schedulable=True openshift_node_labels="{'region': 'infra', 'zone': 'default'}"</pre>
<h2>Install OpenShift</h2>
<p>Once the hosts and configurations options are set in the inventory file, simply run the playbook. There are many playbooks, even ones that integrate directly with infrastructure platforms like (Amazon, Google, Microsoft). These can also setup VMs and infrastructure components, like load balancers that are required. In our case we will go with BYO (bring your own), basically means we have taken care of infrastructure and it is all ready to go. This is typically what you want for on-premise deployments.</p>
<pre># ansible-playbook -vv /usr/share/ansible/openshift-ansible/playbooks/byo/config.yml</pre>
<pre>PLAY RECAP *************************************************************************************************
localhost : ok=13 changed=0 unreachable=0 failed=0 
ose37.lab.com : ok=738 changed=266 unreachable=0 failed=0



INSTALLER STATUS *******************************************************************************************
Initialization : Complete
Health Check : Complete
etcd Install : Complete
NFS Install : Complete
Master Install : Complete
Master Additional Install : Complete
Node Install : Complete
Hosted Install : Complete
Metrics Install : Complete</pre>
<p>Success!!! Okay if you are paying attention and know Ansible, the recap shows few tasks actually changed anything. This is because I had an error in my storage configuration due to using wrong disk device, hence my playbook run failed previously. Not a big deal though, I simply fixed the issue in inventory file and re-ran Ansible. This is the beauty, simplicity and power of Ansible. Fail, Fail until you succeed and then you can reproduce it every time successfully.</p>
<h2>Verifying OpenShift Environment</h2>
<p>Once installation is complete, check to make sure additional services (Storage, Logging and Metrics) are functioning properly.</p>
<p><strong>Create Admin User</strong></p>
<p><strong>[On Master]</strong></p>
<p>Login to the master and create an admin user for external access through UI or CLI.</p>
<pre>[root@master1 ~]# oc login -u system:admin -n default</pre>
<pre>[root@master1 ~]# htpasswd -c /etc/origin/master/htpasswd admin</pre>
<pre>[root@master1 ~]# oadm policy add-cluster-role-to-user \
cluster-admin admin</pre>
<pre>[root@master1 ~]# oc login -u admin -n default</pre>
<p><strong>Ensure Name Resolution Correct</strong></p>
<p>Make su</p>
<pre># vi /etc/resolv.conf
search cluster.local
nameserver 192.168.122.60</pre>
<p>[root@ose37 ~]# oc get pv<br />
NAME CAPACITY ACCESSMODES RECLAIMPOLICY STATUS CLAIM STORAGECLASS REASON AGE<br />
logging-volume 10Gi RWO Retain Bound logging/logging-volume 42m<br />
metrics-volume 10Gi RWO Retain Bound openshift-infra/metrics-1 21h<br />
registry-volume 10Gi RWX Retain Bound default/registry-claim 21h</p>
<p><strong>GUI</strong></p>
<p>To access GUI simply point browser from laptop at the master on port 8443 (https://master.lab:8443).</p>
<p><strong>Storage</strong></p>
<p>These steps should be done on OpenShift master node(s).</p>
<p><strong>Enable repository for CNS</strong></p>
<pre class="screen">subscription-manager repos --enable=rh-gluster-3-for-rhel-7-server-rpms</pre>
<p><strong>Install CNS Tools</strong></p>
<pre class="screen">yum install cns-deploy heketi-client</pre>
<p>&nbsp;</p>
<pre class="screen">subscription-manager repos --disable=rh-gluster-3-for-rhel-7-server-rpms</pre>
<p>ON app all nodes</p>
<p>subscription-manager repos --enable=rh-gluster-3-for-rhel-7-server-rpms</p>
<p>yum install -y glusterfs-client</p>
<p>subscription-manager repos --disable=rh-gluster-3-for-rhel-7-server-rpms</p>
<p>&nbsp;</p>
<pre class="screen">-A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 24007 -j ACCEPT
-A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 24008 -j ACCEPT
-A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 2222 -j ACCEPT
-A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m multiport --dports 49152:49664 -j ACCEPT
-A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 24010 -j ACCEPT
-A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 3260 -j ACCEPT
-A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 111 -j ACCEPT</pre>
<p>&nbsp;</p>
<pre># systemctl reload iptables</pre>
<p>&nbsp;</p>
<pre class="screen"># lsmod | grep dm_thin_pool</pre>
<pre class="screen"># lsmod | grep dm_multipath</pre>
<pre class="screen"># lsmod | grep target_core_user</pre>
<div class="para">If the modules are not loaded, then execute the following command to load the modules:</div>
<pre class="screen"># modprobe dm_thin_pool</pre>
<pre class="screen"># modprobe dm_multipath</pre>
<pre class="screen"># modprobe target_core_user</pre>
<p>&nbsp;</p>
<pre class="screen"># cat /etc/modules-load.d/dm_thin_pool.conf
dm_thin_pool</pre>
<pre class="screen"># cat /etc/modules-load.d/dm_multipath.conf
dm_multipath</pre>
<pre class="screen"># cat /etc/modules-load.d/target_core_user.conf
target_core_user</pre>
<p>&nbsp;</p>
<pre class="screen"># systemctl add-wants multi-user rpcbind.service
# systemctl enable rpcbind.service
# systemctl start rpcbind.service</pre>
<p>&nbsp;</p>
<ol>
<li class="listitem">
<pre class="screen"># yum install iscsi-initiator-utils device-mapper-multipath</pre>
</li>
<li class="listitem">
<div class="para">To enable multipath, execute the following command:</div>
<pre class="screen"># mpathconf --enable</pre>
</li>
<li class="listitem">
<div class="para">Add the following content to the <code class="command">devices</code> section in the <code class="filename">/etc/multipath.conf</code>file</div>
<pre class="screen">device {
                vendor "LIO-ORG"
                user_friendly_names "yes" # names like mpatha
                path_grouping_policy "failover" # one path per group
                path_selector "round-robin 0"
                failback immediate
                path_checker "tur"
                prio "const"
                no_path_retry 120
                rr_weight "uniform"
        }</pre>
</li>
<li class="listitem">
<div class="para">Execute the following command to restart services:</div>
<pre class="screen"># systemctl restart multipathd</pre>
</li>
</ol>
<p>&nbsp;</p>
<p>On all nodes</p>
<pre># vi /etc/dnsmasq.conf
...
address=/.apps.lab.com/192.168.122.61
...</pre>
<pre># systectl restart dnsmasq</pre>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>[root@ose37m1 ~]# systemctl restart atomic-openshift-master-controllers<br />
[root@ose37m1 ~]# systemctl restart atomic-openshift-master-api</p>
<p>&nbsp;</p>
<pre class="screen"># oc new-project storage</pre>
<p>&nbsp;</p>
<pre class="screen"># oadm policy add-scc-to-user privileged -z storage</pre>
<p>&nbsp;</p>
<pre class="screen"># oadm policy add-scc-to-user privileged -z router
# oadm policy add-scc-to-user privileged -z default</pre>
<p>&nbsp;</p>
<p>deploy storage router</p>
<pre class="screen">oadm router storage-router --replicas=1</pre>
<p>&nbsp;</p>
<p>[root@ose37m1 ~]# oc get pods<br />
NAME READY STATUS RESTARTS AGE<br />
storage-router-1-8hpfz 1/1 Running 0 37s</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>cns-deploy -n storage /usr/share/heketi/topology.json</p>
<p>&nbsp;</p>
<p><strong>Logging</strong></p>
<p>Open the logging project. Under Kibana you will see a route or URL (https://kibana.apps.lab). This will change depending on what domain was configured in OpenShift. Connect and you will be prompted to login. OpenShift uses elastic search and automatically populates it with projects and users from OpenShift. Users only see logs for projects and applications where they have access.</p>
<p><img class="alignnone size-full wp-image-11317" src="{{ site.baseurl }}/assets/ocp_logging.png" alt="ocp_logging" width="1745" height="906" /></p>
<p><strong>Metrics</strong></p>
<p>In the logging project you will notice CPU, Memory and Network utilization are shown. This is coming from data collected in Hawkular and a 15 minute sample is shown.</p>
<p><img class="alignnone size-full wp-image-11319" src="{{ site.baseurl }}/assets/ocp_metrics.png" alt="ocp_metrics" width="1537" height="552" /></p>
<p>Under the pods you can view more details and see the performance data historically over much longer period of time.</p>
<p><img class="alignnone size-full wp-image-11320" src="{{ site.baseurl }}/assets/ocp_metrics_2.png" alt="ocp_metrics_2" width="1531" height="632" /></p>
<p><strong>Miscellaneous</strong></p>
<p>In OpenShift 3.6 not only OpenShift itself but also additional services are installed with Ansible. Here is an example of how to install and remove logging.</p>
<p><strong>[On Master]</strong></p>
<p><strong>Remove Logging</strong></p>
<pre># vi /etc/ansible/hosts
openshift_logging_install_logging=false</pre>
<pre>ansible-playbook -vv /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-cluster/openshift-metrics.yml</pre>
<p><strong>Remove Metrics</strong></p>
<pre># vi /etc/ansible/hosts
openshift_metrics_install_metrics=false</pre>
<pre>ansible-playbook -vv /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-cluster/openshift-logging.yml -e openshift_logging_install_logging=False</pre>
<h2>Coolstore</h2>
<p>Coolstore is a polygot demo application comprised of many different services that form a shopping car application. It is a great example of the value OpenShift as a platform brings to a microservice orientated architecture. It illustrates how various containerized services can communicate with one another and how services can be independently released.</p>
<p><strong>Clone Git Repo</strong></p>
<pre># git clone https://github.com/jbossdemocentral/coolstore-microservice.git</pre>
<p><strong>Change Dir</strong></p>
<pre># cd coolstore-microservice</pre>
<p><strong>Checkout Stable 3.7 Branch</strong></p>
<pre># git checkout stable-ocp-3.7</pre>
<p><strong>Create Project</strong></p>
<pre># oc new-project coolstore</pre>
<p><strong>Deploy Coolstore App</strong></p>
<pre># <code>oc process -f openshift/coolstore-template.yaml | oc create -f -</code></pre>
<p><strong>Add Coolstore Routes to Hosts File</strong></p>
<pre># vi /etc/hosts
...
192.168.122.60 web-ui-coolstore.apps.lab
192.168.122.60 turbine-server-coolstore.apps.lab
192.168.122.60 coolstore-gw-coolstore.apps.lab
192.168.122.60 hystrix-dashboard-coolstore.apps.lab</pre>
<p>Open Coolstore App</p>
<pre>http://web-ui-coolstore.apps.lab</pre>
<p><img class="alignnone size-full wp-image-11573" src="{{ site.baseurl }}/assets/ocp_coolstore.png" alt="ocp_coolstore.PNG" width="1834" height="786" /></p>
<p>&nbsp;</p>
<h2>Summary</h2>
<p>In this article we introduced OpenShift being a platform for containerized applications and spoke of the value it provides above the underlying technology (Kubernetes / Docker). We discussed various architectures and detailed the design for a lab or PoC configuration. Finally we went through all the steps to configure infrastructure, prepare nodes, configure, install and verify a running OpenShift environment.</p>
<p>Happy OpenShifting!</p>
<p>(c) 2017 Keith Tenzer</p>
