<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.22.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>OpenShift v3: Unlocking the Power of Persistent Storage - Keith Tenzer’s Blog</title>
<meta name="description" content="Overview In this article we will discuss and implement persistent storage in OpenShift v3. If you are new to OpenShift v3 you should first read the OpenShift v3 Lab Configuration article to get going.   Docker images are immutable and it is not possible to simply store persistent data within containers. When applications write to the Docker union file system, that data is lost as soon as the container is stopped. Docker provides a solution for persisting data, that allows administrator to mount a mount point existing on the container host (OpenShift node) within the container itself. It is similar to concept of raw device maps in virtual machines except with file systems.OpenShift v3 interfaces with Kubernetes and Kubernetes interfaces with Docker. As such we will mostly be referring to Kubernetes in this article. Kubernetes has a concept of pods which is a grouping of Docker containers that are co-existed. All Docker containers within a pod share same resources, including storage. Ephemeral Storage OpenShift v3 supports using ephemeral storage for all database templates. As mentioned using ephemeral storage means application data is written to the Docker union file system. All data is lost, as soon as the Kubernetes pod and as such container is stopped. In addition, since using ephemeral storage uses the Docker union file system, writes will be slow. If performance is desired, it is recommend to use persistent storage. The use case for ephemeral storage is mainly around automated testing. You don&#39;t need performance or to save data in order to test application functionality. Persistent Storage OpenShift v3 supports using persistent storage through Kubernetes storage plugins. Red Hat has contributed plugins for NFS, ISCSI, Ceph RBD and GlusterFS to Kubernetes. OpenShift v3 supports NFS, ISCSI, Ceph RBD or GlusterFS for persistent storage. As mentioned, Kubernetes deploys Docker containers within a pod and as such, is responsible for storage configuration. Details about the implementation of persistent storage in Kubernetes can be found here. Kubernetes allows you to create a pool of persistent volumes. Each persistent volume is mapped to a external storage file system. When persistent storage is requested from a pod, Kubernetes will claim a persistent volume from the pool of available volumes. The Kubernetes scheduler decides where to deploy pod. External storage is mounted on that node and presented to all containers within pod. If persistent storage is no longer needed, it can be reclaimed and made available to other pods. OpenShift v3 makes this all seamless to the user and hides the underlying complexity, as we will see. Below is a snippet from the Docker configuration of a container using persistent storage. If you didn&#39;t have OpenShift v3 and Kubernetes, you would have to deal with this for every single Docker container. [code language=&quot;java&quot;] &quot;Volumes&quot;: {    &quot;/dev/termination-log&quot;: &quot;/var/lib/openshift/openshift.local.volumes/pods/6e1d5a40-471b-11e5-9680-525400bca113/containers/mysql/960dd543dc5f790ff2be72858b79c9df20bfae00ec9bffa333cd6e34e7aa36f9&quot;,    &quot;/var/lib/mysql/data&quot;: &quot;/var/lib/openshift/openshift.local.volumes/pods/6e1d5a40-471b-11e5-9680-525400bca113/volumes/kubernetes.io~nfs/pv0016&quot;,    &quot;/var/run/secrets/kubernetes.io/serviceaccount&quot;: &quot;/var/lib/openshift/openshift.local.volumes/pods/6e1d5a40-471b-11e5-9680-525400bca113/volumes/kubernetes.io~secret/default-token-ck4x7&quot; }, &quot;VolumesRW&quot;: {    &quot;/dev/termination-log&quot;: true,    &quot;/var/lib/mysql/data&quot;: true,    &quot;/var/run/secrets/kubernetes.io/serviceaccount&quot;: false }, &quot;VolumesRelabel&quot;: {    &quot;/dev/termination-log&quot;: &quot;&quot;,    &quot;/var/lib/mysql/data&quot;: &quot;&quot;,    &quot;/var/run/secrets/kubernetes.io/serviceaccount&quot;: &quot;ro&quot; } [/code] Configure Persistent Storage In order to configure persistent storage, the storage must be available to all OpenShift v3 nodes using NFS, ISCSI, Ceph RDB or GlusterFS. In this example, we will configure an NFS server on the OpenShift v3 master. For a lab environment this is fine but for production environments you will want to use external storage for obvious reasons. Configure NFS Server on OpenShift v3 Master The first step is to install NFS server and start the services. #yum groupinstall -y file-server #systemctl enable rpcbind #systemctl enable nfs-server #systemctl start rpcbind #systemctl start nfs-server Once the services are running, we need to allow access through iptables. OpenShift v3 uses iptables and not firewalld. #iptables-save &gt; pre-nfs-firewall-rules-server #iptables -I INPUT -m state --state NEW -p tcp -m multiport --dport 111,892,2049,32803 -s 0.0.0.0/0 -j ACCEPT #iptables -I INPUT -m state --state NEW -p udp -m multiport --dport 111,892,2049,32769 -s 0.0.0.0/0 -j ACCEPT #service iptables save Allow SELinux policy for sVirt to write to nfs shares. By default the SELinux sVirt policy prevents containers from writing to NFS shares. #setsebool -P virt_use_nfs 1 Configure NFS Client on OpenShift v3 Nodes On all nodes, we will want to install the nfs-utils so that nodes can mount NFS shares. #yum install -y nfs-utils Configure Persistent Volumes In order to configure persistent volumes we need to create a JSON or YAML template file. In this example we will use JSON but Kubernetes supports both. We will also create a pool of 20 persistent volumes. From here all steps will be performed on the OpenShift v3 master. Create a JSON file that will be used as template for adding persistent volumes. Note: you need to replace the IP address with the IP of your OpenShift v3 master. vi /root/PV.json {  &quot;apiVersion&quot;: &quot;v1&quot;,  &quot;kind&quot;: &quot;PersistentVolume&quot;,  &quot;metadata&quot;: {  &quot;name&quot;: &quot;pv0001&quot; }, &quot;spec&quot;: {    &quot;capacity&quot;: {    &quot;storage&quot;: &quot;10Gi&quot;    },    &quot;accessModes&quot;: [ &quot;ReadWriteOnce&quot; ],    &quot;nfs&quot;: {       &quot;path&quot;: &quot;/mnt/RBD/pv0001&quot;,       &quot;server&quot;: &quot;192.168.122.60&quot;    },    &quot;persistentVolumeReclaimPolicy&quot;: &quot;Recycle&quot;    } } In order to automate things we will create a for loop, that will create the NFS shares, set permissions and create persistent volumes in OpenShift v3. for i in `seq -w 0001 0020`; do SHARE=/mnt/RBD/pv$i; mkdir -p $SHARE; chmod 777 $SHARE; chown nfsnobody:nfsnobody $SHARE; echo &quot;$SHARE 192.168.122.0/24(rw,all_squash)&quot; &gt;&gt;/etc/exports; sed s/pv0001/pv$i/g /root/PV.json | oc create -f -; done We can now list the persistent storage volumes in OpenShift v3. Notice we have no claims yet. #oc get pv NAME   LABELS CAPACITY    ACCESSMODES STATUS    CLAIM         REASON pv0001 &lt;none&gt; 10737418240 RWO         Available  pv0002 &lt;none&gt; 10737418240 RWO         Available  pv0003 &lt;none&gt; 10737418240 RWO         Available  pv0004 &lt;none&gt; 10737418240 RWO         Available  pv0005 &lt;none&gt; 10737418240 RWO         Available  pv0006 &lt;none&gt; 10737418240 RWO         Available  pv0007 &lt;none&gt; 10737418240 RWO         Available  pv0008 &lt;none&gt; 10737418240 RWO         Available  pv0009 &lt;none&gt; 10737418240 RWO         Available  pv0010 &lt;none&gt; 10737418240 RWO         Available  pv0011 &lt;none&gt; 10737418240 RWO         Available  pv0012 &lt;none&gt; 10737418240 RWO         Available  pv0013 &lt;none&gt; 10737418240 RWO         Available  pv0014 &lt;none&gt; 10737418240 RWO         Available  pv0015 &lt;none&gt; 10737418240 RWO         Available  pv0016 &lt;none&gt; 10737418240 RWO         Available pv0017 &lt;none&gt; 10737418240 RWO         Available  pv0018 &lt;none&gt; 10737418240 RWO         Available  pv0019 &lt;none&gt; 10737418240 RWO         Available  pv0020 &lt;none&gt; 10737418240 RWO         Available Create OpenShift v3 Application Using Persistent Storage Now that everything is configured, we can do a quick demo of how persistent storage in OpenShift v3 actually works. We will deploy a ruby hello world application from GitHub that uses a persistent MySQL database. First create a new project in OpenShift v3. This creates a namespace in Kubernetes. #oc new-project demo Next deploy our ruby hello world application from GitHub. This will deploy a pod that builds the code from GitHub and then using STI (Source To Image), will deploy a running pod with our built application. The reason for this is that we have different dependencies required for building and running applications. You are hopefully starting to see the power of OpenShift v3. If not stay tuned! #oc new-app https://github.com/openshift/ruby-hello-world Since we will want to access our application our service also needs to be exposed. OpenShift v3 will configure an HA proxy using router in openVswitch and an Apache vHost. Traffic is routed to the appropriate host based on the service name. The Apache vHost exposes the application based on the service name. Using vHosts allows OpenShift services to use the same ports and thus doesn&#39;t require unique ports. Behind the scenes Kubernetes is handling the routing from the OpenShift v3 node to the Docker container. #oc expose service ruby-hello-world We can check OpenShift v3 UI or console to see the status of our build and deployment.  Once complete we should have a builder pod that has exited with 0 and a running pod. #oc get pods NAME                     READY     REASON     RESTARTS AGE ruby-hello-world-1-build 0/1       ExitCode:0 0        3m ruby-hello-world-3-3gtig 1/1       Running    0        1m If we open a web browser and point it at the service name we should see the application. In this case it won&#39;t show us much since we don&#39;t yet have a database.  Lets add a persistent MySQL database and connect it to our ruby hello world application. #oc process -n openshift mysql-persistent -v DATABASE_SERVICE_NAME=database | oc create -f - #oc env dc database --list | oc env dc ruby-hello-world -e - A MySQL database will be deployed using persistent storage. Once the database is deployed the pod should be running and the we should also see a persistent storage volume claim. #oc get pods NAME                     READY     REASON     RESTARTS AGE database-1-2gv6j         1/1       Running    0        1m ruby-hello-world-1-build 0/1       ExitCode:0 0        3m ruby-hello-world-3-3gtig 1/1       Running    0        1m #oc get pv NAME   LABELS CAPACITY    ACCESSMODES STATUS    CLAIM           REASON pv0001 &lt;none&gt; 10737418240 RWO         Available  pv0002 &lt;none&gt; 10737418240 RWO         Available  pv0003 &lt;none&gt; 10737418240 RWO         Available  pv0004 &lt;none&gt; 10737418240 RWO         Available  pv0005 &lt;none&gt; 10737418240 RWO         Available  pv0006 &lt;none&gt; 10737418240 RWO         Available  pv0007 &lt;none&gt; 10737418240 RWO         Available  pv0008 &lt;none&gt; 10737418240 RWO         Available  pv0009 &lt;none&gt; 10737418240 RWO         Available  pv0010 &lt;none&gt; 10737418240 RWO         Available  pv0011 &lt;none&gt; 10737418240 RWO         Available  pv0012 &lt;none&gt; 10737418240 RWO         Available  pv0013 &lt;none&gt; 10737418240 RWO         Available  pv0014 &lt;none&gt; 10737418240 RWO         Available  pv0015 &lt;none&gt; 10737418240 RWO         Available  pv0016 &lt;none&gt; 10737418240 RWO         Bound      demo/database pv0017 &lt;none&gt; 10737418240 RWO         Available  pv0018 &lt;none&gt; 10737418240 RWO         Available  pv0019 &lt;none&gt; 10737418240 RWO         Available  pv0020 &lt;none&gt; 10737418240 RWO         Available Once we have verified everything we should also be able to use our ruby hello world application. Lets do a put for a key/value pair.  In order to demonstrate persistent storage, let us now delete the MySQL database pod. Don&#39;t worry the replication controller will automatically deploy a new pod and of course our data will be saved. If we were using ephemeral storage the data would be lost at this step. #oc delete pod database-1-2gv6j Finally lets go back to our ruby hello world application and and do a get for our key &quot;keith&quot;. We should see the value is &quot;tenzer&quot; thus confirming persistent storage is working.  Summary In this article we have seen the power of OpenShift v3 in delivering a complete platform for building, deploying and running container-based applications. We have discussed the use cases behind ephemeral and persistent storage within OpenShift v3 ecosystem. Finally we have implemented and shown a compelling use case for persistent storage. OpenShift v3 is a platform for building and running next-gen applications using immutable container infrastructure. The goal is to deliver innovation faster. Hopefully this article has given you a glimpse at what is available today and inspired you to try things yourself. If you have any feedback or use cases for OpenShift v3, lets hear it! A special thanks goes to Wolfram Richter, a mentor and colleague who helped tremendously in creating the content for this article. Happy OpenShifting! (c) 2015 Keith Tenzer">


  <meta name="author" content="Keith Tenzer">
  
  <meta property="article:author" content="Keith Tenzer">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Keith Tenzer's Blog">
<meta property="og:title" content="OpenShift v3: Unlocking the Power of Persistent Storage">
<meta property="og:url" content="https://keithtenzer.com/openshift/openshift-v3-unlocking-the-power-of-persistent-storage/">


  <meta property="og:description" content="Overview In this article we will discuss and implement persistent storage in OpenShift v3. If you are new to OpenShift v3 you should first read the OpenShift v3 Lab Configuration article to get going.   Docker images are immutable and it is not possible to simply store persistent data within containers. When applications write to the Docker union file system, that data is lost as soon as the container is stopped. Docker provides a solution for persisting data, that allows administrator to mount a mount point existing on the container host (OpenShift node) within the container itself. It is similar to concept of raw device maps in virtual machines except with file systems.OpenShift v3 interfaces with Kubernetes and Kubernetes interfaces with Docker. As such we will mostly be referring to Kubernetes in this article. Kubernetes has a concept of pods which is a grouping of Docker containers that are co-existed. All Docker containers within a pod share same resources, including storage. Ephemeral Storage OpenShift v3 supports using ephemeral storage for all database templates. As mentioned using ephemeral storage means application data is written to the Docker union file system. All data is lost, as soon as the Kubernetes pod and as such container is stopped. In addition, since using ephemeral storage uses the Docker union file system, writes will be slow. If performance is desired, it is recommend to use persistent storage. The use case for ephemeral storage is mainly around automated testing. You don&#39;t need performance or to save data in order to test application functionality. Persistent Storage OpenShift v3 supports using persistent storage through Kubernetes storage plugins. Red Hat has contributed plugins for NFS, ISCSI, Ceph RBD and GlusterFS to Kubernetes. OpenShift v3 supports NFS, ISCSI, Ceph RBD or GlusterFS for persistent storage. As mentioned, Kubernetes deploys Docker containers within a pod and as such, is responsible for storage configuration. Details about the implementation of persistent storage in Kubernetes can be found here. Kubernetes allows you to create a pool of persistent volumes. Each persistent volume is mapped to a external storage file system. When persistent storage is requested from a pod, Kubernetes will claim a persistent volume from the pool of available volumes. The Kubernetes scheduler decides where to deploy pod. External storage is mounted on that node and presented to all containers within pod. If persistent storage is no longer needed, it can be reclaimed and made available to other pods. OpenShift v3 makes this all seamless to the user and hides the underlying complexity, as we will see. Below is a snippet from the Docker configuration of a container using persistent storage. If you didn&#39;t have OpenShift v3 and Kubernetes, you would have to deal with this for every single Docker container. [code language=&quot;java&quot;] &quot;Volumes&quot;: {    &quot;/dev/termination-log&quot;: &quot;/var/lib/openshift/openshift.local.volumes/pods/6e1d5a40-471b-11e5-9680-525400bca113/containers/mysql/960dd543dc5f790ff2be72858b79c9df20bfae00ec9bffa333cd6e34e7aa36f9&quot;,    &quot;/var/lib/mysql/data&quot;: &quot;/var/lib/openshift/openshift.local.volumes/pods/6e1d5a40-471b-11e5-9680-525400bca113/volumes/kubernetes.io~nfs/pv0016&quot;,    &quot;/var/run/secrets/kubernetes.io/serviceaccount&quot;: &quot;/var/lib/openshift/openshift.local.volumes/pods/6e1d5a40-471b-11e5-9680-525400bca113/volumes/kubernetes.io~secret/default-token-ck4x7&quot; }, &quot;VolumesRW&quot;: {    &quot;/dev/termination-log&quot;: true,    &quot;/var/lib/mysql/data&quot;: true,    &quot;/var/run/secrets/kubernetes.io/serviceaccount&quot;: false }, &quot;VolumesRelabel&quot;: {    &quot;/dev/termination-log&quot;: &quot;&quot;,    &quot;/var/lib/mysql/data&quot;: &quot;&quot;,    &quot;/var/run/secrets/kubernetes.io/serviceaccount&quot;: &quot;ro&quot; } [/code] Configure Persistent Storage In order to configure persistent storage, the storage must be available to all OpenShift v3 nodes using NFS, ISCSI, Ceph RDB or GlusterFS. In this example, we will configure an NFS server on the OpenShift v3 master. For a lab environment this is fine but for production environments you will want to use external storage for obvious reasons. Configure NFS Server on OpenShift v3 Master The first step is to install NFS server and start the services. #yum groupinstall -y file-server #systemctl enable rpcbind #systemctl enable nfs-server #systemctl start rpcbind #systemctl start nfs-server Once the services are running, we need to allow access through iptables. OpenShift v3 uses iptables and not firewalld. #iptables-save &gt; pre-nfs-firewall-rules-server #iptables -I INPUT -m state --state NEW -p tcp -m multiport --dport 111,892,2049,32803 -s 0.0.0.0/0 -j ACCEPT #iptables -I INPUT -m state --state NEW -p udp -m multiport --dport 111,892,2049,32769 -s 0.0.0.0/0 -j ACCEPT #service iptables save Allow SELinux policy for sVirt to write to nfs shares. By default the SELinux sVirt policy prevents containers from writing to NFS shares. #setsebool -P virt_use_nfs 1 Configure NFS Client on OpenShift v3 Nodes On all nodes, we will want to install the nfs-utils so that nodes can mount NFS shares. #yum install -y nfs-utils Configure Persistent Volumes In order to configure persistent volumes we need to create a JSON or YAML template file. In this example we will use JSON but Kubernetes supports both. We will also create a pool of 20 persistent volumes. From here all steps will be performed on the OpenShift v3 master. Create a JSON file that will be used as template for adding persistent volumes. Note: you need to replace the IP address with the IP of your OpenShift v3 master. vi /root/PV.json {  &quot;apiVersion&quot;: &quot;v1&quot;,  &quot;kind&quot;: &quot;PersistentVolume&quot;,  &quot;metadata&quot;: {  &quot;name&quot;: &quot;pv0001&quot; }, &quot;spec&quot;: {    &quot;capacity&quot;: {    &quot;storage&quot;: &quot;10Gi&quot;    },    &quot;accessModes&quot;: [ &quot;ReadWriteOnce&quot; ],    &quot;nfs&quot;: {       &quot;path&quot;: &quot;/mnt/RBD/pv0001&quot;,       &quot;server&quot;: &quot;192.168.122.60&quot;    },    &quot;persistentVolumeReclaimPolicy&quot;: &quot;Recycle&quot;    } } In order to automate things we will create a for loop, that will create the NFS shares, set permissions and create persistent volumes in OpenShift v3. for i in `seq -w 0001 0020`; do SHARE=/mnt/RBD/pv$i; mkdir -p $SHARE; chmod 777 $SHARE; chown nfsnobody:nfsnobody $SHARE; echo &quot;$SHARE 192.168.122.0/24(rw,all_squash)&quot; &gt;&gt;/etc/exports; sed s/pv0001/pv$i/g /root/PV.json | oc create -f -; done We can now list the persistent storage volumes in OpenShift v3. Notice we have no claims yet. #oc get pv NAME   LABELS CAPACITY    ACCESSMODES STATUS    CLAIM         REASON pv0001 &lt;none&gt; 10737418240 RWO         Available  pv0002 &lt;none&gt; 10737418240 RWO         Available  pv0003 &lt;none&gt; 10737418240 RWO         Available  pv0004 &lt;none&gt; 10737418240 RWO         Available  pv0005 &lt;none&gt; 10737418240 RWO         Available  pv0006 &lt;none&gt; 10737418240 RWO         Available  pv0007 &lt;none&gt; 10737418240 RWO         Available  pv0008 &lt;none&gt; 10737418240 RWO         Available  pv0009 &lt;none&gt; 10737418240 RWO         Available  pv0010 &lt;none&gt; 10737418240 RWO         Available  pv0011 &lt;none&gt; 10737418240 RWO         Available  pv0012 &lt;none&gt; 10737418240 RWO         Available  pv0013 &lt;none&gt; 10737418240 RWO         Available  pv0014 &lt;none&gt; 10737418240 RWO         Available  pv0015 &lt;none&gt; 10737418240 RWO         Available  pv0016 &lt;none&gt; 10737418240 RWO         Available pv0017 &lt;none&gt; 10737418240 RWO         Available  pv0018 &lt;none&gt; 10737418240 RWO         Available  pv0019 &lt;none&gt; 10737418240 RWO         Available  pv0020 &lt;none&gt; 10737418240 RWO         Available Create OpenShift v3 Application Using Persistent Storage Now that everything is configured, we can do a quick demo of how persistent storage in OpenShift v3 actually works. We will deploy a ruby hello world application from GitHub that uses a persistent MySQL database. First create a new project in OpenShift v3. This creates a namespace in Kubernetes. #oc new-project demo Next deploy our ruby hello world application from GitHub. This will deploy a pod that builds the code from GitHub and then using STI (Source To Image), will deploy a running pod with our built application. The reason for this is that we have different dependencies required for building and running applications. You are hopefully starting to see the power of OpenShift v3. If not stay tuned! #oc new-app https://github.com/openshift/ruby-hello-world Since we will want to access our application our service also needs to be exposed. OpenShift v3 will configure an HA proxy using router in openVswitch and an Apache vHost. Traffic is routed to the appropriate host based on the service name. The Apache vHost exposes the application based on the service name. Using vHosts allows OpenShift services to use the same ports and thus doesn&#39;t require unique ports. Behind the scenes Kubernetes is handling the routing from the OpenShift v3 node to the Docker container. #oc expose service ruby-hello-world We can check OpenShift v3 UI or console to see the status of our build and deployment.  Once complete we should have a builder pod that has exited with 0 and a running pod. #oc get pods NAME                     READY     REASON     RESTARTS AGE ruby-hello-world-1-build 0/1       ExitCode:0 0        3m ruby-hello-world-3-3gtig 1/1       Running    0        1m If we open a web browser and point it at the service name we should see the application. In this case it won&#39;t show us much since we don&#39;t yet have a database.  Lets add a persistent MySQL database and connect it to our ruby hello world application. #oc process -n openshift mysql-persistent -v DATABASE_SERVICE_NAME=database | oc create -f - #oc env dc database --list | oc env dc ruby-hello-world -e - A MySQL database will be deployed using persistent storage. Once the database is deployed the pod should be running and the we should also see a persistent storage volume claim. #oc get pods NAME                     READY     REASON     RESTARTS AGE database-1-2gv6j         1/1       Running    0        1m ruby-hello-world-1-build 0/1       ExitCode:0 0        3m ruby-hello-world-3-3gtig 1/1       Running    0        1m #oc get pv NAME   LABELS CAPACITY    ACCESSMODES STATUS    CLAIM           REASON pv0001 &lt;none&gt; 10737418240 RWO         Available  pv0002 &lt;none&gt; 10737418240 RWO         Available  pv0003 &lt;none&gt; 10737418240 RWO         Available  pv0004 &lt;none&gt; 10737418240 RWO         Available  pv0005 &lt;none&gt; 10737418240 RWO         Available  pv0006 &lt;none&gt; 10737418240 RWO         Available  pv0007 &lt;none&gt; 10737418240 RWO         Available  pv0008 &lt;none&gt; 10737418240 RWO         Available  pv0009 &lt;none&gt; 10737418240 RWO         Available  pv0010 &lt;none&gt; 10737418240 RWO         Available  pv0011 &lt;none&gt; 10737418240 RWO         Available  pv0012 &lt;none&gt; 10737418240 RWO         Available  pv0013 &lt;none&gt; 10737418240 RWO         Available  pv0014 &lt;none&gt; 10737418240 RWO         Available  pv0015 &lt;none&gt; 10737418240 RWO         Available  pv0016 &lt;none&gt; 10737418240 RWO         Bound      demo/database pv0017 &lt;none&gt; 10737418240 RWO         Available  pv0018 &lt;none&gt; 10737418240 RWO         Available  pv0019 &lt;none&gt; 10737418240 RWO         Available  pv0020 &lt;none&gt; 10737418240 RWO         Available Once we have verified everything we should also be able to use our ruby hello world application. Lets do a put for a key/value pair.  In order to demonstrate persistent storage, let us now delete the MySQL database pod. Don&#39;t worry the replication controller will automatically deploy a new pod and of course our data will be saved. If we were using ephemeral storage the data would be lost at this step. #oc delete pod database-1-2gv6j Finally lets go back to our ruby hello world application and and do a get for our key &quot;keith&quot;. We should see the value is &quot;tenzer&quot; thus confirming persistent storage is working.  Summary In this article we have seen the power of OpenShift v3 in delivering a complete platform for building, deploying and running container-based applications. We have discussed the use cases behind ephemeral and persistent storage within OpenShift v3 ecosystem. Finally we have implemented and shown a compelling use case for persistent storage. OpenShift v3 is a platform for building and running next-gen applications using immutable container infrastructure. The goal is to deliver innovation faster. Hopefully this article has given you a glimpse at what is available today and inspired you to try things yourself. If you have any feedback or use cases for OpenShift v3, lets hear it! A special thanks goes to Wolfram Richter, a mentor and colleague who helped tremendously in creating the content for this article. Happy OpenShifting! (c) 2015 Keith Tenzer">





  <meta name="twitter:site" content="@keithtenzer">
  <meta name="twitter:title" content="OpenShift v3: Unlocking the Power of Persistent Storage">
  <meta name="twitter:description" content="Overview In this article we will discuss and implement persistent storage in OpenShift v3. If you are new to OpenShift v3 you should first read the OpenShift v3 Lab Configuration article to get going.   Docker images are immutable and it is not possible to simply store persistent data within containers. When applications write to the Docker union file system, that data is lost as soon as the container is stopped. Docker provides a solution for persisting data, that allows administrator to mount a mount point existing on the container host (OpenShift node) within the container itself. It is similar to concept of raw device maps in virtual machines except with file systems.OpenShift v3 interfaces with Kubernetes and Kubernetes interfaces with Docker. As such we will mostly be referring to Kubernetes in this article. Kubernetes has a concept of pods which is a grouping of Docker containers that are co-existed. All Docker containers within a pod share same resources, including storage. Ephemeral Storage OpenShift v3 supports using ephemeral storage for all database templates. As mentioned using ephemeral storage means application data is written to the Docker union file system. All data is lost, as soon as the Kubernetes pod and as such container is stopped. In addition, since using ephemeral storage uses the Docker union file system, writes will be slow. If performance is desired, it is recommend to use persistent storage. The use case for ephemeral storage is mainly around automated testing. You don&#39;t need performance or to save data in order to test application functionality. Persistent Storage OpenShift v3 supports using persistent storage through Kubernetes storage plugins. Red Hat has contributed plugins for NFS, ISCSI, Ceph RBD and GlusterFS to Kubernetes. OpenShift v3 supports NFS, ISCSI, Ceph RBD or GlusterFS for persistent storage. As mentioned, Kubernetes deploys Docker containers within a pod and as such, is responsible for storage configuration. Details about the implementation of persistent storage in Kubernetes can be found here. Kubernetes allows you to create a pool of persistent volumes. Each persistent volume is mapped to a external storage file system. When persistent storage is requested from a pod, Kubernetes will claim a persistent volume from the pool of available volumes. The Kubernetes scheduler decides where to deploy pod. External storage is mounted on that node and presented to all containers within pod. If persistent storage is no longer needed, it can be reclaimed and made available to other pods. OpenShift v3 makes this all seamless to the user and hides the underlying complexity, as we will see. Below is a snippet from the Docker configuration of a container using persistent storage. If you didn&#39;t have OpenShift v3 and Kubernetes, you would have to deal with this for every single Docker container. [code language=&quot;java&quot;] &quot;Volumes&quot;: {    &quot;/dev/termination-log&quot;: &quot;/var/lib/openshift/openshift.local.volumes/pods/6e1d5a40-471b-11e5-9680-525400bca113/containers/mysql/960dd543dc5f790ff2be72858b79c9df20bfae00ec9bffa333cd6e34e7aa36f9&quot;,    &quot;/var/lib/mysql/data&quot;: &quot;/var/lib/openshift/openshift.local.volumes/pods/6e1d5a40-471b-11e5-9680-525400bca113/volumes/kubernetes.io~nfs/pv0016&quot;,    &quot;/var/run/secrets/kubernetes.io/serviceaccount&quot;: &quot;/var/lib/openshift/openshift.local.volumes/pods/6e1d5a40-471b-11e5-9680-525400bca113/volumes/kubernetes.io~secret/default-token-ck4x7&quot; }, &quot;VolumesRW&quot;: {    &quot;/dev/termination-log&quot;: true,    &quot;/var/lib/mysql/data&quot;: true,    &quot;/var/run/secrets/kubernetes.io/serviceaccount&quot;: false }, &quot;VolumesRelabel&quot;: {    &quot;/dev/termination-log&quot;: &quot;&quot;,    &quot;/var/lib/mysql/data&quot;: &quot;&quot;,    &quot;/var/run/secrets/kubernetes.io/serviceaccount&quot;: &quot;ro&quot; } [/code] Configure Persistent Storage In order to configure persistent storage, the storage must be available to all OpenShift v3 nodes using NFS, ISCSI, Ceph RDB or GlusterFS. In this example, we will configure an NFS server on the OpenShift v3 master. For a lab environment this is fine but for production environments you will want to use external storage for obvious reasons. Configure NFS Server on OpenShift v3 Master The first step is to install NFS server and start the services. #yum groupinstall -y file-server #systemctl enable rpcbind #systemctl enable nfs-server #systemctl start rpcbind #systemctl start nfs-server Once the services are running, we need to allow access through iptables. OpenShift v3 uses iptables and not firewalld. #iptables-save &gt; pre-nfs-firewall-rules-server #iptables -I INPUT -m state --state NEW -p tcp -m multiport --dport 111,892,2049,32803 -s 0.0.0.0/0 -j ACCEPT #iptables -I INPUT -m state --state NEW -p udp -m multiport --dport 111,892,2049,32769 -s 0.0.0.0/0 -j ACCEPT #service iptables save Allow SELinux policy for sVirt to write to nfs shares. By default the SELinux sVirt policy prevents containers from writing to NFS shares. #setsebool -P virt_use_nfs 1 Configure NFS Client on OpenShift v3 Nodes On all nodes, we will want to install the nfs-utils so that nodes can mount NFS shares. #yum install -y nfs-utils Configure Persistent Volumes In order to configure persistent volumes we need to create a JSON or YAML template file. In this example we will use JSON but Kubernetes supports both. We will also create a pool of 20 persistent volumes. From here all steps will be performed on the OpenShift v3 master. Create a JSON file that will be used as template for adding persistent volumes. Note: you need to replace the IP address with the IP of your OpenShift v3 master. vi /root/PV.json {  &quot;apiVersion&quot;: &quot;v1&quot;,  &quot;kind&quot;: &quot;PersistentVolume&quot;,  &quot;metadata&quot;: {  &quot;name&quot;: &quot;pv0001&quot; }, &quot;spec&quot;: {    &quot;capacity&quot;: {    &quot;storage&quot;: &quot;10Gi&quot;    },    &quot;accessModes&quot;: [ &quot;ReadWriteOnce&quot; ],    &quot;nfs&quot;: {       &quot;path&quot;: &quot;/mnt/RBD/pv0001&quot;,       &quot;server&quot;: &quot;192.168.122.60&quot;    },    &quot;persistentVolumeReclaimPolicy&quot;: &quot;Recycle&quot;    } } In order to automate things we will create a for loop, that will create the NFS shares, set permissions and create persistent volumes in OpenShift v3. for i in `seq -w 0001 0020`; do SHARE=/mnt/RBD/pv$i; mkdir -p $SHARE; chmod 777 $SHARE; chown nfsnobody:nfsnobody $SHARE; echo &quot;$SHARE 192.168.122.0/24(rw,all_squash)&quot; &gt;&gt;/etc/exports; sed s/pv0001/pv$i/g /root/PV.json | oc create -f -; done We can now list the persistent storage volumes in OpenShift v3. Notice we have no claims yet. #oc get pv NAME   LABELS CAPACITY    ACCESSMODES STATUS    CLAIM         REASON pv0001 &lt;none&gt; 10737418240 RWO         Available  pv0002 &lt;none&gt; 10737418240 RWO         Available  pv0003 &lt;none&gt; 10737418240 RWO         Available  pv0004 &lt;none&gt; 10737418240 RWO         Available  pv0005 &lt;none&gt; 10737418240 RWO         Available  pv0006 &lt;none&gt; 10737418240 RWO         Available  pv0007 &lt;none&gt; 10737418240 RWO         Available  pv0008 &lt;none&gt; 10737418240 RWO         Available  pv0009 &lt;none&gt; 10737418240 RWO         Available  pv0010 &lt;none&gt; 10737418240 RWO         Available  pv0011 &lt;none&gt; 10737418240 RWO         Available  pv0012 &lt;none&gt; 10737418240 RWO         Available  pv0013 &lt;none&gt; 10737418240 RWO         Available  pv0014 &lt;none&gt; 10737418240 RWO         Available  pv0015 &lt;none&gt; 10737418240 RWO         Available  pv0016 &lt;none&gt; 10737418240 RWO         Available pv0017 &lt;none&gt; 10737418240 RWO         Available  pv0018 &lt;none&gt; 10737418240 RWO         Available  pv0019 &lt;none&gt; 10737418240 RWO         Available  pv0020 &lt;none&gt; 10737418240 RWO         Available Create OpenShift v3 Application Using Persistent Storage Now that everything is configured, we can do a quick demo of how persistent storage in OpenShift v3 actually works. We will deploy a ruby hello world application from GitHub that uses a persistent MySQL database. First create a new project in OpenShift v3. This creates a namespace in Kubernetes. #oc new-project demo Next deploy our ruby hello world application from GitHub. This will deploy a pod that builds the code from GitHub and then using STI (Source To Image), will deploy a running pod with our built application. The reason for this is that we have different dependencies required for building and running applications. You are hopefully starting to see the power of OpenShift v3. If not stay tuned! #oc new-app https://github.com/openshift/ruby-hello-world Since we will want to access our application our service also needs to be exposed. OpenShift v3 will configure an HA proxy using router in openVswitch and an Apache vHost. Traffic is routed to the appropriate host based on the service name. The Apache vHost exposes the application based on the service name. Using vHosts allows OpenShift services to use the same ports and thus doesn&#39;t require unique ports. Behind the scenes Kubernetes is handling the routing from the OpenShift v3 node to the Docker container. #oc expose service ruby-hello-world We can check OpenShift v3 UI or console to see the status of our build and deployment.  Once complete we should have a builder pod that has exited with 0 and a running pod. #oc get pods NAME                     READY     REASON     RESTARTS AGE ruby-hello-world-1-build 0/1       ExitCode:0 0        3m ruby-hello-world-3-3gtig 1/1       Running    0        1m If we open a web browser and point it at the service name we should see the application. In this case it won&#39;t show us much since we don&#39;t yet have a database.  Lets add a persistent MySQL database and connect it to our ruby hello world application. #oc process -n openshift mysql-persistent -v DATABASE_SERVICE_NAME=database | oc create -f - #oc env dc database --list | oc env dc ruby-hello-world -e - A MySQL database will be deployed using persistent storage. Once the database is deployed the pod should be running and the we should also see a persistent storage volume claim. #oc get pods NAME                     READY     REASON     RESTARTS AGE database-1-2gv6j         1/1       Running    0        1m ruby-hello-world-1-build 0/1       ExitCode:0 0        3m ruby-hello-world-3-3gtig 1/1       Running    0        1m #oc get pv NAME   LABELS CAPACITY    ACCESSMODES STATUS    CLAIM           REASON pv0001 &lt;none&gt; 10737418240 RWO         Available  pv0002 &lt;none&gt; 10737418240 RWO         Available  pv0003 &lt;none&gt; 10737418240 RWO         Available  pv0004 &lt;none&gt; 10737418240 RWO         Available  pv0005 &lt;none&gt; 10737418240 RWO         Available  pv0006 &lt;none&gt; 10737418240 RWO         Available  pv0007 &lt;none&gt; 10737418240 RWO         Available  pv0008 &lt;none&gt; 10737418240 RWO         Available  pv0009 &lt;none&gt; 10737418240 RWO         Available  pv0010 &lt;none&gt; 10737418240 RWO         Available  pv0011 &lt;none&gt; 10737418240 RWO         Available  pv0012 &lt;none&gt; 10737418240 RWO         Available  pv0013 &lt;none&gt; 10737418240 RWO         Available  pv0014 &lt;none&gt; 10737418240 RWO         Available  pv0015 &lt;none&gt; 10737418240 RWO         Available  pv0016 &lt;none&gt; 10737418240 RWO         Bound      demo/database pv0017 &lt;none&gt; 10737418240 RWO         Available  pv0018 &lt;none&gt; 10737418240 RWO         Available  pv0019 &lt;none&gt; 10737418240 RWO         Available  pv0020 &lt;none&gt; 10737418240 RWO         Available Once we have verified everything we should also be able to use our ruby hello world application. Lets do a put for a key/value pair.  In order to demonstrate persistent storage, let us now delete the MySQL database pod. Don&#39;t worry the replication controller will automatically deploy a new pod and of course our data will be saved. If we were using ephemeral storage the data would be lost at this step. #oc delete pod database-1-2gv6j Finally lets go back to our ruby hello world application and and do a get for our key &quot;keith&quot;. We should see the value is &quot;tenzer&quot; thus confirming persistent storage is working.  Summary In this article we have seen the power of OpenShift v3 in delivering a complete platform for building, deploying and running container-based applications. We have discussed the use cases behind ephemeral and persistent storage within OpenShift v3 ecosystem. Finally we have implemented and shown a compelling use case for persistent storage. OpenShift v3 is a platform for building and running next-gen applications using immutable container infrastructure. The goal is to deliver innovation faster. Hopefully this article has given you a glimpse at what is available today and inspired you to try things yourself. If you have any feedback or use cases for OpenShift v3, lets hear it! A special thanks goes to Wolfram Richter, a mentor and colleague who helped tremendously in creating the content for this article. Happy OpenShifting! (c) 2015 Keith Tenzer">
  <meta name="twitter:url" content="https://keithtenzer.com/openshift/openshift-v3-unlocking-the-power-of-persistent-storage/">

  
    <meta name="twitter:card" content="summary">
    
  

  



  <meta property="article:published_time" content="2015-08-20T00:00:00-07:00">





  

  


<link rel="canonical" href="https://keithtenzer.com/openshift/openshift-v3-unlocking-the-power-of-persistent-storage/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Keith Tenzer",
      "url": "https://keithtenzer.com/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Keith Tenzer's Blog Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Keith Tenzer's Blog
          <span class="site-subtitle">Cloud Computing and Code</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about/index.html">About</a>
            </li><li class="masthead__menu-item">
              <a href="/conferences-and-events/index.html">Conferences and Events</a>
            </li><li class="masthead__menu-item">
              <a href="/videos/index.html">Videos</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16">
            <path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path>
          </svg>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      


  
    



<nav class="breadcrumbs">
  <ol itemscope itemtype="https://schema.org/BreadcrumbList">
    
    
    
      
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="https://keithtenzer.com/" itemprop="item"><span itemprop="name">Home</span></a>
          <meta itemprop="position" content="1" />
        </li>
        <span class="sep">/</span>
      
      
        
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/categories/#openshift" itemprop="item"><span itemprop="name">Openshift</span></a>
          <meta itemprop="position" content="2" />
        </li>
        <span class="sep">/</span>
      
    
      
      
        <li class="current">OpenShift v3: Unlocking the Power of Persistent Storage</li>
      
    
  </ol>
</nav>

  


<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      
        <img src="/assets/main/me.png" alt="Keith Tenzer" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Keith Tenzer</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>Principal Solutions Architect at Red Hat</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Los Angeles, CA</span>
        </li>
      

      
        
          
        
          
        
          
            <li><a href="https://twitter.com/keithtenzer" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i><span class="label">Twitter</span></a></li>
          
        
          
        
          
            <li><a href="https://github.com/ktenzer" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="OpenShift v3: Unlocking the Power of Persistent Storage">
    <meta itemprop="description" content="OverviewIn this article we will discuss and implement persistent storage in OpenShift v3. If you are new to OpenShift v3 you should first read the OpenShift v3 Lab Configuration article to get going.Docker images are immutable and it is not possible to simply store persistent data within containers. When applications write to the Docker union file system, that data is lost as soon as the container is stopped. Docker provides a solution for persisting data, that allows administrator to mount a mount point existing on the container host (OpenShift node) within the container itself. It is similar to concept of raw device maps in virtual machines except with file systems.OpenShift v3 interfaces with Kubernetes and Kubernetes interfaces with Docker. As such we will mostly be referring to Kubernetes in this article. Kubernetes has a concept of pods which is a grouping of Docker containers that are co-existed. All Docker containers within a pod share same resources, including storage.Ephemeral StorageOpenShift v3 supports using ephemeral storage for all database templates. As mentioned using ephemeral storage means application data is written to the Docker union file system. All data is lost, as soon as the Kubernetes pod and as such container is stopped. In addition, since using ephemeral storage uses the Docker union file system, writes will be slow. If performance is desired, it is recommend to use persistent storage. The use case for ephemeral storage is mainly around automated testing. You don&#39;t need performance or to save data in order to test application functionality.Persistent StorageOpenShift v3 supports using persistent storage through Kubernetes storage plugins. Red Hat has contributed plugins for NFS, ISCSI, Ceph RBD and GlusterFS to Kubernetes. OpenShift v3 supports NFS, ISCSI, Ceph RBD or GlusterFS for persistent storage. As mentioned, Kubernetes deploys Docker containers within a pod and as such, is responsible for storage configuration. Details about the implementation of persistent storage in Kubernetes can be found here. Kubernetes allows you to create a pool of persistent volumes. Each persistent volume is mapped to a external storage file system. When persistent storage is requested from a pod, Kubernetes will claim a persistent volume from the pool of available volumes. The Kubernetes scheduler decides where to deploy pod. External storage is mounted on that node and presented to all containers within pod. If persistent storage is no longer needed, it can be reclaimed and made available to other pods. OpenShift v3 makes this all seamless to the user and hides the underlying complexity, as we will see.Below is a snippet from the Docker configuration of a container using persistent storage. If you didn&#39;t have OpenShift v3 and Kubernetes, you would have to deal with this for every single Docker container.[code language=&quot;java&quot;]&quot;Volumes&quot;: {   &quot;/dev/termination-log&quot;: &quot;/var/lib/openshift/openshift.local.volumes/pods/6e1d5a40-471b-11e5-9680-525400bca113/containers/mysql/960dd543dc5f790ff2be72858b79c9df20bfae00ec9bffa333cd6e34e7aa36f9&quot;,   &quot;/var/lib/mysql/data&quot;: &quot;/var/lib/openshift/openshift.local.volumes/pods/6e1d5a40-471b-11e5-9680-525400bca113/volumes/kubernetes.io~nfs/pv0016&quot;,   &quot;/var/run/secrets/kubernetes.io/serviceaccount&quot;: &quot;/var/lib/openshift/openshift.local.volumes/pods/6e1d5a40-471b-11e5-9680-525400bca113/volumes/kubernetes.io~secret/default-token-ck4x7&quot;},&quot;VolumesRW&quot;: {   &quot;/dev/termination-log&quot;: true,   &quot;/var/lib/mysql/data&quot;: true,   &quot;/var/run/secrets/kubernetes.io/serviceaccount&quot;: false},&quot;VolumesRelabel&quot;: {   &quot;/dev/termination-log&quot;: &quot;&quot;,   &quot;/var/lib/mysql/data&quot;: &quot;&quot;,   &quot;/var/run/secrets/kubernetes.io/serviceaccount&quot;: &quot;ro&quot;}[/code]Configure Persistent StorageIn order to configure persistent storage, the storage must be available to all OpenShift v3 nodes using NFS, ISCSI, Ceph RDB or GlusterFS. In this example, we will configure an NFS server on the OpenShift v3 master. For a lab environment this is fine but for production environments you will want to use external storage for obvious reasons.Configure NFS Server on OpenShift v3 MasterThe first step is to install NFS server and start the services.#yum groupinstall -y file-server#systemctl enable rpcbind#systemctl enable nfs-server#systemctl start rpcbind#systemctl start nfs-serverOnce the services are running, we need to allow access through iptables. OpenShift v3 uses iptables and not firewalld.#iptables-save &gt; pre-nfs-firewall-rules-server#iptables -I INPUT -m state --state NEW -p tcp -m multiport --dport 111,892,2049,32803 -s 0.0.0.0/0 -j ACCEPT#iptables -I INPUT -m state --state NEW -p udp -m multiport --dport 111,892,2049,32769 -s 0.0.0.0/0 -j ACCEPT#service iptables saveAllow SELinux policy for sVirt to write to nfs shares. By default the SELinux sVirt policy prevents containers from writing to NFS shares.#setsebool -P virt_use_nfs 1Configure NFS Client on OpenShift v3 NodesOn all nodes, we will want to install the nfs-utils so that nodes can mount NFS shares.#yum install -y nfs-utilsConfigure Persistent VolumesIn order to configure persistent volumes we need to create a JSON or YAML template file. In this example we will use JSON but Kubernetes supports both. We will also create a pool of 20 persistent volumes. From here all steps will be performed on the OpenShift v3 master.Create a JSON file that will be used as template for adding persistent volumes. Note: you need to replace the IP address with the IP of your OpenShift v3 master.vi /root/PV.json{ &quot;apiVersion&quot;: &quot;v1&quot;, &quot;kind&quot;: &quot;PersistentVolume&quot;, &quot;metadata&quot;: { &quot;name&quot;: &quot;pv0001&quot;},&quot;spec&quot;: {   &quot;capacity&quot;: {   &quot;storage&quot;: &quot;10Gi&quot;   },   &quot;accessModes&quot;: [ &quot;ReadWriteOnce&quot; ],   &quot;nfs&quot;: {      &quot;path&quot;: &quot;/mnt/RBD/pv0001&quot;,      &quot;server&quot;: &quot;192.168.122.60&quot;   },   &quot;persistentVolumeReclaimPolicy&quot;: &quot;Recycle&quot;   }}In order to automate things we will create a for loop, that will create the NFS shares, set permissions and create persistent volumes in OpenShift v3.for i in `seq -w 0001 0020`; do SHARE=/mnt/RBD/pv$i; mkdir -p $SHARE; chmod 777 $SHARE; chown nfsnobody:nfsnobody $SHARE; echo &quot;$SHARE 192.168.122.0/24(rw,all_squash)&quot; &gt;&gt;/etc/exports; sed s/pv0001/pv$i/g /root/PV.json | oc create -f -; doneWe can now list the persistent storage volumes in OpenShift v3. Notice we have no claims yet.#oc get pvNAME   LABELS CAPACITY    ACCESSMODES STATUS    CLAIM         REASONpv0001 &lt;none&gt; 10737418240 RWO         Available pv0002 &lt;none&gt; 10737418240 RWO         Available pv0003 &lt;none&gt; 10737418240 RWO         Available pv0004 &lt;none&gt; 10737418240 RWO         Available pv0005 &lt;none&gt; 10737418240 RWO         Available pv0006 &lt;none&gt; 10737418240 RWO         Available pv0007 &lt;none&gt; 10737418240 RWO         Available pv0008 &lt;none&gt; 10737418240 RWO         Available pv0009 &lt;none&gt; 10737418240 RWO         Available pv0010 &lt;none&gt; 10737418240 RWO         Available pv0011 &lt;none&gt; 10737418240 RWO         Available pv0012 &lt;none&gt; 10737418240 RWO         Available pv0013 &lt;none&gt; 10737418240 RWO         Available pv0014 &lt;none&gt; 10737418240 RWO         Available pv0015 &lt;none&gt; 10737418240 RWO         Available pv0016 &lt;none&gt; 10737418240 RWO         Availablepv0017 &lt;none&gt; 10737418240 RWO         Available pv0018 &lt;none&gt; 10737418240 RWO         Available pv0019 &lt;none&gt; 10737418240 RWO         Available pv0020 &lt;none&gt; 10737418240 RWO         AvailableCreate OpenShift v3 Application Using Persistent StorageNow that everything is configured, we can do a quick demo of how persistent storage in OpenShift v3 actually works. We will deploy a ruby hello world application from GitHub that uses a persistent MySQL database.First create a new project in OpenShift v3. This creates a namespace in Kubernetes.#oc new-project demoNext deploy our ruby hello world application from GitHub. This will deploy a pod that builds the code from GitHub and then using STI (Source To Image), will deploy a running pod with our built application. The reason for this is that we have different dependencies required for building and running applications. You are hopefully starting to see the power of OpenShift v3. If not stay tuned!#oc new-app https://github.com/openshift/ruby-hello-worldSince we will want to access our application our service also needs to be exposed. OpenShift v3 will configure an HA proxy using router in openVswitch and an Apache vHost. Traffic is routed to the appropriate host based on the service name. The Apache vHost exposes the application based on the service name. Using vHosts allows OpenShift services to use the same ports and thus doesn&#39;t require unique ports. Behind the scenes Kubernetes is handling the routing from the OpenShift v3 node to the Docker container.#oc expose service ruby-hello-worldWe can check OpenShift v3 UI or console to see the status of our build and deployment.Once complete we should have a builder pod that has exited with 0 and a running pod.#oc get podsNAME                     READY     REASON     RESTARTS AGEruby-hello-world-1-build 0/1       ExitCode:0 0        3mruby-hello-world-3-3gtig 1/1       Running    0        1mIf we open a web browser and point it at the service name we should see the application. In this case it won&#39;t show us much since we don&#39;t yet have a database.Lets add a persistent MySQL database and connect it to our ruby hello world application.#oc process -n openshift mysql-persistent -v DATABASE_SERVICE_NAME=database | oc create -f -#oc env dc database --list | oc env dc ruby-hello-world -e -A MySQL database will be deployed using persistent storage. Once the database is deployed the pod should be running and the we should also see a persistent storage volume claim.#oc get podsNAME                     READY     REASON     RESTARTS AGEdatabase-1-2gv6j         1/1       Running    0        1mruby-hello-world-1-build 0/1       ExitCode:0 0        3mruby-hello-world-3-3gtig 1/1       Running    0        1m#oc get pvNAME   LABELS CAPACITY    ACCESSMODES STATUS    CLAIM           REASONpv0001 &lt;none&gt; 10737418240 RWO         Available pv0002 &lt;none&gt; 10737418240 RWO         Available pv0003 &lt;none&gt; 10737418240 RWO         Available pv0004 &lt;none&gt; 10737418240 RWO         Available pv0005 &lt;none&gt; 10737418240 RWO         Available pv0006 &lt;none&gt; 10737418240 RWO         Available pv0007 &lt;none&gt; 10737418240 RWO         Available pv0008 &lt;none&gt; 10737418240 RWO         Available pv0009 &lt;none&gt; 10737418240 RWO         Available pv0010 &lt;none&gt; 10737418240 RWO         Available pv0011 &lt;none&gt; 10737418240 RWO         Available pv0012 &lt;none&gt; 10737418240 RWO         Available pv0013 &lt;none&gt; 10737418240 RWO         Available pv0014 &lt;none&gt; 10737418240 RWO         Available pv0015 &lt;none&gt; 10737418240 RWO         Available pv0016 &lt;none&gt; 10737418240 RWO         Bound      demo/databasepv0017 &lt;none&gt; 10737418240 RWO         Available pv0018 &lt;none&gt; 10737418240 RWO         Available pv0019 &lt;none&gt; 10737418240 RWO         Available pv0020 &lt;none&gt; 10737418240 RWO         AvailableOnce we have verified everything we should also be able to use our ruby hello world application. Lets do a put for a key/value pair.In order to demonstrate persistent storage, let us now delete the MySQL database pod. Don&#39;t worry the replication controller will automatically deploy a new pod and of course our data will be saved. If we were using ephemeral storage the data would be lost at this step.#oc delete pod database-1-2gv6jFinally lets go back to our ruby hello world application and and do a get for our key &quot;keith&quot;. We should see the value is &quot;tenzer&quot; thus confirming persistent storage is working.SummaryIn this article we have seen the power of OpenShift v3 in delivering a complete platform for building, deploying and running container-based applications. We have discussed the use cases behind ephemeral and persistent storage within OpenShift v3 ecosystem. Finally we have implemented and shown a compelling use case for persistent storage. OpenShift v3 is a platform for building and running next-gen applications using immutable container infrastructure. The goal is to deliver innovation faster. Hopefully this article has given you a glimpse at what is available today and inspired you to try things yourself. If you have any feedback or use cases for OpenShift v3, lets hear it!A special thanks goes to Wolfram Richter, a mentor and colleague who helped tremendously in creating the content for this article.Happy OpenShifting!(c) 2015 Keith Tenzer">
    <meta itemprop="datePublished" content="2015-08-20T00:00:00-07:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">OpenShift v3: Unlocking the Power of Persistent Storage
</h1>
          

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2015-08-20T00:00:00-07:00">August 20, 2015</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          17 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
        <h3>Overview</h3>
<p>In this article we will discuss and implement persistent storage in OpenShift v3. If you are new to OpenShift v3 you should first read the <a href="http://keithtenzer.com/2015/08/03/openshift-enterprise-v3-lab-configuration-innovate-faster-deliver-sooner/">OpenShift v3 Lab Configuration</a> article to get going.</p>
<p><a href="https://keithtenzer.files.wordpress.com/2015/08/ose-arch-diagram.png"><img class="alignnone wp-image-1176" src="/assets/2015/08/ose-arch-diagram.png?w=300" alt="ose-arch-diagram" width="1200" height="732" /></a><br />
<!--more--></p>
<p>Docker images are immutable and it is not possible to simply store persistent data within containers. When applications write to the Docker union file system, that data is lost as soon as the container is stopped. Docker provides a solution for persisting data, that allows administrator to mount a mount point existing on the container host (OpenShift node) within the container itself. It is similar to concept of raw device maps in virtual machines except with file systems.OpenShift v3 interfaces with Kubernetes and Kubernetes interfaces with Docker. As such we will mostly be referring to Kubernetes in this article. Kubernetes has a concept of pods which is a grouping of Docker containers that are co-existed. All Docker containers within a pod share same resources, including storage.</p>
<h3>Ephemeral Storage</h3>
<p>OpenShift v3 supports using ephemeral storage for all database templates. As mentioned using ephemeral storage means application data is written to the Docker union file system. All data is lost, as soon as the Kubernetes pod and as such container is stopped. In addition, since using ephemeral storage uses the Docker union file system, writes will be slow. If performance is desired, it is recommend to use persistent storage. The use case for ephemeral storage is mainly around automated testing. You don't need performance or to save data in order to test application functionality.</p>
<h3>Persistent Storage</h3>
<p>OpenShift v3 supports using persistent storage through Kubernetes storage plugins. Red Hat has contributed plugins for NFS, ISCSI, Ceph RBD and GlusterFS to Kubernetes. OpenShift v3 supports NFS, ISCSI, Ceph RBD or GlusterFS for persistent storage. As mentioned, Kubernetes deploys Docker containers within a pod and as such, is responsible for storage configuration. Details about the implementation of persistent storage in Kubernetes can be found <a href="https://github.com/kubernetes/kubernetes/blob/master/docs/design/persistent-storage.md">here</a>. Kubernetes allows you to create a pool of persistent volumes. Each persistent volume is mapped to a external storage file system. When persistent storage is requested from a pod, Kubernetes will claim a persistent volume from the pool of available volumes. The Kubernetes scheduler decides where to deploy pod. External storage is mounted on that node and presented to all containers within pod. If persistent storage is no longer needed, it can be reclaimed and made available to other pods. OpenShift v3 makes this all seamless to the user and hides the underlying complexity, as we will see.</p>
<p>Below is a snippet from the Docker configuration of a container using persistent storage. If you didn't have OpenShift v3 and Kubernetes, you would have to deal with this for every single Docker container.</p>
<p>[code language="java"]<br />
&quot;Volumes&quot;: {<br />
   &quot;/dev/termination-log&quot;: &quot;/var/lib/openshift/openshift.local.volumes/pods/6e1d5a40-471b-11e5-9680-525400bca113/containers/mysql/960dd543dc5f790ff2be72858b79c9df20bfae00ec9bffa333cd6e34e7aa36f9&quot;,<br />
   &quot;/var/lib/mysql/data&quot;: &quot;/var/lib/openshift/openshift.local.volumes/pods/6e1d5a40-471b-11e5-9680-525400bca113/volumes/kubernetes.io~nfs/pv0016&quot;,<br />
   &quot;/var/run/secrets/kubernetes.io/serviceaccount&quot;: &quot;/var/lib/openshift/openshift.local.volumes/pods/6e1d5a40-471b-11e5-9680-525400bca113/volumes/kubernetes.io~secret/default-token-ck4x7&quot;<br />
},<br />
&quot;VolumesRW&quot;: {<br />
   &quot;/dev/termination-log&quot;: true,<br />
   &quot;/var/lib/mysql/data&quot;: true,<br />
   &quot;/var/run/secrets/kubernetes.io/serviceaccount&quot;: false<br />
},<br />
&quot;VolumesRelabel&quot;: {<br />
   &quot;/dev/termination-log&quot;: &quot;&quot;,<br />
   &quot;/var/lib/mysql/data&quot;: &quot;&quot;,<br />
   &quot;/var/run/secrets/kubernetes.io/serviceaccount&quot;: &quot;ro&quot;<br />
}</p>
<p>[/code]</p>
<h3>Configure Persistent Storage</h3>
<p>In order to configure persistent storage, the storage must be available to all OpenShift v3 nodes using NFS, ISCSI, Ceph RDB or GlusterFS. In this example, we will configure an NFS server on the OpenShift v3 master. For a lab environment this is fine but for production environments you will want to use external storage for obvious reasons.</p>
<h4>Configure NFS Server on OpenShift v3 Master</h4>
<p>The first step is to install NFS server and start the services.</p>
<pre>#yum groupinstall -y file-server</pre>
<pre>#systemctl enable rpcbind</pre>
<pre>#systemctl enable nfs-server</pre>
<pre>#systemctl start rpcbind</pre>
<pre>#systemctl start nfs-server</pre>
<p>Once the services are running, we need to allow access through iptables. OpenShift v3 uses iptables and not firewalld.</p>
<pre>#iptables-save &gt; pre-nfs-firewall-rules-server</pre>
<pre>#iptables -I INPUT -m state --state NEW -p tcp -m multiport --dport 111,892,2049,32803 -s 0.0.0.0/0 -j ACCEPT</pre>
<pre>#iptables -I INPUT -m state --state NEW -p udp -m multiport --dport 111,892,2049,32769 -s 0.0.0.0/0 -j ACCEPT</pre>
<pre>#service iptables save</pre>
<p>Allow SELinux policy for sVirt to write to nfs shares. By default the SELinux sVirt policy prevents containers from writing to NFS shares.</p>
<pre class="nowrap">#setsebool -P virt_use_nfs 1</pre>
<h4>Configure NFS Client on OpenShift v3 Nodes</h4>
<p>On all nodes, we will want to install the nfs-utils so that nodes can mount NFS shares.</p>
<pre>#yum install -y nfs-utils</pre>
<h4>Configure Persistent Volumes</h4>
<p>In order to configure persistent volumes we need to create a JSON or YAML template file. In this example we will use JSON but Kubernetes supports both. We will also create a pool of 20 persistent volumes. From here all steps will be performed on the OpenShift v3 master.</p>
<p>Create a JSON file that will be used as template for adding persistent volumes. Note: you need to replace the IP address with the IP of your OpenShift v3 master.</p>
<pre>vi /root/PV.json</pre>
<pre>{
 "apiVersion": "v1",
 "kind": "PersistentVolume",
 "metadata": {
 "name": "pv0001"
},
"spec": {
   "capacity": {
   "storage": "10Gi"
   },
   "accessModes": [ "ReadWriteOnce" ],
   "nfs": {
      "path": "/mnt/RBD/pv0001",
      "server": "192.168.122.60"
   },
   "persistentVolumeReclaimPolicy": "Recycle"
   }
}</pre>
<p>In order to automate things we will create a for loop, that will create the NFS shares, set permissions and create persistent volumes in OpenShift v3.</p>
<pre>for i in `seq -w 0001 0020`; do SHARE=/mnt/RBD/pv$i; mkdir -p $SHARE; chmod 777 $SHARE; chown nfsnobody:nfsnobody $SHARE; echo "$SHARE 192.168.122.0/24(rw,all_squash)" &gt;&gt;/etc/exports; sed s/pv0001/pv$i/g /root/PV.json | oc create -f -; done</pre>
<p>We can now list the persistent storage volumes in OpenShift v3. Notice we have no claims yet.</p>
<pre>#oc get pv
NAME   LABELS CAPACITY    ACCESSMODES STATUS    CLAIM         REASON
pv0001 &lt;none&gt; 10737418240 RWO         Available 
pv0002 &lt;none&gt; 10737418240 RWO         Available 
pv0003 &lt;none&gt; 10737418240 RWO         Available 
pv0004 &lt;none&gt; 10737418240 RWO         Available 
pv0005 &lt;none&gt; 10737418240 RWO         Available 
pv0006 &lt;none&gt; 10737418240 RWO         Available 
pv0007 &lt;none&gt; 10737418240 RWO         Available 
pv0008 &lt;none&gt; 10737418240 RWO         Available 
pv0009 &lt;none&gt; 10737418240 RWO         Available 
pv0010 &lt;none&gt; 10737418240 RWO         Available 
pv0011 &lt;none&gt; 10737418240 RWO         Available 
pv0012 &lt;none&gt; 10737418240 RWO         Available 
pv0013 &lt;none&gt; 10737418240 RWO         Available 
pv0014 &lt;none&gt; 10737418240 RWO         Available 
pv0015 &lt;none&gt; 10737418240 RWO         Available 
pv0016 &lt;none&gt; 10737418240 RWO         Available
pv0017 &lt;none&gt; 10737418240 RWO         Available 
pv0018 &lt;none&gt; 10737418240 RWO         Available 
pv0019 &lt;none&gt; 10737418240 RWO         Available 
pv0020 &lt;none&gt; 10737418240 RWO         Available</pre>
<h3>Create OpenShift v3 Application Using Persistent Storage</h3>
<p>Now that everything is configured, we can do a quick demo of how persistent storage in OpenShift v3 actually works. We will deploy a ruby hello world application from GitHub that uses a persistent MySQL database.</p>
<p>First create a new project in OpenShift v3. This creates a namespace in Kubernetes.</p>
<pre>#oc new-project demo</pre>
<p>Next deploy our ruby hello world application from GitHub. This will deploy a pod that builds the code from GitHub and then using <a href="https://github.com/openshift/source-to-image">STI</a> (Source To Image), will deploy a running pod with our built application. The reason for this is that we have different dependencies required for building and running applications. You are hopefully starting to see the power of OpenShift v3. If not stay tuned!</p>
<pre>#oc new-app https://github.com/openshift/ruby-hello-world</pre>
<p>Since we will want to access our application our service also needs to be exposed. OpenShift v3 will configure an HA proxy using router in openVswitch and an Apache vHost. Traffic is routed to the appropriate host based on the service name. The Apache vHost exposes the application based on the service name. Using vHosts allows OpenShift services to use the same ports and thus doesn't require unique ports. Behind the scenes Kubernetes is handling the routing from the OpenShift v3 node to the Docker container.</p>
<pre>#oc expose service ruby-hello-world</pre>
<p>We can check OpenShift v3 UI or console to see the status of our build and deployment.</p>
<p><a href="https://keithtenzer.files.wordpress.com/2015/08/ruby_hello_world_build.png"><img class="alignnone wp-image-1166" src="/assets/2015/08/ruby_hello_world_build.png?w=300" alt="RUBY_HELLO_WORLD_BUILD" width="1187" height="376" /></a></p>
<p>Once complete we should have a builder pod that has exited with 0 and a running pod.</p>
<pre>#oc get pods
NAME                     READY     REASON     RESTARTS AGE
ruby-hello-world-1-build 0/1       ExitCode:0 0        3m
ruby-hello-world-3-3gtig 1/1       Running    0        1m</pre>
<p>If we open a web browser and point it at the service name we should see the application. In this case it won't show us much since we don't yet have a database.</p>
<p><a href="https://keithtenzer.files.wordpress.com/2015/08/ruby_openshift_wo_db.png"><img class="alignnone wp-image-1169" src="/assets/2015/08/ruby_openshift_wo_db.png?w=300" alt="RUBY_OPENSHIFT_WO_DB" width="1185" height="316" /></a></p>
<p>Lets add a persistent MySQL database and connect it to our ruby hello world application.</p>
<pre>#oc process -n openshift mysql-persistent -v DATABASE_SERVICE_NAME=database | oc create -f -</pre>
<pre>#oc env dc database --list | oc env dc ruby-hello-world -e -</pre>
<p>A MySQL database will be deployed using persistent storage. Once the database is deployed the pod should be running and the we should also see a persistent storage volume claim.</p>
<pre>#oc get pods
NAME                     READY     REASON     RESTARTS AGE
database-1-2gv6j         1/1       Running    0        1m
ruby-hello-world-1-build 0/1       ExitCode:0 0        3m
ruby-hello-world-3-3gtig 1/1       Running    0        1m</pre>
<pre>#oc get pv
NAME   LABELS CAPACITY    ACCESSMODES STATUS    CLAIM           REASON
pv0001 &lt;none&gt; 10737418240 RWO         Available 
pv0002 &lt;none&gt; 10737418240 RWO         Available 
pv0003 &lt;none&gt; 10737418240 RWO         Available 
pv0004 &lt;none&gt; 10737418240 RWO         Available 
pv0005 &lt;none&gt; 10737418240 RWO         Available 
pv0006 &lt;none&gt; 10737418240 RWO         Available 
pv0007 &lt;none&gt; 10737418240 RWO         Available 
pv0008 &lt;none&gt; 10737418240 RWO         Available 
pv0009 &lt;none&gt; 10737418240 RWO         Available 
pv0010 &lt;none&gt; 10737418240 RWO         Available 
pv0011 &lt;none&gt; 10737418240 RWO         Available 
pv0012 &lt;none&gt; 10737418240 RWO         Available 
pv0013 &lt;none&gt; 10737418240 RWO         Available 
pv0014 &lt;none&gt; 10737418240 RWO         Available 
pv0015 &lt;none&gt; 10737418240 RWO         Available 
pv0016 &lt;none&gt; 10737418240 RWO         Bound      demo/database
pv0017 &lt;none&gt; 10737418240 RWO         Available 
pv0018 &lt;none&gt; 10737418240 RWO         Available 
pv0019 &lt;none&gt; 10737418240 RWO         Available 
pv0020 &lt;none&gt; 10737418240 RWO         Available</pre>
<p>Once we have verified everything we should also be able to use our ruby hello world application. Lets do a put for a key/value pair.</p>
<p><a href="https://keithtenzer.files.wordpress.com/2015/08/ruby_with_db_put.png"><img class="alignnone wp-image-1170" src="/assets/2015/08/ruby_with_db_put.png?w=300" alt="RUBY_WITH_DB_PUT" width="1155" height="408" /></a></p>
<p>In order to demonstrate persistent storage, let us now delete the MySQL database pod. Don't worry the replication controller will automatically deploy a new pod and of course our data will be saved. If we were using ephemeral storage the data would be lost at this step.</p>
<pre>#oc delete pod database-1-2gv6j</pre>
<p>Finally lets go back to our ruby hello world application and and do a get for our key "keith". We should see the value is "tenzer" thus confirming persistent storage is working.</p>
<p><a href="https://keithtenzer.files.wordpress.com/2015/08/ruby_db_get.png"><img class="alignnone wp-image-1171" src="/assets/2015/08/ruby_db_get.png?w=300" alt="RUBY_DB_GET" width="1195" height="462" /></a></p>
<h3>Summary</h3>
<p>In this article we have seen the power of OpenShift v3 in delivering a complete platform for building, deploying and running container-based applications. We have discussed the use cases behind ephemeral and persistent storage within OpenShift v3 ecosystem. Finally we have implemented and shown a compelling use case for persistent storage. OpenShift v3 is a platform for building and running next-gen applications using immutable container infrastructure. The goal is to deliver innovation faster. Hopefully this article has given you a glimpse at what is available today and inspired you to try things yourself. If you have any feedback or use cases for OpenShift v3, lets hear it!</p>
<p>A special thanks goes to Wolfram Richter, a mentor and colleague who helped tremendously in creating the content for this article.</p>
<p>Happy OpenShifting!</p>
<p>(c) 2015 Keith Tenzer</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#containers" class="page__taxonomy-item" rel="tag">Containers</a><span class="sep">, </span>
    
      <a href="/tags/#docker" class="page__taxonomy-item" rel="tag">Docker</a><span class="sep">, </span>
    
      <a href="/tags/#ephemeral-storage" class="page__taxonomy-item" rel="tag">Ephemeral Storage</a><span class="sep">, </span>
    
      <a href="/tags/#kubernetes" class="page__taxonomy-item" rel="tag">Kubernetes</a><span class="sep">, </span>
    
      <a href="/tags/#nfs" class="page__taxonomy-item" rel="tag">NFS</a><span class="sep">, </span>
    
      <a href="/tags/#openshift" class="page__taxonomy-item" rel="tag">OpenShift</a><span class="sep">, </span>
    
      <a href="/tags/#persistent-storage" class="page__taxonomy-item" rel="tag">Persistent Storage</a><span class="sep">, </span>
    
      <a href="/tags/#storage" class="page__taxonomy-item" rel="tag">Storage</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#openshift" class="page__taxonomy-item" rel="tag">OpenShift</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2015-08-20T00:00:00-07:00">August 20, 2015</time></p>


      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?via=keithtenzer&text=OpenShift+v3%3A+Unlocking+the+Power+of+Persistent+Storage%20https%3A%2F%2Fkeithtenzer.com%2Fopenshift%2Fopenshift-v3-unlocking-the-power-of-persistent-storage%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fkeithtenzer.com%2Fopenshift%2Fopenshift-v3-unlocking-the-power-of-persistent-storage%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fkeithtenzer.com%2Fopenshift%2Fopenshift-v3-unlocking-the-power-of-persistent-storage%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/openshift/openshift-enterprise-v3-lab-configuration-innovate-faster-deliver-sooner/" class="pagination--pager" title="OpenShift Enterprise v3 Lab Configuration: Innovate Faster, Deliver Sooner
">Previous</a>
    
    
      <a href="/openstack/openstack-kilo-lab-installation-and-configuration-guide/" class="pagination--pager" title="OpenStack Kilo Lab Installation and Configuration Guide
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/openshift/openshift-service-mesh-getting-started-guide/" rel="permalink">OpenShift Service Mesh Getting Started Guide
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2021-04-27T00:00:00-07:00">April 27, 2021</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          11 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">






Overview
In this article we will explore the OpenShift Service Mesh and deploy a demo application to better understand the various concepts. First you...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/openshift/openshift-4-aws-ipi-installation-getting-started-guide/" rel="permalink">OpenShift 4 AWS IPI Installation Getting Started Guide
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2021-01-18T00:00:00-08:00">January 18, 2021</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          8 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">


Happy new year as this will be the first post of 2021! 2020 was obviously a challenging year, my hope is I will have more time to devote to blogging in 20...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ansible/windows-automation-with-ansible-getting-started-guide/" rel="permalink">Windows Automation with Ansible: Getting Started Guide
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2020-05-19T00:00:00-07:00">May 19, 2020</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          14 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
Overview
In this article we will focus on how to get started with automation of windows using Ansible. Specifically we will look at installing 3rd party sof...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/general/red-hat-subscription-reporting-guide/" rel="permalink">Red Hat Subscription Reporting Guide
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2020-05-13T00:00:00-07:00">May 13, 2020</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          6 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
source: https://www.cio.com/article/3199910/zuora-blazes-a-new-trail-to-the-subscription-economy-and-a-post-erp-world.html
Overview
This article will look a...</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="https://twitter.com/keithtenzer"" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
      
        
          <li><a href="https://github.com/ktenzer" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2021 Keith Tenzer. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>







  </body>
</html>
