<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.22.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
<link rel="icon" href="/assets/main/me.png">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Storage for Containers Using Ceph RBD - Part IV - Keith Tenzer’s Blog</title>
<meta name="description" content="Overview In this article we will look at how to integrate Ceph RBD (Rados Block Device) with Kubernetes and OpenShift. Ceph is of course a scale-out software-defined storage system that provides block, file and object storage. It focuses primarily on cloud-storage use cases. Providing storage for Kubernetes and OpenShift is just one of many use cases that fit very well with Ceph.  Storage for Containers Overview – Part I Storage for Containers using Gluster – Part II Storage for Containers using Container Native Storage – Part III Storage for Containers using Ceph – Part IV Storage for Containers using NetApp ONTAP NAS – Part V Storage for Containers using NetApp SolidFire – Part VI   Ceph integrates with Kubernetes and OpenShift using the kubernetes.io/rbd driver or provisioner. As you will see this enables dynamic provisioning, allowing storage to be provisioned automatically when developers or users request it. Essentially a developer or OpenShift user, requests a PVC (Persistent Volume Claim) against a storage-class where the the kubernetes.io/rbd provisionier is configured. A PVC will create a RBD image in a specific RBD pool on Ceph and map it to a PV (Persistent Volume) in OpenShift. Once a container starts, the PV pointing to an RBD image is mounted. RBD client then maps the RBD image to the appropriate OpenShift node and mounts it using desired filesystem (ext4). The wonderful thing is everything happens automatically and the Ceph storage administrators only need to manage OpenShift project quotas for storage. Since each storage-class maps to an RBD pool it is possible to create various storage SLAs (gold, silver, bronze, etc). Prerequisites Before going any further an OpenShift 3.4 or higher cluster is required as well as a Ceph 1.3 or higher cluster. Not to worry, you can set this all up using below guides in a few hours and learn more about OpenShift and Ceph. I ran everything required OpenShift (all-in-one) and Ceph (3 node cluster) on my laptop with 12GB RAM.  OpenShift 3.4 all-in-one lab setup Ceph 2.0 lab setup  Configuring Ceph In Ceph we need to create an RBD pool for OpenShift and also create a Ceph authx keyring to access the Ceph cluster from OpenShift. [Ceph Monitor] Create RBD Pool [ceph@ceph1]$ sudo ceph osd pool create ose 128 Note: if you don&#39;t have enough free PGs (placement groups) you can go with 32 or even 16 for lab setups. Create Keyring [ceph@ceph1]$ sudo ceph auth get-or-create client.ose mon &#39;allow r&#39; \ osd &#39;allow class-read object_prefix rdb_children, allow rwx pool=ose&#39; \ -o /etc/ceph/ceph.client.ose.keyring Copy Keyring file to all OpenShift Nodes [ceph@ceph1]$ scp /etc/ceph/ceph.client.ose.keyring \ root@192.168.122.60:/etc/ceph Convert Ceph key to base64 for client.admin user [ceph@ceph1]$ sudo ceph auth get client.admin  exported keyring for client.admin  [client.admin]  key = AQA8nJBYZAQDKxAAuSX4mzY1YODtPU8gzmIufQ==  caps mds = &quot;allow *&quot;  caps mon = &quot;allow *&quot;  caps osd = &quot;allow *&quot; [ceph@ceph1]$  echo AQA8nJBYZAQDKxAAuSX4mzY1YODtPU8gzmIufQ== |base64 QVFBOG5KQllaQVFES3hBQXVTWDRtelkxWU9EdFBVOGd6bUl1ZlE9PQo= Note: save the new base64 key you will need it later. Convert Ceph key to base64 for client.ose user [ceph@ceph1]$ sudo ceph auth get client.ose  exported keyring for client.ose  [client.ose]  key = AQDvU+ZYooHxHBAANGVNCfRpA24iYiTtMgt/tQ==  caps mon = &quot;allow r&quot;  caps osd = &quot;allow class-read object_prefix rdb_children, allow rwx pool=ose&quot; [ceph@ceph1]$ echo AQDvU+ZYooHxHBAANGVNCfRpA24iYiTtMgt/tQ== |base64  QVFEdlUrWllvb0h4SEJBQU5HVk5DZlJwQTI0aVlpVHRNZ3QvdFE9PQo= Note: save the new base64 key you will need it later. Configure Ceph Storage Class in OpenShift [OpenShift Master] Install ceph-common on all OpenShift nodes [root@ose3-master ~]# yum install -y ceph-common Create /var/run/ceph directory [root@ose3-master ~]# mkdir /var/run/ceph Create New Project [root@ose3-master ~]# oc login -u admin [root@ose3-master ~]# oc new-project ceph Create Secret for Ceph client.admin user The key in the secret should be the Ceph authx key converted to base64. [root@ose3-master ~]# vi /root/ceph-secret.yaml apiVersion: v1 kind: Secret metadata:   name: ceph-secret   namespace: default data:   key: QVFBOG5KQllaQVFES3hBQXVTWDRtelkxWU9EdFBVOGd6bUl1ZlE9PQo= [root@ose3-master ~]# oc create -f /root/ceph-secret.yaml Note: Ceph admin secret should be in default project [root@ose3-master ~]# oc get secret ceph-secret -n default  NAME TYPE DATA AGE  ceph-secret Opaque 1 25s Create Secret for Ceph client.ose user The key in the secret should be the Ceph authx key converted to base64. [root@ose3-master ~]# vi /root/ceph-secret-user.yaml apiVersion: v1 kind: Secret metadata:   name: ceph-secret data:   key: QVFEdlUrWllvb0h4SEJBQU5HVk5DZlJwQTI0aVlpVHRNZ3QvdFE9PQo= [root@ose3-master ~]# oc create -f /root/ceph-secret-user.yaml Note: Ceph admin secret must be in the project requesting Ceph storage. Create Storage Class for Ceph [root@ose3-master ~]# vi /root/ceph-rbd-storage-class.yaml apiVersion: storage.k8s.io/v1beta1 kind: StorageClass metadata:   name: ceph   annotations:     storageclass.beta.kubernetes.io/is-default-class: &quot;true&quot; provisioner: kubernetes.io/rbd parameters:   monitors: 192.168.122.81:6789,192.168.122.82:6789,192.168.122.83:6789   adminId: admin   adminSecretName: ceph-secret   adminSecretNamespace: default   pool: ose   userId: ose   userSecretName: ceph-secret-user [root@ose3-master ~]# oc create -f /root/ceph-rbd-storage-class.yaml Dynamic Provisioning using Ceph RBD Create PVC (Persistent Volume Claim)  Using the storage-class for Ceph we can now create a PVC. [root@ose3-master ~]# vi /root/ceph-pvc.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata:   name: ceph-claim   annotations:     volume.beta.kubernetes.io/storage-class: ceph spec:   accessModes:     - ReadWriteOnce   resources:     requests:       storage: 2Gi [root@ose3-master ~]# oc create -f /root/ceph-pvc.yaml Examine PVC and PV The PVC will create a RBD image in the ose RBD pool and map it to a PV. [root@ose3-master ~]# oc get pvc  NAME STATUS VOLUME CAPACITY ACCESSMODES AGE  ceph-claim Bound pvc-792ec052-1ae6-11e7-9752-52540057bf27 2Gi RWO 3s [root@ose3-master ~]# oc get pv  NAME CAPACITY ACCESSMODES RECLAIMPOLICY STATUS CLAIM REASON AGE  pvc-792ec052-1ae6-11e7-9752-52540057bf27 2Gi RWO Delete Bound ceph/ceph-claim 50s List RBD Images [root@ose3-master ~]# rbd list -p ose --name client.ose \ --keyring /etc/ceph/ceph.client.ose.keyring  kubernetes-dynamic-pvc-7930af93-1ae6-11e7-9752-52540057bf27 Dynamic Provisioning using OpenShift UI In OpenShift UI login in and go to the project ceph. Under &quot;resources-&gt;storage&quot; you can view the already created PVC.  Request New PVC Clicking &quot;create storage&quot; you can choose storage class and create a PVC. This is repeating what we already did with PVC.  Under storage we now see both PVCs.  Create MariaDB Persistent Database In order to use a PVC we need to mount it&#39;s PV in a running Pod. In this case we will click &quot;add to project&quot;, search for MariaDB and select the MariaDB persistent template from the OpenShift services catalog.  On the next screen accept the defaults and create the MariaDB persistent service. Once the MariaDB Pod is deploying, under &quot;resources-&gt;storage&quot; you will see the newly created PVC for mariadb.  Finally we see that the MariaDB persistent Pod is started and passed health as well as readiness checks. At this point any data written to this database will be saved in an RBD image residing on a Ceph storage cluster.  Summary In this article we discussed how Ceph RBD integrates with OpenShift and Kubernetes. We saw how to configure Ceph and OpenShift to use RBD through a storage-class. Finally we observed how developers or users can easily consume Ceph storage within OpenShift. Dynamic storage in OpenShift is a huge shift in how we consume storage. I view this as a new beginning in storage. Finally we are really providing storage-as-a-service! Happy Cephing in OpenShift! (c) 2017 Keith Tenzer &nbsp; &nbsp;">


  <meta name="author" content="Keith Tenzer">
  
  <meta property="article:author" content="Keith Tenzer">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Keith Tenzer's Blog">
<meta property="og:title" content="Storage for Containers Using Ceph RBD - Part IV">
<meta property="og:url" content="http://localhost:4000/openshift/storage-for-containers-using-ceph-rbd-part-iv/">


  <meta property="og:description" content="Overview In this article we will look at how to integrate Ceph RBD (Rados Block Device) with Kubernetes and OpenShift. Ceph is of course a scale-out software-defined storage system that provides block, file and object storage. It focuses primarily on cloud-storage use cases. Providing storage for Kubernetes and OpenShift is just one of many use cases that fit very well with Ceph.  Storage for Containers Overview – Part I Storage for Containers using Gluster – Part II Storage for Containers using Container Native Storage – Part III Storage for Containers using Ceph – Part IV Storage for Containers using NetApp ONTAP NAS – Part V Storage for Containers using NetApp SolidFire – Part VI   Ceph integrates with Kubernetes and OpenShift using the kubernetes.io/rbd driver or provisioner. As you will see this enables dynamic provisioning, allowing storage to be provisioned automatically when developers or users request it. Essentially a developer or OpenShift user, requests a PVC (Persistent Volume Claim) against a storage-class where the the kubernetes.io/rbd provisionier is configured. A PVC will create a RBD image in a specific RBD pool on Ceph and map it to a PV (Persistent Volume) in OpenShift. Once a container starts, the PV pointing to an RBD image is mounted. RBD client then maps the RBD image to the appropriate OpenShift node and mounts it using desired filesystem (ext4). The wonderful thing is everything happens automatically and the Ceph storage administrators only need to manage OpenShift project quotas for storage. Since each storage-class maps to an RBD pool it is possible to create various storage SLAs (gold, silver, bronze, etc). Prerequisites Before going any further an OpenShift 3.4 or higher cluster is required as well as a Ceph 1.3 or higher cluster. Not to worry, you can set this all up using below guides in a few hours and learn more about OpenShift and Ceph. I ran everything required OpenShift (all-in-one) and Ceph (3 node cluster) on my laptop with 12GB RAM.  OpenShift 3.4 all-in-one lab setup Ceph 2.0 lab setup  Configuring Ceph In Ceph we need to create an RBD pool for OpenShift and also create a Ceph authx keyring to access the Ceph cluster from OpenShift. [Ceph Monitor] Create RBD Pool [ceph@ceph1]$ sudo ceph osd pool create ose 128 Note: if you don&#39;t have enough free PGs (placement groups) you can go with 32 or even 16 for lab setups. Create Keyring [ceph@ceph1]$ sudo ceph auth get-or-create client.ose mon &#39;allow r&#39; \ osd &#39;allow class-read object_prefix rdb_children, allow rwx pool=ose&#39; \ -o /etc/ceph/ceph.client.ose.keyring Copy Keyring file to all OpenShift Nodes [ceph@ceph1]$ scp /etc/ceph/ceph.client.ose.keyring \ root@192.168.122.60:/etc/ceph Convert Ceph key to base64 for client.admin user [ceph@ceph1]$ sudo ceph auth get client.admin  exported keyring for client.admin  [client.admin]  key = AQA8nJBYZAQDKxAAuSX4mzY1YODtPU8gzmIufQ==  caps mds = &quot;allow *&quot;  caps mon = &quot;allow *&quot;  caps osd = &quot;allow *&quot; [ceph@ceph1]$  echo AQA8nJBYZAQDKxAAuSX4mzY1YODtPU8gzmIufQ== |base64 QVFBOG5KQllaQVFES3hBQXVTWDRtelkxWU9EdFBVOGd6bUl1ZlE9PQo= Note: save the new base64 key you will need it later. Convert Ceph key to base64 for client.ose user [ceph@ceph1]$ sudo ceph auth get client.ose  exported keyring for client.ose  [client.ose]  key = AQDvU+ZYooHxHBAANGVNCfRpA24iYiTtMgt/tQ==  caps mon = &quot;allow r&quot;  caps osd = &quot;allow class-read object_prefix rdb_children, allow rwx pool=ose&quot; [ceph@ceph1]$ echo AQDvU+ZYooHxHBAANGVNCfRpA24iYiTtMgt/tQ== |base64  QVFEdlUrWllvb0h4SEJBQU5HVk5DZlJwQTI0aVlpVHRNZ3QvdFE9PQo= Note: save the new base64 key you will need it later. Configure Ceph Storage Class in OpenShift [OpenShift Master] Install ceph-common on all OpenShift nodes [root@ose3-master ~]# yum install -y ceph-common Create /var/run/ceph directory [root@ose3-master ~]# mkdir /var/run/ceph Create New Project [root@ose3-master ~]# oc login -u admin [root@ose3-master ~]# oc new-project ceph Create Secret for Ceph client.admin user The key in the secret should be the Ceph authx key converted to base64. [root@ose3-master ~]# vi /root/ceph-secret.yaml apiVersion: v1 kind: Secret metadata:   name: ceph-secret   namespace: default data:   key: QVFBOG5KQllaQVFES3hBQXVTWDRtelkxWU9EdFBVOGd6bUl1ZlE9PQo= [root@ose3-master ~]# oc create -f /root/ceph-secret.yaml Note: Ceph admin secret should be in default project [root@ose3-master ~]# oc get secret ceph-secret -n default  NAME TYPE DATA AGE  ceph-secret Opaque 1 25s Create Secret for Ceph client.ose user The key in the secret should be the Ceph authx key converted to base64. [root@ose3-master ~]# vi /root/ceph-secret-user.yaml apiVersion: v1 kind: Secret metadata:   name: ceph-secret data:   key: QVFEdlUrWllvb0h4SEJBQU5HVk5DZlJwQTI0aVlpVHRNZ3QvdFE9PQo= [root@ose3-master ~]# oc create -f /root/ceph-secret-user.yaml Note: Ceph admin secret must be in the project requesting Ceph storage. Create Storage Class for Ceph [root@ose3-master ~]# vi /root/ceph-rbd-storage-class.yaml apiVersion: storage.k8s.io/v1beta1 kind: StorageClass metadata:   name: ceph   annotations:     storageclass.beta.kubernetes.io/is-default-class: &quot;true&quot; provisioner: kubernetes.io/rbd parameters:   monitors: 192.168.122.81:6789,192.168.122.82:6789,192.168.122.83:6789   adminId: admin   adminSecretName: ceph-secret   adminSecretNamespace: default   pool: ose   userId: ose   userSecretName: ceph-secret-user [root@ose3-master ~]# oc create -f /root/ceph-rbd-storage-class.yaml Dynamic Provisioning using Ceph RBD Create PVC (Persistent Volume Claim)  Using the storage-class for Ceph we can now create a PVC. [root@ose3-master ~]# vi /root/ceph-pvc.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata:   name: ceph-claim   annotations:     volume.beta.kubernetes.io/storage-class: ceph spec:   accessModes:     - ReadWriteOnce   resources:     requests:       storage: 2Gi [root@ose3-master ~]# oc create -f /root/ceph-pvc.yaml Examine PVC and PV The PVC will create a RBD image in the ose RBD pool and map it to a PV. [root@ose3-master ~]# oc get pvc  NAME STATUS VOLUME CAPACITY ACCESSMODES AGE  ceph-claim Bound pvc-792ec052-1ae6-11e7-9752-52540057bf27 2Gi RWO 3s [root@ose3-master ~]# oc get pv  NAME CAPACITY ACCESSMODES RECLAIMPOLICY STATUS CLAIM REASON AGE  pvc-792ec052-1ae6-11e7-9752-52540057bf27 2Gi RWO Delete Bound ceph/ceph-claim 50s List RBD Images [root@ose3-master ~]# rbd list -p ose --name client.ose \ --keyring /etc/ceph/ceph.client.ose.keyring  kubernetes-dynamic-pvc-7930af93-1ae6-11e7-9752-52540057bf27 Dynamic Provisioning using OpenShift UI In OpenShift UI login in and go to the project ceph. Under &quot;resources-&gt;storage&quot; you can view the already created PVC.  Request New PVC Clicking &quot;create storage&quot; you can choose storage class and create a PVC. This is repeating what we already did with PVC.  Under storage we now see both PVCs.  Create MariaDB Persistent Database In order to use a PVC we need to mount it&#39;s PV in a running Pod. In this case we will click &quot;add to project&quot;, search for MariaDB and select the MariaDB persistent template from the OpenShift services catalog.  On the next screen accept the defaults and create the MariaDB persistent service. Once the MariaDB Pod is deploying, under &quot;resources-&gt;storage&quot; you will see the newly created PVC for mariadb.  Finally we see that the MariaDB persistent Pod is started and passed health as well as readiness checks. At this point any data written to this database will be saved in an RBD image residing on a Ceph storage cluster.  Summary In this article we discussed how Ceph RBD integrates with OpenShift and Kubernetes. We saw how to configure Ceph and OpenShift to use RBD through a storage-class. Finally we observed how developers or users can easily consume Ceph storage within OpenShift. Dynamic storage in OpenShift is a huge shift in how we consume storage. I view this as a new beginning in storage. Finally we are really providing storage-as-a-service! Happy Cephing in OpenShift! (c) 2017 Keith Tenzer &nbsp; &nbsp;">





  <meta name="twitter:site" content="@keithtenzer">
  <meta name="twitter:title" content="Storage for Containers Using Ceph RBD - Part IV">
  <meta name="twitter:description" content="Overview In this article we will look at how to integrate Ceph RBD (Rados Block Device) with Kubernetes and OpenShift. Ceph is of course a scale-out software-defined storage system that provides block, file and object storage. It focuses primarily on cloud-storage use cases. Providing storage for Kubernetes and OpenShift is just one of many use cases that fit very well with Ceph.  Storage for Containers Overview – Part I Storage for Containers using Gluster – Part II Storage for Containers using Container Native Storage – Part III Storage for Containers using Ceph – Part IV Storage for Containers using NetApp ONTAP NAS – Part V Storage for Containers using NetApp SolidFire – Part VI   Ceph integrates with Kubernetes and OpenShift using the kubernetes.io/rbd driver or provisioner. As you will see this enables dynamic provisioning, allowing storage to be provisioned automatically when developers or users request it. Essentially a developer or OpenShift user, requests a PVC (Persistent Volume Claim) against a storage-class where the the kubernetes.io/rbd provisionier is configured. A PVC will create a RBD image in a specific RBD pool on Ceph and map it to a PV (Persistent Volume) in OpenShift. Once a container starts, the PV pointing to an RBD image is mounted. RBD client then maps the RBD image to the appropriate OpenShift node and mounts it using desired filesystem (ext4). The wonderful thing is everything happens automatically and the Ceph storage administrators only need to manage OpenShift project quotas for storage. Since each storage-class maps to an RBD pool it is possible to create various storage SLAs (gold, silver, bronze, etc). Prerequisites Before going any further an OpenShift 3.4 or higher cluster is required as well as a Ceph 1.3 or higher cluster. Not to worry, you can set this all up using below guides in a few hours and learn more about OpenShift and Ceph. I ran everything required OpenShift (all-in-one) and Ceph (3 node cluster) on my laptop with 12GB RAM.  OpenShift 3.4 all-in-one lab setup Ceph 2.0 lab setup  Configuring Ceph In Ceph we need to create an RBD pool for OpenShift and also create a Ceph authx keyring to access the Ceph cluster from OpenShift. [Ceph Monitor] Create RBD Pool [ceph@ceph1]$ sudo ceph osd pool create ose 128 Note: if you don&#39;t have enough free PGs (placement groups) you can go with 32 or even 16 for lab setups. Create Keyring [ceph@ceph1]$ sudo ceph auth get-or-create client.ose mon &#39;allow r&#39; \ osd &#39;allow class-read object_prefix rdb_children, allow rwx pool=ose&#39; \ -o /etc/ceph/ceph.client.ose.keyring Copy Keyring file to all OpenShift Nodes [ceph@ceph1]$ scp /etc/ceph/ceph.client.ose.keyring \ root@192.168.122.60:/etc/ceph Convert Ceph key to base64 for client.admin user [ceph@ceph1]$ sudo ceph auth get client.admin  exported keyring for client.admin  [client.admin]  key = AQA8nJBYZAQDKxAAuSX4mzY1YODtPU8gzmIufQ==  caps mds = &quot;allow *&quot;  caps mon = &quot;allow *&quot;  caps osd = &quot;allow *&quot; [ceph@ceph1]$  echo AQA8nJBYZAQDKxAAuSX4mzY1YODtPU8gzmIufQ== |base64 QVFBOG5KQllaQVFES3hBQXVTWDRtelkxWU9EdFBVOGd6bUl1ZlE9PQo= Note: save the new base64 key you will need it later. Convert Ceph key to base64 for client.ose user [ceph@ceph1]$ sudo ceph auth get client.ose  exported keyring for client.ose  [client.ose]  key = AQDvU+ZYooHxHBAANGVNCfRpA24iYiTtMgt/tQ==  caps mon = &quot;allow r&quot;  caps osd = &quot;allow class-read object_prefix rdb_children, allow rwx pool=ose&quot; [ceph@ceph1]$ echo AQDvU+ZYooHxHBAANGVNCfRpA24iYiTtMgt/tQ== |base64  QVFEdlUrWllvb0h4SEJBQU5HVk5DZlJwQTI0aVlpVHRNZ3QvdFE9PQo= Note: save the new base64 key you will need it later. Configure Ceph Storage Class in OpenShift [OpenShift Master] Install ceph-common on all OpenShift nodes [root@ose3-master ~]# yum install -y ceph-common Create /var/run/ceph directory [root@ose3-master ~]# mkdir /var/run/ceph Create New Project [root@ose3-master ~]# oc login -u admin [root@ose3-master ~]# oc new-project ceph Create Secret for Ceph client.admin user The key in the secret should be the Ceph authx key converted to base64. [root@ose3-master ~]# vi /root/ceph-secret.yaml apiVersion: v1 kind: Secret metadata:   name: ceph-secret   namespace: default data:   key: QVFBOG5KQllaQVFES3hBQXVTWDRtelkxWU9EdFBVOGd6bUl1ZlE9PQo= [root@ose3-master ~]# oc create -f /root/ceph-secret.yaml Note: Ceph admin secret should be in default project [root@ose3-master ~]# oc get secret ceph-secret -n default  NAME TYPE DATA AGE  ceph-secret Opaque 1 25s Create Secret for Ceph client.ose user The key in the secret should be the Ceph authx key converted to base64. [root@ose3-master ~]# vi /root/ceph-secret-user.yaml apiVersion: v1 kind: Secret metadata:   name: ceph-secret data:   key: QVFEdlUrWllvb0h4SEJBQU5HVk5DZlJwQTI0aVlpVHRNZ3QvdFE9PQo= [root@ose3-master ~]# oc create -f /root/ceph-secret-user.yaml Note: Ceph admin secret must be in the project requesting Ceph storage. Create Storage Class for Ceph [root@ose3-master ~]# vi /root/ceph-rbd-storage-class.yaml apiVersion: storage.k8s.io/v1beta1 kind: StorageClass metadata:   name: ceph   annotations:     storageclass.beta.kubernetes.io/is-default-class: &quot;true&quot; provisioner: kubernetes.io/rbd parameters:   monitors: 192.168.122.81:6789,192.168.122.82:6789,192.168.122.83:6789   adminId: admin   adminSecretName: ceph-secret   adminSecretNamespace: default   pool: ose   userId: ose   userSecretName: ceph-secret-user [root@ose3-master ~]# oc create -f /root/ceph-rbd-storage-class.yaml Dynamic Provisioning using Ceph RBD Create PVC (Persistent Volume Claim)  Using the storage-class for Ceph we can now create a PVC. [root@ose3-master ~]# vi /root/ceph-pvc.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata:   name: ceph-claim   annotations:     volume.beta.kubernetes.io/storage-class: ceph spec:   accessModes:     - ReadWriteOnce   resources:     requests:       storage: 2Gi [root@ose3-master ~]# oc create -f /root/ceph-pvc.yaml Examine PVC and PV The PVC will create a RBD image in the ose RBD pool and map it to a PV. [root@ose3-master ~]# oc get pvc  NAME STATUS VOLUME CAPACITY ACCESSMODES AGE  ceph-claim Bound pvc-792ec052-1ae6-11e7-9752-52540057bf27 2Gi RWO 3s [root@ose3-master ~]# oc get pv  NAME CAPACITY ACCESSMODES RECLAIMPOLICY STATUS CLAIM REASON AGE  pvc-792ec052-1ae6-11e7-9752-52540057bf27 2Gi RWO Delete Bound ceph/ceph-claim 50s List RBD Images [root@ose3-master ~]# rbd list -p ose --name client.ose \ --keyring /etc/ceph/ceph.client.ose.keyring  kubernetes-dynamic-pvc-7930af93-1ae6-11e7-9752-52540057bf27 Dynamic Provisioning using OpenShift UI In OpenShift UI login in and go to the project ceph. Under &quot;resources-&gt;storage&quot; you can view the already created PVC.  Request New PVC Clicking &quot;create storage&quot; you can choose storage class and create a PVC. This is repeating what we already did with PVC.  Under storage we now see both PVCs.  Create MariaDB Persistent Database In order to use a PVC we need to mount it&#39;s PV in a running Pod. In this case we will click &quot;add to project&quot;, search for MariaDB and select the MariaDB persistent template from the OpenShift services catalog.  On the next screen accept the defaults and create the MariaDB persistent service. Once the MariaDB Pod is deploying, under &quot;resources-&gt;storage&quot; you will see the newly created PVC for mariadb.  Finally we see that the MariaDB persistent Pod is started and passed health as well as readiness checks. At this point any data written to this database will be saved in an RBD image residing on a Ceph storage cluster.  Summary In this article we discussed how Ceph RBD integrates with OpenShift and Kubernetes. We saw how to configure Ceph and OpenShift to use RBD through a storage-class. Finally we observed how developers or users can easily consume Ceph storage within OpenShift. Dynamic storage in OpenShift is a huge shift in how we consume storage. I view this as a new beginning in storage. Finally we are really providing storage-as-a-service! Happy Cephing in OpenShift! (c) 2017 Keith Tenzer &nbsp; &nbsp;">
  <meta name="twitter:url" content="http://localhost:4000/openshift/storage-for-containers-using-ceph-rbd-part-iv/">

  
    <meta name="twitter:card" content="summary">
    
  

  



  <meta property="article:published_time" content="2017-04-07T00:00:00-07:00">





  

  


<link rel="canonical" href="http://localhost:4000/openshift/storage-for-containers-using-ceph-rbd-part-iv/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Keith Tenzer",
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Keith Tenzer's Blog Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single wide">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Keith Tenzer's Blog
          <span class="site-subtitle">Cloud Computing and Code</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about/index.html">About</a>
            </li><li class="masthead__menu-item">
              <a href="/conferences-and-events/index.html">Conferences and Events</a>
            </li><li class="masthead__menu-item">
              <a href="/videos/index.html">Videos</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16">
            <path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path>
          </svg>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      


  
    



<nav class="breadcrumbs">
  <ol itemscope itemtype="https://schema.org/BreadcrumbList">
    
    
    
      
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="http://localhost:4000/" itemprop="item"><span itemprop="name">Home</span></a>
          <meta itemprop="position" content="1" />
        </li>
        <span class="sep">/</span>
      
      
        
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/categories/#openshift" itemprop="item"><span itemprop="name">Openshift</span></a>
          <meta itemprop="position" content="2" />
        </li>
        <span class="sep">/</span>
      
    
      
      
        <li class="current">Storage for Containers Using Ceph RBD - Part IV</li>
      
    
  </ol>
</nav>

  


<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      
        <img src="/assets/main/me.png" alt="Keith Tenzer" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Keith Tenzer</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>Principal Solutions Architect at Red Hat</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Los Angeles, CA</span>
        </li>
      

      
        
          
        
          
        
          
            <li><a href="https://twitter.com/keithtenzer" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i><span class="label">Twitter</span></a></li>
          
        
          
        
          
            <li><a href="https://github.com/ktenzer" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Storage for Containers Using Ceph RBD - Part IV">
    <meta itemprop="description" content="OverviewIn this article we will look at how to integrate Ceph RBD (Rados Block Device) with Kubernetes and OpenShift. Ceph is of course a scale-out software-defined storage system that provides block, file and object storage. It focuses primarily on cloud-storage use cases. Providing storage for Kubernetes and OpenShift is just one of many use cases that fit very well with Ceph.Storage for Containers Overview – Part IStorage for Containers using Gluster – Part IIStorage for Containers using Container Native Storage – Part IIIStorage for Containers using Ceph – Part IVStorage for Containers using NetApp ONTAP NAS – Part VStorage for Containers using NetApp SolidFire – Part VICeph integrates with Kubernetes and OpenShift using the kubernetes.io/rbd driver or provisioner. As you will see this enables dynamic provisioning, allowing storage to be provisioned automatically when developers or users request it. Essentially a developer or OpenShift user, requests a PVC (Persistent Volume Claim) against a storage-class where the the kubernetes.io/rbd provisionier is configured. A PVC will create a RBD image in a specific RBD pool on Ceph and map it to a PV (Persistent Volume) in OpenShift. Once a container starts, the PV pointing to an RBD image is mounted. RBD client then maps the RBD image to the appropriate OpenShift node and mounts it using desired filesystem (ext4). The wonderful thing is everything happens automatically and the Ceph storage administrators only need to manage OpenShift project quotas for storage. Since each storage-class maps to an RBD pool it is possible to create various storage SLAs (gold, silver, bronze, etc).PrerequisitesBefore going any further an OpenShift 3.4 or higher cluster is required as well as a Ceph 1.3 or higher cluster. Not to worry, you can set this all up using below guides in a few hours and learn more about OpenShift and Ceph. I ran everything required OpenShift (all-in-one) and Ceph (3 node cluster) on my laptop with 12GB RAM.OpenShift 3.4 all-in-one lab setupCeph 2.0 lab setupConfiguring CephIn Ceph we need to create an RBD pool for OpenShift and also create a Ceph authx keyring to access the Ceph cluster from OpenShift.[Ceph Monitor]Create RBD Pool[ceph@ceph1]$ sudo ceph osd pool create ose 128Note: if you don&#39;t have enough free PGs (placement groups) you can go with 32 or even 16 for lab setups.Create Keyring[ceph@ceph1]$ sudo ceph auth get-or-create client.ose mon &#39;allow r&#39; \osd &#39;allow class-read object_prefix rdb_children, allow rwx pool=ose&#39; \-o /etc/ceph/ceph.client.ose.keyringCopy Keyring file to all OpenShift Nodes[ceph@ceph1]$ scp /etc/ceph/ceph.client.ose.keyring \root@192.168.122.60:/etc/cephConvert Ceph key to base64 for client.admin user[ceph@ceph1]$ sudo ceph auth get client.admin exported keyring for client.admin [client.admin] key = AQA8nJBYZAQDKxAAuSX4mzY1YODtPU8gzmIufQ== caps mds = &quot;allow *&quot; caps mon = &quot;allow *&quot; caps osd = &quot;allow *&quot;[ceph@ceph1]$  echo AQA8nJBYZAQDKxAAuSX4mzY1YODtPU8gzmIufQ== |base64QVFBOG5KQllaQVFES3hBQXVTWDRtelkxWU9EdFBVOGd6bUl1ZlE9PQo=Note: save the new base64 key you will need it later.Convert Ceph key to base64 for client.ose user[ceph@ceph1]$ sudo ceph auth get client.ose exported keyring for client.ose [client.ose] key = AQDvU+ZYooHxHBAANGVNCfRpA24iYiTtMgt/tQ== caps mon = &quot;allow r&quot; caps osd = &quot;allow class-read object_prefix rdb_children, allow rwx pool=ose&quot;[ceph@ceph1]$ echo AQDvU+ZYooHxHBAANGVNCfRpA24iYiTtMgt/tQ== |base64 QVFEdlUrWllvb0h4SEJBQU5HVk5DZlJwQTI0aVlpVHRNZ3QvdFE9PQo=Note: save the new base64 key you will need it later.Configure Ceph Storage Class in OpenShift[OpenShift Master]Install ceph-common on all OpenShift nodes[root@ose3-master ~]# yum install -y ceph-commonCreate /var/run/ceph directory[root@ose3-master ~]# mkdir /var/run/cephCreate New Project[root@ose3-master ~]# oc login -u admin[root@ose3-master ~]# oc new-project cephCreate Secret for Ceph client.admin userThe key in the secret should be the Ceph authx key converted to base64.[root@ose3-master ~]# vi /root/ceph-secret.yamlapiVersion: v1kind: Secretmetadata:  name: ceph-secret  namespace: defaultdata:  key: QVFBOG5KQllaQVFES3hBQXVTWDRtelkxWU9EdFBVOGd6bUl1ZlE9PQo=[root@ose3-master ~]# oc create -f /root/ceph-secret.yamlNote: Ceph admin secret should be in default project[root@ose3-master ~]# oc get secret ceph-secret -n default NAME TYPE DATA AGE ceph-secret Opaque 1 25sCreate Secret for Ceph client.ose userThe key in the secret should be the Ceph authx key converted to base64.[root@ose3-master ~]# vi /root/ceph-secret-user.yamlapiVersion: v1kind: Secretmetadata:  name: ceph-secretdata:  key: QVFEdlUrWllvb0h4SEJBQU5HVk5DZlJwQTI0aVlpVHRNZ3QvdFE9PQo=[root@ose3-master ~]# oc create -f /root/ceph-secret-user.yamlNote: Ceph admin secret must be in the project requesting Ceph storage.Create Storage Class for Ceph[root@ose3-master ~]# vi /root/ceph-rbd-storage-class.yamlapiVersion: storage.k8s.io/v1beta1kind: StorageClassmetadata:  name: ceph  annotations:    storageclass.beta.kubernetes.io/is-default-class: &quot;true&quot;provisioner: kubernetes.io/rbdparameters:  monitors: 192.168.122.81:6789,192.168.122.82:6789,192.168.122.83:6789  adminId: admin  adminSecretName: ceph-secret  adminSecretNamespace: default  pool: ose  userId: ose  userSecretName: ceph-secret-user[root@ose3-master ~]# oc create -f /root/ceph-rbd-storage-class.yamlDynamic Provisioning using Ceph RBDCreate PVC (Persistent Volume Claim) Using the storage-class for Ceph we can now create a PVC.[root@ose3-master ~]# vi /root/ceph-pvc.yamlkind: PersistentVolumeClaimapiVersion: v1metadata:  name: ceph-claim  annotations:    volume.beta.kubernetes.io/storage-class: cephspec:  accessModes:    - ReadWriteOnce  resources:    requests:      storage: 2Gi[root@ose3-master ~]# oc create -f /root/ceph-pvc.yamlExamine PVC and PVThe PVC will create a RBD image in the ose RBD pool and map it to a PV.[root@ose3-master ~]# oc get pvc NAME STATUS VOLUME CAPACITY ACCESSMODES AGE ceph-claim Bound pvc-792ec052-1ae6-11e7-9752-52540057bf27 2Gi RWO 3s[root@ose3-master ~]# oc get pv NAME CAPACITY ACCESSMODES RECLAIMPOLICY STATUS CLAIM REASON AGE pvc-792ec052-1ae6-11e7-9752-52540057bf27 2Gi RWO Delete Bound ceph/ceph-claim 50sList RBD Images[root@ose3-master ~]# rbd list -p ose --name client.ose \--keyring /etc/ceph/ceph.client.ose.keyring kubernetes-dynamic-pvc-7930af93-1ae6-11e7-9752-52540057bf27Dynamic Provisioning using OpenShift UIIn OpenShift UI login in and go to the project ceph. Under &quot;resources-&gt;storage&quot; you can view the already created PVC.Request New PVCClicking &quot;create storage&quot; you can choose storage class and create a PVC. This is repeating what we already did with PVC.Under storage we now see both PVCs.Create MariaDB Persistent DatabaseIn order to use a PVC we need to mount it&#39;s PV in a running Pod. In this case we will click &quot;add to project&quot;, search for MariaDB and select the MariaDB persistent template from the OpenShift services catalog.On the next screen accept the defaults and create the MariaDB persistent service. Once the MariaDB Pod is deploying, under &quot;resources-&gt;storage&quot; you will see the newly created PVC for mariadb.Finally we see that the MariaDB persistent Pod is started and passed health as well as readiness checks. At this point any data written to this database will be saved in an RBD image residing on a Ceph storage cluster.SummaryIn this article we discussed how Ceph RBD integrates with OpenShift and Kubernetes. We saw how to configure Ceph and OpenShift to use RBD through a storage-class. Finally we observed how developers or users can easily consume Ceph storage within OpenShift. Dynamic storage in OpenShift is a huge shift in how we consume storage. I view this as a new beginning in storage. Finally we are really providing storage-as-a-service!Happy Cephing in OpenShift!(c) 2017 Keith Tenzer&nbsp;&nbsp;">
    <meta itemprop="datePublished" content="2017-04-07T00:00:00-07:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Storage for Containers Using Ceph RBD - Part IV
</h1>
          

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2017-04-07T00:00:00-07:00">April 7, 2017</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          10 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
        <h3><img class="alignnone size-full wp-image-8150" src="/assets/2017/04/storage_article_008.jpg" alt="Shipping containers" width="1200" height="400" /></h3>
<h3>Overview</h3>
<p>In this article we will look at how to integrate Ceph RBD (Rados Block Device) with Kubernetes and OpenShift. Ceph is of course a scale-out software-defined storage system that provides block, file and object storage. It focuses primarily on cloud-storage use cases. Providing storage for Kubernetes and OpenShift is just one of many use cases that fit very well with Ceph.</p>
<ul>
<li><a href="https://keithtenzer.com/2017/03/07/storage-for-containers-overview-part-i/">Storage for Containers Overview – Part I</a></li>
<li><a href="https://keithtenzer.com/2017/03/24/storage-for-containers-using-gluster-part-ii/">Storage for Containers using Gluster – Part II</a></li>
<li><a href="https://keithtenzer.com/2017/03/29/storage-for-containers-using-container-native-storage-part-iii/">Storage for Containers using Container Native Storage – Part III</a></li>
<li><a href="https://keithtenzer.com/2017/04/07/storage-for-containers-using-ceph-rbd-part-iv/">Storage for Containers using Ceph – Part IV</a></li>
<li><a href="https://keithtenzer.com/2017/04/05/storage-for-containers-using-netapp-ontap-nas-part-v/">Storage for Containers using NetApp ONTAP NAS – Part V</a></li>
<li><a href="https://keithtenzer.com/2017/04/05/storage-for-containers-using-netapp-solidfire-part-vi/">Storage for Containers using NetApp SolidFire – Part VI</a></li>
</ul>
<p><!--more--></p>
<p>Ceph integrates with Kubernetes and OpenShift using the kubernetes.io/rbd driver or provisioner. As you will see this enables dynamic provisioning, allowing storage to be provisioned automatically when developers or users request it. Essentially a developer or OpenShift user, requests a PVC (Persistent Volume Claim) against a storage-class where the the kubernetes.io/rbd provisionier is configured. A PVC will create a RBD image in a specific RBD pool on Ceph and map it to a PV (Persistent Volume) in OpenShift. Once a container starts, the PV pointing to an RBD image is mounted. RBD client then maps the RBD image to the appropriate OpenShift node and mounts it using desired filesystem (ext4). The wonderful thing is everything happens automatically and the Ceph storage administrators only need to manage OpenShift project quotas for storage. Since each storage-class maps to an RBD pool it is possible to create various storage SLAs (gold, silver, bronze, etc).</p>
<h3>Prerequisites</h3>
<p>Before going any further an OpenShift 3.4 or higher cluster is required as well as a Ceph 1.3 or higher cluster. Not to worry, you can set this all up using below guides in a few hours and learn more about OpenShift and Ceph. I ran everything required OpenShift (all-in-one) and Ceph (3 node cluster) on my laptop with 12GB RAM.</p>
<ul>
<li><a href="https://keithtenzer.com/2017/03/13/openshift-enterprise-3-4-all-in-one-lab-environment/">OpenShift 3.4 all-in-one lab setup</a></li>
<li><a href="https://keithtenzer.com/2017/02/03/red-hat-ceph-storage-2-0-lab-object-storage-configuration-guide/">Ceph 2.0 lab setup</a></li>
</ul>
<h3>Configuring Ceph</h3>
<p>In Ceph we need to create an RBD pool for OpenShift and also create a Ceph authx keyring to access the Ceph cluster from OpenShift.</p>
<p>[Ceph Monitor]</p>
<p><strong>Create RBD Pool</strong></p>
<pre>[ceph@ceph1]$ sudo ceph osd pool create ose 128</pre>
<p>Note: if you don't have enough free PGs (placement groups) you can go with 32 or even 16 for lab setups.</p>
<p><strong>Create Keyring</strong></p>
<pre>[ceph@ceph1]$ sudo ceph auth get-or-create client.ose mon 'allow r' \
osd 'allow class-read object_prefix rdb_children, allow rwx pool=ose' \
-o /etc/ceph/ceph.client.ose.keyring</pre>
<p><strong>Copy Keyring file to all OpenShift Nodes</strong></p>
<pre>[ceph@ceph1]$ scp /etc/ceph/ceph.client.ose.keyring \
root@192.168.122.60:/etc/ceph</pre>
<p><strong>Convert Ceph key to base64 for client.admin user</strong></p>
<pre>[ceph@ceph1]$ sudo ceph auth get client.admin
 exported keyring for client.admin
 [client.admin]
 key = AQA8nJBYZAQDKxAAuSX4mzY1YODtPU8gzmIufQ==
 caps mds = "allow *"
 caps mon = "allow *"
 caps osd = "allow *"</pre>
<pre>[ceph@ceph1]$  echo AQA8nJBYZAQDKxAAuSX4mzY1YODtPU8gzmIufQ== |base64
<strong>QVFBOG5KQllaQVFES3hBQXVTWDRtelkxWU9EdFBVOGd6bUl1ZlE9PQo=</strong></pre>
<p>Note: save the new base64 key you will need it later.</p>
<p><strong>Convert Ceph key to base64 for client.ose user</strong></p>
<pre>[ceph@ceph1]$ sudo ceph auth get client.ose
 exported keyring for client.ose
 [client.ose]
 key = AQDvU+ZYooHxHBAANGVNCfRpA24iYiTtMgt/tQ==
 caps mon = "allow r"
 caps osd = "allow class-read object_prefix rdb_children, allow rwx pool=ose"</pre>
<pre>[ceph@ceph1]$ echo AQDvU+ZYooHxHBAANGVNCfRpA24iYiTtMgt/tQ== |base64
 <strong>QVFEdlUrWllvb0h4SEJBQU5HVk5DZlJwQTI0aVlpVHRNZ3QvdFE9PQo=</strong></pre>
<p>Note: save the new base64 key you will need it later.</p>
<h3>Configure Ceph Storage Class in OpenShift</h3>
<p>[OpenShift Master]</p>
<p><strong>Install ceph-common on all OpenShift nodes</strong></p>
<pre class="nowrap">[root@ose3-master ~]# yum install -y ceph-common</pre>
<p><strong>Create /var/run/ceph directory</strong></p>
<pre>[root@ose3-master ~]# mkdir /var/run/ceph</pre>
<p><strong>Create New Project</strong></p>
<pre>[root@ose3-master ~]# oc login -u admin</pre>
<pre>[root@ose3-master ~]# oc new-project ceph</pre>
<p><strong>Create Secret for Ceph client.admin user</strong></p>
<p>The key in the secret should be the Ceph authx key converted to base64.</p>
<pre>[root@ose3-master ~]# vi /root/ceph-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
  namespace: default
data:
  key: <strong>QVFBOG5KQllaQVFES3hBQXVTWDRtelkxWU9EdFBVOGd6bUl1ZlE9PQo=</strong></pre>
<pre>[root@ose3-master ~]# oc create -f /root/ceph-secret.yaml</pre>
<p>Note: Ceph admin secret should be in default project</p>
<pre>[root@ose3-master ~]# oc get secret ceph-secret -n default
 NAME TYPE DATA AGE
 ceph-secret Opaque 1 25s</pre>
<p><strong>Create Secret for Ceph client.ose user</strong></p>
<p>The key in the secret should be the Ceph authx key converted to base64.</p>
<pre>[root@ose3-master ~]# vi /root/ceph-secret-user.yaml
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
data:
  key: <strong>QVFEdlUrWllvb0h4SEJBQU5HVk5DZlJwQTI0aVlpVHRNZ3QvdFE9PQo=</strong></pre>
<pre>[root@ose3-master ~]# oc create -f /root/ceph-secret-user.yaml</pre>
<p>Note: Ceph admin secret <strong>must</strong> be in the project requesting Ceph storage.</p>
<p><strong>Create Storage Class for Ceph</strong></p>
<pre>[root@ose3-master ~]# vi /root/ceph-rbd-storage-class.yaml
apiVersion: storage.k8s.io/v1beta1
kind: StorageClass
metadata:
  name: ceph
  annotations:
    storageclass.beta.kubernetes.io/is-default-class: "true"
provisioner: kubernetes.io/rbd
parameters:
  monitors: 192.168.122.81:6789,192.168.122.82:6789,192.168.122.83:6789
  adminId: admin
  adminSecretName: ceph-secret
  adminSecretNamespace: default
  pool: ose
  userId: ose
  userSecretName: ceph-secret-user</pre>
<pre>[root@ose3-master ~]# oc create -f /root/ceph-rbd-storage-class.yaml</pre>
<h3>Dynamic Provisioning using Ceph RBD</h3>
<p><strong>Create PVC (Persistent Volume Claim) </strong></p>
<p>Using the storage-class for Ceph we can now create a PVC.</p>
<pre>[root@ose3-master ~]# vi /root/ceph-pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: ceph-claim
  annotations:
    volume.beta.kubernetes.io/storage-class: ceph
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi</pre>
<pre>[root@ose3-master ~]# oc create -f /root/ceph-pvc.yaml</pre>
<p><strong>Examine PVC and PV</strong></p>
<p>The PVC will create a RBD image in the ose RBD pool and map it to a PV.</p>
<pre>[root@ose3-master ~]# oc get pvc
 NAME STATUS VOLUME CAPACITY ACCESSMODES AGE
 ceph-claim Bound pvc-792ec052-1ae6-11e7-9752-52540057bf27 2Gi RWO 3s</pre>
<pre>[root@ose3-master ~]# oc get pv
 NAME CAPACITY ACCESSMODES RECLAIMPOLICY STATUS CLAIM REASON AGE
 pvc-792ec052-1ae6-11e7-9752-52540057bf27 2Gi RWO Delete Bound ceph/ceph-claim 50s</pre>
<p><strong>List RBD Images</strong></p>
<pre>[root@ose3-master ~]# rbd list -p ose --name client.ose \
--keyring /etc/ceph/ceph.client.ose.keyring
 kubernetes-dynamic-pvc-7930af93-1ae6-11e7-9752-52540057bf27</pre>
<h3>Dynamic Provisioning using OpenShift UI</h3>
<p>In OpenShift UI login in and go to the project ceph. Under "resources-&gt;storage" you can view the already created PVC.</p>
<p><img class="alignnone size-full wp-image-9097" src="/assets/2017/04/ceph-pvc.png" alt="ceph-pvc" width="1679" height="354" /></p>
<p><strong>Request New PVC</strong></p>
<p>Clicking "create storage" you can choose storage class and create a PVC. This is repeating what we already did with PVC.</p>
<p><img class="alignnone size-full wp-image-9099" src="/assets/2017/04/ceph-provision.png" alt="ceph-provision" width="965" height="728" /></p>
<p>Under storage we now see both PVCs.</p>
<p><img class="alignnone size-full wp-image-9100" src="/assets/2017/04/ceph-pvc2.png" alt="ceph-pvc2" width="1692" height="386" /></p>
<p><strong>Create MariaDB Persistent Database</strong></p>
<p>In order to use a PVC we need to mount it's PV in a running Pod. In this case we will click "add to project", search for MariaDB and select the MariaDB persistent template from the OpenShift services catalog.</p>
<p><img class="alignnone size-full wp-image-9104" src="/assets/2017/04/ceph-prov-mariadb.png" alt="ceph-prov-mariadb" width="622" height="573" /></p>
<p>On the next screen accept the defaults and create the MariaDB persistent service. Once the MariaDB Pod is deploying, under "resources-&gt;storage" you will see the newly created PVC for mariadb.</p>
<p><img class="alignnone size-full wp-image-9102" src="/assets/2017/04/ceph-mariadb-provisioned.png" alt="ceph-mariadb-provisioned" width="1680" height="461" /></p>
<p>Finally we see that the MariaDB persistent Pod is started and passed health as well as readiness checks. At this point any data written to this database will be saved in an RBD image residing on a Ceph storage cluster.</p>
<p><img class="alignnone size-full wp-image-9106" src="/assets/2017/04/ceph-mariadb-complete.png" alt="ceph-mariadb-complete" width="853" height="432" /></p>
<h3>Summary</h3>
<p>In this article we discussed how Ceph RBD integrates with OpenShift and Kubernetes. We saw how to configure Ceph and OpenShift to use RBD through a storage-class. Finally we observed how developers or users can easily consume Ceph storage within OpenShift. Dynamic storage in OpenShift is a huge shift in how we consume storage. I view this as a new beginning in storage. Finally we are really providing storage-as-a-service!</p>
<p>Happy Cephing in OpenShift!</p>
<p>(c) 2017 Keith Tenzer</p>
<p>&nbsp;</p>
<p>&nbsp;</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#ceph" class="page__taxonomy-item" rel="tag">Ceph</a><span class="sep">, </span>
    
      <a href="/tags/#containers" class="page__taxonomy-item" rel="tag">Containers</a><span class="sep">, </span>
    
      <a href="/tags/#kubernetes" class="page__taxonomy-item" rel="tag">Kubernetes</a><span class="sep">, </span>
    
      <a href="/tags/#openshift" class="page__taxonomy-item" rel="tag">OpenShift</a><span class="sep">, </span>
    
      <a href="/tags/#rbd" class="page__taxonomy-item" rel="tag">RBD</a><span class="sep">, </span>
    
      <a href="/tags/#storage" class="page__taxonomy-item" rel="tag">Storage</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#openshift" class="page__taxonomy-item" rel="tag">OpenShift</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2017-04-07T00:00:00-07:00">April 7, 2017</time></p>


      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?via=keithtenzer&text=Storage+for+Containers+Using+Ceph+RBD+-+Part+IV%20http%3A%2F%2Flocalhost%3A4000%2Fopenshift%2Fstorage-for-containers-using-ceph-rbd-part-iv%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fopenshift%2Fstorage-for-containers-using-ceph-rbd-part-iv%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fopenshift%2Fstorage-for-containers-using-ceph-rbd-part-iv%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/openshift/storage-for-containers-using-netapp-solidfire-part-vi/" class="pagination--pager" title="Storage for Containers using NetApp SolidFire– Part VI
">Previous</a>
    
    
      <a href="/openstack/red-hat-openstack-platform-10-newton-installation-and-configuration-guide/" class="pagination--pager" title="Red Hat OpenStack Platform 10 (Newton) Installation and Configuration Guide
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/linux/The-Fedora-Workstation-Experience/" rel="permalink">The Fedora Workstation Experience
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2022-01-10T00:00:00-08:00">January 10, 2022</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          11 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
Overview
In 2022 choice matters, talk is even cheaper and action is many decibels louder than it used to be. In addition, more and more people are realizing...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/openshift/building-ansible-operators-1-2-3/" rel="permalink">Building Ansible Operators 1-2-3
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2021-12-03T00:00:00-08:00">December 3, 2021</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          10 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Overview
In this article we will go step by step to build a Kubernetes Operator using Ansible and the Operator Framework. Operators provide the ability to no...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/openshift/openshift-service-mesh-getting-started-guide/" rel="permalink">OpenShift Service Mesh Getting Started Guide
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2021-04-27T00:00:00-07:00">April 27, 2021</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          11 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">






Overview
In this article we will explore the OpenShift Service Mesh and deploy a demo application to better understand the various concepts. First you...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/openshift/openshift-4-aws-ipi-installation-getting-started-guide/" rel="permalink">OpenShift 4 AWS IPI Installation Getting Started Guide
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2021-01-18T00:00:00-08:00">January 18, 2021</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          8 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">


Happy new year as this will be the first post of 2021! 2020 was obviously a challenging year, my hope is I will have more time to devote to blogging in 20...</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="https://twitter.com/keithtenzer"" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
      
        
          <li><a href="https://github.com/ktenzer" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2022 Keith Tenzer. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>







  </body>
</html>
