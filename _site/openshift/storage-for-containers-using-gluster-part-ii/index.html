<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.22.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
<link rel="icon" href="/assets/main/me.png">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Storage for Containers using Gluster – Part II - Keith Tenzer’s Blog</title>
<meta name="description" content="Overview This article is a collaboration between Daniel Messer (Technical Marketing Manager Storage @RedHat) and Keith Tenzer (Solutions Architect @RedHat).  Storage for Containers Overview – Part I Storage for Containers using Gluster – Part II Storage for Containers using Container Native Storage – Part III Storage for Containers using Ceph – Part IV Storage for Containers using NetApp ONTAP NAS – Part V Storage for Containers using NetApp SolidFire – Part VI  Gluster as Container-Ready Storage (CRS) In this article we will look at one of the first options of storage for containers and how to deploy it. Support for GlusterFS has been in Kubernetes and OpenShift for some time. GlusterFS is a good fit because it is available across all deployment options: bare-metal, virtual, on-premise and public cloud. The recent addition of GlusterFS running in a container will be discussed later in this series. GlusterFS is a distributed filesystem at heart with a native protocol (GlusterFS) and various other protocols (NFS, SMB,...). For integration with OpenShift, nodes will use the native protocol via FUSE to mount GlusterFS volumes on the node itself and then have them bind-mount&#39;ed into the target containers. OpenShift/Kubernetes has a native provisioner that implements requesting, releasing and (un-)mounting GlusterFS volumes.  CRS Overview On the storage side there is an additional component managing the cluster at the request of OpenShift/Kubernetes called &quot;heketi&quot;. This is effectively a REST API for GlusterFS and also ships a CLI version. In the following steps we will deploy heketi among 3 GlusterFS nodes, use it to deploy a GlusterFS storage pool, connect it to OpenShift and use it to provision storage for containers via PersistentVolumeClaims. In total we will deploy 4 Virtual Machines. One for OpenShift (lab setup) and three for GlusterFS. Note: your system should have at least a quad-core CPU, 16GB RAM and 20 GB of free disk space. Deploying OpenShift At first you will need an OpenShift deployment. An All-in-One deployment in a VM is sufficient, instructions can be found in the &quot;OpenShift Enterprise 3.4 all-in-one Lab Environment&quot; article. Make sure your OpenShift VM can resolve external domain names. Edit /etc/dnsmasq.conf and add the following line to use Google DNS: server=8.8.8.8 Restart it: # systemctl restart dnsmasq # ping -c1 google.com Deploying Gluster For GlusterFS at least 3 VMs are required with the following specs:  RHEL 7.3 2 CPUs 2 GB RAM 30 GB disk for OS 10 GB disk for GlusterFS bricks  It is necessary to provide local name resolution for the 3 VMs and the OpenShift VM via a common /etc/hosts file. For example (feel free to adjust the domain and host names to your environment): # cat /etc/hosts 127.0.0.1      localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1            localhost localhost.localdomain localhost6 localhost6.localdomain6 172.16.99.144  ocp-master.lab ocp-master 172.16.128.7   crs-node1.lab crs-node1 172.16.128.8   crs-node2.lab crs-node2 172.16.128.9   crs-node3.lab crs-node3 Execute the following steps on all 3 GlusterFS VMs: # subscription-manager repos --disable=&quot;*&quot; # subscription-manager repos --enable=rhel-7-server-rpms If you have a GlusterFS subscription you can use it and enable the rh-gluster-3-for-rhel-7-server-rpms repository. If you don&#39;t you can use the unsupported GlusterFS community repositories for testing via EPEL: # yum -y install http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm # rpm --import http://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-7 Create a file named glusterfs-3.10.repo in /etc/yum.repos.d/ [glusterfs-3.10] name=glusterfs-3.10 description=&quot;GlusterFS 3.10 Community Version&quot; baseurl=https://buildlogs.centos.org/centos/7/storage/x86_64/gluster-3.10/ gpgcheck=0 enabled=1 Verify the repository is active: # yum repolist You should now be able to install GlusterFS # yum -y install glusterfs-server A couple of basic TCP ports need to be opened for GlusterFS peers to communicate and provide storage to OpenShift: # firewall-cmd --add-port=24007-24008/tcp --add-port=49152-49664/tcp --add-port=2222/tcp # firewall-cmd --runtime-to-permanent Now we are ready to start the GlusterFS daemon: # systemctl enable glusterd # systemctl start glusterd That&#39;s it. GlusterFS is up and running. The rest of the configuration will be done via heketi. Install heketi on one of the GlusterFS VMs: [root@crs-node1 ~]# yum -y install heketi heketi-client   Update for for EPEL users If you don&#39;t have a Red Hat Gluster Storage subscription you will get heketi from EPEL. At the time of writing this is version 3.0.0-1.el7 from October 2016 which is not working with OpenShift 3.4. You will need to update to a more current version: [root@crs-node1 ~]# yum -y install wget [root@crs-node1 ~]# wget https://github.com/heketi/heketi/releases/download/v4.0.0/heketi-v4.0.0.linux.amd64.tar.gz [root@crs-node1 ~]# tar -xzf heketi-v4.0.0.linux.amd64.tar.gz [root@crs-node1 ~]# systemctl stop heketi [root@crs-node1 ~]# cp heketi/heketi* /usr/bin/ [root@crs-node1 ~]# chown heketi:heketi /usr/bin/heketi* Create a file in /etc/systemd/system/heketi.service for the updated syntax of the v4 heketi binary: [Unit] Description=Heketi Server">


  <meta name="author" content="Keith Tenzer">
  
  <meta property="article:author" content="Keith Tenzer">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Keith Tenzer's Blog">
<meta property="og:title" content="Storage for Containers using Gluster – Part II">
<meta property="og:url" content="http://localhost:4000/openshift/storage-for-containers-using-gluster-part-ii/">


  <meta property="og:description" content="Overview This article is a collaboration between Daniel Messer (Technical Marketing Manager Storage @RedHat) and Keith Tenzer (Solutions Architect @RedHat).  Storage for Containers Overview – Part I Storage for Containers using Gluster – Part II Storage for Containers using Container Native Storage – Part III Storage for Containers using Ceph – Part IV Storage for Containers using NetApp ONTAP NAS – Part V Storage for Containers using NetApp SolidFire – Part VI  Gluster as Container-Ready Storage (CRS) In this article we will look at one of the first options of storage for containers and how to deploy it. Support for GlusterFS has been in Kubernetes and OpenShift for some time. GlusterFS is a good fit because it is available across all deployment options: bare-metal, virtual, on-premise and public cloud. The recent addition of GlusterFS running in a container will be discussed later in this series. GlusterFS is a distributed filesystem at heart with a native protocol (GlusterFS) and various other protocols (NFS, SMB,...). For integration with OpenShift, nodes will use the native protocol via FUSE to mount GlusterFS volumes on the node itself and then have them bind-mount&#39;ed into the target containers. OpenShift/Kubernetes has a native provisioner that implements requesting, releasing and (un-)mounting GlusterFS volumes.  CRS Overview On the storage side there is an additional component managing the cluster at the request of OpenShift/Kubernetes called &quot;heketi&quot;. This is effectively a REST API for GlusterFS and also ships a CLI version. In the following steps we will deploy heketi among 3 GlusterFS nodes, use it to deploy a GlusterFS storage pool, connect it to OpenShift and use it to provision storage for containers via PersistentVolumeClaims. In total we will deploy 4 Virtual Machines. One for OpenShift (lab setup) and three for GlusterFS. Note: your system should have at least a quad-core CPU, 16GB RAM and 20 GB of free disk space. Deploying OpenShift At first you will need an OpenShift deployment. An All-in-One deployment in a VM is sufficient, instructions can be found in the &quot;OpenShift Enterprise 3.4 all-in-one Lab Environment&quot; article. Make sure your OpenShift VM can resolve external domain names. Edit /etc/dnsmasq.conf and add the following line to use Google DNS: server=8.8.8.8 Restart it: # systemctl restart dnsmasq # ping -c1 google.com Deploying Gluster For GlusterFS at least 3 VMs are required with the following specs:  RHEL 7.3 2 CPUs 2 GB RAM 30 GB disk for OS 10 GB disk for GlusterFS bricks  It is necessary to provide local name resolution for the 3 VMs and the OpenShift VM via a common /etc/hosts file. For example (feel free to adjust the domain and host names to your environment): # cat /etc/hosts 127.0.0.1      localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1            localhost localhost.localdomain localhost6 localhost6.localdomain6 172.16.99.144  ocp-master.lab ocp-master 172.16.128.7   crs-node1.lab crs-node1 172.16.128.8   crs-node2.lab crs-node2 172.16.128.9   crs-node3.lab crs-node3 Execute the following steps on all 3 GlusterFS VMs: # subscription-manager repos --disable=&quot;*&quot; # subscription-manager repos --enable=rhel-7-server-rpms If you have a GlusterFS subscription you can use it and enable the rh-gluster-3-for-rhel-7-server-rpms repository. If you don&#39;t you can use the unsupported GlusterFS community repositories for testing via EPEL: # yum -y install http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm # rpm --import http://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-7 Create a file named glusterfs-3.10.repo in /etc/yum.repos.d/ [glusterfs-3.10] name=glusterfs-3.10 description=&quot;GlusterFS 3.10 Community Version&quot; baseurl=https://buildlogs.centos.org/centos/7/storage/x86_64/gluster-3.10/ gpgcheck=0 enabled=1 Verify the repository is active: # yum repolist You should now be able to install GlusterFS # yum -y install glusterfs-server A couple of basic TCP ports need to be opened for GlusterFS peers to communicate and provide storage to OpenShift: # firewall-cmd --add-port=24007-24008/tcp --add-port=49152-49664/tcp --add-port=2222/tcp # firewall-cmd --runtime-to-permanent Now we are ready to start the GlusterFS daemon: # systemctl enable glusterd # systemctl start glusterd That&#39;s it. GlusterFS is up and running. The rest of the configuration will be done via heketi. Install heketi on one of the GlusterFS VMs: [root@crs-node1 ~]# yum -y install heketi heketi-client   Update for for EPEL users If you don&#39;t have a Red Hat Gluster Storage subscription you will get heketi from EPEL. At the time of writing this is version 3.0.0-1.el7 from October 2016 which is not working with OpenShift 3.4. You will need to update to a more current version: [root@crs-node1 ~]# yum -y install wget [root@crs-node1 ~]# wget https://github.com/heketi/heketi/releases/download/v4.0.0/heketi-v4.0.0.linux.amd64.tar.gz [root@crs-node1 ~]# tar -xzf heketi-v4.0.0.linux.amd64.tar.gz [root@crs-node1 ~]# systemctl stop heketi [root@crs-node1 ~]# cp heketi/heketi* /usr/bin/ [root@crs-node1 ~]# chown heketi:heketi /usr/bin/heketi* Create a file in /etc/systemd/system/heketi.service for the updated syntax of the v4 heketi binary: [Unit] Description=Heketi Server">





  <meta name="twitter:site" content="@keithtenzer">
  <meta name="twitter:title" content="Storage for Containers using Gluster – Part II">
  <meta name="twitter:description" content="Overview This article is a collaboration between Daniel Messer (Technical Marketing Manager Storage @RedHat) and Keith Tenzer (Solutions Architect @RedHat).  Storage for Containers Overview – Part I Storage for Containers using Gluster – Part II Storage for Containers using Container Native Storage – Part III Storage for Containers using Ceph – Part IV Storage for Containers using NetApp ONTAP NAS – Part V Storage for Containers using NetApp SolidFire – Part VI  Gluster as Container-Ready Storage (CRS) In this article we will look at one of the first options of storage for containers and how to deploy it. Support for GlusterFS has been in Kubernetes and OpenShift for some time. GlusterFS is a good fit because it is available across all deployment options: bare-metal, virtual, on-premise and public cloud. The recent addition of GlusterFS running in a container will be discussed later in this series. GlusterFS is a distributed filesystem at heart with a native protocol (GlusterFS) and various other protocols (NFS, SMB,...). For integration with OpenShift, nodes will use the native protocol via FUSE to mount GlusterFS volumes on the node itself and then have them bind-mount&#39;ed into the target containers. OpenShift/Kubernetes has a native provisioner that implements requesting, releasing and (un-)mounting GlusterFS volumes.  CRS Overview On the storage side there is an additional component managing the cluster at the request of OpenShift/Kubernetes called &quot;heketi&quot;. This is effectively a REST API for GlusterFS and also ships a CLI version. In the following steps we will deploy heketi among 3 GlusterFS nodes, use it to deploy a GlusterFS storage pool, connect it to OpenShift and use it to provision storage for containers via PersistentVolumeClaims. In total we will deploy 4 Virtual Machines. One for OpenShift (lab setup) and three for GlusterFS. Note: your system should have at least a quad-core CPU, 16GB RAM and 20 GB of free disk space. Deploying OpenShift At first you will need an OpenShift deployment. An All-in-One deployment in a VM is sufficient, instructions can be found in the &quot;OpenShift Enterprise 3.4 all-in-one Lab Environment&quot; article. Make sure your OpenShift VM can resolve external domain names. Edit /etc/dnsmasq.conf and add the following line to use Google DNS: server=8.8.8.8 Restart it: # systemctl restart dnsmasq # ping -c1 google.com Deploying Gluster For GlusterFS at least 3 VMs are required with the following specs:  RHEL 7.3 2 CPUs 2 GB RAM 30 GB disk for OS 10 GB disk for GlusterFS bricks  It is necessary to provide local name resolution for the 3 VMs and the OpenShift VM via a common /etc/hosts file. For example (feel free to adjust the domain and host names to your environment): # cat /etc/hosts 127.0.0.1      localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1            localhost localhost.localdomain localhost6 localhost6.localdomain6 172.16.99.144  ocp-master.lab ocp-master 172.16.128.7   crs-node1.lab crs-node1 172.16.128.8   crs-node2.lab crs-node2 172.16.128.9   crs-node3.lab crs-node3 Execute the following steps on all 3 GlusterFS VMs: # subscription-manager repos --disable=&quot;*&quot; # subscription-manager repos --enable=rhel-7-server-rpms If you have a GlusterFS subscription you can use it and enable the rh-gluster-3-for-rhel-7-server-rpms repository. If you don&#39;t you can use the unsupported GlusterFS community repositories for testing via EPEL: # yum -y install http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm # rpm --import http://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-7 Create a file named glusterfs-3.10.repo in /etc/yum.repos.d/ [glusterfs-3.10] name=glusterfs-3.10 description=&quot;GlusterFS 3.10 Community Version&quot; baseurl=https://buildlogs.centos.org/centos/7/storage/x86_64/gluster-3.10/ gpgcheck=0 enabled=1 Verify the repository is active: # yum repolist You should now be able to install GlusterFS # yum -y install glusterfs-server A couple of basic TCP ports need to be opened for GlusterFS peers to communicate and provide storage to OpenShift: # firewall-cmd --add-port=24007-24008/tcp --add-port=49152-49664/tcp --add-port=2222/tcp # firewall-cmd --runtime-to-permanent Now we are ready to start the GlusterFS daemon: # systemctl enable glusterd # systemctl start glusterd That&#39;s it. GlusterFS is up and running. The rest of the configuration will be done via heketi. Install heketi on one of the GlusterFS VMs: [root@crs-node1 ~]# yum -y install heketi heketi-client   Update for for EPEL users If you don&#39;t have a Red Hat Gluster Storage subscription you will get heketi from EPEL. At the time of writing this is version 3.0.0-1.el7 from October 2016 which is not working with OpenShift 3.4. You will need to update to a more current version: [root@crs-node1 ~]# yum -y install wget [root@crs-node1 ~]# wget https://github.com/heketi/heketi/releases/download/v4.0.0/heketi-v4.0.0.linux.amd64.tar.gz [root@crs-node1 ~]# tar -xzf heketi-v4.0.0.linux.amd64.tar.gz [root@crs-node1 ~]# systemctl stop heketi [root@crs-node1 ~]# cp heketi/heketi* /usr/bin/ [root@crs-node1 ~]# chown heketi:heketi /usr/bin/heketi* Create a file in /etc/systemd/system/heketi.service for the updated syntax of the v4 heketi binary: [Unit] Description=Heketi Server">
  <meta name="twitter:url" content="http://localhost:4000/openshift/storage-for-containers-using-gluster-part-ii/">

  
    <meta name="twitter:card" content="summary">
    
  

  



  <meta property="article:published_time" content="2017-03-24T00:00:00-07:00">





  

  


<link rel="canonical" href="http://localhost:4000/openshift/storage-for-containers-using-gluster-part-ii/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Keith Tenzer",
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Keith Tenzer's Blog Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single wide">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Keith Tenzer's Blog
          <span class="site-subtitle">Cloud Computing and Code</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about/index.html">About</a>
            </li><li class="masthead__menu-item">
              <a href="/conferences-and-events/index.html">Conferences and Events</a>
            </li><li class="masthead__menu-item">
              <a href="/videos/index.html">Videos</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16">
            <path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path>
          </svg>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      


  
    



<nav class="breadcrumbs">
  <ol itemscope itemtype="https://schema.org/BreadcrumbList">
    
    
    
      
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="http://localhost:4000/" itemprop="item"><span itemprop="name">Home</span></a>
          <meta itemprop="position" content="1" />
        </li>
        <span class="sep">/</span>
      
      
        
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/categories/#openshift" itemprop="item"><span itemprop="name">Openshift</span></a>
          <meta itemprop="position" content="2" />
        </li>
        <span class="sep">/</span>
      
    
      
      
        <li class="current">Storage for Containers using Gluster – Part II</li>
      
    
  </ol>
</nav>

  


<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      
        <img src="/assets/main/me.png" alt="Keith Tenzer" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Keith Tenzer</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>Principal Solutions Architect at Red Hat</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Los Angeles, CA</span>
        </li>
      

      
        
          
        
          
        
          
            <li><a href="https://twitter.com/keithtenzer" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i><span class="label">Twitter</span></a></li>
          
        
          
        
          
            <li><a href="https://github.com/ktenzer" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Storage for Containers using Gluster – Part II">
    <meta itemprop="description" content="OverviewThis article is a collaboration between Daniel Messer (Technical Marketing Manager Storage @RedHat) and Keith Tenzer (Solutions Architect @RedHat).Storage for Containers Overview – Part IStorage for Containers using Gluster – Part IIStorage for Containers using Container Native Storage – Part IIIStorage for Containers using Ceph – Part IVStorage for Containers using NetApp ONTAP NAS – Part VStorage for Containers using NetApp SolidFire – Part VIGluster as Container-Ready Storage (CRS)In this article we will look at one of the first options of storage for containers and how to deploy it. Support for GlusterFS has been in Kubernetes and OpenShift for some time. GlusterFS is a good fit because it is available across all deployment options: bare-metal, virtual, on-premise and public cloud. The recent addition of GlusterFS running in a container will be discussed later in this series.GlusterFS is a distributed filesystem at heart with a native protocol (GlusterFS) and various other protocols (NFS, SMB,...). For integration with OpenShift, nodes will use the native protocol via FUSE to mount GlusterFS volumes on the node itself and then have them bind-mount&#39;ed into the target containers. OpenShift/Kubernetes has a native provisioner that implements requesting, releasing and (un-)mounting GlusterFS volumes.CRS OverviewOn the storage side there is an additional component managing the cluster at the request of OpenShift/Kubernetes called &quot;heketi&quot;. This is effectively a REST API for GlusterFS and also ships a CLI version. In the following steps we will deploy heketi among 3 GlusterFS nodes, use it to deploy a GlusterFS storage pool, connect it to OpenShift and use it to provision storage for containers via PersistentVolumeClaims. In total we will deploy 4 Virtual Machines. One for OpenShift (lab setup) and three for GlusterFS.Note: your system should have at least a quad-core CPU, 16GB RAM and 20 GB of free disk space.Deploying OpenShiftAt first you will need an OpenShift deployment. An All-in-One deployment in a VM is sufficient, instructions can be found in the &quot;OpenShift Enterprise 3.4 all-in-one Lab Environment&quot; article.Make sure your OpenShift VM can resolve external domain names. Edit /etc/dnsmasq.conf and add the following line to use Google DNS:server=8.8.8.8Restart it:# systemctl restart dnsmasq# ping -c1 google.comDeploying GlusterFor GlusterFS at least 3 VMs are required with the following specs:RHEL 7.32 CPUs2 GB RAM30 GB disk for OS10 GB disk for GlusterFS bricksIt is necessary to provide local name resolution for the 3 VMs and the OpenShift VM via a common /etc/hosts file.For example (feel free to adjust the domain and host names to your environment):# cat /etc/hosts127.0.0.1      localhost localhost.localdomain localhost4 localhost4.localdomain4::1            localhost localhost.localdomain localhost6 localhost6.localdomain6172.16.99.144  ocp-master.lab ocp-master172.16.128.7   crs-node1.lab crs-node1172.16.128.8   crs-node2.lab crs-node2172.16.128.9   crs-node3.lab crs-node3Execute the following steps on all 3 GlusterFS VMs:# subscription-manager repos --disable=&quot;*&quot;# subscription-manager repos --enable=rhel-7-server-rpmsIf you have a GlusterFS subscription you can use it and enable the rh-gluster-3-for-rhel-7-server-rpms repository.If you don&#39;t you can use the unsupported GlusterFS community repositories for testing via EPEL:# yum -y install http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm# rpm --import http://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-7Create a file named glusterfs-3.10.repo in /etc/yum.repos.d/[glusterfs-3.10]name=glusterfs-3.10description=&quot;GlusterFS 3.10 Community Version&quot;baseurl=https://buildlogs.centos.org/centos/7/storage/x86_64/gluster-3.10/gpgcheck=0enabled=1Verify the repository is active:# yum repolistYou should now be able to install GlusterFS# yum -y install glusterfs-serverA couple of basic TCP ports need to be opened for GlusterFS peers to communicate and provide storage to OpenShift:# firewall-cmd --add-port=24007-24008/tcp --add-port=49152-49664/tcp --add-port=2222/tcp# firewall-cmd --runtime-to-permanentNow we are ready to start the GlusterFS daemon:# systemctl enable glusterd# systemctl start glusterdThat&#39;s it. GlusterFS is up and running. The rest of the configuration will be done via heketi.Install heketi on one of the GlusterFS VMs:[root@crs-node1 ~]# yum -y install heketi heketi-clientUpdate for for EPEL usersIf you don&#39;t have a Red Hat Gluster Storage subscription you will get heketi from EPEL. At the time of writing this is version 3.0.0-1.el7 from October 2016 which is not working with OpenShift 3.4. You will need to update to a more current version:[root@crs-node1 ~]# yum -y install wget[root@crs-node1 ~]# wget https://github.com/heketi/heketi/releases/download/v4.0.0/heketi-v4.0.0.linux.amd64.tar.gz[root@crs-node1 ~]# tar -xzf heketi-v4.0.0.linux.amd64.tar.gz[root@crs-node1 ~]# systemctl stop heketi[root@crs-node1 ~]# cp heketi/heketi* /usr/bin/[root@crs-node1 ~]# chown heketi:heketi /usr/bin/heketi*Create a file in /etc/systemd/system/heketi.service for the updated syntax of the v4 heketi binary:[Unit]Description=Heketi Server">
    <meta itemprop="datePublished" content="2017-03-24T00:00:00-07:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Storage for Containers using Gluster – Part II
</h1>
          

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2017-03-24T00:00:00-07:00">March 24, 2017</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          29 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
        <h3><img class="alignnone size-full wp-image-8150" src="/assets/2017/03/storage_article_008.jpg" alt="Shipping containers" width="1200" height="400" /></h3>
<h3>Overview</h3>
<p>This article is a collaboration between Daniel Messer (Technical Marketing Manager Storage @RedHat) and Keith Tenzer (Solutions Architect @RedHat).</p>
<ul>
<li><a href="https://keithtenzer.com/2017/03/07/storage-for-containers-overview-part-i/">Storage for Containers Overview – Part I</a></li>
<li><a href="https://keithtenzer.com/2017/03/24/storage-for-containers-using-gluster-part-ii/">Storage for Containers using Gluster – Part II</a></li>
<li><a href="https://keithtenzer.com/2017/03/29/storage-for-containers-using-container-native-storage-part-iii/">Storage for Containers using Container Native Storage – Part III</a></li>
<li><a href="https://keithtenzer.com/2017/04/07/storage-for-containers-using-ceph-rbd-part-iv/">Storage for Containers using Ceph – Part IV</a></li>
<li><a href="https://keithtenzer.com/2017/04/05/storage-for-containers-using-netapp-ontap-nas-part-v/">Storage for Containers using NetApp ONTAP NAS – Part V</a></li>
<li><a href="https://keithtenzer.com/2017/04/05/storage-for-containers-using-netapp-solidfire-part-vi/">Storage for Containers using NetApp SolidFire – Part VI</a></li>
</ul>
<h3>Gluster as Container-Ready Storage (CRS)</h3>
<p>In this article we will look at one of the first options of storage for containers and how to deploy it. Support for GlusterFS has been in Kubernetes and OpenShift for some time. GlusterFS is a good fit because it is available across all deployment options: bare-metal, virtual, on-premise and public cloud. The recent addition of GlusterFS running in a container will be discussed later in this series.</p>
<p>GlusterFS is a distributed filesystem at heart with a native protocol (GlusterFS) and various other protocols (NFS, SMB,...). For integration with OpenShift, nodes will use the native protocol via FUSE to mount GlusterFS volumes on the node itself and then have them bind-mount'ed into the target containers. OpenShift/Kubernetes has a native provisioner that implements requesting, releasing and (un-)mounting GlusterFS volumes.</p>
<p><!--more--></p>
<h3>CRS Overview</h3>
<p>On the storage side there is an additional component managing the cluster at the request of OpenShift/Kubernetes called "heketi". This is effectively a REST API for GlusterFS and also ships a CLI version. In the following steps we will deploy heketi among 3 GlusterFS nodes, use it to deploy a GlusterFS storage pool, connect it to OpenShift and use it to provision storage for containers via PersistentVolumeClaims. In total we will deploy 4 Virtual Machines. One for OpenShift (lab setup) and three for GlusterFS.</p>
<p>Note: your system should have at least a quad-core CPU, 16GB RAM and 20 GB of free disk space.</p>
<h3>Deploying OpenShift</h3>
<p>At first you will need an OpenShift deployment. An All-in-One deployment in a VM is sufficient, instructions can be found in <a href="http://keithtenzer.com/2017/03/13/openshift-enterprise-3-4-all-in-one-lab-environment/">the "OpenShift Enterprise 3.4 all-in-one Lab Environment" article.</a></p>
<p>Make sure your OpenShift VM can resolve external domain names. Edit /etc/dnsmasq.conf and add the following line to use Google DNS:</p>
<pre>server=8.8.8.8</pre>
<p>Restart it:</p>
<pre># systemctl restart dnsmasq
# ping -c1 google.com</pre>
<h3>Deploying Gluster</h3>
<p>For GlusterFS at least 3 VMs are required with the following specs:</p>
<ul>
<li>RHEL 7.3</li>
<li>2 CPUs</li>
<li>2 GB RAM</li>
<li>30 GB disk for OS</li>
<li>10 GB disk for GlusterFS bricks</li>
</ul>
<p>It is necessary to provide local name resolution for the 3 VMs and the OpenShift VM via a common /etc/hosts file.</p>
<p>For example (feel free to adjust the domain and host names to your environment):</p>
<pre># cat /etc/hosts
127.0.0.1      localhost localhost.localdomain localhost4 localhost4.localdomain4
::1            localhost localhost.localdomain localhost6 localhost6.localdomain6
172.16.99.144  ocp-master.lab ocp-master
172.16.128.7   crs-node1.lab crs-node1
172.16.128.8   crs-node2.lab crs-node2
172.16.128.9   crs-node3.lab crs-node3</pre>
<p><span style="text-decoration:underline;"><strong>Execute the following steps on all 3 GlusterFS VMs:</strong></span></p>
<pre># subscription-manager repos --disable="*"
# subscription-manager repos --enable=rhel-7-server-rpms</pre>
<p>If you have a GlusterFS subscription you can use it and enable the rh-gluster-3-for-rhel-7-server-rpms repository.</p>
<p>If you don't you can use the unsupported GlusterFS community repositories for testing via EPEL:</p>
<pre># yum -y install http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
# rpm --import http://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-7</pre>
<p>Create a file named glusterfs-3.10.repo in /etc/yum.repos.d/</p>
<pre>[glusterfs-3.10]
name=glusterfs-3.10
description="GlusterFS 3.10 Community Version"
baseurl=https://buildlogs.centos.org/centos/7/storage/x86_64/gluster-3.10/
gpgcheck=0
enabled=1</pre>
<p>Verify the repository is active:</p>
<pre># yum repolist</pre>
<p>You should now be able to install GlusterFS</p>
<pre># yum -y install glusterfs-server</pre>
<p>A couple of basic TCP ports need to be opened for GlusterFS peers to communicate and provide storage to OpenShift:</p>
<pre># firewall-cmd --add-port=24007-24008/tcp --add-port=49152-49664/tcp --add-port=2222/tcp
# firewall-cmd --runtime-to-permanent</pre>
<p>Now we are ready to start the GlusterFS daemon:</p>
<pre># systemctl enable glusterd
# systemctl start glusterd</pre>
<p>That's it. GlusterFS is up and running. The rest of the configuration will be done via heketi.</p>
<p><span style="text-decoration:underline;"><strong>Install heketi on one of the GlusterFS VMs:</strong></span></p>
<pre>[root@crs-node1 ~]# yum -y install heketi heketi-client</pre>
<hr />
<h3><span style="color:#ff0000;">Update for for EPEL users</span></h3>
<p>If you don't have a Red Hat Gluster Storage subscription you will get heketi from EPEL. At the time of writing this is version 3.0.0-1.el7 from October 2016 which is not working with OpenShift 3.4. You will need to update to a more current version:</p>
<pre>[root@crs-node1 ~]# yum -y install wget
[root@crs-node1 ~]# wget https://github.com/heketi/heketi/releases/download/v4.0.0/heketi-v4.0.0.linux.amd64.tar.gz
[root@crs-node1 ~]# tar -xzf heketi-v4.0.0.linux.amd64.tar.gz
[root@crs-node1 ~]# systemctl stop heketi
[root@crs-node1 ~]# cp heketi/heketi* /usr/bin/
[root@crs-node1 ~]# chown heketi:heketi /usr/bin/heketi*</pre>
<p>Create a file in /etc/systemd/system/heketi.service for the updated syntax of the v4 heketi binary:</p>
<pre>[Unit]
Description=Heketi Server

[Service]
Type=simple
WorkingDirectory=/var/lib/heketi
EnvironmentFile=-/etc/heketi/heketi.json
User=heketi
ExecStart=/usr/bin/heketi --config=/etc/heketi/heketi.json
Restart=on-failure
StandardOutput=syslog
StandardError=syslog

[Install]
WantedBy=multi-user.target</pre>
<pre>[root@crs-node1 ~]# systemctl daemon-reload
[root@crs-node1 ~]# systemctl start heketi</pre>
<hr />
<p>Heketi will use SSH to configure GlusterFS on all nodes. Create an SSH key pair and copy the public key to all 3 nodes (including the first node you are logged on):</p>
<pre>[root@crs-node1 ~]# ssh-keygen -f /etc/heketi/heketi_key -t rsa -N ''
[root@crs-node1 ~]# ssh-copy-id -i /etc/heketi/heketi_key.pub root@crs-node1.lab
[root@crs-node1 ~]# ssh-copy-id -i /etc/heketi/heketi_key.pub root@crs-node2.lab
[root@crs-node1 ~]# ssh-copy-id -i /etc/heketi/heketi_key.pub root@crs-node3.lab
[root@crs-node1 ~]# chown heketi:heketi /etc/heketi/heketi_key*</pre>
<p>The only thing left is to configure heketi to use SSH. Edit the /etc/heketi/heketi.json to look like the below (changed parts highlighted are <span style="text-decoration:underline;">underlined</span>):</p>
<pre>{
   "_port_comment":"Heketi Server Port Number",
   "port":"8080",
   "_use_auth":"Enable JWT authorization. Please enable for deployment",
   "use_auth":false,
   "_jwt":"Private keys for access",
   "jwt":{
      "_admin":"Admin has access to all APIs",
      "admin":{
         "key":"My Secret"
      },
      "_user":"User only has access to /volumes endpoint",
      "user":{
         "key":"My Secret"
      }
   },
   "_glusterfs_comment":"GlusterFS Configuration",
   "glusterfs":{
      "_executor_comment":[
         "Execute plugin. Possible choices: mock, ssh",
         "mock: This setting is used for testing and development.",
         " It will not send commands to any node.",
         "ssh: This setting will notify Heketi to ssh to the nodes.",
         " It will need the values in sshexec to be configured.",
         "kubernetes: Communicate with GlusterFS containers over",
         " Kubernetes exec api."
      ],
      "executor":"<span style="text-decoration:underline;">ssh</span>",
      "_sshexec_comment":"SSH username and private key file information",
      "sshexec":{
         "keyfile":"<span style="text-decoration:underline;">/etc/heketi/heketi_key</span>",
         "user":"<span style="text-decoration:underline;">root</span>",
         "port":"<span style="text-decoration:underline;">22</span>",
         "fstab":"<span style="text-decoration:underline;">/etc/fstab</span>"
      },
      "_kubeexec_comment":"Kubernetes configuration",
      "kubeexec":{
         "host":"https://kubernetes.host:8443",
         "cert":"/path/to/crt.file",
         "insecure":false,
         "user":"kubernetes username",
         "password":"password for kubernetes user",
         "namespace":"OpenShift project or Kubernetes namespace",
         "fstab":"Optional: Specify fstab file on node. Default is /etc/fstab"
      },
      "_db_comment":"Database file name",
      "db":"/var/lib/heketi/heketi.db",
      "_loglevel_comment":[
         "Set log level. Choices are:",
         " none, critical, error, warning, info, debug",
         "Default is warning"
      ],
      "loglevel":"debug"
   }
}</pre>
<p>Finished. heketi will listen on port 8080, let's make sure the firewall allows that:</p>
<pre># firewall-cmd --add-port=8080/tcp
# firewall-cmd --runtime-to-permanent</pre>
<p>Now restart heketi:</p>
<pre># systemctl enable heketi
# systemctl restart heketi</pre>
<p>Test if it's running:</p>
<pre># curl http://crs-node1.lab:8080/hello
Hello from Heketi</pre>
<p>Good. Time to put heketi to work. We will use it to configure our GlusterFS storage pool. The software is already running on all our VMs but it's unconfigured. To change that to a functional storage system we will describe our desired GlusterFS storage pool in a topology file, like below:</p>
<pre># vi topology.json
{
  "clusters": [
    {
      "nodes": [
        {
          "node": {
            "hostnames": {
              "manage": [
                "crs-node1.lab"
              ],
              "storage": [
                "172.16.128.7"
              ]
            },
            "zone": 1
          },
          "devices": [
            "/dev/sdb"
          ]
        },
        {
          "node": {
            "hostnames": {
              "manage": [
                "crs-node2.lab"
              ],
              "storage": [
                "172.16.128.8"
              ]
            },
            "zone": 1
          },
          "devices": [
            "/dev/sdb"
          ]
        },
        {
          "node": {
            "hostnames": {
              "manage": [
                "crs-node3.lab"
              ],
              "storage": [
                "172.16.128.9"
              ]
            },
            "zone": 1
          },
          "devices": [
            "/dev/sdb"
          ]
        }
      ]
    }
  ]
}</pre>
<p>Despite the formatting the file is relatively simple. It basically tells heketi to create a 3 node cluster with each node being known by a FQDN, an IP address and with at least one spare block device which will be used as a GlusterFS brick.</p>
<p>Now feed this file to heketi:</p>
<pre># export HEKETI_CLI_SERVER=http://crs-node1.lab:8080
# heketi-cli topology load --json=topology.json
Creating cluster ... ID: 78cdb57aa362f5284bc95b2549bc7e7d
 Creating node crs-node1.lab ... ID: ffd7671c0083d88aeda9fd1cb40b339b
 Adding device /dev/sdb ... OK
 Creating node crs-node2.lab ... ID: 8220975c0a4479792e684584153050a9
 Adding device /dev/sdb ... OK
 Creating node crs-node3.lab ... ID: b94f14c4dbd8850f6ac589ac3b39cc8e
 Adding device /dev/sdb ... OK</pre>
<p>Now heketi has configured a 3 node GlusterFS storage pool. Easy! You can see that the 3 VMs have successfully formed what's called a Trusted Storage Pool in GlusterFS</p>
<pre>[root@crs-node1 ~]# gluster peer status
Number of Peers: 2

Hostname: crs-node2.lab
Uuid: 93b34946-9571-46a8-983c-c9f128557c0e
State: Peer in Cluster (Connected)
Other names:
crs-node2.lab

Hostname: 172.16.128.9
Uuid: e3c1f9b0-be97-42e5-beda-f70fc05f47ea
State: Peer in Cluster (Connected)</pre>
<p>Now back to OpenShift!</p>
<h3>Integrating Gluster with OpenShift</h3>
<p>For integration in OpenShift two things are needed: a dynamic Kubernetes Storage Provisioner and a StorageClass. The provisioner ships out of the box with OpenShift. It does the actual heavy lifting of attaching storage to containers. The StorageClass is an entity that users in OpenShift can make PersistentVolumeClaims against, which will in turn trigger a provisioner to implement the actual provisioning and represent the result as Kubernetes PersistentVolume (PV).</p>
<p>Like everything else in OpenShift the StorageClass is simply defined as a YAML file:</p>
<pre># cat crs-storageclass.yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1beta1
metadata:
 name: container-ready-storage
 annotations:
 storageclass.beta.kubernetes.io/is-default-class: "true"
provisioner: kubernetes.io/glusterfs
parameters:
 resturl: "http://crs-node1.lab:8080"
 restauthenabled: "false"</pre>
<p>Our provisioner is kubernetes.io/glusterfs and we make it point to our heketi instance. We name the class "container-ready-storage" and at the same time make it the default StorageClass for all PersistentVolumeClaims that do not explicitly specify one.<br />
Create the StorageClass for your GlusterFS pool:</p>
<pre># oc create -f crs-storageclass.yaml</pre>
<h3>Using Gluster with OpenShift</h3>
<p>Let's look at how we would use GlusterFS in OpenShift. First create a playground project on the OpenShift VM:</p>
<pre># oc new-project crs-storage --display-name="Container-Ready Storage"</pre>
<p>To request storage in Kubernetes/OpenShift a PersistentVolumeClaim (PVC) is issued. It's a simple object describing at the minimum how much capacity we need and in which access mode it should be supplied (non-shared, shared, read-only). It's usually part of an application template but let's just create a standalone PVC:</p>
<pre># cat crs-claim.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
 name: my-crs-storage
 namespace: crs-storage
spec:
 accessModes:
 - ReadWriteOnce
 resources:
 requests:
 storage: 1Gi</pre>
<p>Issue the claim:</p>
<pre># oc create -f crs-claim.yaml</pre>
<p>Watch the PVC being processed and fulfilled with a dynamically created volume in OpenShift:</p>
<pre># oc get pvc
NAME             STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
my-crs-storage   Bound     pvc-41ad5adb-107c-11e7-afae-000c2949cce7   1Gi        RWO           58s</pre>
<p>Great! You have now storage capacity available for use in OpenShift without any interaction with the storage system directly. Let's look at the volume that got created:</p>
<pre># oc get pv/pvc-41ad5adb-107c-11e7-afae-000c2949cce7
Name:		pvc-41ad5adb-107c-11e7-afae-000c2949cce7
Labels:		
StorageClass:	container-ready-storage
Status:		Bound
Claim:		crs-storage/my-crs-storage
Reclaim Policy:	Delete
Access Modes:	RWO
Capacity:	1Gi
Message:
Source:
    Type:		Glusterfs (a Glusterfs mount on the host that shares a pod's lifetime)
    EndpointsName:	gluster-dynamic-my-crs-storage
    Path:		vol_85e444ee3bc154de084976a9aef16025
    ReadOnly:		false</pre>
<p>The volume has been created specifically according to the design specifications in the PVC. In the PVC we did not explicitly specify which StorageClass we wanted to use because the GlusterFS StorageClass using heketi was defined as the system-wide default.</p>
<p>What happened in the background was that when the PVC reached the system, our default StorageClass reached out to the GlusterFS Provisioner with the volume specs from the PVC. The provisioner in turn communicates with our heketi instance which facilitates the creation of the GlusterFS volume, which we can trace in it's log messages:</p>
<pre>[root@crs-node1 ~]# journalctl -l -u heketi.service
...
Mar 24 11:25:52 crs-node1.lab heketi[2598]: [heketi] DEBUG 2017/03/24 11:25:52 /src/github.com/heketi/heketi/apps/glusterfs/volume_entry.go:298: Volume to be created on cluster e
Mar 24 11:25:52 crs-node1.lab heketi[2598]: [heketi] INFO 2017/03/24 11:25:52 Creating brick 9e791b1daa12af783c9195941fe63103
Mar 24 11:25:52 crs-node1.lab heketi[2598]: [heketi] INFO 2017/03/24 11:25:52 Creating brick 3e06af2f855bef521a95ada91680d14b
Mar 24 11:25:52 crs-node1.lab heketi[2598]: [heketi] INFO 2017/03/24 11:25:52 Creating brick e4daa240f1359071e3f7ea22618cfbab
...
Mar 24 11:25:52 crs-node1.lab heketi[2598]: [sshexec] INFO 2017/03/24 11:25:52 Creating volume vol_85e444ee3bc154de084976a9aef16025 replica 3
...
Mar 24 11:25:53 crs-node1.lab heketi[2598]: Result: volume create: vol_85e444ee3bc154de084976a9aef16025: success: please start the volume to access data
...
Mar 24 11:25:55 crs-node1.lab heketi[2598]: Result: volume start: vol_85e444ee3bc154de084976a9aef16025: success
...
Mar 24 11:25:55 crs-node1.lab heketi[2598]: [asynchttp] INFO 2017/03/24 11:25:55 Completed job c3d6c4f9fc74796f4a5262647dc790fe in 3.176522702s
...</pre>
<p>Success! In just about 3 seconds the GlusterFS pool was configured and provisioned a volume. The default as of today is replica 3, which means the data will be replicated across 3 bricks (GlusterFS speak for backend storage) of 3 distinct nodes. The process is orchestrated via heketi on behalf of OpenShift.</p>
<p>You can see this information on the volume from GlusterFS perspective:</p>
<pre>[root@crs-node1 ~]# gluster volume list
vol_85e444ee3bc154de084976a9aef16025
[root@crs-node1 ~]# gluster volume info vol_85e444ee3bc154de084976a9aef16025

Volume Name: vol_85e444ee3bc154de084976a9aef16025
Type: Replicate
Volume ID: a32168c8-858e-472a-b145-08c20192082b
Status: Started
Snapshot Count: 0
Number of Bricks: 1 x 3 = 3
Transport-type: tcp
Bricks:
Brick1: 172.16.128.8:/var/lib/heketi/mounts/vg_147b43f6f6903be8b23209903b7172ae/brick_9e791b1daa12af783c9195941fe63103/brick
Brick2: 172.16.128.9:/var/lib/heketi/mounts/vg_72c0f520b0c57d807be21e9c90312f85/brick_3e06af2f855bef521a95ada91680d14b/brick
Brick3: 172.16.128.7:/var/lib/heketi/mounts/vg_67314f879686de975f9b8936ae43c5c5/brick_e4daa240f1359071e3f7ea22618cfbab/brick
Options Reconfigured:
transport.address-family: inet
nfs.disable: on</pre>
<p>Notice how the volume name in GlusterFS corresponds to the "path" of the Kubernetes Persistent Volume in OpenShift.</p>
<p>Alternatively you can also use the OpenShift UI to provision storage, which allows you to conveniently select among all known StorageClasses in the system:</p>
<p><img class="alignnone size-full wp-image-8387" src="/assets/2017/03/screen-shot-2017-03-23-at-21-50-34.png" alt="Screen Shot 2017-03-23 at 21.50.34" width="993" height="738" /></p>
<p><img class="alignnone size-full wp-image-8392" src="/assets/2017/03/screen-shot-2017-03-24-at-11-09-341.png" alt="Screen Shot 2017-03-24 at 11.09.34.png" width="1108" height="362" /></p>
<p>Let's make this a little more interesting and run a workload on OpenShift.</p>
<p>On our OpenShift VM still being in the crs-storage project:</p>
<pre># oc get templates -n openshift</pre>
<p>You should see a nice list of application and database templates for easy consumption in OpenShift to get your app development project kickstarted.</p>
<p>We will use  MySQL to demonstrate how to host a stateful application on OpenShift with persistent and elastic storage. The mysql-persistent template includes a PVC of 1G for the MySQL database directory. For demonstration purposes all default values are fine.</p>
<pre># oc process mysql-persistent -n openshift | oc create -f -</pre>
<p>Wait for the deployment to finish. You can observe the progress in the UI or via</p>
<pre># oc get pods
NAME            READY     STATUS    RESTARTS   AGE
mysql-1-h4afb   1/1       Running   0          2m</pre>
<p>Nice. This template created a service, secrets, a PVC and a pod. Let's use it (your pod name will differ):</p>
<pre># oc rsh mysql-1-h4afb</pre>
<p>You have successfully attached to the MySQL pod. Let's connect to the database:</p>
<pre>sh-4.2$ mysql -u $MYSQL_USER -p$MYSQL_PASSWORD -h $HOSTNAME $MYSQL_DATABASE</pre>
<p>Conveniently all vital configuration like MySQL credentials, database name, etc are part of environment variables in the pod template and hence available in the pod as shell environment variables too. Let's create some data:</p>
<pre>mysql&gt; show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| sampledb           |
+--------------------+
2 rows in set (0.02 sec)

mysql&gt; \u sampledb
Database changed
mysql&gt; CREATE TABLE IF NOT EXISTS equipment (
    -&gt;     equip_id int(5) NOT NULL AUTO_INCREMENT,
    -&gt;     type varchar(50) DEFAULT NULL,
    -&gt;     install_date DATE DEFAULT NULL,
    -&gt;     color varchar(20) DEFAULT NULL,
    -&gt;     working bool DEFAULT NULL,
    -&gt;     location varchar(250) DEFAULT NULL,
    -&gt;     PRIMARY KEY(equip_id)
    -&gt;     );
Query OK, 0 rows affected (0.13 sec)

mysql&gt; INSERT INTO equipment (type, install_date, color, working, location)
    -&gt; VALUES
    -&gt; ("Slide", Now(), "blue", 1, "Southwest Corner");
Query OK, 1 row affected, 1 warning (0.01 sec)

mysql&gt; SELECT * FROM equipment;
+----------+-------+--------------+-------+---------+------------------+
| equip_id | type  | install_date | color | working | location         |
+----------+-------+--------------+-------+---------+------------------+
|        1 | Slide | 2017-03-24   | blue  |       1 | Southwest Corner |
+----------+-------+--------------+-------+---------+------------------+
1 row in set (0.00 sec)</pre>
<p>This means the database is functional. Great!</p>
<p>Do you want to see where the data is stored? Easy! Look at the mysql volume that got created as part of the template:</p>
<pre># oc get pvc/mysql
NAME      STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
mysql     Bound     pvc-a678b583-1082-11e7-afae-000c2949cce7   1Gi        RWO           11m
# oc describe pv/pvc-a678b583-1082-11e7-afae-000c2949cce7
Name:		pvc-a678b583-1082-11e7-afae-000c2949cce7
Labels:		
StorageClass:	container-ready-storage
Status:		Bound
Claim:		crs-storage/mysql
Reclaim Policy:	Delete
Access Modes:	RWO
Capacity:	1Gi
Message:
Source:
    Type:		Glusterfs (a Glusterfs mount on the host that shares a pod's lifetime)
    EndpointsName:	gluster-dynamic-mysql
    Path:		vol_6299fc74eee513119dafd43f8a438db1
    ReadOnly:		false</pre>
<p>Note the path to GlusterFS volume name vol_6299fc74eee513119dafd43f8a438db1.</p>
<p>Return to one of your GlusterFS VMs and issue:</p>
<pre># gluster volume info vol_6299fc74eee513119dafd43f8a438db

Volume Name: vol_6299fc74eee513119dafd43f8a438db1
Type: Replicate
Volume ID: 4115918f-28f7-4d4a-b3f5-4b9afe5b391f
Status: Started
Snapshot Count: 0
Number of Bricks: 1 x 3 = 3
Transport-type: tcp
Bricks:
Brick1: 172.16.128.7:/var/lib/heketi/mounts/vg_67314f879686de975f9b8936ae43c5c5/brick_f264a47aa32be5d595f83477572becf8/brick
Brick2: 172.16.128.8:/var/lib/heketi/mounts/vg_147b43f6f6903be8b23209903b7172ae/brick_f5731fe7175cbe6e6567e013c2591343/brick
Brick3: 172.16.128.9:/var/lib/heketi/mounts/vg_72c0f520b0c57d807be21e9c90312f85/brick_ac6add804a6a467cd81cd1404841bbf1/brick
Options Reconfigured:
transport.address-family: inet
nfs.disable: on</pre>
<p>You can see how the data is replicated across 3 GlusterFS bricks. Let's pick one of them (ideally the host you are logged on to and look at the directory contents):</p>
<pre># ll /var/lib/heketi/mounts/vg_67314f879686de975f9b8936ae43c5c5/brick_f264a47aa32be5d595f83477572becf8/brick
total 180300
-rw-r-----. 2 1000070000 2001       56 Mar 24 12:11 auto.cnf
-rw-------. 2 1000070000 2001     1676 Mar 24 12:11 ca-key.pem
-rw-r--r--. 2 1000070000 2001     1075 Mar 24 12:11 ca.pem
-rw-r--r--. 2 1000070000 2001     1079 Mar 24 12:12 client-cert.pem
-rw-------. 2 1000070000 2001     1680 Mar 24 12:12 client-key.pem
-rw-r-----. 2 1000070000 2001      352 Mar 24 12:12 ib_buffer_pool
-rw-r-----. 2 1000070000 2001 12582912 Mar 24 12:20 ibdata1
-rw-r-----. 2 1000070000 2001 79691776 Mar 24 12:20 ib_logfile0
-rw-r-----. 2 1000070000 2001 79691776 Mar 24 12:11 ib_logfile1
-rw-r-----. 2 1000070000 2001 12582912 Mar 24 12:12 ibtmp1
drwxr-s---. 2 1000070000 2001     8192 Mar 24 12:12 mysql
-rw-r-----. 2 1000070000 2001        2 Mar 24 12:12 mysql-1-h4afb.pid
drwxr-s---. 2 1000070000 2001     8192 Mar 24 12:12 performance_schema
-rw-------. 2 1000070000 2001     1676 Mar 24 12:12 private_key.pem
-rw-r--r--. 2 1000070000 2001      452 Mar 24 12:12 public_key.pem
drwxr-s---. 2 1000070000 2001       62 Mar 24 12:20 sampledb
-rw-r--r--. 2 1000070000 2001     1079 Mar 24 12:11 server-cert.pem
-rw-------. 2 1000070000 2001     1676 Mar 24 12:11 server-key.pem
drwxr-s---. 2 1000070000 2001     8192 Mar 24 12:12 sys</pre>
<p>You can see the MySQL database directory here. This is how it's stored in GlusterFS backend and presented to the MySQL container as a bind-mount. If you check your mount table on the OpenShift VM you will see the GlusterFS mount.</p>
<h3>Summary</h3>
<p>What we have done here is create a simple but functional GlusterFS storage pool outside of OpenShift. This pool can grow and shrink independently of the applications. The entire lifecycle of this pool is managed by a simple front-end known as heketi which only needs manual intervention when the deployment grows. For daily provisioning operations it's API is used via OpenShifts dynamic provisioner, eliminating the need for Developers to interact with Infrastructure teams directly.<br />
This is how we bring storage into the DevOps world - painless, and available directly via developer tooling of the OpenShift PaaS system.<br />
GlusterFS and OpenShift run across all foot-prints: bare-metal, virtual, private and public cloud (Azure, Google Cloud, AWS...) ensuring application portability and avoiding cloud provider lock-in.</p>
<p>Happy Glustering your Containers!</p>
<p>(c) 2017 Keith Tenzer</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#crs" class="page__taxonomy-item" rel="tag">CRS</a><span class="sep">, </span>
    
      <a href="/tags/#docker" class="page__taxonomy-item" rel="tag">Docker</a><span class="sep">, </span>
    
      <a href="/tags/#gluster" class="page__taxonomy-item" rel="tag">Gluster</a><span class="sep">, </span>
    
      <a href="/tags/#heketi" class="page__taxonomy-item" rel="tag">heketi</a><span class="sep">, </span>
    
      <a href="/tags/#kubernetes" class="page__taxonomy-item" rel="tag">Kubernetes</a><span class="sep">, </span>
    
      <a href="/tags/#persistent-storage" class="page__taxonomy-item" rel="tag">Persistent Storage</a><span class="sep">, </span>
    
      <a href="/tags/#software-defined-storage" class="page__taxonomy-item" rel="tag">software-defined storage</a><span class="sep">, </span>
    
      <a href="/tags/#storage" class="page__taxonomy-item" rel="tag">Storage</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#openshift" class="page__taxonomy-item" rel="tag">OpenShift</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2017-03-24T00:00:00-07:00">March 24, 2017</time></p>


      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?via=keithtenzer&text=Storage+for+Containers+using+Gluster+%E2%80%93+Part+II%20http%3A%2F%2Flocalhost%3A4000%2Fopenshift%2Fstorage-for-containers-using-gluster-part-ii%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fopenshift%2Fstorage-for-containers-using-gluster-part-ii%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fopenshift%2Fstorage-for-containers-using-gluster-part-ii%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/cloudforms/deploying-cloudforms-in-the-azure-cloud/" class="pagination--pager" title="Deploying CloudForms in the Azure Cloud
">Previous</a>
    
    
      <a href="/openstack/openstack-10-newton-lab-installation-and-configuration-guide/" class="pagination--pager" title="OpenStack 10 (Newton) Lab Installation and Configuration Guide
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/openshift/building-ansible-operators-1-2-3/" rel="permalink">Building Ansible Operators 1-2-3
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2021-12-03T00:00:00-08:00">December 3, 2021</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          10 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Overview
In this article we will go step by step in build a Kubernetes Operator using Ansible and the Operator Framework. Operators provide ability to not on...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/openshift/openshift-service-mesh-getting-started-guide/" rel="permalink">OpenShift Service Mesh Getting Started Guide
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2021-04-27T00:00:00-07:00">April 27, 2021</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          11 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">






Overview
In this article we will explore the OpenShift Service Mesh and deploy a demo application to better understand the various concepts. First you...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/openshift/openshift-4-aws-ipi-installation-getting-started-guide/" rel="permalink">OpenShift 4 AWS IPI Installation Getting Started Guide
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2021-01-18T00:00:00-08:00">January 18, 2021</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          8 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">


Happy new year as this will be the first post of 2021! 2020 was obviously a challenging year, my hope is I will have more time to devote to blogging in 20...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ansible/windows-automation-with-ansible-getting-started-guide/" rel="permalink">Windows Automation with Ansible: Getting Started Guide
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2020-05-19T00:00:00-07:00">May 19, 2020</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          14 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
Overview
In this article we will focus on how to get started with automation of windows using Ansible. Specifically we will look at installing 3rd party sof...</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="https://twitter.com/keithtenzer"" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
      
        
          <li><a href="https://github.com/ktenzer" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2021 Keith Tenzer. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>







  </body>
</html>
