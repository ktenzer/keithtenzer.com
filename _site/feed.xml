<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-08-29T18:05:43-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Keith Tenzer’s Blog</title><subtitle></subtitle><author><name>Keith Tenzer</name></author><entry><title type="html">Replay of Replay</title><link href="http://localhost:4000/temporal/replay-of-replay/" rel="alternate" type="text/html" title="Replay of Replay" /><published>2022-08-29T00:00:00-07:00</published><updated>2022-08-29T00:00:00-07:00</updated><id>http://localhost:4000/temporal/replay-of-replay</id><content type="html" xml:base="http://localhost:4000/temporal/replay-of-replay/">&lt;p&gt;&lt;img src=&quot;/assets/2022-08-29/temporal-replay.png&quot; alt=&quot;Replay&quot; /&gt;&lt;/p&gt;
&lt;h1 id=&quot;overview&quot;&gt;Overview&lt;/h1&gt;
&lt;p&gt;Temporal just completed it’s inaugural user conference called Replay in Seattle. As such I wanted to do a quick replay of Replay. First, why Dinosaurs? Well, Dinosaurs are cool right? If Dinosaurs were a Temporal application we could replay them, experiencing the past over and over, now wouldn’t that be cool!&lt;/p&gt;

&lt;h1 id=&quot;impressions&quot;&gt;Impressions&lt;/h1&gt;
&lt;p&gt;Honestly, replay blew my mind. I’ve been to a lot of conferences but this was the first time I felt I could relate to everyone. That we all not only shared similar challenges, but also a vision for the future. It was collaborative, informative and truly visionary. The community that came together represented some of the most iconic companies on our planet, disruptive startups and large global enterprises. All with incredible challenges to solve and looking at Temporal for at least part of the solution. 
Scaling applications at the cost of complexity or increasing developer velocity at the cost of reliability and safety, simply are not tradeoffs we are willing to accept. There is a better way and this community seems on the cusp of re-inventing or even disrupting an entire industry. How we build, connect, deploy applications and the platform services they depend on.&lt;/p&gt;

&lt;h1 id=&quot;day-1&quot;&gt;Day 1&lt;/h1&gt;
&lt;p&gt;The first day had split tracks with birds of the feather discussions in the lounge area and workshops in the main hall. I attended the workshop sessions which had a great mix of hands-on, real-world customer examples and theoretical.&lt;/p&gt;

&lt;h3 id=&quot;temporal-service-and-application-architecture&quot;&gt;Temporal Service and Application Architecture&lt;/h3&gt;
&lt;p&gt;This was a workshop led by several Temporalers (including one it’s co-founders Samar Abbas) to dive into real customer applications and answer their questions. Samar began by eloquently explaining how using Temporal really forces you to think differently. Most (including myself) think about a workflow as having a start and an end but Temporal challenges that by removing constraints of time. In Temporal, a workflow encapsulates processes and entities, reacting to signals sent and as such can run infinitely. Temporal only executes a process when it has something to do. The only real limitation of a workflow is 50k events or 50Mb of storage in the event history. These limitations however, have a simple solution, using &lt;a href=&quot;https://docs.temporal.io/concepts/what-is-continue-as-new/&quot;&gt;continue-as-new&lt;/a&gt; we can simply refresh the event history for a given workflow.&lt;/p&gt;

&lt;p&gt;In this workshop we reviewed code and some of the challenges from two Temporal users: Tock and Coalition.&lt;/p&gt;

&lt;h4 id=&quot;tock&quot;&gt;Tock&lt;/h4&gt;
&lt;p&gt;Provides a reservation system used by many restaurants. They are using Temporal for the following use cases:  Determining reservations offered by restaurants, sending emails via typescript, e-commerce flows and scheduled workflows.&lt;/p&gt;

&lt;h6 id=&quot;what-is-best-practice-when-we-dont-want-a-retry-policy&quot;&gt;What is best practice when we don’t want a retry policy?&lt;/h6&gt;
&lt;p&gt;Temporal will change how you think about failure and will motivate you to think about retrying until it is repaired. Don’t fail a workflow, instead fix what is preventing workflow from proceeding.&lt;/p&gt;

&lt;h6 id=&quot;how-to-keep-long-running-workflows-without-ifthenelse-logic-to-handle-versioning&quot;&gt;How to keep long running workflows without if/then/else logic to handle versioning?&lt;/h6&gt;
&lt;p&gt;Provide versioning primitive to batch long runnings apps. Use continue-as-new which restarts a workflow with same input, just clearing it’s history. Have guarantee that no workflows rely on a version older than X of the client. If you do have really long running workflows and require introducing a new worker, consider keeping old worker around till those workflows complete. New workflows can then execute on a new worker.&lt;/p&gt;

&lt;h6 id=&quot;what-is-best-way-to-share-output-types-from-workflows-written-in-different-languages&quot;&gt;What is best way to share output types from workflows written in different languages?&lt;/h6&gt;
&lt;p&gt;Everything in temporal is serialized, can use data converters and write your own or protobuff wrappers at gRPC level.&lt;/p&gt;

&lt;h6 id=&quot;how-to-prioritize-different-types-of-workflows&quot;&gt;How to prioritize different types of workflows?&lt;/h6&gt;
&lt;p&gt;Higher priority workflows should have dedicated task queue with dedicated pool of workers. Works well if you can create pools that have a few different priority workflow types and not large numbers of priorities. Can also be done at workflow level, each signal is an order to execute a child workflow and business logic can determine priority in this case.&lt;/p&gt;

&lt;h6 id=&quot;how-do-you-access-longer-retentions-3-6-months&quot;&gt;How do you access longer retentions (3-6 months)?&lt;/h6&gt;
&lt;p&gt;All workflow state is kept up to 30 days after a workflow closes in the event history. Some want to keep this around for a longer, due to compliance issues. Recommendation is to use archive feature only currently available in the OS. Archive empties the state in Temporal’s backend and pushes it to a blob.&lt;/p&gt;

&lt;h6 id=&quot;general-recommended-strategy-for-scaling-workers-if-sharing-same-resources-does-it-make-sense-to-have-separate-workers-what-about-serverless-approach-to-workers&quot;&gt;General recommended strategy for scaling workers? If sharing same resources, does it make sense to have separate workers? What about serverless approach to workers?&lt;/h6&gt;
&lt;p&gt;Not opinionated on how to deploy workers, workers are stateless, they maintain a cache for performance reasons. You can create dynamic autoscale approaches to workers. Other piece is isolation which is at Temporal namespace level. Temporal has strong guarantee for availability and performance of a given namespace. For example limiting blast radius or noisy neighbor scenario. Example, a bug in a worker for a given workflow could cause OOM issues and that could propagate through workers affecting other workflows that may be of higher priority.&lt;/p&gt;

&lt;h6 id=&quot;how-to-find-backlog-of-task-queue-to-understand-how-to-schedule-workers&quot;&gt;How to find backlog of task queue to understand how to schedule workers?&lt;/h6&gt;
&lt;p&gt;Temporal does not yet allow directly controlling task queues but the Schedule-To-Start timeout allows you to limit backlog on task queues by re-routing or draining them to a different task queue / worker.&lt;/p&gt;

&lt;h4 id=&quot;coalition&quot;&gt;Coalition&lt;/h4&gt;
&lt;p&gt;Focuses on active side of cyber security insurance. Helping customers avoid security breaches. They are using Temporal for the following use cases: lifecycle of policy, sending emails and anything in general that comes from a request should go through Temporal.&lt;/p&gt;

&lt;h6 id=&quot;when-does-it-make-sense-to-do-activity-vs-job-workflow&quot;&gt;When does it make sense to do activity vs job workflow?&lt;/h6&gt;
&lt;p&gt;Everything that does not have to do with orchestration should be done via Activity. For example, using timers, business logic or integrating with external systems should all be done within activities. Job workflow can be used in order to partition some limitations of histories for long running workflows.&lt;/p&gt;

&lt;h6 id=&quot;when-to-use-new-workflow-vs-child-workflow&quot;&gt;When to use new workflow vs child workflow?&lt;/h6&gt;
&lt;p&gt;Temporal scales on number of workflow executions but an individual workflow execution has limits as already explained. Child workflows can be used to partition a workflow as each child workflow has it’s own event history. A child workflow execution should maintain a one to one resource mapping and be considered a separate service.&lt;/p&gt;

&lt;h6 id=&quot;where-to-use-sleep&quot;&gt;Where to use Sleep?&lt;/h6&gt;
&lt;p&gt;Sleep is one of blocking APIs to create a durable timer which is duration of sleep. Polling in workflow however can blowup history of workflow execution. A way around this is to use signals. For example, might have external system polling for events in Temporal and sending signal to workflow when polling completes to continue with next activity.&lt;/p&gt;

&lt;h6 id=&quot;how-much-data-should-i-send-to-temporal&quot;&gt;How much data should I send to Temporal?&lt;/h6&gt;
&lt;p&gt;Metadata is generated internally but the majority of overall data within Temporal is from input parameters and result of activity functions. All those payloads get serialized and stored in the history. One shouldn’t pass large amounts of data, i.e. megabytes. Better to pass pointers to a blob that contains the actual data.&lt;/p&gt;

&lt;h3 id=&quot;temporal-101-workshop&quot;&gt;Temporal 101 Workshop&lt;/h3&gt;
&lt;p&gt;This was a two part hands-on workshop intended for new users of Temporal (like me). An entire development environment was provided via GitPod. The exercises involved bootstrapping a Temporal worker, creating some basic workflows that consume a service and using UI/CLI utilities. Everything is available &lt;a href=&quot;https://learn.temporal.io/replay2022&quot;&gt;here&lt;/a&gt; and I highly recommend this for anyone new to Temporal. In addition if you want to setup your own local development environment, I provided a consolidated guide &lt;a href=&quot;https://keithtenzer.com/temporal/temporal_getting_started_guide/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;building-event-driven-reactive-applications&quot;&gt;Building event-driven, reactive applications&lt;/h3&gt;
&lt;p&gt;This session was very thought provoking. It started with discussing around processes; which essentially do two things: coordination and execution. In Temporal coordination is the workflow and execution is the activity. Generally coordination is very long lived while execution tends to be short lived. There are two very different notions for how to achieve long running coordination: orchestration vs choreography.&lt;/p&gt;

&lt;h4 id=&quot;orchestration&quot;&gt;Orchestration&lt;/h4&gt;
&lt;p&gt;Command based (step 1, step 2, step 3), explicit control flow and direct. Behavior is pre-determined&lt;/p&gt;

&lt;h4 id=&quot;choreography&quot;&gt;Choreography&lt;/h4&gt;
&lt;p&gt;Does not depict behavior, event based, implicit flow control and indirect. Behavior comes to life after a process executes.&lt;/p&gt;

&lt;p&gt;While both notions work, providing solution to long running coordination; only orchestration does it simply with much less complexity. Choreographed systems require event buses, streams, retry logic and many databases to store persistency at every change. Behavior as such is inherently difficult to understand and when things go wrong, challenging to troubleshoot. Temporal is an orchestrated system, solving the problem with less code and complexity, resulting in higher reliability and simplicity.&lt;/p&gt;

&lt;h1 id=&quot;day-2&quot;&gt;Day 2&lt;/h1&gt;
&lt;p&gt;The second day featured keynotes and Temporal user sessions from some quite amazing speakers.&lt;/p&gt;

&lt;h3 id=&quot;keynote-maxim-fateev&quot;&gt;Keynote: Maxim Fateev&lt;/h3&gt;
&lt;p&gt;Maxim is Temporal’s CEO and co-founder. He provided not only a brilliant history of temporal but also compared building applications to fruits. Often we start with a fruit (microservice) but then it needs to interact with other fruits, since we are of course mixing fruits, to make an amazing desert (application). The result is a smoothie. While it can taste great, it is impossible to actually know what ingredients are in there without reading a step-by-step of how it was created. The smoothie approach doesn’t work well for applications as it is complicated to understand and replay. Reliability and durability are much lower as a result. Maxim explains a better way is to make a cake where each fruit is a layer. Contracts and boundaries are easily understood. If you want banana and strawberry but not blueberry then you just get a piece of those layers. This is the simplicity we can have with Temporal.&lt;/p&gt;

&lt;p&gt;Mike Nichols did a fantastic live demo showcasing a sneak peak at the upcoming Temporal namespace-as-a-service cloud offering. The Temporal cloud not only provides a fully supported managed namespace but also helps get customers going quickly. Few users really want to standup and be responsible for the infrastructure running the Temporal server or namespace. It is easy to get something working (day 1) but building a scalable, reliable operating model (day 2) is a whole other story. While Temporal has been in production with a cloud service for a while, we have not yet offered namespace-as-a-service, namespaces were provided via support tickets. Soon, anyone will be able to enjoy the Temporal cloud and self provision their namespaces.&lt;/p&gt;

&lt;h3 id=&quot;datadog-jeremy-legrone&quot;&gt;DataDog: Jeremy LeGrone&lt;/h3&gt;
&lt;p&gt;At DataDog the main uses of Temporal are for deployments, infra, control planes, datastore operators, code merges, queuing systems and incident response.&lt;/p&gt;

&lt;h4 id=&quot;getting-started-recommendations&quot;&gt;Getting started recommendations&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Define payload types as protobuf messages&lt;/li&gt;
  &lt;li&gt;Stick with single SDK language&lt;/li&gt;
  &lt;li&gt;Consider one of new SDKs like typescript&lt;/li&gt;
  &lt;li&gt;Safeguard against making network or other blocking calls inside workflow&lt;/li&gt;
  &lt;li&gt;Don’t allow Temporal API to be exposed to public, does not have protection against DDOS attack&lt;/li&gt;
  &lt;li&gt;Invest in core Temporal enablement team&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Two challenges that DataDog encountered were around workflow versioning and large payloads.&lt;/p&gt;

&lt;h4 id=&quot;workflow-versioning&quot;&gt;Workflow versioning&lt;/h4&gt;
&lt;p&gt;Proposal in place using build IDs to provide solution. It would enable A/B deployments for example, allowing old v1 workflows to run on the v1 workers while new workflows go to v2 workers. In meantime until feature is implemented, you can do CI checks that do replay tests or use continue-as-new to allow long running workflows to restart on new v2 workers.&lt;/p&gt;

&lt;h4 id=&quot;large-payloads&quot;&gt;Large payloads&lt;/h4&gt;
&lt;p&gt;Storage is over the entire history of workflow, not just the current execution. Large payloads should be kept in blob storage space and you should pass only pointers as inputs. Downside is activity code needs special treatment and overall it can be complex to deal with lifecycle management.&lt;/p&gt;

&lt;h3 id=&quot;square-ryan-walls&quot;&gt;Square: Ryan Walls&lt;/h3&gt;
&lt;p&gt;At Square the main use cases for Temporal are service orchestration as code, better mechanism for retries, cancellations and increased durability for massive amounts of microservices.&lt;/p&gt;

&lt;p&gt;Ryan focused his session about taking innovation from idea to broad enterprise adoption. It was very relevant, as many developers using Temporal today or who are just learning about it need to grow adoption within their organizations. That only happens if you can influence and be a catalyst for change. Instead of winging it like most of us, Ryan shared his learnings from taking idea to adoption in very large innovative organizations. He shared great insights and also captured some of the pitfalls.&lt;/p&gt;

&lt;h3 id=&quot;stripe-drew-hoskins&quot;&gt;Stripe: Drew Hoskins&lt;/h3&gt;
&lt;p&gt;Drew stated that Temporal is a paradigm shift for distributed systems. I think one of the key takeaways from replay was that we as a community are at the beginning of something big that will re-shape how we think about and build future applications.
At Stripe, Temporal is used by both infrastructure and product teams. The priorities at Stripe are developer productivity and reliability. It isn’t good enough to move fast, they have to do so safely and reliably. 
Client teams own Temporal workers and monitoring. Platform at Stripe offers fully wrapped Ruby / Java SDKs, lightly extended GO SDK for infra team, Temporal server / UI, proxy and management plane.&lt;/p&gt;

&lt;h4 id=&quot;why-wrap-the-sdk&quot;&gt;Why wrap the SDK?&lt;/h4&gt;
&lt;p&gt;Stripe wanted to build sorbet types (strong typing), actionable error messages and inspect URLs (links to Temporal docs) which explains how to debug certain known issues that Stripe developers typically encounter. It also lets you guide developers better to make the correct implementation choices,keeping certain guardrails around them.&lt;/p&gt;

&lt;h4 id=&quot;art-of-timeouts&quot;&gt;Art of timeouts&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;A workflow has been stuck for hours and nobody knows why?
    &lt;ul&gt;
      &lt;li&gt;Its waiting on activity which has long timeout. Either set shorter timeout or heartbeat&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Workflow failed because activity took too long to start
    &lt;ul&gt;
      &lt;li&gt;Never use schedule-to-start timeout, monitor instead&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Workflow timeout after 30 mins due to a bug (use tctl to fix)&lt;/li&gt;
  &lt;li&gt;Use long workflow timeouts, stripe defaults to 3 weeks so there is time to deploy fixes&lt;/li&gt;
  &lt;li&gt;Discourage start-to-close timeout of 2 hours or more&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;art-of-retries&quot;&gt;Art of retries&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Workflow activity that moves money, succeeds but failed to report due to network issue could result in money moving twice
    &lt;ul&gt;
      &lt;li&gt;Make all activities idempotent&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Cluster goes down and need to fail running workflows over to another region
    &lt;ul&gt;
      &lt;li&gt;Ensure all activities are idempotent allowing them to be re-run safely&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Have test framework that tests for idempotency
    &lt;ul&gt;
      &lt;li&gt;Once activity goes into integration, test behind the scenes for idempotency&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;workflow-latency-slas&quot;&gt;Workflow latency SLAs&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;A workflow encounters transient failure
    &lt;ul&gt;
      &lt;li&gt;Track failure metrics and alert if too high&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Workflows encounter extended outage
    &lt;ul&gt;
      &lt;li&gt;Same but notify developer&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Code bug, invalid argument
    &lt;ul&gt;
      &lt;li&gt;Same developer deploys fix and workflow resumes&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Have ton of workflows that are stuck
    &lt;ul&gt;
      &lt;li&gt;Add OutOfTimeSla search to elastic search, teams can monitor with a recurring check&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;batches&quot;&gt;Batches&lt;/h4&gt;
&lt;p&gt;Workflows that perform an operation on set of items (batched for efficiency)&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Batch activity takes long. Still running?
    &lt;ul&gt;
      &lt;li&gt;Use activity heartbeats&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;In failure loop, every time it fails, have to start activity from beginning
    &lt;ul&gt;
      &lt;li&gt;Pull out cursor and pass to heartbeat so you can resume where you left off&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Processing huge number of records in parallel across many activities, workflow history gets too long
    &lt;ul&gt;
      &lt;li&gt;Use continue-as-new or child workflows diving work among multiple workers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;keynote-paul-nordstrom&quot;&gt;Keynote: Paul Nordstrom&lt;/h3&gt;
&lt;p&gt;Paul took us on a wonderful journey which compared application ecosystems to our own biological ecosystems. It paralleled how things evolved in both software and the biological world. Paul shared a powerful vision based on observing biological worlds, what is the logical next step for software and applications? 
Paul theorized that the next step in software evolution would be application or services communities. The biological world depends on contracts and a certain balance between all life organisms within the ecosystem. Similarly applications too should develop well-defined contracts in how they interact with themselves and other critical dependencies. Something like Temporal could evolve to provide a way for applications or services to create these contracts which enforce durability and reliability, allowing interactions to be quite predictable.&lt;/p&gt;

&lt;h3 id=&quot;yum-brands-matt-mcdole&quot;&gt;Yum! Brands: Matt McDole&lt;/h3&gt;
&lt;p&gt;After hearing from Temporal’s CEO, some of the most iconic voices in our industry and a peak through a visionary’s lens, it was time to hear a story from a large scale enterprise user. I felt like we saved the best for last. Matt delivered an extremely insightful, thoughtful presentation on not only how but why Yum! brands is using Temporal.
Using Temporal Yum! brands achieved: simplified CD, increased oder flow reliability for KFC (Taco Bell/Pizza Hut are next), improved idempotency and simplified production support. Just using Temporal alone removed an estimated quarter of their code base. They no longer needed to maintain their own pollers, task queues, retry logic or state machine. The result was an increased overall reliability and serviceability. Matt gave us a real world understanding of why you never want to be debugging a state machine during a production down event.
What I learned from Matt was how easy it is to start with Temporal. You don’t need to boil the ocean; instead you can start small, for example replace retry logic of a single service and then build form there. With each successful use case the value of Temporal grows. Using the Temporal cloud is a very fast, low cost way to get started. You don’t need to stand-up a Kubernetes cluster, databases or even an SRE. If something isn’t easy or cost effective it won’t get broad adoption no matter what.&lt;/p&gt;

&lt;h3 id=&quot;pannel-shaping-future-of-backend-development&quot;&gt;Pannel: Shaping future of backend development&lt;/h3&gt;
&lt;p&gt;The replay conference ended with a panel of experts. Folks from Temporal, Snap and Netflix all represented the panel. They discussed what they would like to see in the future of backend development. A lot of the ideas discussed revolved around two kep points: moving fast (high development velocity, which we achieve today) but doing it safely (which is the missing piece).&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this article we replayed replay, Temporal’s inaugural user conference. The first one of (I hope) many such conferences. My main takeaway is that we are at the beginning of a journey, launch of a new better way to build applications for durability. We don’t even yet know how to call our new industry of durable applications but we will figure it out together, like everything else one step at a time! Best of luck in your journey to more reliable applications with Temporal! If you have any questions reach out on &lt;a href=&quot;https://temporal.io/slack&quot;&gt;Temporal community slack&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;(c) 2022 Keith Tenzer&lt;/p&gt;</content><author><name>Keith Tenzer</name></author><category term="Temporal" /><category term="Temporal" /><category term="Opensource" /><category term="Workflows" /><category term="Activities" /><category term="Go" /><category term="Replay" /><summary type="html">Overview Temporal just completed it’s inaugural user conference called Replay in Seattle. As such I wanted to do a quick replay of Replay. First, why Dinosaurs? Well, Dinosaurs are cool right? If Dinosaurs were a Temporal application we could replay them, experiencing the past over and over, now wouldn’t that be cool!</summary></entry><entry><title type="html">Temporal Getting Started Guide</title><link href="http://localhost:4000/temporal/temporal_getting_started_guide/" rel="alternate" type="text/html" title="Temporal Getting Started Guide" /><published>2022-08-24T00:00:00-07:00</published><updated>2022-08-24T00:00:00-07:00</updated><id>http://localhost:4000/temporal/temporal_getting_started_guide</id><content type="html" xml:base="http://localhost:4000/temporal/temporal_getting_started_guide/">&lt;p&gt;&lt;img src=&quot;/assets/2022-08-15/logo-temporal-with-copy.svg&quot; alt=&quot;Temporal&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;In this article we will walk through setup of a development environment for Temporal. There are of course, several ways you can run the Temporal server locally. You can use Docker compose, helm or an operator on minikube and temporalite. In addition you can also consume Temporal namespaces as-a-service via the Temporal cloud. I would recommend temporalite if you want to run the Temporal server disconnected on your laptop otherwise the Temporal cloud is definitely the best option. I will cover getting started with the Temporal cloud in a future post.&lt;/p&gt;

&lt;h2 id=&quot;temporalite-install&quot;&gt;Temporalite Install&lt;/h2&gt;
&lt;p&gt;Temporalite is a simple packaging of the Temporal server. It uses SQLite, instead of a full blown backend database for simplicity and less resource consumption. It is intended only for development or testing and not production use. In order to install it you need to have a Go environment to build the binary.&lt;/p&gt;

&lt;h3 id=&quot;configure-go&quot;&gt;Configure Go&lt;/h3&gt;
&lt;h4 id=&quot;create-directory-structure-for-go-in-your-home-directory&quot;&gt;Create directory structure for Go in your home directory.&lt;/h4&gt;
&lt;pre&gt;$ mkdir -p ~/go/src/github.com&lt;/pre&gt;

&lt;h4 id=&quot;install-go&quot;&gt;Install Go&lt;/h4&gt;
&lt;pre&gt;$ sudo dnf install -y go&lt;/pre&gt;

&lt;h4 id=&quot;update-profile-with-go-environment&quot;&gt;Update Profile with Go Environment&lt;/h4&gt;
&lt;pre&gt;
$ vi ~/.bash_profile
export GOBIN=/home/username
export GOPATH=/home/username/src
export EDITOR=vim
&lt;/pre&gt;

&lt;h3 id=&quot;build-temporalite&quot;&gt;Build Temporalite&lt;/h3&gt;
&lt;h4 id=&quot;create-directory-for-temporal&quot;&gt;Create directory for Temporal&lt;/h4&gt;
&lt;pre&gt;$ mkdir ~/go/src/github.com/temporalio&lt;/pre&gt;

&lt;h4 id=&quot;change-directory-to-temporalio&quot;&gt;Change directory to temporalio&lt;/h4&gt;
&lt;pre&gt;$ cd temporalio&lt;/pre&gt;

&lt;h4 id=&quot;clone-temporalite-github-repository&quot;&gt;Clone temporalite GitHub repository&lt;/h4&gt;
&lt;pre&gt;$ git clone https://github.com/temporalio/temporalite.git&lt;/pre&gt;

&lt;h4 id=&quot;build-temporalite-binary&quot;&gt;Build temporalite binary&lt;/h4&gt;
&lt;pre&gt;$ go install github.com/temporalio/temporalite/cmd/temporalite@latest&lt;/pre&gt;

&lt;h4 id=&quot;copy-temporalite-binary-to-bin-directory&quot;&gt;Copy temporalite binary to bin Directory&lt;/h4&gt;
&lt;pre&gt;$ sudo cp /home/ktenzer/go/bin/temporalite /usr/local/bin&lt;/pre&gt;

&lt;h4 id=&quot;create-directory-for-local-temporal-database&quot;&gt;Create Directory for local Temporal Database&lt;/h4&gt;
&lt;pre&gt;mkdir -p /home/ktenzer/.config/temporalite/db&lt;/pre&gt;

&lt;h4 id=&quot;start-temporalite&quot;&gt;Start temporalite&lt;/h4&gt;
&lt;pre&gt;$ temporalite start --namespace default&lt;/pre&gt;

&lt;h2 id=&quot;build-cli&quot;&gt;Build CLI&lt;/h2&gt;
&lt;p&gt;Now that we have the Temporal server running via temporalite we need to build the CLI.&lt;/p&gt;

&lt;h4 id=&quot;change-directory-to-temporalio-1&quot;&gt;Change directory to temporalio&lt;/h4&gt;
&lt;pre&gt;$ cd ~/go/src/github.com/temporalio&lt;/pre&gt;

&lt;h4 id=&quot;clone-temporal-github-repository&quot;&gt;Clone Temporal GitHub repository&lt;/h4&gt;
&lt;pre&gt;$ git clone https://github.com/temporalio/temporal.git&lt;/pre&gt;

&lt;h4 id=&quot;run-tctl-makefile&quot;&gt;Run tctl Makefile&lt;/h4&gt;
&lt;pre&gt;$ make update-tctl&lt;/pre&gt;

&lt;h4 id=&quot;copy-tctl-binary-to-bin-directory&quot;&gt;Copy tctl binary to bin directory&lt;/h4&gt;
&lt;pre&gt;$ sudo cp ~/go/bin/tctl /usr/local/bin&lt;/pre&gt;

&lt;h2 id=&quot;start-temporal-workflow&quot;&gt;Start Temporal Workflow&lt;/h2&gt;
&lt;p&gt;In this case we will just run the helloworld sample from the many samples that Temporal provides.&lt;/p&gt;

&lt;h4 id=&quot;change-directory-to-temporalio-2&quot;&gt;Change directory to temporalio&lt;/h4&gt;
&lt;pre&gt;$ cd ~/go/src/github.com/temporalio&lt;/pre&gt;

&lt;h4 id=&quot;clone-samples-github--repository&quot;&gt;Clone samples GitHub  repository&lt;/h4&gt;
&lt;pre&gt;$ git clone https://github.com/temporalio/samples-go.git&lt;/pre&gt;

&lt;h4 id=&quot;change-directory-to-samples&quot;&gt;Change directory to samples&lt;/h4&gt;
&lt;pre&gt;$ cd samples-go&lt;/pre&gt;

&lt;h4 id=&quot;start-workflow&quot;&gt;Start workflow&lt;/h4&gt;
&lt;pre&gt;
$ go run helloworld/starter/main.go
2022/08/24 11:46:11 INFO  No logger configured for temporal client. Created default one.
2022/08/24 11:46:11 Started workflow WorkflowID hello_world_workflowID RunID b93ef614-e6df-4578-a31b-7ebd3f59df55
&lt;/pre&gt;

&lt;p&gt;Notice that the workflow starts but is waiting to execute. This is because there is no worker. The starter program will tell the Temporal server to execute a workflow however, since no worker is attached the Temporal server namespace, the workflow will be in running state until a worker becomes available.&lt;/p&gt;

&lt;p&gt;The Temporal UI will also show that we are waiting for a worker.
&lt;img src=&quot;/assets/2022-08-24/temporal_workflow_started.png&quot; alt=&quot;Temporal Workflow Started&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;start-temporal-worker&quot;&gt;Start Temporal Worker&lt;/h2&gt;
&lt;p&gt;Now that we have our Temporal server and a workflow running all that is needed is a worker. We will start the corresponding helloworld worker.&lt;/p&gt;

&lt;pre&gt;$ go run helloworld/worker/main.go&lt;/pre&gt;

&lt;p&gt;The worker should immediately run and complete the workflow.&lt;/p&gt;
&lt;pre&gt;
2022/08/24 11:55:26 INFO  HelloWorld workflow completed. Namespace default TaskQueue hello-world WorkerID 7576@fedora@ WorkflowType Workflow WorkflowID hello_world_workflowID RunID b93ef614-e6df-4578-a31b-7ebd3f59df55 Attempt 1 result Hello Temporal!
&lt;/pre&gt;

&lt;p&gt;If you look at your starter it also has now returned and displays the workflow result returned from the workflow.&lt;/p&gt;
&lt;pre&gt;
2022/08/24 11:55:26 Workflow result: Hello Temporal!
&lt;/pre&gt;

&lt;p&gt;In the UI we can now also see the workflow has completed.
&lt;img src=&quot;/assets/2022-08-24/temporal_workflow_completed.png&quot; alt=&quot;Temporal Workflow Completed&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;view-workflow-in-cli&quot;&gt;View Workflow in CLI&lt;/h2&gt;
&lt;p&gt;Temporal provides the tctl CLI for interacting with the Temporal server.&lt;/p&gt;

&lt;h4 id=&quot;list-workflows&quot;&gt;List workflows&lt;/h4&gt;
&lt;pre&gt;
$ tctl workflow list
  WORKFLOW TYPE |      WORKFLOW ID       |                RUN ID                | TASK QUEUE  | START TIME | EXECUTION TIME | END TIME  
  Workflow      | hello_world_workflowID | b93ef614-e6df-4578-a31b-7ebd3f59df55 | hello-world | 18:46:11   | 18:46:11       | 18:55:26  
&lt;/pre&gt;

&lt;h4 id=&quot;show-workflow-details&quot;&gt;Show workflow details&lt;/h4&gt;
&lt;pre&gt;
$ tctl workflow show --workflow_id hello_world_workflowID --run_id b93ef614-e6df-4578-a31b-7ebd3f59df55
   1  WorkflowExecutionStarted    {WorkflowType:{Name:Workflow},                                 
                                  ParentInitiatedEventId:0, TaskQueue:{Name:hello-world,         
                                  Kind:Normal}, Input:[&quot;Temporal&quot;],                              
                                  WorkflowExecutionTimeout:0s, WorkflowRunTimeout:0s,            
                                  WorkflowTaskTimeout:10s, Initiator:Unspecified,                
                                  OriginalExecutionRunId:b93ef614-e6df-4578-a31b-7ebd3f59df55,   
                                  Identity:5846@fedora@,                                         
                                  FirstExecutionRunId:b93ef614-e6df-4578-a31b-7ebd3f59df55,      
                                  Attempt:1, FirstWorkflowTaskBackoff:0s,                        
                                  ParentInitiatedEventVersion:0}                                 
   2  WorkflowTaskScheduled       {TaskQueue:{Name:hello-world,                                  
                                  Kind:Normal},                                                  
                                  StartToCloseTimeout:10s,                                       
                                  Attempt:1}                                                     
   3  WorkflowTaskStarted         {ScheduledEventId:2, Identity:7576@fedora@,                    
                                  RequestId:3c597463-65e4-4d61-b014-484634f32c37,                
                                  SuggestContinueAsNew:false, HistorySizeBytes:0}                
   4  WorkflowTaskCompleted       {ScheduledEventId:2, StartedEventId:3,                         
                                  Identity:7576@fedora@,                                         
                                  BinaryChecksum:0990e4a32efe7cd9c6c127b9cb51ecfc}               
   5  ActivityTaskScheduled       {ActivityId:5,                                                 
                                  ActivityType:{Name:Activity},                                  
                                  TaskQueue:{Name:hello-world,                                   
                                  Kind:Normal},                                                  
                                  Input:[&quot;Temporal&quot;],                                            
                                  ScheduleToCloseTimeout:0s,                                     
                                  ScheduleToStartTimeout:0s,                                     
                                  StartToCloseTimeout:10s,                                       
                                  HeartbeatTimeout:0s,                                           
                                  WorkflowTaskCompletedEventId:4,                                
                                  RetryPolicy:{InitialInterval:1s,                               
                                  BackoffCoefficient:2,                                          
                                  MaximumInterval:1m40s,                                         
                                  MaximumAttempts:0,                                             
                                  NonRetryableErrorTypes:[]}}                                    
   6  ActivityTaskStarted         {ScheduledEventId:5, Identity:7576@fedora@,                    
                                  RequestId:9a1da09d-2b00-4c46-a945-9e24adc10c04,                
                                  Attempt:1}                                                     
   7  ActivityTaskCompleted       {Result:[&quot;Hello                                                
                                  Temporal!&quot;],                                                   
                                  ScheduledEventId:5,                                            
                                  StartedEventId:6,                                              
                                  Identity:7576@fedora@}                                         
   8  WorkflowTaskScheduled       {TaskQueue:{Name:fedora:e78b4021-eed7-4cb9-880e-9a7354471838,  
                                  Kind:Sticky}, StartToCloseTimeout:10s, Attempt:1}              
   9  WorkflowTaskStarted         {ScheduledEventId:8, Identity:7576@fedora@,                    
                                  RequestId:8cc430d3-3d73-4261-961e-7dacd1b13005,                
                                  SuggestContinueAsNew:false, HistorySizeBytes:0}                
  10  WorkflowTaskCompleted       {ScheduledEventId:8, StartedEventId:9,                         
                                  Identity:7576@fedora@,                                         
                                  BinaryChecksum:0990e4a32efe7cd9c6c127b9cb51ecfc}               
  11  WorkflowExecutionCompleted  {Result:[&quot;Hello                                                
                                  Temporal!&quot;],                                                   
                                  WorkflowTaskCompletedEventId:10} 
&lt;/pre&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this article we discussed several options for running the Temporal server, including using the Temporal cloud. We walked through the steps; deploying a quick local development environment using temporalite. Finally we showed how to execute and get workflow details using both the Temporal CLI and UI. Best of luck in your journey to more reliable applications with Temporal! If you have any questions reach out on &lt;a href=&quot;https://temporal.io/slack&quot;&gt;Temporal community slack&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;(c) 2022 Keith Tenzer&lt;/p&gt;</content><author><name>Keith Tenzer</name></author><category term="Temporal" /><category term="Temporal" /><category term="Opensource" /><category term="Workflows" /><category term="Activities" /><category term="Go" /><summary type="html">Overview In this article we will walk through setup of a development environment for Temporal. There are of course, several ways you can run the Temporal server locally. You can use Docker compose, helm or an operator on minikube and temporalite. In addition you can also consume Temporal namespaces as-a-service via the Temporal cloud. I would recommend temporalite if you want to run the Temporal server disconnected on your laptop otherwise the Temporal cloud is definitely the best option. I will cover getting started with the Temporal cloud in a future post.</summary></entry><entry><title type="html">My First Day at Temporal</title><link href="http://localhost:4000/temporal/my-first-day-at-temporal/" rel="alternate" type="text/html" title="My First Day at Temporal" /><published>2022-08-15T00:00:00-07:00</published><updated>2022-08-15T00:00:00-07:00</updated><id>http://localhost:4000/temporal/my-first-day-at-temporal</id><content type="html" xml:base="http://localhost:4000/temporal/my-first-day-at-temporal/">&lt;p&gt;&lt;img src=&quot;/assets/2022-08-15/logo-temporal-with-copy.svg&quot; alt=&quot;Temporal&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Today is my first day at temporal and with that I wanted to share some thoughts around my decision, why Temporal and my experience thus far. As you can imagine, making any career change is always a very careful thought process and as we get older, have more responsibilities, the gravity around those decisions becomes stronger.&lt;/p&gt;

&lt;h2 id=&quot;why-temporal&quot;&gt;Why Temporal?&lt;/h2&gt;
&lt;p&gt;After IBM acquired Red Hat in 2018, I had the opportunity to visit the IBM research and development lab in Boeblingen, Germany. The idea was so RHatters could learn about some of the IBM history and why mainframes even today are great, at least with Linux. As you can imagine, I was more than skeptical. We spent years modernizing monolithic applications and moving workloads to Linux. Mainframe technology was a dinosaur that predated anything I ever worked on and as such I dismissed it, what could I possibly learn? Even once we arrived at the facility; the gray, lifeless, government looking buildings exhumed anything but innovation or creativity.&lt;/p&gt;

&lt;p&gt;Of course as the cliche goes, Looks can and are deceiving. Our minds judge everything and blind us if we let them. So what did I learn on that day? Reliability, you can shoot an IBM mainframe with a high-powered weapon and it will continue to run. That is why still today, well into the era of cloud some of the most critical computing workflows (government, banks, aviation, insurance and healthcare) are still done on mainframes. One of the challenges with cloud-native is of course, reliability across many disparate services. Monolithic environments are simply much easier to control and anticipate. Developers are left to solve reliability on their own with 10s or even 100s of microservices potentially involved in a workflow. Of course, this ends up being a lot of code, technical debt and reliability is never quite certain, certainly not to the level of a mainframe.&lt;/p&gt;

&lt;h3 id=&quot;opportunity&quot;&gt;Opportunity&lt;/h3&gt;
&lt;p&gt;Temporal is solving this problem through a workflow-as-code approach. Handling workflow state, execution, retries and allowing developers to orchestrate microservices into a workflow that is reliable as running water. It is a simple purpose with enormous potential. What could we accomplish if we further increased reliability of cloud-native applications and even started to modernize legacy applications shackled in their moorings? That has me extremely excited!&lt;/p&gt;

&lt;h3 id=&quot;execution&quot;&gt;Execution&lt;/h3&gt;
&lt;p&gt;Still it is one thing to have great potential but it isn’t worth that much if you can’t execute. It is far better to have great execution of a poor plan, than poor execution of a great plan. At the end of the day, execution comes down to people and the will to overcome adversity or whatever else gets thrown in the way. My interview process at Temporal was not an interview at all. It was a conversation involving sharing ideas and thoughts bi-directionally. Everyone I met was passionate, focused, professional and curious. All seemed united, being part of the same cause but each with their own role to play, led by their passion.&lt;/p&gt;

&lt;h3 id=&quot;opensource&quot;&gt;Opensource&lt;/h3&gt;
&lt;p&gt;While I needed to understand Temporal’s purpose, believe the vision could be executed, there is still something more. For me, especially coming from Red Hat, the technology needed to be opensource. I saw a great interview with Maxim Fateev (CEO of Temporal) where he explains the reasoning why Temporal needs to be opensource? He says that anything developer orientated today, needs to be opensource or it simply won’t be seriously considered. This was not only reassuring but also gave me great satisfaction in just how far opensource adoption has come. The problems of today are simply bigger than any one company and opensource is the only way to collaborate together across organizations, to solve them.&lt;/p&gt;

&lt;h3 id=&quot;the-right-fit&quot;&gt;The Right Fit&lt;/h3&gt;
&lt;p&gt;Finally the role also had to be the right fit. I had a desire to get closer to developers after being mostly on the platform side the last 7+ years. I am in fact a closet software hacker, it is one of the things I do for fun. The solution architect role at Temporal required someone of course with pre-sales skills but also software development skills (a less common combination). Temporal is consumed through an SDK by developers and as such being able to provide recommended practice or guidance to help customers also means understanding the code. I have long waited for a pre-sales role where my software development skills and passion were not just an asset but a requirement.&lt;/p&gt;

&lt;h2 id=&quot;my-experience-so-far&quot;&gt;My Experience So Far&lt;/h2&gt;
&lt;p&gt;It might seem odd to talk about experience when this is literally day one but there is actually a lot to talk about. I have been on Temporal’s community Slack for several weeks. Following some of the discussions and after I accepted an offer folks started to reach out. Even asking for my feedback or ideas about certain things. I attended a pre-screen of a 101 training session that will be presented at Replay, Temporal’s first user conference August 25-26 in Seattle. I even got to provide feedback all before my actual start date. I have had my laptop now for about two weeks which I am proud to say runs Fedora 36 (Gnome 42) and the Friday before my start date I had all my accounts setup. It is day 1 and I am ready to rock!&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this article I discussed thoughts around my decision to make a career change and pursue a new opportunity at Temporal. I am extremely excited about taking my first steps on this new journey and cannot wait to discover what is around the corner. I hope that my perspective may be of help, in your career or life journey.&lt;/p&gt;

&lt;p&gt;(c) 2022 Keith Tenzer&lt;/p&gt;</content><author><name>Keith Tenzer</name></author><category term="Temporal" /><category term="Temporal" /><category term="Opensource" /><category term="Workflows" /><category term="Activities" /><summary type="html">Overview Today is my first day at temporal and with that I wanted to share some thoughts around my decision, why Temporal and my experience thus far. As you can imagine, making any career change is always a very careful thought process and as we get older, have more responsibilities, the gravity around those decisions becomes stronger.</summary></entry><entry><title type="html">Blog with Gitops Practices and GitHub</title><link href="http://localhost:4000/linux/blog-with-gitops-practices-and-github/" rel="alternate" type="text/html" title="Blog with Gitops Practices and GitHub" /><published>2022-02-10T00:00:00-08:00</published><updated>2022-02-10T00:00:00-08:00</updated><id>http://localhost:4000/linux/blog-with-gitops-practices-and-github</id><content type="html" xml:base="http://localhost:4000/linux/blog-with-gitops-practices-and-github/">&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Want to build your brand, while living the gitops revolution and not paying anything for it? That is exactly what this article will walk you through. I will show you how to setup your customized blog hosted in Github and create blog posts, doing it the way developers do it, one commit at a time.&lt;/p&gt;

&lt;p&gt;Now most people go to wordpress or some blog platform and pay money for it. I think that is a shame because it is a missed opportunity to live gitops which is at the center of automating everything. Github has long provided the ability to create a basic blogs using gh-pages. Github uses a framework written in ruby called Jekyll which is available outside of Github, it’s opensource. Jekyll provides a templating framework and handles all the html, css stuff, allowing you to just write your blog in markdown (which anyone can do). The Jekyll themes Github provides are seriously limited and no fun. Most people don’t know this but with a little extra effort you can actually use any Jekyll themes and there are 100’s if not 1000’s so you can create your brand just the way you like it.&lt;/p&gt;

&lt;h2 id=&quot;setup-github-repository&quot;&gt;Setup Github Repository&lt;/h2&gt;
&lt;p&gt;First if you don’t have a &lt;a href=&quot;https://github.com/&quot;&gt;github&lt;/a&gt; account get one. Next once you have an account create a new repository. This should be the name of the blog. In my case my blog is keithtenzer.com so that is also the name of the &lt;a href=&quot;https://github.com/ktenzer/keithtenzer.com&quot;&gt;repository&lt;/a&gt;. Next decide if you are okay with a github domain name or if you want your own custom domain. I strongly recommend you pay the $10 a year and get your name. It is your brand after all and using your name makes it that way and also allows you to blog about anything.&lt;/p&gt;

&lt;h2 id=&quot;create-your-domain-optional&quot;&gt;Create Your Domain (optional)&lt;/h2&gt;
&lt;p&gt;If you aren’t interested in your own domain you can skip this part.
Decide where you want your domain to be hosted. I strongly recommend &lt;a href=&quot;https://www.cloudflare.com/&quot;&gt;cloudflare&lt;/a&gt;. Purchase your domain by selecting buy domain.
&lt;img src=&quot;/assets/2022-02-10/buy_domain.png&quot; alt=&quot;Buy Domain&quot; /&gt;
Once you have a domain you need to point it at Github. Under websites, click DNS and configure DNS as follows. Simply replace &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;keithtenzer.com&lt;/code&gt; with your domain. Also make sure you add a CNAME for www to &amp;lt;pre&amp;gt;username.github.io&amp;lt;/pre&amp;gt;.
&lt;img src=&quot;/assets/2022-02-10/buy_domain.png&quot; alt=&quot;Buy Domain&quot; /&gt;
Once DNS changes are done it can take 24 hours to propagate so I recommend waiting. You can check by querying DNS using nslookup or dig and seeing when your domain shows the IPs you entered for Github.&lt;/p&gt;

&lt;h2 id=&quot;initialize-blog-in-github&quot;&gt;Initialize Blog in Github&lt;/h2&gt;
&lt;p&gt;Log in to Github and under repositories and create a new one.
&lt;img src=&quot;/assets/2022-02-10/blog_repo.png&quot; alt=&quot;Blog Repository&quot; /&gt;
Clone your repository by copying the code link and running it in a CLI terminal.&lt;/p&gt;
&lt;pre&gt;$ git clone https://github.com/ktenzer/helloblog.git&lt;/pre&gt;
&lt;p&gt;You may need to install &lt;a href=&quot;https://github.com/git-guides/install-git&quot;&gt;git&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;enable-blog&quot;&gt;Enable Blog&lt;/h2&gt;
&lt;p&gt;Once the repository is initialized we can enable the blog by going to settings. Under pages you simply need to select the branch (main) and fill out your custom domain if you have one.
&lt;img src=&quot;/assets/2022-02-10/enable_blog.png&quot; alt=&quot;Enable Blog&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;select-a-jekyll-themes&quot;&gt;Select a Jekyll Themes&lt;/h2&gt;
&lt;p&gt;As I mentioned there are many Jekyll themes. You can search for curated ones &lt;a href=&quot;https://jekyllthemes.io/&quot;&gt;here&lt;/a&gt;. For my blog I use &lt;a href=&quot;https://mmistakes.github.io/minimal-mistakes/&quot;&gt;minimal-mistakes&lt;/a&gt;. Once you have chosen a theme simply clone it from Github in a separate directory from where you cloned your blog following same steps as above.&lt;/p&gt;
&lt;pre&gt;$ git clone https://github.com/mmistakes/minimal-mistakes.git&lt;/pre&gt;
&lt;p&gt;Next copy the contents into your blog repository.&lt;/p&gt;
&lt;pre&gt;cp -r minimal-mistakes/* helloblog/&lt;/pre&gt;

&lt;h2 id=&quot;customize-blog&quot;&gt;Customize Blog&lt;/h2&gt;
&lt;p&gt;Now that you have a theme it is time to customize. Under your blog directory edit the _config.yml. You need to uncomment the remote_theme. In addition you can add title, author information and whatever else.&lt;/p&gt;
&lt;pre&gt;$ vi _config.yml
remote_theme           : &quot;mmistakes/minimal-mistakes&quot;
&lt;/pre&gt;

&lt;h2 id=&quot;setup-jekyll&quot;&gt;Setup Jekyll&lt;/h2&gt;
&lt;p&gt;Now that you have a blog, you are dying to publish it right? Not so fast, now it is time to test it locally. After all you wouldn’t want to publish anything you didn’t review first, right? In order to test locally you need to install and setup jekyll. This part is much easier if you are running Linux like me. If not maybe you should also consider &lt;a href=&quot;https://keithtenzer.com/linux/The-Fedora-Workstation-Experience/&quot;&gt;switching&lt;/a&gt; to Linux, your Macbook is holding you back from greatness (trust me). You could also run Fedora as a virtual machine, well as long as you don’t have one of those shiny M1 Macbook’s (maybe someone will make that work in future though).
Either way below are the steps for Fedora Linux.&lt;/p&gt;
&lt;pre&gt;$ cd helloblog
$ sudo dnf install ruby
$ sudo dnf install -y ruby-devel
$ sudo dnf install -y g++
$ bundler install
$ gem install eventmachine -v &apos;1.2.7&apos; -- --with-cppflags=-I/usr/local/opt/openssl/include
$ gem install jekyll bundler
$ sudo gem update --system 3.2.30 --install-dir=/usr/share/gems --bindir /usr/local/bin
$ sudo gem pristine ffi --version 1.15.4
$ sudo gem pristine http_parser.rb --version 0.8.0
$ sudo gem pristine sassc --version 2.4.0
$ bundle install
$ bundle add webrick
&lt;/pre&gt;

&lt;p&gt;It may want you to pristine different version of gems. If that is the case just change the version to what it wants.&lt;/p&gt;

&lt;h2 id=&quot;test-blog&quot;&gt;Test Blog&lt;/h2&gt;
&lt;p&gt;Now that Jekyll is setup we can test our new blog.&lt;/p&gt;
&lt;pre&gt;$ bundle exec jekyll serve

Configuration file: /home/ktenzer/helloblog/_config.yml
            Source: /home/ktenzer/helloblog
       Destination: /home/ktenzer/helloblog/_site
 Incremental build: disabled. Enable with --incremental
      Generating... 
       Jekyll Feed: Generating feed for posts
                    done in 0.192 seconds.
 Auto-regeneration: enabled for &apos;/home/ktenzer/helloblog&apos;
    Server address: http://127.0.0.1:4000
  Server running... press ctrl-c to stop.
&lt;/pre&gt;
&lt;p&gt;Once Jekyll is running simply open your web browser of choice and go to the server address link.
&lt;img src=&quot;/assets/2022-02-10/hello_blog.png&quot; alt=&quot;Hello World Blog&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;push-blog-to-github&quot;&gt;Push Blog to Github&lt;/h1&gt;
&lt;p&gt;Now that we have tested our blog and are happy lets commit the code! If you want to brush up on Github I recommend the &lt;a href=&quot;https://docs.github.com/en/get-started&quot;&gt;tutorials&lt;/a&gt;.
First lets add our files locally from within our blog directory.&lt;/p&gt;
&lt;pre&gt;$ git add .&lt;/pre&gt;
&lt;p&gt;To see the files that will be commited (optional).&lt;/p&gt;
&lt;pre&gt;$ git status -s&lt;/pre&gt;
&lt;p&gt;Commit files locally.&lt;/p&gt;
&lt;pre&gt;$ git commit -a -m &quot;first blog entry&quot;&lt;/pre&gt;
&lt;p&gt;Push changes into Github.&lt;/p&gt;
&lt;pre&gt;$ git push origin&lt;/pre&gt;
&lt;p&gt;In order to push changes you will need a token. You can create a token under account settings-&amp;gt;developer settings-&amp;gt;personal access tokens in Github.
Once changes are pushed it goes through CI/CD (Github Actions) and then is published. It usually takes just few minutes. You can check the status by looking at workflows under actions. Once the action which does CI/CD is complete check out your blog in production &lt;a href=&quot;https://ktenzer.github.io/helloblog/&quot;&gt;hello blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2022-02-10/actions.png&quot; alt=&quot;Github Actions&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;add-post-to-blog&quot;&gt;Add Post to Blog&lt;/h2&gt;
&lt;p&gt;Now that the blog is up and running let’s create a blog entry. Create the _posts directory and then create a new file with the format YYYY-MM-DD-post-name.md. Add the header information that defines title, layout, categories and tags. Layouts are located in the _layouts directory and provided by the Jekyll theme.&lt;/p&gt;
&lt;pre&gt;$ mkdir _posts&lt;/pre&gt;
&lt;pre&gt;$ vi _posts/2022-02-10-hello-world.md&lt;/pre&gt;
&lt;pre&gt;--- 
layout: single
title:  &quot;Hello World&quot;
categories:
- Linux
tags:
- Linux
- Fedora
- Opensource
---
![Hello World Image](/assets/2022-02-10/hello_world_md.png)
## Overview
This is a hellow world blog
## Hello
## World
## Summary
This was a hello world blog
(c) 2022 Keith Tenzer&lt;/pre&gt;

&lt;h3 id=&quot;add-image-to-post&quot;&gt;Add Image to Post&lt;/h3&gt;
&lt;p&gt;Images, videos and other artifacts go in the assets directory. It is best to organize under the date of your post so you can find stuff.
First create a directory.&lt;/p&gt;
&lt;pre&gt;$ mkdir assets/2022-02-10&lt;/pre&gt;
&lt;p&gt;Next copy image to that directory and then you can link it in your post using the /assets/2022-02-10 path.&lt;/p&gt;
&lt;pre&gt;$ cp /home/ktenzer/Pictures/hello_world_md.png assets/2022-02-10/ &lt;/pre&gt;

&lt;h2 id=&quot;test-post&quot;&gt;Test Post&lt;/h2&gt;
&lt;p&gt;Simply run Jekyll again to test locally.&lt;/p&gt;
&lt;pre&gt;$ bundle exec jekyll serve&lt;/pre&gt;
&lt;h2 id=&quot;push-blog-post-to-github&quot;&gt;Push Blog Post to Github&lt;/h2&gt;
&lt;p&gt;Repeat same steps above when we originally pushed our first commit. 
First lets add our files locally from without our blog directory.&lt;/p&gt;
&lt;pre&gt;$ git add .&lt;/pre&gt;
&lt;p&gt;To see the files that will be commited (optional).&lt;/p&gt;
&lt;pre&gt;$ git status -s&lt;/pre&gt;
&lt;p&gt;Commit files locally.&lt;/p&gt;
&lt;pre&gt;$ git commit -a -m &quot;first blog entry&quot;&lt;/pre&gt;
&lt;p&gt;Push changes into Github.&lt;/p&gt;
&lt;pre&gt;$ git push origin&lt;/pre&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;Blogging is a great way to build your own brand and be an ambassador for what is important to you. It allows you to share your experiences and knowledge with the rest of the world. In this article we went through the steps to create your own unique blog platform using Jekyll and hosting it in Github (for free). One thing I learned that I will always take with me, sharing is caring.&lt;/p&gt;

&lt;p&gt;(c) 2022 Keith Tenzer&lt;/p&gt;</content><author><name>Keith Tenzer</name></author><category term="Linux" /><category term="Linux" /><category term="Fedora" /><category term="Opensource" /><summary type="html">Overview Want to build your brand, while living the gitops revolution and not paying anything for it? That is exactly what this article will walk you through. I will show you how to setup your customized blog hosted in Github and create blog posts, doing it the way developers do it, one commit at a time.</summary></entry><entry><title type="html">The Fedora Workstation Experience</title><link href="http://localhost:4000/linux/The-Fedora-Workstation-Experience/" rel="alternate" type="text/html" title="The Fedora Workstation Experience" /><published>2022-01-10T00:00:00-08:00</published><updated>2022-01-10T00:00:00-08:00</updated><id>http://localhost:4000/linux/The-Fedora-Workstation-Experience</id><content type="html" xml:base="http://localhost:4000/linux/The-Fedora-Workstation-Experience/">&lt;p&gt;&lt;img src=&quot;/assets/2022-01-10/desktop.png&quot; alt=&quot;Fedora&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;A lot of people always ask me what is the best way to contribute to opensource? Of course contributing code, documentation, spreading the gospel or even talking about opensource are great. However the easiest and seemingly least obvious method is to actually use opensource. The best way to do that is by running Linux on your workstation. There are a lot of great Linux distro’s to choose from and me being a RHatter, well let’s just say I have an impartial view.&lt;/p&gt;

&lt;p&gt;Being an ex-macbooker, I love the simplicity and minimalist approach of Apple’s UI design and experience but I dislike their proprietary, closed system approach. As such I aim to find the best of both worlds: great UI experience without compromising on freedom, choice and promoting the power of opensource!&lt;/p&gt;

&lt;h2 id=&quot;hardware&quot;&gt;Hardware&lt;/h2&gt;
&lt;p&gt;Currently I have a Lenovo Thinkpad X1 but great thing here is you have choice. I get that Apple’s Macbook hardware has a great look and feel (touchpad works great) but those are pretty minor things and your going to cover your laptop in stickers anyway, right?&lt;/p&gt;

&lt;h2 id=&quot;installing-fedora-latest&quot;&gt;Installing Fedora Latest&lt;/h2&gt;
&lt;p&gt;Fedora has a &lt;a href=&quot;https://getfedora.org/en/workstation/download/&quot;&gt;media writer&lt;/a&gt; and you can download this to create a USB Image to install Fedora. You can provide your own ISO or just let it create an image using the latest Fedora. Once you have USB image simply reboot your laptop/desktop press F12 or F8 to enter boot menu and boot from your USB. The rest is just accepting defaults. If your more advanced user you may want to also setup your own filesystem partitions and I always recommend turning on disk encryption with LUKS.&lt;/p&gt;

&lt;h2 id=&quot;customizing-fedora-workstation&quot;&gt;Customizing Fedora Workstation&lt;/h2&gt;
&lt;p&gt;Once you get booted into Fedora it’s time to customize. Unlike MacOS or Windows you can truly customize the desktop environment and while that is powerful and rewarding it also can be time consuming as well as turn most people off. The point here is to get something great with limited effort.&lt;/p&gt;

&lt;h3 id=&quot;update-fedora&quot;&gt;Update Fedora&lt;/h3&gt;
&lt;p&gt;Just like any OS clicking the update button is usually the first step. You can of course click on Applications-&amp;gt;System Tools-&amp;gt;Software which launches the UI software package manager but this is Linux right?&lt;/p&gt;

&lt;pre&gt;$ sudo dnf update -y&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2022-01-10/update.png&quot; alt=&quot;Fedora&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;install-gnome-extentions&quot;&gt;Install GNOME Extentions&lt;/h3&gt;
&lt;p&gt;GNOME is the UI operating environment and has a modular plugin framework for extending it. There have been huge advances in GNOME performance and while developers do all the hard work, users make those contributions meaningful which in turns leads to more development resources.&lt;/p&gt;
&lt;pre&gt;$ sudo dnf install gnome-extensions-app&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2022-01-10/extentions.png&quot; alt=&quot;GNOME Extentions&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;install-dock-from-dash-extensions&quot;&gt;Install Dock from Dash Extensions&lt;/h4&gt;
&lt;p&gt;This extension will add a docking bar in center for favorite applications just like MacOS.
Navigate to &lt;a href=&quot;https://extensions.gnome.org/&quot;&gt;https://extensions.gnome.org/&lt;/a&gt; and in search filled enter “dock from dash”. Click on the extension and in upper right there is a slider to enable the extension.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2022-01-10/dock_and_dash.png&quot; alt=&quot;Dock and Dash&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;install-gnome-tweaks&quot;&gt;Install GNOME Tweaks&lt;/h4&gt;
&lt;p&gt;Using GNOME tweaks you can configure many aspects of the UI we will be using later&lt;/p&gt;

&lt;pre&gt;$ sudo dnf install -y gnome-tweaks&lt;/pre&gt;

&lt;h3 id=&quot;install-google-chrome&quot;&gt;Install Google Chrome&lt;/h3&gt;

&lt;pre&gt;$ sudo dnf install google-chrome&lt;/pre&gt;

&lt;h3 id=&quot;disable-wayland&quot;&gt;Disable Wayland&lt;/h3&gt;
&lt;p&gt;Overall wayland works but one area there is still issues is in screen sharing. If you care about that I would recommend disabling it.&lt;/p&gt;

&lt;pre&gt;
$ sudo vi /etc/gdm/custom.conf
WaylandEnable=false
&lt;/pre&gt;

&lt;h3 id=&quot;configure-macos-theme&quot;&gt;Configure MacOS Theme&lt;/h3&gt;
&lt;p&gt;These steps will enable MacOS icons and desktop theme.&lt;/p&gt;

&lt;pre&gt;$ sudo dnf install la-capitaine-icon-theme&lt;/pre&gt;

&lt;pre&gt;$ git clone https://github.com/paullinuxthemer/Mc-OS-themes.git&lt;/pre&gt;

&lt;pre&gt;$ mkdir ~/.themes&lt;/pre&gt;

&lt;pre&gt;$ cp -r Mc-OS-themes/McOS-MJV ~/.themes&lt;/pre&gt;

&lt;p&gt;Open GNOME Tweak tool Applications-&amp;gt;Utilities-&amp;gt;Tweak and navigate to appearance section. Set application theme to McOS-MJV and icons to La-Capitane. Navigate to window titlebars section and enable maximize/minimize under titlebar buttons. This adds buttons to all windows that let you maximize or minimize them.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2022-01-10/themes.png&quot; alt=&quot;Themes&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2022-01-10/appearance.png&quot; alt=&quot;Appearance&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2022-01-10/minimize_buttons.png&quot; alt=&quot;Window Buttons&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;install-rpm-fusion&quot;&gt;Install RPM Fusion&lt;/h3&gt;
&lt;p&gt;This is a tool that gives you access to a lot of community developed tools and is very useful for workstations.&lt;/p&gt;

&lt;pre&gt;$ sudo dnf install https://download1.rpmfusion.org/free/fedora/rpmfusion-free-release-$(rpm -E %fedora).noarch.rpm&lt;/pre&gt;

&lt;h3 id=&quot;optimize-battery-usage&quot;&gt;Optimize Battery Usage&lt;/h3&gt;
&lt;p&gt;There are additional drivers needed to ensure your battery is used efficiently. Not installing these leads to quicker battery drain.&lt;/p&gt;

&lt;pre&gt;$ sudo dnf install tlp tlp-rdw&lt;/pre&gt;

&lt;p&gt;For thinkpads add additional driver from RPM fusion.&lt;/p&gt;
&lt;pre&gt;$ dnf install https://download1.rpmfusion.org/free/fedora/rpmfusion-free-release-$(rpm -E %fedora).noarch.rpm
&lt;/pre&gt;

&lt;h3 id=&quot;install-multimedia-codecs&quot;&gt;Install Multimedia Codecs&lt;/h3&gt;
&lt;pre&gt;$ sudo dnf groupupdate multimedia --setop=&quot;install_weak_deps=False&quot; --exclude=PackageKit-gstreamer-plugin&lt;/pre&gt;
&lt;pre&gt;$ sudo dnf groupupdate sound-and-video&lt;/pre&gt;

&lt;h3 id=&quot;setup-solarized&quot;&gt;Setup Solarized&lt;/h3&gt;
&lt;p&gt;Solarized is a color scheme for terminal sessions. Let’s face it staring at a black screen with green text is bad and tiring for your eyes. These color patters are soothing and will let you enjoy starring at CLI terminals.&lt;/p&gt;
&lt;pre&gt;$ sudo dnf install -y p7zip&lt;/pre&gt;
&lt;pre&gt;$ mkdir ~/solarized&lt;/pre&gt;
&lt;pre&gt;$ cd ~/solarized&lt;/pre&gt;
&lt;pre&gt;$ git clone https://github.com/sigurdga/gnome-terminal-colors-solarized.git&lt;/pre&gt;
&lt;pre&gt;$ gnome-terminal-colors-solarized/install.sh&lt;/pre&gt;

&lt;p&gt;Setup vim and install solarized colors for vim.&lt;/p&gt;
&lt;pre&gt;$ sudo dnf install -y vim&lt;/pre&gt;
&lt;pre&gt;$ git clone https://github.com/altercation/vim-colors-solarized.git&lt;/pre&gt;
&lt;pre&gt;$ mkdir -p ~/.vim/colors&lt;/pre&gt;
&lt;pre&gt;$ cp vim-colors-solarized/colors/solarized.vim ~/.vim/colors/&lt;/pre&gt;

&lt;h3 id=&quot;install-vscode&quot;&gt;Install VSCODE&lt;/h3&gt;
&lt;p&gt;Pretty much the standard IDE for software development, infrastructure-as-code, blogging or anything that requires editing text.&lt;/p&gt;
&lt;pre&gt;$ sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc&lt;/pre&gt;

&lt;pre&gt;$ cat &amp;lt;&amp;lt;EOF | sudo tee /etc/yum.repos.d/vscode.repo
[code]
name=Visual Studio Code
baseurl=https://packages.microsoft.com/yumrepos/vscode
enabled=1
gpgcheck=1
gpgkey=https://packages.microsoft.com/keys/microsoft.asc
EOF
&lt;/pre&gt;

&lt;pre&gt;$ sudo dnf check-update&lt;/pre&gt;
&lt;pre&gt;$ sudo dnf install -y code&lt;/pre&gt;

&lt;h3 id=&quot;install-ansible-optional&quot;&gt;Install Ansible (Optional)&lt;/h3&gt;
&lt;p&gt;If you aren’t automating stuff with Ansible it is never to late to change your life.&lt;/p&gt;
&lt;pre&gt;$ sudo dnf install -y ansible&lt;/pre&gt;
&lt;pre&gt;$ sudo dnf install -y python-ansible-runner&lt;/pre&gt;
&lt;pre&gt;$ pip install ansible-runner-http&lt;/pre&gt;
&lt;pre&gt;$ pip install openshift&lt;/pre&gt;
&lt;pre&gt;$ ansible-galaxy collection install kubernetes.core&lt;/pre&gt;

&lt;h3 id=&quot;install-go-optional&quot;&gt;Install Go (Optional)&lt;/h3&gt;
&lt;p&gt;Golang is by far my language of choice for it’s simplicity and elegance.&lt;/p&gt;
&lt;pre&gt;$ mkdir -p ~/go/src/github.com&lt;/pre&gt;
&lt;pre&gt;$ sudo dnf install -y go&lt;/pre&gt;
&lt;pre&gt;$ vi ~/.bash_profile
export GOBIN=/home/username
export GOPATH=/home/username/src
export EDITOR=vim
&lt;/pre&gt;

&lt;h3 id=&quot;install-openshift-and-kubectl-optional&quot;&gt;Install OpenShift and Kubectl (Optional)&lt;/h3&gt;
&lt;p&gt;Likely there are newer releases so grab latest.
&lt;a href=&quot;https://access.redhat.com/downloads/content/290/ver=4.9/rhel---8/4.9.13/x86_64/product-software&quot;&gt;OpenShift and Kubectl 4.9&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;install-operator-sdk-optional&quot;&gt;Install Operator SDK (Optional)&lt;/h3&gt;
&lt;p&gt;If you want to put all those Ansible skills to use in cloud-native world start writing Operators.&lt;/p&gt;

&lt;p&gt;Likely there are newer releases so grab latest or release for your OCP release.
&lt;a href=&quot;https://docs.openshift.com/container-platform/4.9/operators/operator_sdk/osdk-installing-cli.html#osdk-installing-cli-linux-macos_osdk-installing-cli&quot;&gt;Operator Framework 4.9&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this article we discussed the importance of choice and at least why you might consider Linux for your next workstation operating system. We also go through a step-by-step guide in configuring your Fedora workstation and give it similar look and feel to MacOS. If I only convinced one person to give Linux a try then it was all worth it!&lt;/p&gt;

&lt;p&gt;(c) 2022 Keith Tenzer&lt;/p&gt;</content><author><name>Keith Tenzer</name></author><category term="Linux" /><category term="Linux" /><category term="Fedora" /><category term="Opensource" /><summary type="html">Overview A lot of people always ask me what is the best way to contribute to opensource? Of course contributing code, documentation, spreading the gospel or even talking about opensource are great. However the easiest and seemingly least obvious method is to actually use opensource. The best way to do that is by running Linux on your workstation. There are a lot of great Linux distro’s to choose from and me being a RHatter, well let’s just say I have an impartial view.</summary></entry><entry><title type="html">Building Ansible Operators 1-2-3</title><link href="http://localhost:4000/openshift/building-ansible-operators-1-2-3/" rel="alternate" type="text/html" title="Building Ansible Operators 1-2-3" /><published>2021-12-03T00:00:00-08:00</published><updated>2021-12-03T00:00:00-08:00</updated><id>http://localhost:4000/openshift/building-ansible-operators-1-2-3</id><content type="html" xml:base="http://localhost:4000/openshift/building-ansible-operators-1-2-3/">&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;In this article we will go step by step to build a Kubernetes Operator using Ansible and the Operator Framework. Operators provide the ability to not only deploy an application but also manage the entire lifecycle while also baking in higher degrees of intelligence as well as resilience. Before we get to the Operator lets briefly revisit the ways we can deploy and manage applications in k8s.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Deployment&lt;/li&gt;
  &lt;li&gt;Deployment Template&lt;/li&gt;
  &lt;li&gt;Helm&lt;/li&gt;
  &lt;li&gt;Operator&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A k8s deployment is the simplest method but there is no way to parameterize unless you are doing it through Ansible k8s module or something else that handles that. A deployment template does provide parameterization but is only available on OpenShift and also doesn’t handle packaging or management of the application itself. Helm provides parameterization and also packaging but doesn’t provide application lifecycle management or allow for building extended intelligence for added resilience. Operators provide the full package and provide the pattern to improve operability over time. Some suggest just using the simplest tool for the job. If you just need to deploy an app for example and don’t need parameterization, use a k8s deployment. Personally my view is that with Ansible, Operators are just about as easy as a k8s deployment but with so much added benefit that the Operator approach always makes sense. My hope and goal with this article is to maybe influence a few more Operators and show that it isn’t really any additional work.&lt;/p&gt;

&lt;h2 id=&quot;pre-requisites&quot;&gt;Pre-requisites&lt;/h2&gt;
&lt;p&gt;Your starting point should be an application you can deploy using a k8s deployment or deployment config. From there the next thing is to setup a development environment for building Operators using the Operator Framework and Ansible.
&lt;a href=&quot;https://docs.openshift.com/container-platform/4.8/operators/operator_sdk/ansible/osdk-ansible-quickstart.html&quot;&gt;Operator Pre-requisites&lt;/a&gt;
Once you have operator-sdk and openshift or kubectl client you need some Ansible dependencies.&lt;/p&gt;
&lt;pre&gt;$ sudo dnf install -y ansible&lt;/pre&gt;
&lt;pre&gt;$ sudo dnf install -y python-ansible-runner&lt;/pre&gt;
&lt;pre&gt;$ sudo dnf install -y python-ansible-runner&lt;/pre&gt;
&lt;pre&gt;$ sudo pip install ansible-runner-http&lt;/pre&gt;
&lt;pre&gt;$ sudo pip install openshift&lt;/pre&gt;
&lt;pre&gt;$ ansible-galaxy collection install kubernetes.core &lt;/pre&gt;
&lt;pre&gt;$ ansible-galaxy collection install operator_sdk.util &lt;/pre&gt;

&lt;h2 id=&quot;create-operator-scaffolding&quot;&gt;Create Operator Scaffolding&lt;/h2&gt;
&lt;p&gt;Before we can begin coding the Operator we need boiler plate code and thankfully the operator sdk does all that.&lt;/p&gt;
&lt;pre&gt;$ operator-sdk init --plugins=ansible --domain=kubecraft.com&lt;/pre&gt;
&lt;pre&gt;$ operator-sdk create api     --group cache     --version v1     --kind Kubecraft     --generate-role&lt;/pre&gt;

&lt;h2 id=&quot;customizing-operator&quot;&gt;Customizing Operator&lt;/h2&gt;
&lt;p&gt;At this point we need an application. My approach is to first create a k8s deployment and test deploying my application before building the Operator. In this example we will use an app called &lt;a href=&quot;https://github.com/ktenzer/kubecraftadmin&quot;&gt;Kubekraft&lt;/a&gt;. It is a fun app that connects k8s world to minecraft through a minecraft websocket server written in Go. Browse to the yaml folder and you will see the k8s deployment.yaml. This is what we will use to build out our Operator.
Under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;roles/kubecraft/tasks&lt;/code&gt; directory we create tasks in Ansible to deploy what we had in the k8s deployment yaml which is a deployment, service and route. In addition we added a task to get the application domain dynamically so we can build our route properly. This also demonstrates how to query and use other k8s resources in our Ansible code.&lt;/p&gt;

&lt;p&gt;In addition, if your operator creates k8s resources you need to ensure proper permissions. Under the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;config/rbac&lt;/code&gt; directory you can add permissions to the role.yaml. For this operator I added services and routes so that those resources  can be created by the Operator.&lt;/p&gt;

&lt;h2 id=&quot;testing-operator&quot;&gt;Testing Operator&lt;/h2&gt;
&lt;p&gt;The operator sdk provides a simple way to test the operator locally. Once we have our tasks complete under the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;role/project/tasks&lt;/code&gt; directory, we simply need to create a new project, run the operator locally and create a custom resource (CR) which executes our role and tasks we defined.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Create project&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;$ oc new-project kubecraft&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Run operator locally&lt;/strong&gt;
Run these commands from the root directory of your operator project where the Makefile resides.&lt;/p&gt;
&lt;pre&gt;$ make install&lt;/pre&gt;
&lt;pre&gt;$ make run&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Create custom resource&lt;/strong&gt;
In a new terminal create CR. Also note that our operator expects as user input, a comma separated list of namespaces to monitor. User input is parameterized via the custom resource so if you look inside you will see the namespaces parameter set and being consumed within the Ansible role.&lt;/p&gt;
&lt;pre&gt;$ oc create -f config/samples/cache_v1_kubecraft.yaml&lt;/pre&gt;

&lt;h2 id=&quot;building-operator&quot;&gt;Building Operator&lt;/h2&gt;
&lt;p&gt;Once we have tested the operator locally we can build and publish our operator to a registry.
&lt;strong&gt;Authenticate to registry&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;$ sudo docker login https://registry.redhat.io&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Build operator image and push to registry&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;$ sudo make docker-build docker-push IMG=quay.io/ktenzer/kubecraft-operator:latest&lt;/pre&gt;
&lt;p&gt;If you are using quay.io as your registry make sure to login and make the image is public so it can be accessed.&lt;/p&gt;

&lt;h2 id=&quot;running-operator&quot;&gt;Running Operator&lt;/h2&gt;
&lt;p&gt;Now that we have the operator tested and the image built we can simply deploy it, create a CR and rule the world!
 The operator sdk makes this really easy and streamlines everything into a single command.&lt;/p&gt;
&lt;pre&gt;$ make deploy IMG=quay.io/ktenzer/kubecraft-operator:latest&lt;/pre&gt;

&lt;p&gt;BY default the operator will be installed into a project &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;operatorName-system&lt;/code&gt; however you can change that by updating the project name in the PROJECT file under the root of the operator project. In this case we changed it to kubecraft-operator.&lt;/p&gt;

&lt;p&gt;We can remove the operator also using make.&lt;/p&gt;
&lt;pre&gt;$ make undeploy&lt;/pre&gt;

&lt;h2 id=&quot;create-operator-bundle&quot;&gt;Create Operator Bundle&lt;/h2&gt;
&lt;p&gt;The operator bundle allows integration with operator lifecycle manager (olm) which provides a facility for upgrading operator seamlessly as well as integrating with operator hub. First we will generate the bundle boiler plate.&lt;/p&gt;
&lt;pre&gt;$ make bundle&lt;/pre&gt;
&lt;p&gt;If you want to change anything, like add image you can update &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bundle/manifests&lt;/code&gt; clusterserviceversion. When your ready you will build bundle and then push it to your repository. Remember if using quay.io to make the image public.&lt;/p&gt;
&lt;pre&gt;$ sudo make bundle-build BUNDLE_IMG=quay.io/ktenzer/kubecraft-operator-bundle:latest&lt;/pre&gt;
&lt;p&gt;Push bundle to registry.&lt;/p&gt;
&lt;pre&gt;$ sudo docker push quay.io/ktenzer/kubecraft-operator-bundle:latest&lt;/pre&gt;
&lt;h2 id=&quot;run-operator-bundle&quot;&gt;Run operator bundle&lt;/h2&gt;
&lt;p&gt;Now that we have the bundle built and pushed to registry we can deploy it to a cluster.&lt;/p&gt;
&lt;pre&gt;$ operator-sdk run bundle -n kubecraft-operator quay.io/ktenzer/kubecraft-operator-bundle:latest&lt;/pre&gt;
&lt;p&gt;Once our operator bundle is running simply create a new project, a CR and watch the magic happen.&lt;/p&gt;
&lt;pre&gt;$ oc project kubecraft&lt;/pre&gt;
&lt;pre&gt;$ oc create -f config/samples/cache_v1_kubecraft.yaml&lt;/pre&gt;
&lt;pre&gt;$ oc get deployment
NAME        READY   UP-TO-DATE   AVAILABLE   AGE
kubecraft   1/1     1            1           5s
&lt;/pre&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this article we created an operator using the operator sdk and Ansible. We saw how to take a simple k8s deployment and turn it into a fully managed operator. Finally we created a bundle showing the integration with operator lifecycle manager and operator hub. Hopefully this article helps you get started with Ansible operators and next time you deploy an application in k8s you would consider the operator approach.&lt;/p&gt;

&lt;p&gt;(c) 2021 Keith Tenzer&lt;/p&gt;</content><author><name>Keith Tenzer</name></author><category term="OpenShift" /><category term="Containers" /><category term="OpenShift" /><category term="Operators" /><category term="Operator Framework" /><category term="Ansible" /><summary type="html">Overview In this article we will go step by step to build a Kubernetes Operator using Ansible and the Operator Framework. Operators provide the ability to not only deploy an application but also manage the entire lifecycle while also baking in higher degrees of intelligence as well as resilience. Before we get to the Operator lets briefly revisit the ways we can deploy and manage applications in k8s. Deployment Deployment Template Helm Operator</summary></entry><entry><title type="html">OpenShift Service Mesh Getting Started Guide</title><link href="http://localhost:4000/openshift/openshift-service-mesh-getting-started-guide/" rel="alternate" type="text/html" title="OpenShift Service Mesh Getting Started Guide" /><published>2021-04-27T00:00:00-07:00</published><updated>2021-04-27T00:00:00-07:00</updated><id>http://localhost:4000/openshift/openshift-service-mesh-getting-started-guide</id><content type="html" xml:base="http://localhost:4000/openshift/openshift-service-mesh-getting-started-guide/">&lt;h2&gt;&lt;/h2&gt;
&lt;p&gt;&lt;!-- wp:image {&quot;id&quot;:12890,&quot;width&quot;:79,&quot;height&quot;:83,&quot;sizeSlug&quot;:&quot;large&quot;,&quot;linkDestination&quot;:&quot;media&quot;} --&gt;&lt;/p&gt;
&lt;figure class=&quot;wp-block-image size-large is-resized&quot;&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2018/12/OpenShift-LogoType.svg_.png&quot;&gt;&lt;img src=&quot;/assets/2021/04/OpenShift-LogoType.svg_.png?w=959&quot; alt=&quot;&quot; class=&quot;wp-image-12890&quot; width=&quot;79&quot; height=&quot;83&quot; /&gt;&lt;/a&gt;&lt;/figure&gt;
&lt;p&gt;&lt;!-- /wp:image --&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2018/12/OpenShift-LogoType.svg_.png&quot;&gt;&lt;!-- /wp:image --&gt;&lt;/a&gt;&lt;!-- /wp:image --&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/04/istio.png&quot;&gt;&lt;!-- /wp:image --&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;!-- /wp:image --&gt;&lt;/h2&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;In this article we will explore the OpenShift Service Mesh and deploy a demo application to better understand the various concepts. First you might be asking yourself, why do I need a Service Mesh? If you have a microservice architecture then you likely have many services that interact with one another in various ways. If a downstream service breaks, how will you know? How can you trace an error through all services to find out where it originated? How will you manage exposing new APIs/capabilities to users? Service Mesh provides the answer to those questions providing 1) Visibility across all microservices 2) Traceability through all of the microservice interactions and 3) Ruleset governing service versions and capabilities that are introduced into the environment. OpenShift Service Mesh uses Istio as the mesh, Kiali for the dashboard and Jaeger for traceability. &lt;/p&gt;
&lt;p&gt;&lt;!--more--&gt;&lt;/p&gt;
&lt;h2&gt;Install OpenShift Service Mesh&lt;/h2&gt;
&lt;p&gt;OpenShift Service Mesh enables visibility of traffic as is traverses all services within the mesh. In case of an error you can not only see what service is actually the source for the error but also are able to trace the API communications between services. Finally it allows us to implement rules, like balancing traffic between multiple ratings API endpoints or even sending specific users to an API version. You can even create a chaos monkey using the ruleset that will inject various errors so you can observe how failures are handled. A Service Mesh is critical for any complex microservice application and without it you are literally flying blind while adding technical debt unable to manage or monitor service interactions properly.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Create a new project&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First we will create a project for hosting the service mesh control plane.&lt;/p&gt;
&lt;pre&gt;$ oc new-project bookinfo-mesh&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Install OpenShift Service Mesh&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under the project booking-mesh go to Operator Hub and install Red Hat OpenShift Service Mesh, Red Hat OpenShift Jaeger and Kiali Operator.&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:image {&quot;id&quot;:14769,&quot;sizeSlug&quot;:&quot;large&quot;,&quot;linkDestination&quot;:&quot;media&quot;} --&gt;&lt;/p&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/04/operators_servicemesh.png&quot;&gt;&lt;img class=&quot;wp-image-14769&quot; src=&quot;/assets/2021/04/operators_servicemesh.png?w=1024&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/figure&gt;
&lt;p&gt;&lt;!-- /wp:image --&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Configure OpenShift Service Mesh&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Create a service mesh control plane using defaults.&lt;/p&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/04/booking_mesh-1.png&quot;&gt;&lt;img class=&quot;wp-image-14742&quot; src=&quot;/assets/2021/04/booking_mesh-1.png?w=1024&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;Create a service mesh member. Update the yaml and change namespace under controlPlaneRef to bookinfo-mesh.&lt;/figure&gt;
&lt;pre&gt;apiVersion: maistra.io/v1&lt;br /&gt;kind: ServiceMeshMember&lt;br /&gt;metadata:&lt;br /&gt;  namespace: bookinfo-mesh&lt;br /&gt;  name: default&lt;br /&gt;spec:&lt;br /&gt;  controlPlaneRef:&lt;br /&gt;    name: basic&lt;br /&gt;    namespace: bookinfo-mesh&lt;/pre&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/04/mesh_role.png&quot;&gt;&lt;img class=&quot;wp-image-14744&quot; src=&quot;/assets/2021/04/mesh_role.png?w=1024&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;Finally create a service member role adding name of the project that will access the service mesh..&lt;/figure&gt;
&lt;div&gt;
&lt;pre&gt;apiVersion: maistra.io/v1&lt;br /&gt;kind: ServiceMeshMemberRoll&lt;br /&gt;metadata:&lt;br /&gt;  namespace: bookinfo-mesh&lt;br /&gt;  name: default&lt;br /&gt;spec:&lt;br /&gt;  members:&lt;br /&gt;    - bookinfo&lt;/pre&gt;
&lt;/div&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/04/mesh_member_role.png&quot;&gt;&lt;img class=&quot;wp-image-14746&quot; src=&quot;/assets/2021/04/mesh_member_role.png?w=1024&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/figure&gt;
&lt;h2&gt;Deploy Demo Application&lt;/h2&gt;
&lt;p&gt;In order to look into the capabilities provided by the OpenShift Service Mesh we will deploy a simple book review app. The application is static but shows reviews for a book and has several microservices. The product page is the entry point service. It provides information on books and their reviews. It accesses book reviews through the reviews service which is also accesses the ratings service downstream to allow users to give a book a rating. It also gets details about a book via the details service. Like a true polyglot all services are written in a different programming language. The ratings service provides three different API versions, v1 doesn&apos;t display ratings, v2 shows ratings in black and v3 shows ratings in red. The idea is all about innovating quickly and getting real user feedback to take the app in the right direction.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/04/demoapp.png&quot;&gt;&lt;img class=&quot;wp-image-14764&quot; src=&quot;/assets/2021/04/demoapp.png?w=1024&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Create a project for the book app.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Create a project to host the book app.&lt;/p&gt;
&lt;pre&gt;$ oc new-project bookinfo&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Deploy book app.&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;$ oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-2.0/samples/bookinfo/platform/kube/bookinfo.yaml&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Create a service mesh gateway.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Once the app is deployed we need to create a gateway and setup the URI matches.&lt;/p&gt;
&lt;pre&gt;$ oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-2.0/samples/bookinfo/networking/bookinfo-gateway.yaml&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Create service mesh rule set.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Since the ratings service has 3 API versions we need some rules to govern the traffic.&lt;/p&gt;
&lt;pre&gt;$ oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-2.0/samples/bookinfo/networking/destination-rule-all.yaml&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Access the application.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Get the route to the application and add the /productpage to access via web browser.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ export GATEWAY_URL=$(oc -n istio-system get route istio-ingressgateway -o jsonpath=&apos;{.spec.host}&apos;)&lt;br /&gt;$ echo $GATEWAY_URL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;http://istio-ingressgateway-bookinfo-mesh.apps.ocp4.rh-southwest.com/productpage&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:image {&quot;id&quot;:14775,&quot;sizeSlug&quot;:&quot;large&quot;,&quot;linkDestination&quot;:&quot;media&quot;} --&gt;&lt;/p&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/04/bookinfo2.png&quot;&gt;&lt;img class=&quot;wp-image-14775&quot; src=&quot;/assets/2021/04/bookinfo2.png?w=1024&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/figure&gt;
&lt;p&gt;&lt;!-- /wp:image --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:image {&quot;id&quot;:14752,&quot;sizeSlug&quot;:&quot;large&quot;,&quot;linkDestination&quot;:&quot;media&quot;} --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:image --&gt;If you refresh the web page you should see the ratings change between no ratings, black and red. This is because the ruleset is sending traffic to all three API endpoints (v1, v2 and v3).&lt;/p&gt;
&lt;h2&gt;Update Service Mesh Ruleset&lt;/h2&gt;
&lt;p&gt;As mentioned we can create rules that enable us to send certain traffic to a different API endpoint. This could be for a canary deployment where we want to test how users interact with different capabilities (red or black ratings stars), to perform a blue/green deployment upgrading to a newer API version or even sending different users to different APIs.&lt;/p&gt;
&lt;p&gt;In this case we will simply apply a new rule that only sends traffic to v2 (black) and v3 (red) ratings API.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Apply a new ruleset.&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ oc replace -f https://raw.githubusercontent.com/istio/istio/master/samples/bookinfo/networking/virtual-service-reviews-v2-v3.yaml&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now when you access the book app and refresh you should see it switched between red and black ratings.&lt;/p&gt;
&lt;p&gt;Looking at Kiali versioned app graph we can see that traffic is only going to v2 and v3 as we would expect.&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:image {&quot;id&quot;:14760,&quot;sizeSlug&quot;:&quot;large&quot;,&quot;linkDestination&quot;:&quot;media&quot;} --&gt;&lt;/p&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;!-- wp:image {&quot;id&quot;:14784,&quot;sizeSlug&quot;:&quot;large&quot;,&quot;linkDestination&quot;:&quot;media&quot;} --&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/04/kiali_ratings_v2_v3-1.png&quot;&gt;&lt;img class=&quot;wp-image-14784&quot; src=&quot;/assets/2021/04/kiali_ratings_v2_v3-1.png?w=1024&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/figure&gt;
&lt;p&gt;&lt;!-- /wp:image --&gt;&lt;/p&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;!-- /wp:image --&gt;&lt;/p&gt;
&lt;h2&gt;Troubleshooting Errors&lt;/h2&gt;
&lt;p&gt;As mentioned one of the key values of the OpenShift Service Mesh is the visualization and tracing. In order to generate an error we will scale down the ratings v1 deployment to 0.&lt;/p&gt;
&lt;pre&gt;$ oc scale deployment/ratings-v1 -n bookinfo --replicas 0&lt;/pre&gt;
&lt;p&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/04/bookinfo_error.png&quot;&gt;&lt;img class=&quot;wp-image-14752&quot; src=&quot;/assets/2021/04/bookinfo_error.png?w=1024&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You should now see that the ratings service is currently unavailable when you refresh the app in a browser.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Check Kiali dashboard.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You can access Kiali via route under networking in the bookinfo-mesh project. In Kiali we clearly see the issue is the reviews service as we would expect. &lt;/p&gt;
&lt;p&gt;&lt;!-- wp:image {&quot;id&quot;:14751,&quot;sizeSlug&quot;:&quot;large&quot;,&quot;linkDestination&quot;:&quot;media&quot;} --&gt;&lt;/p&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/04/kali_error.png&quot;&gt;&lt;img class=&quot;wp-image-14751&quot; src=&quot;/assets/2021/04/kali_error.png?w=1024&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/figure&gt;
&lt;p&gt;&lt;!-- /wp:image --&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Open Jaeger to trace the calls that are failing.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Next we can dig into the requests by opening distributed tracing (Jaeger) from the Kiali dashboard. We can see the flow of all calls grouped and identify all request to the ratings service that are throwing an error. We see a 503 is returned which means service in unavailable.&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:image {&quot;id&quot;:14753,&quot;sizeSlug&quot;:&quot;large&quot;,&quot;linkDestination&quot;:&quot;media&quot;} --&gt;&lt;/p&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/04/jaeger_error.png&quot;&gt;&lt;img class=&quot;wp-image-14753&quot; src=&quot;/assets/2021/04/jaeger_error.png?w=1024&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/figure&gt;
&lt;p&gt;&lt;!-- /wp:image --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Kiali dashboard also shows request response (latency in ms) for slowest 50%, 95% and 99% of requests. This allows us to not only see the average latency but compare it to the slowest requests which indicates if we have spikes that could cause user slowness that we might not easily see when looking at just the average.&lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:paragraph --&gt; &lt;!-- wp:image {&quot;id&quot;:14779,&quot;sizeSlug&quot;:&quot;large&quot;,&quot;linkDestination&quot;:&quot;media&quot;} --&gt;&lt;/p&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/04/kali_error2.png&quot;&gt;&lt;img class=&quot;wp-image-14779&quot; src=&quot;/assets/2021/04/kali_error2.png?w=337&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/figure&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this article we discussed the capabilities OpenShift Service Mesh provides for microservice applications. We deployed a service mesh and a book demo polyglot application that leverages the OpenShift Service Mesh. Using Kiali we saw how to gain visibility into our microservice application and easily identify problems as well as trends. Through Jaeger and distributed tracing we were able to identify the exact API causing error conditions. Microservice architectures provide a lot of value but they are harder to manage, control and troubleshoot in certain aspects than their monolithic peers which is why OpenShift Service Mesh is so critical.&lt;/p&gt;
&lt;p&gt;(c) 2021 Keith Tenzer&lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:image --&gt;&lt;/p&gt;</content><author><name>Keith Tenzer</name></author><category term="OpenShift" /><category term="Containers" /><category term="istio" /><category term="jaeger" /><category term="kiali" /><category term="microservice" /><category term="OpenShift" /><category term="polyglot" /><category term="service mesh" /><summary type="html">Overview In this article we will explore the OpenShift Service Mesh and deploy a demo application to better understand the various concepts. First you might be asking yourself, why do I need a Service Mesh? If you have a microservice architecture then you likely have many services that interact with one another in various ways. If a downstream service breaks, how will you know? How can you trace an error through all services to find out where it originated? How will you manage exposing new APIs/capabilities to users? Service Mesh provides the answer to those questions providing 1) Visibility across all microservices 2) Traceability through all of the microservice interactions and 3) Ruleset governing service versions and capabilities that are introduced into the environment. OpenShift Service Mesh uses Istio as the mesh, Kiali for the dashboard and Jaeger for traceability.  Install OpenShift Service Mesh OpenShift Service Mesh enables visibility of traffic as is traverses all services within the mesh. In case of an error you can not only see what service is actually the source for the error but also are able to trace the API communications between services. Finally it allows us to implement rules, like balancing traffic between multiple ratings API endpoints or even sending specific users to an API version. You can even create a chaos monkey using the ruleset that will inject various errors so you can observe how failures are handled. A Service Mesh is critical for any complex microservice application and without it you are literally flying blind while adding technical debt unable to manage or monitor service interactions properly. Create a new project First we will create a project for hosting the service mesh control plane. $ oc new-project bookinfo-mesh Install OpenShift Service Mesh Under the project booking-mesh go to Operator Hub and install Red Hat OpenShift Service Mesh, Red Hat OpenShift Jaeger and Kiali Operator. Configure OpenShift Service Mesh Create a service mesh control plane using defaults. Create a service mesh member. Update the yaml and change namespace under controlPlaneRef to bookinfo-mesh. apiVersion: maistra.io/v1kind: ServiceMeshMembermetadata:  namespace: bookinfo-mesh  name: defaultspec:  controlPlaneRef:    name: basic    namespace: bookinfo-mesh Finally create a service member role adding name of the project that will access the service mesh.. apiVersion: maistra.io/v1kind: ServiceMeshMemberRollmetadata:  namespace: bookinfo-mesh  name: defaultspec:  members:    - bookinfo Deploy Demo Application In order to look into the capabilities provided by the OpenShift Service Mesh we will deploy a simple book review app. The application is static but shows reviews for a book and has several microservices. The product page is the entry point service. It provides information on books and their reviews. It accesses book reviews through the reviews service which is also accesses the ratings service downstream to allow users to give a book a rating. It also gets details about a book via the details service. Like a true polyglot all services are written in a different programming language. The ratings service provides three different API versions, v1 doesn&apos;t display ratings, v2 shows ratings in black and v3 shows ratings in red. The idea is all about innovating quickly and getting real user feedback to take the app in the right direction. Create a project for the book app. Create a project to host the book app. $ oc new-project bookinfo Deploy book app. $ oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-2.0/samples/bookinfo/platform/kube/bookinfo.yaml Create a service mesh gateway. Once the app is deployed we need to create a gateway and setup the URI matches. $ oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-2.0/samples/bookinfo/networking/bookinfo-gateway.yaml Create service mesh rule set. Since the ratings service has 3 API versions we need some rules to govern the traffic. $ oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-2.0/samples/bookinfo/networking/destination-rule-all.yaml Access the application. Get the route to the application and add the /productpage to access via web browser. $ export GATEWAY_URL=$(oc -n istio-system get route istio-ingressgateway -o jsonpath=&apos;{.spec.host}&apos;)$ echo $GATEWAY_URL http://istio-ingressgateway-bookinfo-mesh.apps.ocp4.rh-southwest.com/productpage If you refresh the web page you should see the ratings change between no ratings, black and red. This is because the ruleset is sending traffic to all three API endpoints (v1, v2 and v3). Update Service Mesh Ruleset As mentioned we can create rules that enable us to send certain traffic to a different API endpoint. This could be for a canary deployment where we want to test how users interact with different capabilities (red or black ratings stars), to perform a blue/green deployment upgrading to a newer API version or even sending different users to different APIs. In this case we will simply apply a new rule that only sends traffic to v2 (black) and v3 (red) ratings API. Apply a new ruleset. $ oc replace -f https://raw.githubusercontent.com/istio/istio/master/samples/bookinfo/networking/virtual-service-reviews-v2-v3.yaml Now when you access the book app and refresh you should see it switched between red and black ratings. Looking at Kiali versioned app graph we can see that traffic is only going to v2 and v3 as we would expect.   Troubleshooting Errors As mentioned one of the key values of the OpenShift Service Mesh is the visualization and tracing. In order to generate an error we will scale down the ratings v1 deployment to 0. $ oc scale deployment/ratings-v1 -n bookinfo --replicas 0 You should now see that the ratings service is currently unavailable when you refresh the app in a browser. Check Kiali dashboard. You can access Kiali via route under networking in the bookinfo-mesh project. In Kiali we clearly see the issue is the reviews service as we would expect.  Open Jaeger to trace the calls that are failing. Next we can dig into the requests by opening distributed tracing (Jaeger) from the Kiali dashboard. We can see the flow of all calls grouped and identify all request to the ratings service that are throwing an error. We see a 503 is returned which means service in unavailable. Kiali dashboard also shows request response (latency in ms) for slowest 50%, 95% and 99% of requests. This allows us to not only see the average latency but compare it to the slowest requests which indicates if we have spikes that could cause user slowness that we might not easily see when looking at just the average. Summary In this article we discussed the capabilities OpenShift Service Mesh provides for microservice applications. We deployed a service mesh and a book demo polyglot application that leverages the OpenShift Service Mesh. Using Kiali we saw how to gain visibility into our microservice application and easily identify problems as well as trends. Through Jaeger and distributed tracing we were able to identify the exact API causing error conditions. Microservice architectures provide a lot of value but they are harder to manage, control and troubleshoot in certain aspects than their monolithic peers which is why OpenShift Service Mesh is so critical. (c) 2021 Keith Tenzer</summary></entry><entry><title type="html">OpenShift 4 AWS IPI Installation Getting Started Guide</title><link href="http://localhost:4000/openshift/openshift-4-aws-ipi-installation-getting-started-guide/" rel="alternate" type="text/html" title="OpenShift 4 AWS IPI Installation Getting Started Guide" /><published>2021-01-18T00:00:00-08:00</published><updated>2021-01-18T00:00:00-08:00</updated><id>http://localhost:4000/openshift/openshift-4-aws-ipi-installation-getting-started-guide</id><content type="html" xml:base="http://localhost:4000/openshift/openshift-4-aws-ipi-installation-getting-started-guide/">&lt;p&gt;&lt;!-- wp:gallery {&quot;ids&quot;:[5513,5506,14709],&quot;imageCrop&quot;:false,&quot;linkTo&quot;:&quot;file&quot;,&quot;sizeSlug&quot;:&quot;medium&quot;} --&gt;&lt;/p&gt;
&lt;h2&gt;&lt;img class=&quot;alignnone  wp-image-2609&quot; style=&quot;color:var(--color-text);font-size:16px;font-weight:400;&quot; src=&quot;/assets/2021/01/aws-logo-1280x720-2.png?w=300&quot; alt=&quot;ansible_2&quot; width=&quot;100&quot; height=&quot;100&quot; /&gt;&lt;img class=&quot;alignnone size-full wp-image-12472&quot; style=&quot;color:var(--color-text);font-size:16px;font-weight:400;&quot; src=&quot;/assets/2021/01/plus_sign.gif?w=300&quot; alt=&quot;plus_sign&quot; width=&quot;80&quot; height=&quot;80&quot; /&gt;&lt;img class=&quot;alignnone  wp-image-14629&quot; src=&quot;/assets/2021/01/openshiftlogo.png?w=170&quot; alt=&quot;cropped-Windows-logo1&quot; width=&quot;100&quot; height=&quot;100&quot; /&gt;&lt;/h2&gt;
&lt;p&gt;&lt;!-- /wp:gallery --&gt;&lt;/p&gt;
&lt;p&gt;Happy new year as this will be the first post of 2021! 2020 was obviously a challenging year, my hope is I will have more time to devote to blogging in 2021. Please reach out and let me know what topics would be most helpful.&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:heading {&quot;className&quot;:&quot;has-large-font-size&quot;} --&gt;&lt;/p&gt;
&lt;h2 class=&quot;has-large-font-size&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;&lt;!-- /wp:heading --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;In this Article we will walk through an OpenShift deployment using the IPI (Installer Provisioned Infrastructure) method on AWS. OpenShift offers two possible deployment methods: IPI (as mentioned) and UPI (User Provisioned Infrastructure). The difference is the degree of automation and customization. IPI will not only deploy OpenShift but also all infrastructure components and configurations. IPI is supported in various environments including AWS, Azure, GCE, VMware Vsphere and even Baremetal. IPI is tightly coupled with the infrastructure layer whereas UPI is not and will allow the most customization and work anywhere.&lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Ultimately IPI vs UPI is usually dictated by the requirements. My view is unless you have special requirements (like a stretch cluster or specific integrations with infrastructure that require UPI) always default to IPI. It is far better to have the vendor, in this case Red Hat own more of the share of responsibility and ensure proper, tested infrastructure configurations are being deployed as well as maintained throughout the cluster lifecycle.&lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:more --&gt;&lt;br /&gt;
&lt;!--more--&gt;&lt;br /&gt;
&lt;!-- /wp:more --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:heading {&quot;className&quot;:&quot;has-large-font-size&quot;} --&gt;&lt;/p&gt;
&lt;h2 class=&quot;has-large-font-size&quot;&gt;Create Hosted Zone&lt;/h2&gt;
&lt;p&gt;&lt;!-- /wp:heading --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Before a cluster can be deployed we need to access AWS console and add a hosted zone to the route53 service. In the diagram below, we are adding the domain rh-southwest.com.&lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:image {&quot;id&quot;:14686,&quot;linkDestination&quot;:&quot;custom&quot;,&quot;className&quot;:&quot;size-large&quot;} --&gt;&lt;/p&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/01/route53new-2.png&quot;&gt;&lt;img src=&quot;/assets/2021/01/route53new-2.png?w=1024&quot; alt=&quot;&quot; class=&quot;wp-image-14686&quot; /&gt;&lt;/a&gt;&lt;/figure&gt;
&lt;p&gt;&lt;!-- /wp:image --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:heading {&quot;className&quot;:&quot;has-large-font-size&quot;} --&gt;&lt;/p&gt;
&lt;h2 class=&quot;has-large-font-size&quot;&gt;Download and Install CLI Tools&lt;/h2&gt;
&lt;p&gt;&lt;!-- /wp:heading --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Red Hat provides a CLI tools for deploying and managing OpenShift clusters. The CLI tools wrap kubectl, podman (docker image manipulation) and other tools to provide a single easy-to-use interface. You can of course also use kubectl and individual tooling for manipulating images (docker or podman).&lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Download CLI Tools&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Access&amp;nbsp;&lt;a href=&quot;https://cloud.redhat.com/openshift/install&quot;&gt;https://cloud.redhat.com/openshift/install&lt;/a&gt; using your Red Hat account. Select AWS as infrastructure provider and the installer provisioned option. Download both the CLI and OpenShift install.&lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Copy Pull Secret&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;The pull secret is used to automatically authenticate to Red Hat content repositories for OpenShift during the install. You will need to provide the pull secret as part of a later step.&lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:image {&quot;id&quot;:14696,&quot;sizeSlug&quot;:&quot;large&quot;,&quot;linkDestination&quot;:&quot;media&quot;} --&gt;&lt;/p&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/01/aws_setup-2.png&quot;&gt;&lt;img src=&quot;/assets/2021/01/aws_setup-2.png?w=1024&quot; alt=&quot;&quot; class=&quot;wp-image-14696&quot; /&gt;&lt;/a&gt;&lt;/figure&gt;
&lt;p&gt;&lt;!-- /wp:image --&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Install CLI Tools&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The CLI tools can be simply extracted into /usr/bin.&lt;/p&gt;
&lt;pre class=&quot;wp-block-preformatted&quot;&gt;$ sudo tar xvf openshift-client-linux.tar.gz -C /usr/bin &lt;br /&gt;$ sudo tar xvf openshift-install-linux.tar.gz -C /usr/bin&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Check CLI Version&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&quot;wp-block-preformatted&quot;&gt;$ openshift-install version openshift-install 4.6.9&lt;/pre&gt;
&lt;h2&gt;Install Configuration&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Create Install Config&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When creating an install config you will need to provide the platform, in this case AWS, the domain configured in route53, a name for the OpenShift cluster and of course the pull secret.&lt;/p&gt;
&lt;pre&gt;$ openshift-install create install-config \
--dir=ocp_install
? SSH Public Key /root/.ssh/id_rsa.pub
? Platform aws
? Cloud aws
? Base Domain rh-southwest.com
? Cluster Name ocp4
? Pull Secret [? for help] ************************************************&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Edit Install Config&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&quot;wp-block-preformatted&quot;&gt;$ vi ocp_install/install-config.yaml &lt;br /&gt;... &lt;br /&gt;compute: &lt;br /&gt;  architecture: amd64 &lt;br /&gt;  hyperthreading: Enabled &lt;br /&gt;  name: worker &lt;br /&gt;  platform: &lt;br /&gt;    aws: &lt;br /&gt;      type: m5.large &lt;br /&gt;      userTags: &lt;br /&gt;        Contact: myemail@mydomain.com &lt;br /&gt;        AlwaysUp: True &lt;br /&gt;        DeleteBy: NEVER &lt;br /&gt;  replicas: 3 &lt;br /&gt;controlPlane: &lt;br /&gt;  architecture: amd64 &lt;br /&gt;  hyperthreading: Enabled &lt;br /&gt;  name: master &lt;br /&gt;  platform: &lt;br /&gt;    aws: &lt;br /&gt;      type: m5.xlarge &lt;br /&gt;      userTags: &lt;br /&gt;        Contact: myemail@mydomain.com &lt;br /&gt;        AlwaysUp: True &lt;br /&gt;        DeleteBy: NEVER &lt;br /&gt;  replicas: 3 &lt;br /&gt;...&lt;/pre&gt;
&lt;p&gt;User tags allow you to label or tag the AWS VMs. In this case we have some automation that will shutdown any clusters that are not supposed to be always available or delete any clusters that have an expiration date set.&lt;/p&gt;
&lt;h2&gt;Deploy OpenShift Cluster&lt;/h2&gt;
&lt;p&gt;Now that we have adjusted the configuration we can deploy the cluster and grab a coffee.&lt;/p&gt;
&lt;pre class=&quot;wp-block-preformatted&quot;&gt;$ [ktenzer@localhost ~]$ time openshift-install create cluster --dir=ocp_install --log-level=debug&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Verify Cluster&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Once the deployment has completed successfully we can verify cluster. In order to gain access to cluster we can use the default system account created during deployment by exporting the kubeconfig.&lt;/p&gt;
&lt;pre class=&quot;wp-block-preformatted&quot;&gt;$ export KUBECONFIG=ocp_install/auth/kubeconfig &lt;br /&gt;$ oc get nodes &lt;br /&gt;NAME                                       STATUS ROLES  AGE VERSION &lt;br /&gt;ip-10-0-147-102.us-east-2.compute.internal Ready  worker 27m v1.19.0+7070803 &lt;br /&gt;ip-10-0-159-222.us-east-2.compute.internal Ready  master 33m v1.19.0+7070803 &lt;br /&gt;ip-10-0-161-231.us-east-2.compute.internal Ready  worker 24m v1.19.0+7070803 &lt;br /&gt;ip-10-0-178-131.us-east-2.compute.internal Ready  master 33m v1.19.0+7070803 &lt;br /&gt;ip-10-0-194-232.us-east-2.compute.internal Ready  worker 24m v1.19.0+7070803 &lt;br /&gt;ip-10-0-218-84.us-east-2.compute.internal Ready   master 33m v1.19.0+7070803&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Show Cluster Version&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&quot;wp-block-preformatted&quot;&gt;$ oc get clusterversion &lt;br /&gt;NAME    VERSION AVAILABLE PROGRESSING SINCE STATUS &lt;br /&gt;version 4.6.9   True      False       7m44s Cluster version is 4.6.9&lt;/pre&gt;
&lt;h2&gt;Configure OAUTH&lt;/h2&gt;
&lt;p&gt;OpenShift supports oauth2, there is a lot of choice with the identification provider. Usually you would integrate with your enterprise LDAP provider. In this case or for test environments you can use the htpasswd provider.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Create Htpasswd file&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&quot;wp-block-preformatted&quot;&gt;$ htpasswd -c -B -b ocp_install/ocp.htpasswd admin pasword123&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Create Secret for Htpasswd&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&quot;wp-block-preformatted&quot;&gt;$ oc create secret generic htpass-secret --from-file=ocp.htpasswd -n openshift-config&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Configure Htpasswd in Oauth&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&quot;wp-block-preformatted&quot;&gt;$ vi ocp_install/htpasswd-cr.yaml &lt;/pre&gt;
&lt;pre&gt;apiVersion: config.openshift.io/v1
kind: OAuth
metadata:
  name: cluster
spec:
  identityProviders:
  - name: my_htpasswd_provider
    mappingMethod: claim
    type: HTPasswd
    htpasswd:
      fileData:
        name: htpass-secret&lt;/pre&gt;
&lt;pre class=&quot;wp-block-preformatted&quot;&gt;$ oc apply -f ocp_install/htpasswd-cr.yaml&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Add Cluster Role&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Once we have added htpasswd as a oauth provider we need to add cluster-admin role to the admin user we already created.&lt;/p&gt;
&lt;pre class=&quot;wp-block-preformatted&quot;&gt;$ oc adm policy add-cluster-role-to-user cluster-admin admin&lt;/pre&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;In this article we discussed the IPI and UPI deployment methods provided by OpenShift. Most organizations will want to choose IPI where possible. It not only simplifies the deployment of OpenShift but also improves the supportability likely leading to a higher SLA or quicker resolution of any potential issues. If not possible, UPI provides all the flexibility needed to deploy OpenShift into any environment albeit with a little more oversight and ownership required. Finally we walked through the IPI deployment of OpenShift on AWS including showing how to configure OAUTH htpasswd provider.&lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;(c) 2021 Keith Tenzer &lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;</content><author><name>Keith Tenzer</name></author><category term="OpenShift" /><category term="AWS" /><category term="OpenShift" /><category term="Kubernetes" /><summary type="html">Happy new year as this will be the first post of 2021! 2020 was obviously a challenging year, my hope is I will have more time to devote to blogging in 2021. Please reach out and let me know what topics would be most helpful. Overview In this Article we will walk through an OpenShift deployment using the IPI (Installer Provisioned Infrastructure) method on AWS. OpenShift offers two possible deployment methods: IPI (as mentioned) and UPI (User Provisioned Infrastructure). The difference is the degree of automation and customization. IPI will not only deploy OpenShift but also all infrastructure components and configurations. IPI is supported in various environments including AWS, Azure, GCE, VMware Vsphere and even Baremetal. IPI is tightly coupled with the infrastructure layer whereas UPI is not and will allow the most customization and work anywhere. Ultimately IPI vs UPI is usually dictated by the requirements. My view is unless you have special requirements (like a stretch cluster or specific integrations with infrastructure that require UPI) always default to IPI. It is far better to have the vendor, in this case Red Hat own more of the share of responsibility and ensure proper, tested infrastructure configurations are being deployed as well as maintained throughout the cluster lifecycle. Create Hosted Zone Before a cluster can be deployed we need to access AWS console and add a hosted zone to the route53 service. In the diagram below, we are adding the domain rh-southwest.com. Download and Install CLI Tools Red Hat provides a CLI tools for deploying and managing OpenShift clusters. The CLI tools wrap kubectl, podman (docker image manipulation) and other tools to provide a single easy-to-use interface. You can of course also use kubectl and individual tooling for manipulating images (docker or podman). Download CLI Tools Access&amp;nbsp;https://cloud.redhat.com/openshift/install using your Red Hat account. Select AWS as infrastructure provider and the installer provisioned option. Download both the CLI and OpenShift install. Copy Pull Secret The pull secret is used to automatically authenticate to Red Hat content repositories for OpenShift during the install. You will need to provide the pull secret as part of a later step. Install CLI Tools The CLI tools can be simply extracted into /usr/bin. $ sudo tar xvf openshift-client-linux.tar.gz -C /usr/bin $ sudo tar xvf openshift-install-linux.tar.gz -C /usr/bin Check CLI Version $ openshift-install version openshift-install 4.6.9 Install Configuration Create Install Config When creating an install config you will need to provide the platform, in this case AWS, the domain configured in route53, a name for the OpenShift cluster and of course the pull secret. $ openshift-install create install-config \ --dir=ocp_install ? SSH Public Key /root/.ssh/id_rsa.pub ? Platform aws ? Cloud aws ? Base Domain rh-southwest.com ? Cluster Name ocp4 ? Pull Secret [? for help] ************************************************ Edit Install Config $ vi ocp_install/install-config.yaml ... compute: architecture: amd64 hyperthreading: Enabled name: worker platform: aws: type: m5.large userTags: Contact: myemail@mydomain.com AlwaysUp: True DeleteBy: NEVER replicas: 3 controlPlane: architecture: amd64 hyperthreading: Enabled name: master platform: aws: type: m5.xlarge userTags: Contact: myemail@mydomain.com AlwaysUp: True DeleteBy: NEVER replicas: 3 ... User tags allow you to label or tag the AWS VMs. In this case we have some automation that will shutdown any clusters that are not supposed to be always available or delete any clusters that have an expiration date set. Deploy OpenShift Cluster Now that we have adjusted the configuration we can deploy the cluster and grab a coffee. $ [ktenzer@localhost ~]$ time openshift-install create cluster --dir=ocp_install --log-level=debug Verify Cluster Once the deployment has completed successfully we can verify cluster. In order to gain access to cluster we can use the default system account created during deployment by exporting the kubeconfig. $ export KUBECONFIG=ocp_install/auth/kubeconfig $ oc get nodes NAME STATUS ROLES AGE VERSION ip-10-0-147-102.us-east-2.compute.internal Ready worker 27m v1.19.0+7070803 ip-10-0-159-222.us-east-2.compute.internal Ready master 33m v1.19.0+7070803 ip-10-0-161-231.us-east-2.compute.internal Ready worker 24m v1.19.0+7070803 ip-10-0-178-131.us-east-2.compute.internal Ready master 33m v1.19.0+7070803 ip-10-0-194-232.us-east-2.compute.internal Ready worker 24m v1.19.0+7070803 ip-10-0-218-84.us-east-2.compute.internal Ready master 33m v1.19.0+7070803 Show Cluster Version $ oc get clusterversion NAME VERSION AVAILABLE PROGRESSING SINCE STATUS version 4.6.9 True False 7m44s Cluster version is 4.6.9 Configure OAUTH OpenShift supports oauth2, there is a lot of choice with the identification provider. Usually you would integrate with your enterprise LDAP provider. In this case or for test environments you can use the htpasswd provider. Create Htpasswd file $ htpasswd -c -B -b ocp_install/ocp.htpasswd admin pasword123 Create Secret for Htpasswd $ oc create secret generic htpass-secret --from-file=ocp.htpasswd -n openshift-config Configure Htpasswd in Oauth $ vi ocp_install/htpasswd-cr.yaml apiVersion: config.openshift.io/v1 kind: OAuth metadata: name: cluster spec: identityProviders: - name: my_htpasswd_provider mappingMethod: claim type: HTPasswd htpasswd: fileData: name: htpass-secret $ oc apply -f ocp_install/htpasswd-cr.yaml Add Cluster Role Once we have added htpasswd as a oauth provider we need to add cluster-admin role to the admin user we already created. $ oc adm policy add-cluster-role-to-user cluster-admin admin Summary In this article we discussed the IPI and UPI deployment methods provided by OpenShift. Most organizations will want to choose IPI where possible. It not only simplifies the deployment of OpenShift but also improves the supportability likely leading to a higher SLA or quicker resolution of any potential issues. If not possible, UPI provides all the flexibility needed to deploy OpenShift into any environment albeit with a little more oversight and ownership required. Finally we walked through the IPI deployment of OpenShift on AWS including showing how to configure OAUTH htpasswd provider. (c) 2021 Keith Tenzer</summary></entry><entry><title type="html">Windows Automation with Ansible: Getting Started Guide</title><link href="http://localhost:4000/ansible/windows-automation-with-ansible-getting-started-guide/" rel="alternate" type="text/html" title="Windows Automation with Ansible: Getting Started Guide" /><published>2020-05-19T00:00:00-07:00</published><updated>2020-05-19T00:00:00-07:00</updated><id>http://localhost:4000/ansible/windows-automation-with-ansible-getting-started-guide</id><content type="html" xml:base="http://localhost:4000/ansible/windows-automation-with-ansible-getting-started-guide/">&lt;h2&gt;&lt;img class=&quot;alignnone  wp-image-2609&quot; style=&quot;color:var(--color-text);font-size:16px;font-weight:400;&quot; src=&quot;/assets/2020/05/ansible_2.png&quot; alt=&quot;ansible_2&quot; width=&quot;185&quot; height=&quot;185&quot; /&gt;&lt;img class=&quot;alignnone size-full wp-image-12472&quot; style=&quot;color:var(--color-text);font-size:16px;font-weight:400;&quot; src=&quot;/assets/2020/05/plus_sign.gif&quot; alt=&quot;plus_sign&quot; width=&quot;161&quot; height=&quot;161&quot; /&gt;&lt;img class=&quot;alignnone  wp-image-14629&quot; src=&quot;/assets/2020/05/cropped-windows-logo1.png&quot; alt=&quot;cropped-Windows-logo1&quot; width=&quot;179&quot; height=&quot;179&quot; /&gt;&lt;/h2&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;In this article we will focus on how to get started with automation of windows using Ansible. Specifically we will look at installing 3rd party software and OS updates. Automation is the basis for cloud-computing or cloud-native patterns and breeds a culture of innovation. Let&apos;s face it, we cannot innovate, if we are stuck doing mundane tasks and manual labor. Ansible has revolutionized automation because until Ansible, automating was rather complicated and required lots of domain knowledge. Ansible provides an automation language that the entire organization can use because it is so easy and so flexible. The best thing about Ansible though it it brings teams together and fosters a devops culture. It brings the Linux and Windows world together!&lt;/p&gt;
&lt;p&gt;&lt;!--more--&gt;&lt;/p&gt;
&lt;h2&gt;How Ansible Works&lt;/h2&gt;
&lt;p&gt;Ansible provides a runtime for executing playbooks. A playbook is simply a group of plays which are essentially steps executed in order. A play is one or more tasks. A task is simply the execution of an Ansible module. An Ansible module is python code that does something, like install a software package, update the system, change a configuration file, check if something is set correctly, etc. There are 1000s of Ansible modules and a huge community around it.&lt;/p&gt;
&lt;p&gt;In order to run a playbook against hosts you need an inventory which is simply a grouping of hosts and host vars (parameters). That is about it for the basics. Below is a diagram showing the Ansible automation engine architecture.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;alignnone  wp-image-14622&quot; src=&quot;/assets/2020/05/ansible_arch.png&quot; alt=&quot;ansible_arch&quot; width=&quot;1059&quot; height=&quot;740&quot; /&gt;&lt;/p&gt;
&lt;p&gt;This guide assumes you know some Ansible or are familar with the basics. If not I created an Ansible getting started guide &lt;a href=&quot;https://keithtenzer.com/2017/11/09/ansible-getting-started-guide/&quot;&gt;here&lt;/a&gt;. I highly recommend giving it a look before proceeding.&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;In order for a windows host to be managed by Ansible there are a few prerequisites&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Powershell version 3.0 or higher (Should be fine with Windows 2012 or higher)&lt;/li&gt;
&lt;li&gt;.NET Framework 4.0 or higher (should be fine with Windows 2012 or higher)&lt;/li&gt;
&lt;li&gt;Windows Remote Management Listener or SSH (cygwin)&lt;/li&gt;
&lt;li&gt;Windows 7, 8.1, and 10, and server OSs including Windows Server 2008, 2008 R2, 2012, 2012 R2, 2016, and 2019&lt;/li&gt;
&lt;li&gt;Chocolatey for installing 3rd party software&lt;/li&gt;
&lt;li&gt;WSUS for updating OS packages and patching&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Install Chocalatey and WSUS&lt;/h2&gt;
&lt;p&gt;There are various tools that can be used. I recommend using Chocolatey for installing packages and WSUS for OS updates/patching.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Install Chocalately&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using powershell we can install chocalately.&lt;/p&gt;
&lt;pre&gt;PS C:\windows\system32&amp;gt; Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString(&apos;https://chocolatey.org/install.ps1&apos;))&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Install WSUS&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Open server manager. In the top right under manage you can add or change roles.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;alignnone  wp-image-14632&quot; src=&quot;/assets/2020/05/server_manager.png&quot; alt=&quot;server_manager&quot; width=&quot;814&quot; height=&quot;429&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Under server roles select Windows Service Update Services. This will install WSUS. Once installed there will be a WSUS option in server manager. You can select it and configure what packages should be updated, etc.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;alignnone  wp-image-14633&quot; src=&quot;/assets/2020/05/wsus.png&quot; alt=&quot;wsus&quot; width=&quot;844&quot; height=&quot;409&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Open WSUS and check that the computer is showing up under &apos;All Computers&apos;. If not and you are running outside of domain you may need the following fix.&lt;/p&gt;
&lt;pre&gt;c:\&amp;gt; net stop wuauserv
c:\&amp;gt; regsvr32 /s wuapi.dll
c:\&amp;gt; regsvr32 /s wups.dll
c:\&amp;gt; regsvr32 /s wuaueng.dll
c:\&amp;gt; regsvr32 /s wucltui.dll
c:\&amp;gt; regsvr32 /s msxml3.dll
c:\&amp;gt; cd %windir%\SoftwareDistribution
c:\&amp;gt; rd /s/q DataStore
c:\&amp;gt; mkdir DataStore
c:\&amp;gt; rd /s/q Download
c:\&amp;gt; mkdir Download
c:\&amp;gt; net start wuauserv
c:\&amp;gt; reg delete HKLM\SOFTWARE\Microsoft\Windows\CurrentVersion\WindowsUpdate /v AccountDomainSid /f
c:\&amp;gt; reg delete HKLM\SOFTWARE\Microsoft\Windows\CurrentVersion\WindowsUpdate /v PingID /f
c:\&amp;gt; reg delete HKLM\SOFTWARE\Microsoft\Windows\CurrentVersion\WindowsUpdate /v SusClientId /f
c:\&amp;gt; wuauclt /resetauthorization
c:\&amp;gt; wuauclt /detectnow
c:\&amp;gt; wuauclt /reportnow&lt;/pre&gt;
&lt;h2&gt;Configuring Windows Remote Management for Ansible&lt;/h2&gt;
&lt;p&gt;Ansible requires no agents. It uses what the OS provides for communication. In Windows you can use SSH or Windows Remote Management (WinRM). Since SSH is more or less bolted on, WinRM is usually the preferred choice.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Test to ensure WinRM is working&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First install WinRM if it isn&apos;t installed. Execute the hostname command through WinRM.&lt;/p&gt;
&lt;pre&gt;PS C:\windows\system32&amp;gt; winrs -r:http://:5985/wsman -u: -p: hostname
win2012&lt;/pre&gt;
&lt;p&gt;Optionally you can also test WinRM by making a remote desktop connection from another windows host.&lt;/p&gt;
&lt;p&gt;Note: you may run into an issue where you get an authentication error and a CredSSL issue if you have an older windows version. This can also happen if you don&apos;t have a valid SSL certificate. If you run into this issue, update your registry.&lt;/p&gt;
&lt;pre&gt;c:\&amp;gt;reg add &quot;HKLM\Software\Microsoft\Windows\CurrentVersion\Policies\System\CredSSP\Parameters&quot; /f /v AllowEncryptionOracle /t REG_DWORD /d 2&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Enable basic auth&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There are several authentication methods. In this example we will use basic authentication which is username/password. This is of course the least secure method.&lt;/p&gt;
&lt;pre&gt;&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;PS C:\&amp;gt; Set-Item -Path WSMan:\localhost\Service\Auth\Basic -Value $true&lt;/span&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Update WinRM &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A script is provided by Ansible community to check WinRM and make necessary changes to allow Ansible to connect. In this case we are using basic authentication but likely you will want to use something more secure. The script can be found &lt;a href=&quot;https://raw.githubusercontent.com/ansible/ansible/devel/examples/scripts/ConfigureRemotingForAnsible.ps1&quot;&gt;here&lt;/a&gt; and other authentication options are documented in the script header.&lt;/p&gt;
&lt;p&gt;Open a powershell command prompt.&lt;/p&gt;
&lt;p&gt;Store URL path to script.&lt;/p&gt;
&lt;pre&gt;PS C:\&amp;gt; $url = &quot;https://raw.githubusercontent.com/ansible/ansible/devel/examples/scripts/ConfigureRemotingForAnsible.ps1&quot;&lt;/pre&gt;
&lt;p&gt;Store location for the script&lt;/p&gt;
&lt;pre&gt;PS C:\&amp;gt; $file = &quot;$env:temp\ConfigureRemotingForAnsible.ps1&quot;&lt;/pre&gt;
&lt;p&gt;Download script and output to file locally.&lt;/p&gt;
&lt;pre&gt;PS C:\&amp;gt; (New-Object -TypeName System.Net.WebClient).DownloadFile($url, $file)&lt;/pre&gt;
&lt;p&gt;Execute the script.&lt;/p&gt;
&lt;pre&gt;PS C:\&amp;gt; powershell.exe -ExecutionPolicy ByPass -File $file
Ok.&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Check WinRM connection&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;PS C:\windows\system32&amp;gt; winrm enumerate winrm/config/Listener
Listener
Address = *
Transport = HTTP
Port = 5985
Hostname
Enabled = true
URLPrefix = wsman
CertificateThumbprint
ListeningOn = 10.10.1.139, 127.0.0.1, ::1, fe80::5efe:10.10.1.139%22, fe80::8155:327a:aeaf:365a%12

Listener
Address = *
Transport = HTTPS
Port = 5986
Hostname
Enabled = true
URLPrefix = wsman
CertificateThumbprint = 7a38de2c212764a54de106dc756f7cbc275156a3
ListeningOn = 10.10.1.139, 127.0.0.1, ::1, fe80::5efe:10.10.1.139%22, fe80::8155:327a:aeaf:365a%12&lt;/pre&gt;
&lt;p&gt;Ensure the HTTP/HTTPS ports are open.&lt;/p&gt;
&lt;p&gt;More details about WinRM setup and how to setup a listener manually are documented &lt;a href=&quot;https://docs.ansible.com/ansible/latest/user_guide/windows_setup.html#winrm-setup&quot;&gt;here&lt;/a&gt;. In this case I used the default listener configured by WinRM.&lt;/p&gt;
&lt;h2&gt;Create Inventory File&lt;/h2&gt;
&lt;p&gt;We will create an inventory file with just a single host in a group called windows. From here on out we will be working on a Linux server where we have Ansible installed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Create Ansible inventory file&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;$ vi inventory
[windows]
138.204.12.111&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Test ansible connection&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using the inventory file we can test if Ansible can communicate with our windows server.&lt;/p&gt;
&lt;pre&gt;$ ansible -i ../inventory windows -m win_ping -e ansible_connection=winrm \
-e ansible_user=Admin -e ansible_password=&amp;lt;password&amp;gt; \
-e ansible_winrm_transport=basic \
-e ansible_winrm_server_cert_validation=ignore

138.201.147.202 | SUCCESS =&amp;gt; {
&quot;changed&quot;: false,
&quot;ping&quot;: &quot;pong&quot;
}&lt;/pre&gt;
&lt;h2&gt;Windows Patch Management&lt;/h2&gt;
&lt;p&gt;Now that Ansible is working with WinRM we can automate. In this case we will automate software package installation and updates.&lt;/p&gt;
&lt;p&gt;Playbook and roles are available &lt;a href=&quot;https://github.com/ktenzer/ansible-windows&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Create Playbook&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We haven&apos;t talked about roles. In Ansible roles are how we make playbooks reusable. It is always good practice to create roles. A role essentially allows you to organize Ansible plays and their dependencies together allowing them to be consumed easily. In order to use roles you need to create a certain directory structure and hierarchy. Create a playbook that imports our roles.&lt;/p&gt;
&lt;pre&gt;$ vi windows_baseline.yaml
---
- name: Windows Baseline
  hosts: &quot;&quot;
  connection: winrm
  gather_facts: true
  vars:
    ansible_user: &quot;&quot;
    ansible_password: &quot;&quot;
    ansible_connection: winrm 
    ansible_winrm_transport: basic 
    ansible_winrm_server_cert_validation: ignore

  tasks:
    - name: Install Baseline Packages
      include_role:
        name: install

    - name: Perform Updates
      include_role:
        name: updates
&lt;/pre&gt;
&lt;p&gt;Here we are setting hosts, ansible user and password as variables. These inputs must be provided when executing the playbook. Hosts should be the name of our host group from our inventory file. This playbook has two tasks, a role to install packages and a role to do an OS update.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Create Ansible install role&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;At a minimum your role will need a tasks directory&lt;/p&gt;
&lt;pre&gt;$ mkdir -p roles/install/tasks&lt;/pre&gt;
&lt;p&gt;Create a task to install git using the chocalatey module.&lt;/p&gt;
&lt;pre&gt;$ vi roles/install/tasks/main.yaml
---
- name: Install Git
  win_chocolatey:
    name: git
    state: present
&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Create Ansible patch update role&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;At a minimum your role will need a tasks directory&lt;/p&gt;
&lt;pre&gt;$ mkdir -p roles/updates/tasks&lt;/pre&gt;
&lt;p&gt;Create a task to perform an OS update.&lt;/p&gt;
&lt;pre&gt;vi roles/updates/tasks/main.yaml
---
- name: Update windows packages
  win_updates:
    category_names:
      - CriticalUpdates
      - SecurityUpdates
    reboot: yes
    reboot_timeout: 500
&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Run playbook using inventory&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The &apos;-vvvvv&apos; allows the playbook to run in debug mode for maximum verbosity.&lt;/p&gt;
&lt;pre&gt;$ ansible-playbook -i ./inventory -e target=windows -e user=Admin -e password= windows_baseline.yaml
PLAY [Windows Baseline] ****************************************************************************

TASK [Gathering Facts] *****************************************************************************
ok: [138.201.147.202]

TASK [Install Baseline Packages] *******************************************************************

TASK [install : Install Git] ***********************************************************************
ok: [138.201.147.202]

TASK [Perform Updates] *****************************************************************************

TASK [updates : Update windows packages] ***********************************************************
changed: [138.201.147.202]

PLAY RECAP *****************************************************************************************
138.201.147.202            : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=
&lt;/pre&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ansible/ansible-examples/tree/master/windows&quot;&gt;Here&lt;/a&gt; are some additional examples of windows playbooks that may be of interest on your journey.&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this article we discussed the value of automation and why it is just a game changer. We provided a step-by-step on preparing a windows host for Ansible. Finally using the Ansible automation language, showed how to use native windows tooling to install and update OS patches. Hopefully this will provide a good starting point for a journey into windows automation with Ansible.&lt;/p&gt;
&lt;p&gt;(c) 2020 Keith Tenzer&lt;/p&gt;</content><author><name>Keith Tenzer</name></author><category term="Ansible" /><category term="Automation" /><category term="Windows" /><summary type="html">Overview In this article we will focus on how to get started with automation of windows using Ansible. Specifically we will look at installing 3rd party software and OS updates. Automation is the basis for cloud-computing or cloud-native patterns and breeds a culture of innovation. Let&apos;s face it, we cannot innovate, if we are stuck doing mundane tasks and manual labor. Ansible has revolutionized automation because until Ansible, automating was rather complicated and required lots of domain knowledge. Ansible provides an automation language that the entire organization can use because it is so easy and so flexible. The best thing about Ansible though it it brings teams together and fosters a devops culture. It brings the Linux and Windows world together! How Ansible Works Ansible provides a runtime for executing playbooks. A playbook is simply a group of plays which are essentially steps executed in order. A play is one or more tasks. A task is simply the execution of an Ansible module. An Ansible module is python code that does something, like install a software package, update the system, change a configuration file, check if something is set correctly, etc. There are 1000s of Ansible modules and a huge community around it. In order to run a playbook against hosts you need an inventory which is simply a grouping of hosts and host vars (parameters). That is about it for the basics. Below is a diagram showing the Ansible automation engine architecture. This guide assumes you know some Ansible or are familar with the basics. If not I created an Ansible getting started guide here. I highly recommend giving it a look before proceeding. Prerequisites In order for a windows host to be managed by Ansible there are a few prerequisites Powershell version 3.0 or higher (Should be fine with Windows 2012 or higher) .NET Framework 4.0 or higher (should be fine with Windows 2012 or higher) Windows Remote Management Listener or SSH (cygwin) Windows 7, 8.1, and 10, and server OSs including Windows Server 2008, 2008 R2, 2012, 2012 R2, 2016, and 2019 Chocolatey for installing 3rd party software WSUS for updating OS packages and patching Install Chocalatey and WSUS There are various tools that can be used. I recommend using Chocolatey for installing packages and WSUS for OS updates/patching. Install Chocalately Using powershell we can install chocalately. PS C:\windows\system32&amp;gt; Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString(&apos;https://chocolatey.org/install.ps1&apos;)) Install WSUS Open server manager. In the top right under manage you can add or change roles. Under server roles select Windows Service Update Services. This will install WSUS. Once installed there will be a WSUS option in server manager. You can select it and configure what packages should be updated, etc. Open WSUS and check that the computer is showing up under &apos;All Computers&apos;. If not and you are running outside of domain you may need the following fix. c:\&amp;gt; net stop wuauserv c:\&amp;gt; regsvr32 /s wuapi.dll c:\&amp;gt; regsvr32 /s wups.dll c:\&amp;gt; regsvr32 /s wuaueng.dll c:\&amp;gt; regsvr32 /s wucltui.dll c:\&amp;gt; regsvr32 /s msxml3.dll c:\&amp;gt; cd %windir%\SoftwareDistribution c:\&amp;gt; rd /s/q DataStore c:\&amp;gt; mkdir DataStore c:\&amp;gt; rd /s/q Download c:\&amp;gt; mkdir Download c:\&amp;gt; net start wuauserv c:\&amp;gt; reg delete HKLM\SOFTWARE\Microsoft\Windows\CurrentVersion\WindowsUpdate /v AccountDomainSid /f c:\&amp;gt; reg delete HKLM\SOFTWARE\Microsoft\Windows\CurrentVersion\WindowsUpdate /v PingID /f c:\&amp;gt; reg delete HKLM\SOFTWARE\Microsoft\Windows\CurrentVersion\WindowsUpdate /v SusClientId /f c:\&amp;gt; wuauclt /resetauthorization c:\&amp;gt; wuauclt /detectnow c:\&amp;gt; wuauclt /reportnow Configuring Windows Remote Management for Ansible Ansible requires no agents. It uses what the OS provides for communication. In Windows you can use SSH or Windows Remote Management (WinRM). Since SSH is more or less bolted on, WinRM is usually the preferred choice. Test to ensure WinRM is working First install WinRM if it isn&apos;t installed. Execute the hostname command through WinRM. PS C:\windows\system32&amp;gt; winrs -r:http://:5985/wsman -u: -p: hostname win2012 Optionally you can also test WinRM by making a remote desktop connection from another windows host. Note: you may run into an issue where you get an authentication error and a CredSSL issue if you have an older windows version. This can also happen if you don&apos;t have a valid SSL certificate. If you run into this issue, update your registry. c:\&amp;gt;reg add &quot;HKLM\Software\Microsoft\Windows\CurrentVersion\Policies\System\CredSSP\Parameters&quot; /f /v AllowEncryptionOracle /t REG_DWORD /d 2 Enable basic auth There are several authentication methods. In this example we will use basic authentication which is username/password. This is of course the least secure method. PS C:\&amp;gt; Set-Item -Path WSMan:\localhost\Service\Auth\Basic -Value $true Update WinRM  A script is provided by Ansible community to check WinRM and make necessary changes to allow Ansible to connect. In this case we are using basic authentication but likely you will want to use something more secure. The script can be found here and other authentication options are documented in the script header. Open a powershell command prompt. Store URL path to script. PS C:\&amp;gt; $url = &quot;https://raw.githubusercontent.com/ansible/ansible/devel/examples/scripts/ConfigureRemotingForAnsible.ps1&quot; Store location for the script PS C:\&amp;gt; $file = &quot;$env:temp\ConfigureRemotingForAnsible.ps1&quot; Download script and output to file locally. PS C:\&amp;gt; (New-Object -TypeName System.Net.WebClient).DownloadFile($url, $file) Execute the script. PS C:\&amp;gt; powershell.exe -ExecutionPolicy ByPass -File $file Ok. Check WinRM connection PS C:\windows\system32&amp;gt; winrm enumerate winrm/config/Listener Listener Address = * Transport = HTTP Port = 5985 Hostname Enabled = true URLPrefix = wsman CertificateThumbprint ListeningOn = 10.10.1.139, 127.0.0.1, ::1, fe80::5efe:10.10.1.139%22, fe80::8155:327a:aeaf:365a%12</summary></entry><entry><title type="html">Red Hat Subscription Reporting Guide</title><link href="http://localhost:4000/general/red-hat-subscription-reporting-guide/" rel="alternate" type="text/html" title="Red Hat Subscription Reporting Guide" /><published>2020-05-13T00:00:00-07:00</published><updated>2020-05-13T00:00:00-07:00</updated><id>http://localhost:4000/general/red-hat-subscription-reporting-guide</id><content type="html" xml:base="http://localhost:4000/general/red-hat-subscription-reporting-guide/">&lt;h2&gt;&lt;img class=&quot;alignnone  wp-image-14584&quot; src=&quot;/assets/2020/05/ecommerce-subscription-ts-100621375-large.jpg&quot; alt=&quot;ecommerce-subscription-ts-100621375-large&quot; width=&quot;362&quot; height=&quot;240&quot; /&gt;&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;source: https://www.cio.com/article/3199910/zuora-blazes-a-new-trail-to-the-subscription-economy-and-a-post-erp-world.html&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;This article will look at the various options available, to do subscription reporting for Red Hat products. Many large organizations sometimes struggle to keep track of what subscriptions are being used, often maintaining their own spreadsheets. This can be very error prone and time consuming. Systems can even be subscribed to the wrong subscriptions, for example a virtual machine running RHEL using a physical subscription. Many Red Hat customers have several different products, not just RHEL and being able to actively inventory the entire subscription landscape is critical.&lt;/p&gt;
&lt;p&gt;&lt;!--more--&gt;&lt;/p&gt;
&lt;h2&gt;Options for Subscription Reporting&lt;/h2&gt;
&lt;p&gt;There are various options for subscription reporting depending on if you have Satellite (Red Hat Systems Management) or not.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Subscription Entitlement Report &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Starting with Satellite 6.7 there is now a canned report that is very customizable. This is the recommended way of reporting subscriptions in Satellite 6.7+. Provides granular host level subscription reporting.&lt;/p&gt;
&lt;p&gt;Example of the out-of-the-box entitlement report is &lt;a href=&quot;https://gist.github.com/ktenzer/7632ad9f75ba4424b1b59a9e0ba229ea&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Red Hat Discovery Tool&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The discovery tool can do reporting on subscriptions in Satellite and non-Satellite environments. Provides granular host level subscription reporting. This is the recommendation if you don&apos;t have Satellite 6.7+ or have systems not subscribing through Satellite. The discovery tool can be installed in a connected or disconnected environment.&lt;/p&gt;
&lt;p&gt;The discovery tool also has an upstream project and is available via the community: &lt;a href=&quot;https://github.com/quipucords/quipucords&quot;&gt;https://github.com/quipucords/quipucords&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Red Hat Subscription Watch&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is a cloud service offering that can aggregate subscription reporting across various infrastructure or even organizations. Data is sent either from Red Hat Insights or Satellite to a customers portal on cloud.redhat.com. This is a recommended addition to provide additional visibility but does not provide granular information at the host level.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Subscription Inventory Script&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Provides granular host level subscription reporting for Satellite environments. This is a script not a product so use at your own risk. However this is recommended if you have Satellite 6.6 or below and cannot for whatever reason leverage the Red Hat Discovery tool.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/RedHatSatellite/sat6Inventory/tree/product-cert&quot;&gt;https://github.com/RedHatSatellite/sat6Inventory/tree/product-cert&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Using the Satellite Subscription Inventory Script&lt;/h2&gt;
&lt;p&gt;Since the other options are products and well documented, I will focus on showing how to do subscription reporting using the inventory script on RHEL 7. First you will need to deploy a RHEL 7 system.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Enable Correct RHEL Repos&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First ensure all repositories are disabled and then just enable the rhel-7-server-rpms.&lt;/p&gt;
&lt;pre&gt;$ sudo subscription-manager repos --disable=*
$ sudo subscription-manager repos --enable=rhel-7-server-rpms&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Install python 2.6+&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;$ sudo yum install -y python&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Install Git (Only for connected setup)&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;$ sudo yum install -y git&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Clone git repository (Only for connected setup)&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;$ git clone https://github.com/RedHatSatellite/sat6Inventory.git&lt;/pre&gt;
&lt;p&gt;If the system isn&apos;t connected to the internet then you need to download from the git repository. You can download the script from &lt;a href=&quot;https://github.com/RedHatSatellite/sat6Inventory/tree/product-cert&quot;&gt;https://github.com/RedHatSatellite/sat6Inventory/tree/product-cert&lt;/a&gt; by selecting download. You can then copy the tarball to your system and extract it.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;alignnone  wp-image-14605&quot; src=&quot;/assets/2020/05/download.png&quot; alt=&quot;download&quot; width=&quot;1828&quot; height=&quot;484&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Change directory&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;cd sat6Inventory&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Run the inventory script&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You need to pass the admin user, password and the Satellite organization.&lt;/p&gt;
&lt;pre&gt;./sat6Inventory.py -s sat-bc87.rhpds.opentlc.com -l admin -p  \
-o &apos;EXAMPLE.COM&apos;&lt;/pre&gt;
&lt;p&gt;The script will output a CSV.&lt;/p&gt;
&lt;pre&gt;$ ls
&lt;strong&gt;EXAMPLE.COM_inventory_report.csv&lt;/strong&gt; LICENSE README.md Sample Report sat6Inventory.py&lt;/pre&gt;
&lt;p&gt;You can now import the script output CSV into a spreadsheet program and manipulate as desired.&lt;/p&gt;
&lt;p&gt;A sample report is also provided &lt;a href=&quot;https://github.com/RedHatSatellite/sat6Inventory/blob/product-cert/Sample%20Report/Example_inventory_report.csv&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Reconciling mismatched entitlements&lt;/h2&gt;
&lt;p&gt;If you have a Satellite environment with RHEL physical and VDC (virtual subscriptions) it is possible that a virtual RHEL host is possibly using a physical instead of virtual subscription. To identify such hosts you can run the below command from your Satellite server.&lt;/p&gt;
&lt;pre&gt;$ sudo foreman-rake katello:virt_who_report&lt;/pre&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this article we covered the various options for doing subscription reporting of Red Hat products. We also showed a short hands-on guide for using the inventory script. Hopefully this provided guidance to choose the most appropriate approach to subscription reporting.&lt;/p&gt;
&lt;p&gt;(c) 2020 Keith Tenzer&lt;/p&gt;</content><author><name>Keith Tenzer</name></author><category term="General" /><category term="Red Hat" /><category term="Reporting" /><category term="RHEL" /><category term="Satellite" /><category term="Subscription" /><summary type="html">source: https://www.cio.com/article/3199910/zuora-blazes-a-new-trail-to-the-subscription-economy-and-a-post-erp-world.html Overview This article will look at the various options available, to do subscription reporting for Red Hat products. Many large organizations sometimes struggle to keep track of what subscriptions are being used, often maintaining their own spreadsheets. This can be very error prone and time consuming. Systems can even be subscribed to the wrong subscriptions, for example a virtual machine running RHEL using a physical subscription. Many Red Hat customers have several different products, not just RHEL and being able to actively inventory the entire subscription landscape is critical. Options for Subscription Reporting There are various options for subscription reporting depending on if you have Satellite (Red Hat Systems Management) or not. Subscription Entitlement Report  Starting with Satellite 6.7 there is now a canned report that is very customizable. This is the recommended way of reporting subscriptions in Satellite 6.7+. Provides granular host level subscription reporting. Example of the out-of-the-box entitlement report is here. Red Hat Discovery Tool The discovery tool can do reporting on subscriptions in Satellite and non-Satellite environments. Provides granular host level subscription reporting. This is the recommendation if you don&apos;t have Satellite 6.7+ or have systems not subscribing through Satellite. The discovery tool can be installed in a connected or disconnected environment. The discovery tool also has an upstream project and is available via the community: https://github.com/quipucords/quipucords Red Hat Subscription Watch This is a cloud service offering that can aggregate subscription reporting across various infrastructure or even organizations. Data is sent either from Red Hat Insights or Satellite to a customers portal on cloud.redhat.com. This is a recommended addition to provide additional visibility but does not provide granular information at the host level. Subscription Inventory Script Provides granular host level subscription reporting for Satellite environments. This is a script not a product so use at your own risk. However this is recommended if you have Satellite 6.6 or below and cannot for whatever reason leverage the Red Hat Discovery tool. https://github.com/RedHatSatellite/sat6Inventory/tree/product-cert Using the Satellite Subscription Inventory Script Since the other options are products and well documented, I will focus on showing how to do subscription reporting using the inventory script on RHEL 7. First you will need to deploy a RHEL 7 system. Enable Correct RHEL Repos First ensure all repositories are disabled and then just enable the rhel-7-server-rpms. $ sudo subscription-manager repos --disable=* $ sudo subscription-manager repos --enable=rhel-7-server-rpms Install python 2.6+ $ sudo yum install -y python Install Git (Only for connected setup) $ sudo yum install -y git Clone git repository (Only for connected setup) $ git clone https://github.com/RedHatSatellite/sat6Inventory.git If the system isn&apos;t connected to the internet then you need to download from the git repository. You can download the script from https://github.com/RedHatSatellite/sat6Inventory/tree/product-cert by selecting download. You can then copy the tarball to your system and extract it. Change directory cd sat6Inventory Run the inventory script You need to pass the admin user, password and the Satellite organization. ./sat6Inventory.py -s sat-bc87.rhpds.opentlc.com -l admin -p \ -o &apos;EXAMPLE.COM&apos; The script will output a CSV. $ ls EXAMPLE.COM_inventory_report.csv LICENSE README.md Sample Report sat6Inventory.py You can now import the script output CSV into a spreadsheet program and manipulate as desired. A sample report is also provided here. Reconciling mismatched entitlements If you have a Satellite environment with RHEL physical and VDC (virtual subscriptions) it is possible that a virtual RHEL host is possibly using a physical instead of virtual subscription. To identify such hosts you can run the below command from your Satellite server. $ sudo foreman-rake katello:virt_who_report Summary In this article we covered the various options for doing subscription reporting of Red Hat products. We also showed a short hands-on guide for using the inventory script. Hopefully this provided guidance to choose the most appropriate approach to subscription reporting. (c) 2020 Keith Tenzer</summary></entry></feed>