<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://keithtenzer.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://keithtenzer.com/" rel="alternate" type="text/html" /><updated>2021-12-03T21:44:58-08:00</updated><id>https://keithtenzer.com/feed.xml</id><title type="html">Keith Tenzer’s Blog</title><subtitle></subtitle><author><name>Keith Tenzer</name></author><entry><title type="html">Building Ansible Operators 1-2-3</title><link href="https://keithtenzer.com/openshift/building-ansible-operators-1-2-3/" rel="alternate" type="text/html" title="Building Ansible Operators 1-2-3" /><published>2021-12-03T00:00:00-08:00</published><updated>2021-12-03T00:00:00-08:00</updated><id>https://keithtenzer.com/openshift/building-ansible-operators-1-2-3</id><content type="html" xml:base="https://keithtenzer.com/openshift/building-ansible-operators-1-2-3/">&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;In this article we will go step by step in build a Kubernetes Operator using Ansible and the Operator Framework. Operators provide ability to not only deploy an application but also manage the entire lifecycle while also baking in higher degrees of intelligence and resilience. Before we get to the Operator lets briefly revisit the ways we can deploy and manage applications in k8s.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Deployment Config&lt;/li&gt;
  &lt;li&gt;Deployment Template&lt;/li&gt;
  &lt;li&gt;Helm&lt;/li&gt;
  &lt;li&gt;Operator&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A deployment config is the simplest method but there is no way to parameterize unless you are doing it through Ansible k8s module or something else that handles that. A deployment template does provide parameterization but is only available on OpenShift and also doesn’t handle packaging or management of the application itself. Helm provides parameterization and also packaging but doesn’t provide application lifecycle management or allow for building extended intelligence or resilience. Operators provide the full package and provide the pattern to improve operability over time. Some suggest just use the simplest tool for the job so if you just need to deploy an app for example, don’t need parameterization make a deployment config. Personally my view is that with Ansible, Operators are just about as easy as a deployment config but with so much added benefit that is always makes sense to take this approach from the beginning.
My hope and goal with this article is to maybe influence a few more Operators so let’s see what we can accomplish.&lt;/p&gt;

&lt;h2 id=&quot;pre-requisites&quot;&gt;Pre-requisites&lt;/h2&gt;
&lt;p&gt;Your starting point should be an application you can deploy using a yaml deployment config. From there the next thing is to setup a development environment for building Operators using the Operator Framework.
&lt;a href=&quot;https://docs.openshift.com/container-platform/4.8/operators/operator_sdk/ansible/osdk-ansible-quickstart.html&quot;&gt;Operator Pre-requisites&lt;/a&gt;
Once you have operator-sdk and openshift or kubectl client you need some Ansible dependencies.&lt;/p&gt;
&lt;pre&gt;$ sudo dnf install -y ansible&lt;/pre&gt;
&lt;pre&gt;$ sudo dnf install -y python-ansible-runner&lt;/pre&gt;
&lt;pre&gt;$ sudo dnf install -y python-ansible-runner&lt;/pre&gt;
&lt;pre&gt;$ sudo pip install ansible-runner-http&lt;/pre&gt;
&lt;pre&gt;$ sudo pip install openshift&lt;/pre&gt;

&lt;h2 id=&quot;create-operator-scaffolding&quot;&gt;Create Operator Scaffolding&lt;/h2&gt;
&lt;p&gt;Before we can begin coding the Operator we need boiler plate code and thankfully the operator sdk does that for us.&lt;/p&gt;
&lt;pre&gt;$ operator-sdk init --plugins=ansible --domain=kubecraft.com&lt;/pre&gt;
&lt;pre&gt;$ operator-sdk create api     --group cache     --version v1     --kind Kubecraft     --generate-role&lt;/pre&gt;

&lt;h2 id=&quot;customizing-operator&quot;&gt;Customizing Operator&lt;/h2&gt;
&lt;p&gt;At this point we need an application and what I always do is create a deployment yaml to ensure I have all the pieces and test deploying my application to k8s before building Operator. In this example we will use an app called &lt;a href=&quot;https://github.com/ktenzer/kubecraftadmin&quot;&gt;Kubekraft&lt;/a&gt;. It is a fun app that connects k8s world to minecraft through a minecraft websocket server. Browse to the yaml folder and you will see the deployment.yaml. This is what we will use to build out our Operator.
Under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;roles/kubecraft/tasks&lt;/code&gt; directory we create tasks in Ansible to deploy what we had in the deployment yaml which is a deployment, service and route. In addition we added a task to get the application domain dynamically so we can build our route properly.&lt;/p&gt;

&lt;h2 id=&quot;testing-operator&quot;&gt;Testing Operator&lt;/h2&gt;
&lt;p&gt;The operator sdk provides a simple way to test the operator locally. Once we have our tasks complete under the role we simply need to create a new project, run the operator locally and create a custom resource (CR) which executes our role and tasks we defined.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Create project&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;$ oc new-project kubecraft&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Run operator locally&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;$ make install&lt;/pre&gt;
&lt;pre&gt;$ make run&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Create custom resource&lt;/strong&gt;
In a new terminal create CR. Also note that our operator expects user input a comma separated list of namespaces to monitor. User input is parameterized via the custom resource so if you look inside you will see the namespaces parameter set and being consumed within the Ansible role.&lt;/p&gt;
&lt;pre&gt;$ oc create -f config/samples/cache_v1_kubecraft.yaml&lt;/pre&gt;

&lt;h2 id=&quot;building-operator&quot;&gt;Building Operator&lt;/h2&gt;
&lt;p&gt;Once we have tested the operator locally we can build and publish our operator to a registry.
&lt;strong&gt;Authenticate to registry&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;$ sudo docker login https://registry.redhat.io&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Build operator image and push to registry&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;$ sudo make docker-build docker-push IMG=quay.io/ktenzer/kubecraft-operator:latest&lt;/pre&gt;
&lt;p&gt;If you are using quay.io as your registry make sure to login and make the image public so it can be accessed.&lt;/p&gt;

&lt;h2 id=&quot;running-operator&quot;&gt;Running Operator&lt;/h2&gt;
&lt;p&gt;Now that we have the operator tested and the image built we can simply deploy it, create a CR and rule the world!
 The operator sdk makes this really easy and streamlines everything into a single command.&lt;/p&gt;
&lt;pre&gt;$ make deploy IMG=quay.io/ktenzer/kubecraft-operator:latest&lt;/pre&gt;

&lt;h2 id=&quot;create-operator-bundle&quot;&gt;Create Operator Bundle&lt;/h2&gt;
&lt;p&gt;The operator bundle allows integration with operator lifecycle manager (olm) which provides a facility for upgrading operator seamlessly as well as integrating with operator hub. First we will generate the bundle boiler plate.&lt;/p&gt;
&lt;pre&gt;$ make bundle&lt;/pre&gt;
&lt;p&gt;If you want to change anything, like add image you can update &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bundle/manifests&lt;/code&gt; clusterserviceversion. When your ready you will build bundle and then push it to your repository. Remember if using quay.io to make it public.&lt;/p&gt;
&lt;pre&gt;$ sudo make bundle-build BUNDLE_IMG=quay.io/ktenzer/kubecraft-operator-bundle:latest&lt;/pre&gt;
&lt;p&gt;Push bundle to registry.&lt;/p&gt;
&lt;pre&gt;$ sudo docker push quay.io/ktenzer/kubecraft-operator-bundle:latest&lt;/pre&gt;
&lt;h2 id=&quot;run-operator-bundle&quot;&gt;Run operator bundle&lt;/h2&gt;
&lt;p&gt;Now that we have the bundle built and pushed to registry we can deploy it to a cluster.&lt;/p&gt;
&lt;pre&gt;$ operator-sdk run bundle -n kubecraft-operator quay.io/ktenzer/kubecraft-operator-bundle:latest&lt;/pre&gt;
&lt;p&gt;Once our operator bundle is running simply create a new project, a CR and watch the magic happen.&lt;/p&gt;
&lt;pre&gt;$ oc project kubecraft&lt;/pre&gt;
&lt;pre&gt;$ oc create -f config/samples/cache_v1_kubecraft.yaml&lt;/pre&gt;
&lt;pre&gt;$ oc get deployment
NAME        READY   UP-TO-DATE   AVAILABLE   AGE
kubecraft   1/1     1            1           5s
&lt;/pre&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this article we created an operator using the operator sdk and Ansible. We saw how to take a simple deployment yaml file and turn it into a fully managed operator. Finally we created a bundle showing the integration with operator lifecycle manager and operator hub. Hopefully this article helps you get started with Ansible operators and next time you deploy an application in k8s you would consider the operator approach.&lt;/p&gt;

&lt;p&gt;(c) 2021 Keith Tenzer&lt;/p&gt;</content><author><name>Keith Tenzer</name></author><category term="OpenShift" /><category term="Containers" /><category term="OpenShift" /><category term="Operators" /><category term="Operator Framework" /><category term="Ansible" /><summary type="html">Overview In this article we will go step by step in build a Kubernetes Operator using Ansible and the Operator Framework. Operators provide ability to not only deploy an application but also manage the entire lifecycle while also baking in higher degrees of intelligence and resilience. Before we get to the Operator lets briefly revisit the ways we can deploy and manage applications in k8s. Deployment Config Deployment Template Helm Operator</summary></entry><entry><title type="html">OpenShift Service Mesh Getting Started Guide</title><link href="https://keithtenzer.com/openshift/openshift-service-mesh-getting-started-guide/" rel="alternate" type="text/html" title="OpenShift Service Mesh Getting Started Guide" /><published>2021-04-27T00:00:00-07:00</published><updated>2021-04-27T00:00:00-07:00</updated><id>https://keithtenzer.com/openshift/openshift-service-mesh-getting-started-guide</id><content type="html" xml:base="https://keithtenzer.com/openshift/openshift-service-mesh-getting-started-guide/">&lt;h2&gt;&lt;/h2&gt;
&lt;p&gt;&lt;!-- wp:image {&quot;id&quot;:12890,&quot;width&quot;:79,&quot;height&quot;:83,&quot;sizeSlug&quot;:&quot;large&quot;,&quot;linkDestination&quot;:&quot;media&quot;} --&gt;&lt;/p&gt;
&lt;figure class=&quot;wp-block-image size-large is-resized&quot;&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2018/12/OpenShift-LogoType.svg_.png&quot;&gt;&lt;img src=&quot;/assets/2021/04/OpenShift-LogoType.svg_.png?w=959&quot; alt=&quot;&quot; class=&quot;wp-image-12890&quot; width=&quot;79&quot; height=&quot;83&quot; /&gt;&lt;/a&gt;&lt;/figure&gt;
&lt;p&gt;&lt;!-- /wp:image --&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2018/12/OpenShift-LogoType.svg_.png&quot;&gt;&lt;!-- /wp:image --&gt;&lt;/a&gt;&lt;!-- /wp:image --&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/04/istio.png&quot;&gt;&lt;!-- /wp:image --&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;!-- /wp:image --&gt;&lt;/h2&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;In this article we will explore the OpenShift Service Mesh and deploy a demo application to better understand the various concepts. First you might be asking yourself, why do I need a Service Mesh? If you have a microservice architecture then you likely have many services that interact with one another in various ways. If a downstream service breaks, how will you know? How can you trace an error through all services to find out where it originated? How will you manage exposing new APIs/capabilities to users? Service Mesh provides the answer to those questions providing 1) Visibility across all microservices 2) Traceability through all of the microservice interactions and 3) Ruleset governing service versions and capabilities that are introduced into the environment. OpenShift Service Mesh uses Istio as the mesh, Kiali for the dashboard and Jaeger for traceability. &lt;/p&gt;
&lt;p&gt;&lt;!--more--&gt;&lt;/p&gt;
&lt;h2&gt;Install OpenShift Service Mesh&lt;/h2&gt;
&lt;p&gt;OpenShift Service Mesh enables visibility of traffic as is traverses all services within the mesh. In case of an error you can not only see what service is actually the source for the error but also are able to trace the API communications between services. Finally it allows us to implement rules, like balancing traffic between multiple ratings API endpoints or even sending specific users to an API version. You can even create a chaos monkey using the ruleset that will inject various errors so you can observe how failures are handled. A Service Mesh is critical for any complex microservice application and without it you are literally flying blind while adding technical debt unable to manage or monitor service interactions properly.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Create a new project&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First we will create a project for hosting the service mesh control plane.&lt;/p&gt;
&lt;pre&gt;$ oc new-project bookinfo-mesh&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Install OpenShift Service Mesh&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under the project booking-mesh go to Operator Hub and install Red Hat OpenShift Service Mesh, Red Hat OpenShift Jaeger and Kiali Operator.&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:image {&quot;id&quot;:14769,&quot;sizeSlug&quot;:&quot;large&quot;,&quot;linkDestination&quot;:&quot;media&quot;} --&gt;&lt;/p&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/04/operators_servicemesh.png&quot;&gt;&lt;img class=&quot;wp-image-14769&quot; src=&quot;/assets/2021/04/operators_servicemesh.png?w=1024&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/figure&gt;
&lt;p&gt;&lt;!-- /wp:image --&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Configure OpenShift Service Mesh&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Create a service mesh control plane using defaults.&lt;/p&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/04/booking_mesh-1.png&quot;&gt;&lt;img class=&quot;wp-image-14742&quot; src=&quot;/assets/2021/04/booking_mesh-1.png?w=1024&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;Create a service mesh member. Update the yaml and change namespace under controlPlaneRef to bookinfo-mesh.&lt;/figure&gt;
&lt;pre&gt;apiVersion: maistra.io/v1&lt;br /&gt;kind: ServiceMeshMember&lt;br /&gt;metadata:&lt;br /&gt;  namespace: bookinfo-mesh&lt;br /&gt;  name: default&lt;br /&gt;spec:&lt;br /&gt;  controlPlaneRef:&lt;br /&gt;    name: basic&lt;br /&gt;    namespace: bookinfo-mesh&lt;/pre&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/04/mesh_role.png&quot;&gt;&lt;img class=&quot;wp-image-14744&quot; src=&quot;/assets/2021/04/mesh_role.png?w=1024&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;Finally create a service member role adding name of the project that will access the service mesh..&lt;/figure&gt;
&lt;div&gt;
&lt;pre&gt;apiVersion: maistra.io/v1&lt;br /&gt;kind: ServiceMeshMemberRoll&lt;br /&gt;metadata:&lt;br /&gt;  namespace: bookinfo-mesh&lt;br /&gt;  name: default&lt;br /&gt;spec:&lt;br /&gt;  members:&lt;br /&gt;    - bookinfo&lt;/pre&gt;
&lt;/div&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/04/mesh_member_role.png&quot;&gt;&lt;img class=&quot;wp-image-14746&quot; src=&quot;/assets/2021/04/mesh_member_role.png?w=1024&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/figure&gt;
&lt;h2&gt;Deploy Demo Application&lt;/h2&gt;
&lt;p&gt;In order to look into the capabilities provided by the OpenShift Service Mesh we will deploy a simple book review app. The application is static but shows reviews for a book and has several microservices. The product page is the entry point service. It provides information on books and their reviews. It accesses book reviews through the reviews service which is also accesses the ratings service downstream to allow users to give a book a rating. It also gets details about a book via the details service. Like a true polyglot all services are written in a different programming language. The ratings service provides three different API versions, v1 doesn&apos;t display ratings, v2 shows ratings in black and v3 shows ratings in red. The idea is all about innovating quickly and getting real user feedback to take the app in the right direction.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/04/demoapp.png&quot;&gt;&lt;img class=&quot;wp-image-14764&quot; src=&quot;/assets/2021/04/demoapp.png?w=1024&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Create a project for the book app.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Create a project to host the book app.&lt;/p&gt;
&lt;pre&gt;$ oc new-project bookinfo&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Deploy book app.&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;$ oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-2.0/samples/bookinfo/platform/kube/bookinfo.yaml&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Create a service mesh gateway.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Once the app is deployed we need to create a gateway and setup the URI matches.&lt;/p&gt;
&lt;pre&gt;$ oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-2.0/samples/bookinfo/networking/bookinfo-gateway.yaml&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Create service mesh rule set.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Since the ratings service has 3 API versions we need some rules to govern the traffic.&lt;/p&gt;
&lt;pre&gt;$ oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-2.0/samples/bookinfo/networking/destination-rule-all.yaml&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Access the application.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Get the route to the application and add the /productpage to access via web browser.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ export GATEWAY_URL=$(oc -n istio-system get route istio-ingressgateway -o jsonpath=&apos;{.spec.host}&apos;)&lt;br /&gt;$ echo $GATEWAY_URL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;http://istio-ingressgateway-bookinfo-mesh.apps.ocp4.rh-southwest.com/productpage&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:image {&quot;id&quot;:14775,&quot;sizeSlug&quot;:&quot;large&quot;,&quot;linkDestination&quot;:&quot;media&quot;} --&gt;&lt;/p&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/04/bookinfo2.png&quot;&gt;&lt;img class=&quot;wp-image-14775&quot; src=&quot;/assets/2021/04/bookinfo2.png?w=1024&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/figure&gt;
&lt;p&gt;&lt;!-- /wp:image --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:image {&quot;id&quot;:14752,&quot;sizeSlug&quot;:&quot;large&quot;,&quot;linkDestination&quot;:&quot;media&quot;} --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:image --&gt;If you refresh the web page you should see the ratings change between no ratings, black and red. This is because the ruleset is sending traffic to all three API endpoints (v1, v2 and v3).&lt;/p&gt;
&lt;h2&gt;Update Service Mesh Ruleset&lt;/h2&gt;
&lt;p&gt;As mentioned we can create rules that enable us to send certain traffic to a different API endpoint. This could be for a canary deployment where we want to test how users interact with different capabilities (red or black ratings stars), to perform a blue/green deployment upgrading to a newer API version or even sending different users to different APIs.&lt;/p&gt;
&lt;p&gt;In this case we will simply apply a new rule that only sends traffic to v2 (black) and v3 (red) ratings API.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Apply a new ruleset.&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ oc replace -f https://raw.githubusercontent.com/istio/istio/master/samples/bookinfo/networking/virtual-service-reviews-v2-v3.yaml&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now when you access the book app and refresh you should see it switched between red and black ratings.&lt;/p&gt;
&lt;p&gt;Looking at Kiali versioned app graph we can see that traffic is only going to v2 and v3 as we would expect.&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:image {&quot;id&quot;:14760,&quot;sizeSlug&quot;:&quot;large&quot;,&quot;linkDestination&quot;:&quot;media&quot;} --&gt;&lt;/p&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;!-- wp:image {&quot;id&quot;:14784,&quot;sizeSlug&quot;:&quot;large&quot;,&quot;linkDestination&quot;:&quot;media&quot;} --&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/04/kiali_ratings_v2_v3-1.png&quot;&gt;&lt;img class=&quot;wp-image-14784&quot; src=&quot;/assets/2021/04/kiali_ratings_v2_v3-1.png?w=1024&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/figure&gt;
&lt;p&gt;&lt;!-- /wp:image --&gt;&lt;/p&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;!-- /wp:image --&gt;&lt;/p&gt;
&lt;h2&gt;Troubleshooting Errors&lt;/h2&gt;
&lt;p&gt;As mentioned one of the key values of the OpenShift Service Mesh is the visualization and tracing. In order to generate an error we will scale down the ratings v1 deployment to 0.&lt;/p&gt;
&lt;pre&gt;$ oc scale deployment/ratings-v1 -n bookinfo --replicas 0&lt;/pre&gt;
&lt;p&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/04/bookinfo_error.png&quot;&gt;&lt;img class=&quot;wp-image-14752&quot; src=&quot;/assets/2021/04/bookinfo_error.png?w=1024&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You should now see that the ratings service is currently unavailable when you refresh the app in a browser.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Check Kiali dashboard.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You can access Kiali via route under networking in the bookinfo-mesh project. In Kiali we clearly see the issue is the reviews service as we would expect. &lt;/p&gt;
&lt;p&gt;&lt;!-- wp:image {&quot;id&quot;:14751,&quot;sizeSlug&quot;:&quot;large&quot;,&quot;linkDestination&quot;:&quot;media&quot;} --&gt;&lt;/p&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/04/kali_error.png&quot;&gt;&lt;img class=&quot;wp-image-14751&quot; src=&quot;/assets/2021/04/kali_error.png?w=1024&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/figure&gt;
&lt;p&gt;&lt;!-- /wp:image --&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Open Jaeger to trace the calls that are failing.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Next we can dig into the requests by opening distributed tracing (Jaeger) from the Kiali dashboard. We can see the flow of all calls grouped and identify all request to the ratings service that are throwing an error. We see a 503 is returned which means service in unavailable.&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:image {&quot;id&quot;:14753,&quot;sizeSlug&quot;:&quot;large&quot;,&quot;linkDestination&quot;:&quot;media&quot;} --&gt;&lt;/p&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/04/jaeger_error.png&quot;&gt;&lt;img class=&quot;wp-image-14753&quot; src=&quot;/assets/2021/04/jaeger_error.png?w=1024&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/figure&gt;
&lt;p&gt;&lt;!-- /wp:image --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Kiali dashboard also shows request response (latency in ms) for slowest 50%, 95% and 99% of requests. This allows us to not only see the average latency but compare it to the slowest requests which indicates if we have spikes that could cause user slowness that we might not easily see when looking at just the average.&lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:paragraph --&gt; &lt;!-- wp:image {&quot;id&quot;:14779,&quot;sizeSlug&quot;:&quot;large&quot;,&quot;linkDestination&quot;:&quot;media&quot;} --&gt;&lt;/p&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/04/kali_error2.png&quot;&gt;&lt;img class=&quot;wp-image-14779&quot; src=&quot;/assets/2021/04/kali_error2.png?w=337&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/figure&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this article we discussed the capabilities OpenShift Service Mesh provides for microservice applications. We deployed a service mesh and a book demo polyglot application that leverages the OpenShift Service Mesh. Using Kiali we saw how to gain visibility into our microservice application and easily identify problems as well as trends. Through Jaeger and distributed tracing we were able to identify the exact API causing error conditions. Microservice architectures provide a lot of value but they are harder to manage, control and troubleshoot in certain aspects than their monolithic peers which is why OpenShift Service Mesh is so critical.&lt;/p&gt;
&lt;p&gt;(c) 2021 Keith Tenzer&lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:image --&gt;&lt;/p&gt;</content><author><name>Keith Tenzer</name></author><category term="OpenShift" /><category term="Containers" /><category term="istio" /><category term="jaeger" /><category term="kiali" /><category term="microservice" /><category term="OpenShift" /><category term="polyglot" /><category term="service mesh" /><summary type="html">Overview In this article we will explore the OpenShift Service Mesh and deploy a demo application to better understand the various concepts. First you might be asking yourself, why do I need a Service Mesh? If you have a microservice architecture then you likely have many services that interact with one another in various ways. If a downstream service breaks, how will you know? How can you trace an error through all services to find out where it originated? How will you manage exposing new APIs/capabilities to users? Service Mesh provides the answer to those questions providing 1) Visibility across all microservices 2) Traceability through all of the microservice interactions and 3) Ruleset governing service versions and capabilities that are introduced into the environment. OpenShift Service Mesh uses Istio as the mesh, Kiali for the dashboard and Jaeger for traceability.  Install OpenShift Service Mesh OpenShift Service Mesh enables visibility of traffic as is traverses all services within the mesh. In case of an error you can not only see what service is actually the source for the error but also are able to trace the API communications between services. Finally it allows us to implement rules, like balancing traffic between multiple ratings API endpoints or even sending specific users to an API version. You can even create a chaos monkey using the ruleset that will inject various errors so you can observe how failures are handled. A Service Mesh is critical for any complex microservice application and without it you are literally flying blind while adding technical debt unable to manage or monitor service interactions properly. Create a new project First we will create a project for hosting the service mesh control plane. $ oc new-project bookinfo-mesh Install OpenShift Service Mesh Under the project booking-mesh go to Operator Hub and install Red Hat OpenShift Service Mesh, Red Hat OpenShift Jaeger and Kiali Operator. Configure OpenShift Service Mesh Create a service mesh control plane using defaults. Create a service mesh member. Update the yaml and change namespace under controlPlaneRef to bookinfo-mesh. apiVersion: maistra.io/v1kind: ServiceMeshMembermetadata:  namespace: bookinfo-mesh  name: defaultspec:  controlPlaneRef:    name: basic    namespace: bookinfo-mesh Finally create a service member role adding name of the project that will access the service mesh.. apiVersion: maistra.io/v1kind: ServiceMeshMemberRollmetadata:  namespace: bookinfo-mesh  name: defaultspec:  members:    - bookinfo Deploy Demo Application In order to look into the capabilities provided by the OpenShift Service Mesh we will deploy a simple book review app. The application is static but shows reviews for a book and has several microservices. The product page is the entry point service. It provides information on books and their reviews. It accesses book reviews through the reviews service which is also accesses the ratings service downstream to allow users to give a book a rating. It also gets details about a book via the details service. Like a true polyglot all services are written in a different programming language. The ratings service provides three different API versions, v1 doesn&apos;t display ratings, v2 shows ratings in black and v3 shows ratings in red. The idea is all about innovating quickly and getting real user feedback to take the app in the right direction. Create a project for the book app. Create a project to host the book app. $ oc new-project bookinfo Deploy book app. $ oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-2.0/samples/bookinfo/platform/kube/bookinfo.yaml Create a service mesh gateway. Once the app is deployed we need to create a gateway and setup the URI matches. $ oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-2.0/samples/bookinfo/networking/bookinfo-gateway.yaml Create service mesh rule set. Since the ratings service has 3 API versions we need some rules to govern the traffic. $ oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-2.0/samples/bookinfo/networking/destination-rule-all.yaml Access the application. Get the route to the application and add the /productpage to access via web browser. $ export GATEWAY_URL=$(oc -n istio-system get route istio-ingressgateway -o jsonpath=&apos;{.spec.host}&apos;)$ echo $GATEWAY_URL http://istio-ingressgateway-bookinfo-mesh.apps.ocp4.rh-southwest.com/productpage If you refresh the web page you should see the ratings change between no ratings, black and red. This is because the ruleset is sending traffic to all three API endpoints (v1, v2 and v3). Update Service Mesh Ruleset As mentioned we can create rules that enable us to send certain traffic to a different API endpoint. This could be for a canary deployment where we want to test how users interact with different capabilities (red or black ratings stars), to perform a blue/green deployment upgrading to a newer API version or even sending different users to different APIs. In this case we will simply apply a new rule that only sends traffic to v2 (black) and v3 (red) ratings API. Apply a new ruleset. $ oc replace -f https://raw.githubusercontent.com/istio/istio/master/samples/bookinfo/networking/virtual-service-reviews-v2-v3.yaml Now when you access the book app and refresh you should see it switched between red and black ratings. Looking at Kiali versioned app graph we can see that traffic is only going to v2 and v3 as we would expect.   Troubleshooting Errors As mentioned one of the key values of the OpenShift Service Mesh is the visualization and tracing. In order to generate an error we will scale down the ratings v1 deployment to 0. $ oc scale deployment/ratings-v1 -n bookinfo --replicas 0 You should now see that the ratings service is currently unavailable when you refresh the app in a browser. Check Kiali dashboard. You can access Kiali via route under networking in the bookinfo-mesh project. In Kiali we clearly see the issue is the reviews service as we would expect.  Open Jaeger to trace the calls that are failing. Next we can dig into the requests by opening distributed tracing (Jaeger) from the Kiali dashboard. We can see the flow of all calls grouped and identify all request to the ratings service that are throwing an error. We see a 503 is returned which means service in unavailable. Kiali dashboard also shows request response (latency in ms) for slowest 50%, 95% and 99% of requests. This allows us to not only see the average latency but compare it to the slowest requests which indicates if we have spikes that could cause user slowness that we might not easily see when looking at just the average. Summary In this article we discussed the capabilities OpenShift Service Mesh provides for microservice applications. We deployed a service mesh and a book demo polyglot application that leverages the OpenShift Service Mesh. Using Kiali we saw how to gain visibility into our microservice application and easily identify problems as well as trends. Through Jaeger and distributed tracing we were able to identify the exact API causing error conditions. Microservice architectures provide a lot of value but they are harder to manage, control and troubleshoot in certain aspects than their monolithic peers which is why OpenShift Service Mesh is so critical. (c) 2021 Keith Tenzer</summary></entry><entry><title type="html">OpenShift 4 AWS IPI Installation Getting Started Guide</title><link href="https://keithtenzer.com/openshift/openshift-4-aws-ipi-installation-getting-started-guide/" rel="alternate" type="text/html" title="OpenShift 4 AWS IPI Installation Getting Started Guide" /><published>2021-01-18T00:00:00-08:00</published><updated>2021-01-18T00:00:00-08:00</updated><id>https://keithtenzer.com/openshift/openshift-4-aws-ipi-installation-getting-started-guide</id><content type="html" xml:base="https://keithtenzer.com/openshift/openshift-4-aws-ipi-installation-getting-started-guide/">&lt;p&gt;&lt;!-- wp:gallery {&quot;ids&quot;:[5513,5506,14709],&quot;imageCrop&quot;:false,&quot;linkTo&quot;:&quot;file&quot;,&quot;sizeSlug&quot;:&quot;medium&quot;} --&gt;&lt;/p&gt;
&lt;h2&gt;&lt;img class=&quot;alignnone  wp-image-2609&quot; style=&quot;color:var(--color-text);font-size:16px;font-weight:400;&quot; src=&quot;/assets/2021/01/aws-logo-1280x720-2.png?w=300&quot; alt=&quot;ansible_2&quot; width=&quot;100&quot; height=&quot;100&quot; /&gt;&lt;img class=&quot;alignnone size-full wp-image-12472&quot; style=&quot;color:var(--color-text);font-size:16px;font-weight:400;&quot; src=&quot;/assets/2021/01/plus_sign.gif?w=300&quot; alt=&quot;plus_sign&quot; width=&quot;80&quot; height=&quot;80&quot; /&gt;&lt;img class=&quot;alignnone  wp-image-14629&quot; src=&quot;/assets/2021/01/openshiftlogo.png?w=170&quot; alt=&quot;cropped-Windows-logo1&quot; width=&quot;100&quot; height=&quot;100&quot; /&gt;&lt;/h2&gt;
&lt;p&gt;&lt;!-- /wp:gallery --&gt;&lt;/p&gt;
&lt;p&gt;Happy new year as this will be the first post of 2021! 2020 was obviously a challenging year, my hope is I will have more time to devote to blogging in 2021. Please reach out and let me know what topics would be most helpful.&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:heading {&quot;className&quot;:&quot;has-large-font-size&quot;} --&gt;&lt;/p&gt;
&lt;h2 class=&quot;has-large-font-size&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;&lt;!-- /wp:heading --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;In this Article we will walk through an OpenShift deployment using the IPI (Installer Provisioned Infrastructure) method on AWS. OpenShift offers two possible deployment methods: IPI (as mentioned) and UPI (User Provisioned Infrastructure). The difference is the degree of automation and customization. IPI will not only deploy OpenShift but also all infrastructure components and configurations. IPI is supported in various environments including AWS, Azure, GCE, VMware Vsphere and even Baremetal. IPI is tightly coupled with the infrastructure layer whereas UPI is not and will allow the most customization and work anywhere.&lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Ultimately IPI vs UPI is usually dictated by the requirements. My view is unless you have special requirements (like a stretch cluster or specific integrations with infrastructure that require UPI) always default to IPI. It is far better to have the vendor, in this case Red Hat own more of the share of responsibility and ensure proper, tested infrastructure configurations are being deployed as well as maintained throughout the cluster lifecycle.&lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:more --&gt;&lt;br /&gt;
&lt;!--more--&gt;&lt;br /&gt;
&lt;!-- /wp:more --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:heading {&quot;className&quot;:&quot;has-large-font-size&quot;} --&gt;&lt;/p&gt;
&lt;h2 class=&quot;has-large-font-size&quot;&gt;Create Hosted Zone&lt;/h2&gt;
&lt;p&gt;&lt;!-- /wp:heading --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Before a cluster can be deployed we need to access AWS console and add a hosted zone to the route53 service. In the diagram below, we are adding the domain rh-southwest.com.&lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:image {&quot;id&quot;:14686,&quot;linkDestination&quot;:&quot;custom&quot;,&quot;className&quot;:&quot;size-large&quot;} --&gt;&lt;/p&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/01/route53new-2.png&quot;&gt;&lt;img src=&quot;/assets/2021/01/route53new-2.png?w=1024&quot; alt=&quot;&quot; class=&quot;wp-image-14686&quot; /&gt;&lt;/a&gt;&lt;/figure&gt;
&lt;p&gt;&lt;!-- /wp:image --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:heading {&quot;className&quot;:&quot;has-large-font-size&quot;} --&gt;&lt;/p&gt;
&lt;h2 class=&quot;has-large-font-size&quot;&gt;Download and Install CLI Tools&lt;/h2&gt;
&lt;p&gt;&lt;!-- /wp:heading --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Red Hat provides a CLI tools for deploying and managing OpenShift clusters. The CLI tools wrap kubectl, podman (docker image manipulation) and other tools to provide a single easy-to-use interface. You can of course also use kubectl and individual tooling for manipulating images (docker or podman).&lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Download CLI Tools&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Access&amp;nbsp;&lt;a href=&quot;https://cloud.redhat.com/openshift/install&quot;&gt;https://cloud.redhat.com/openshift/install&lt;/a&gt; using your Red Hat account. Select AWS as infrastructure provider and the installer provisioned option. Download both the CLI and OpenShift install.&lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Copy Pull Secret&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;The pull secret is used to automatically authenticate to Red Hat content repositories for OpenShift during the install. You will need to provide the pull secret as part of a later step.&lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:image {&quot;id&quot;:14696,&quot;sizeSlug&quot;:&quot;large&quot;,&quot;linkDestination&quot;:&quot;media&quot;} --&gt;&lt;/p&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/01/aws_setup-2.png&quot;&gt;&lt;img src=&quot;/assets/2021/01/aws_setup-2.png?w=1024&quot; alt=&quot;&quot; class=&quot;wp-image-14696&quot; /&gt;&lt;/a&gt;&lt;/figure&gt;
&lt;p&gt;&lt;!-- /wp:image --&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Install CLI Tools&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The CLI tools can be simply extracted into /usr/bin.&lt;/p&gt;
&lt;pre class=&quot;wp-block-preformatted&quot;&gt;$ sudo tar xvf openshift-client-linux.tar.gz -C /usr/bin &lt;br /&gt;$ sudo tar xvf openshift-install-linux.tar.gz -C /usr/bin&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Check CLI Version&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&quot;wp-block-preformatted&quot;&gt;$ openshift-install version openshift-install 4.6.9&lt;/pre&gt;
&lt;h2&gt;Install Configuration&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Create Install Config&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When creating an install config you will need to provide the platform, in this case AWS, the domain configured in route53, a name for the OpenShift cluster and of course the pull secret.&lt;/p&gt;
&lt;pre&gt;$ openshift-install create install-config \
--dir=ocp_install
? SSH Public Key /root/.ssh/id_rsa.pub
? Platform aws
? Cloud aws
? Base Domain rh-southwest.com
? Cluster Name ocp4
? Pull Secret [? for help] ************************************************&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Edit Install Config&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&quot;wp-block-preformatted&quot;&gt;$ vi ocp_install/install-config.yaml &lt;br /&gt;... &lt;br /&gt;compute: &lt;br /&gt;  architecture: amd64 &lt;br /&gt;  hyperthreading: Enabled &lt;br /&gt;  name: worker &lt;br /&gt;  platform: &lt;br /&gt;    aws: &lt;br /&gt;      type: m5.large &lt;br /&gt;      userTags: &lt;br /&gt;        Contact: myemail@mydomain.com &lt;br /&gt;        AlwaysUp: True &lt;br /&gt;        DeleteBy: NEVER &lt;br /&gt;  replicas: 3 &lt;br /&gt;controlPlane: &lt;br /&gt;  architecture: amd64 &lt;br /&gt;  hyperthreading: Enabled &lt;br /&gt;  name: master &lt;br /&gt;  platform: &lt;br /&gt;    aws: &lt;br /&gt;      type: m5.xlarge &lt;br /&gt;      userTags: &lt;br /&gt;        Contact: myemail@mydomain.com &lt;br /&gt;        AlwaysUp: True &lt;br /&gt;        DeleteBy: NEVER &lt;br /&gt;  replicas: 3 &lt;br /&gt;...&lt;/pre&gt;
&lt;p&gt;User tags allow you to label or tag the AWS VMs. In this case we have some automation that will shutdown any clusters that are not supposed to be always available or delete any clusters that have an expiration date set.&lt;/p&gt;
&lt;h2&gt;Deploy OpenShift Cluster&lt;/h2&gt;
&lt;p&gt;Now that we have adjusted the configuration we can deploy the cluster and grab a coffee.&lt;/p&gt;
&lt;pre class=&quot;wp-block-preformatted&quot;&gt;$ [ktenzer@localhost ~]$ time openshift-install create cluster --dir=ocp_install --log-level=debug&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Verify Cluster&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Once the deployment has completed successfully we can verify cluster. In order to gain access to cluster we can use the default system account created during deployment by exporting the kubeconfig.&lt;/p&gt;
&lt;pre class=&quot;wp-block-preformatted&quot;&gt;$ export KUBECONFIG=ocp_install/auth/kubeconfig &lt;br /&gt;$ oc get nodes &lt;br /&gt;NAME                                       STATUS ROLES  AGE VERSION &lt;br /&gt;ip-10-0-147-102.us-east-2.compute.internal Ready  worker 27m v1.19.0+7070803 &lt;br /&gt;ip-10-0-159-222.us-east-2.compute.internal Ready  master 33m v1.19.0+7070803 &lt;br /&gt;ip-10-0-161-231.us-east-2.compute.internal Ready  worker 24m v1.19.0+7070803 &lt;br /&gt;ip-10-0-178-131.us-east-2.compute.internal Ready  master 33m v1.19.0+7070803 &lt;br /&gt;ip-10-0-194-232.us-east-2.compute.internal Ready  worker 24m v1.19.0+7070803 &lt;br /&gt;ip-10-0-218-84.us-east-2.compute.internal Ready   master 33m v1.19.0+7070803&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Show Cluster Version&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&quot;wp-block-preformatted&quot;&gt;$ oc get clusterversion &lt;br /&gt;NAME    VERSION AVAILABLE PROGRESSING SINCE STATUS &lt;br /&gt;version 4.6.9   True      False       7m44s Cluster version is 4.6.9&lt;/pre&gt;
&lt;h2&gt;Configure OAUTH&lt;/h2&gt;
&lt;p&gt;OpenShift supports oauth2, there is a lot of choice with the identification provider. Usually you would integrate with your enterprise LDAP provider. In this case or for test environments you can use the htpasswd provider.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Create Htpasswd file&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&quot;wp-block-preformatted&quot;&gt;$ htpasswd -c -B -b ocp_install/ocp.htpasswd admin pasword123&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Create Secret for Htpasswd&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&quot;wp-block-preformatted&quot;&gt;$ oc create secret generic htpass-secret --from-file=ocp.htpasswd -n openshift-config&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Configure Htpasswd in Oauth&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&quot;wp-block-preformatted&quot;&gt;$ vi ocp_install/htpasswd-cr.yaml &lt;/pre&gt;
&lt;pre&gt;apiVersion: config.openshift.io/v1
kind: OAuth
metadata:
  name: cluster
spec:
  identityProviders:
  - name: my_htpasswd_provider
    mappingMethod: claim
    type: HTPasswd
    htpasswd:
      fileData:
        name: htpass-secret&lt;/pre&gt;
&lt;pre class=&quot;wp-block-preformatted&quot;&gt;$ oc apply -f ocp_install/htpasswd-cr.yaml&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Add Cluster Role&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Once we have added htpasswd as a oauth provider we need to add cluster-admin role to the admin user we already created.&lt;/p&gt;
&lt;pre class=&quot;wp-block-preformatted&quot;&gt;$ oc adm policy add-cluster-role-to-user cluster-admin admin&lt;/pre&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;In this article we discussed the IPI and UPI deployment methods provided by OpenShift. Most organizations will want to choose IPI where possible. It not only simplifies the deployment of OpenShift but also improves the supportability likely leading to a higher SLA or quicker resolution of any potential issues. If not possible, UPI provides all the flexibility needed to deploy OpenShift into any environment albeit with a little more oversight and ownership required. Finally we walked through the IPI deployment of OpenShift on AWS including showing how to configure OAUTH htpasswd provider.&lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;(c) 2021 Keith Tenzer &lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;</content><author><name>Keith Tenzer</name></author><category term="OpenShift" /><category term="AWS" /><category term="OpenShift" /><category term="Kubernetes" /><summary type="html">Happy new year as this will be the first post of 2021! 2020 was obviously a challenging year, my hope is I will have more time to devote to blogging in 2021. Please reach out and let me know what topics would be most helpful. Overview In this Article we will walk through an OpenShift deployment using the IPI (Installer Provisioned Infrastructure) method on AWS. OpenShift offers two possible deployment methods: IPI (as mentioned) and UPI (User Provisioned Infrastructure). The difference is the degree of automation and customization. IPI will not only deploy OpenShift but also all infrastructure components and configurations. IPI is supported in various environments including AWS, Azure, GCE, VMware Vsphere and even Baremetal. IPI is tightly coupled with the infrastructure layer whereas UPI is not and will allow the most customization and work anywhere. Ultimately IPI vs UPI is usually dictated by the requirements. My view is unless you have special requirements (like a stretch cluster or specific integrations with infrastructure that require UPI) always default to IPI. It is far better to have the vendor, in this case Red Hat own more of the share of responsibility and ensure proper, tested infrastructure configurations are being deployed as well as maintained throughout the cluster lifecycle. Create Hosted Zone Before a cluster can be deployed we need to access AWS console and add a hosted zone to the route53 service. In the diagram below, we are adding the domain rh-southwest.com. Download and Install CLI Tools Red Hat provides a CLI tools for deploying and managing OpenShift clusters. The CLI tools wrap kubectl, podman (docker image manipulation) and other tools to provide a single easy-to-use interface. You can of course also use kubectl and individual tooling for manipulating images (docker or podman). Download CLI Tools Access&amp;nbsp;https://cloud.redhat.com/openshift/install using your Red Hat account. Select AWS as infrastructure provider and the installer provisioned option. Download both the CLI and OpenShift install. Copy Pull Secret The pull secret is used to automatically authenticate to Red Hat content repositories for OpenShift during the install. You will need to provide the pull secret as part of a later step. Install CLI Tools The CLI tools can be simply extracted into /usr/bin. $ sudo tar xvf openshift-client-linux.tar.gz -C /usr/bin $ sudo tar xvf openshift-install-linux.tar.gz -C /usr/bin Check CLI Version $ openshift-install version openshift-install 4.6.9 Install Configuration Create Install Config When creating an install config you will need to provide the platform, in this case AWS, the domain configured in route53, a name for the OpenShift cluster and of course the pull secret. $ openshift-install create install-config \ --dir=ocp_install ? SSH Public Key /root/.ssh/id_rsa.pub ? Platform aws ? Cloud aws ? Base Domain rh-southwest.com ? Cluster Name ocp4 ? Pull Secret [? for help] ************************************************ Edit Install Config $ vi ocp_install/install-config.yaml ... compute: architecture: amd64 hyperthreading: Enabled name: worker platform: aws: type: m5.large userTags: Contact: myemail@mydomain.com AlwaysUp: True DeleteBy: NEVER replicas: 3 controlPlane: architecture: amd64 hyperthreading: Enabled name: master platform: aws: type: m5.xlarge userTags: Contact: myemail@mydomain.com AlwaysUp: True DeleteBy: NEVER replicas: 3 ... User tags allow you to label or tag the AWS VMs. In this case we have some automation that will shutdown any clusters that are not supposed to be always available or delete any clusters that have an expiration date set. Deploy OpenShift Cluster Now that we have adjusted the configuration we can deploy the cluster and grab a coffee. $ [ktenzer@localhost ~]$ time openshift-install create cluster --dir=ocp_install --log-level=debug Verify Cluster Once the deployment has completed successfully we can verify cluster. In order to gain access to cluster we can use the default system account created during deployment by exporting the kubeconfig. $ export KUBECONFIG=ocp_install/auth/kubeconfig $ oc get nodes NAME STATUS ROLES AGE VERSION ip-10-0-147-102.us-east-2.compute.internal Ready worker 27m v1.19.0+7070803 ip-10-0-159-222.us-east-2.compute.internal Ready master 33m v1.19.0+7070803 ip-10-0-161-231.us-east-2.compute.internal Ready worker 24m v1.19.0+7070803 ip-10-0-178-131.us-east-2.compute.internal Ready master 33m v1.19.0+7070803 ip-10-0-194-232.us-east-2.compute.internal Ready worker 24m v1.19.0+7070803 ip-10-0-218-84.us-east-2.compute.internal Ready master 33m v1.19.0+7070803 Show Cluster Version $ oc get clusterversion NAME VERSION AVAILABLE PROGRESSING SINCE STATUS version 4.6.9 True False 7m44s Cluster version is 4.6.9 Configure OAUTH OpenShift supports oauth2, there is a lot of choice with the identification provider. Usually you would integrate with your enterprise LDAP provider. In this case or for test environments you can use the htpasswd provider. Create Htpasswd file $ htpasswd -c -B -b ocp_install/ocp.htpasswd admin pasword123 Create Secret for Htpasswd $ oc create secret generic htpass-secret --from-file=ocp.htpasswd -n openshift-config Configure Htpasswd in Oauth $ vi ocp_install/htpasswd-cr.yaml apiVersion: config.openshift.io/v1 kind: OAuth metadata: name: cluster spec: identityProviders: - name: my_htpasswd_provider mappingMethod: claim type: HTPasswd htpasswd: fileData: name: htpass-secret $ oc apply -f ocp_install/htpasswd-cr.yaml Add Cluster Role Once we have added htpasswd as a oauth provider we need to add cluster-admin role to the admin user we already created. $ oc adm policy add-cluster-role-to-user cluster-admin admin Summary In this article we discussed the IPI and UPI deployment methods provided by OpenShift. Most organizations will want to choose IPI where possible. It not only simplifies the deployment of OpenShift but also improves the supportability likely leading to a higher SLA or quicker resolution of any potential issues. If not possible, UPI provides all the flexibility needed to deploy OpenShift into any environment albeit with a little more oversight and ownership required. Finally we walked through the IPI deployment of OpenShift on AWS including showing how to configure OAUTH htpasswd provider. (c) 2021 Keith Tenzer</summary></entry><entry><title type="html">Windows Automation with Ansible: Getting Started Guide</title><link href="https://keithtenzer.com/ansible/windows-automation-with-ansible-getting-started-guide/" rel="alternate" type="text/html" title="Windows Automation with Ansible: Getting Started Guide" /><published>2020-05-19T00:00:00-07:00</published><updated>2020-05-19T00:00:00-07:00</updated><id>https://keithtenzer.com/ansible/windows-automation-with-ansible-getting-started-guide</id><content type="html" xml:base="https://keithtenzer.com/ansible/windows-automation-with-ansible-getting-started-guide/">&lt;h2&gt;&lt;img class=&quot;alignnone  wp-image-2609&quot; style=&quot;color:var(--color-text);font-size:16px;font-weight:400;&quot; src=&quot;/assets/2020/05/ansible_2.png&quot; alt=&quot;ansible_2&quot; width=&quot;185&quot; height=&quot;185&quot; /&gt;&lt;img class=&quot;alignnone size-full wp-image-12472&quot; style=&quot;color:var(--color-text);font-size:16px;font-weight:400;&quot; src=&quot;/assets/2020/05/plus_sign.gif&quot; alt=&quot;plus_sign&quot; width=&quot;161&quot; height=&quot;161&quot; /&gt;&lt;img class=&quot;alignnone  wp-image-14629&quot; src=&quot;/assets/2020/05/cropped-windows-logo1.png&quot; alt=&quot;cropped-Windows-logo1&quot; width=&quot;179&quot; height=&quot;179&quot; /&gt;&lt;/h2&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;In this article we will focus on how to get started with automation of windows using Ansible. Specifically we will look at installing 3rd party software and OS updates. Automation is the basis for cloud-computing or cloud-native patterns and breeds a culture of innovation. Let&apos;s face it, we cannot innovate, if we are stuck doing mundane tasks and manual labor. Ansible has revolutionized automation because until Ansible, automating was rather complicated and required lots of domain knowledge. Ansible provides an automation language that the entire organization can use because it is so easy and so flexible. The best thing about Ansible though it it brings teams together and fosters a devops culture. It brings the Linux and Windows world together!&lt;/p&gt;
&lt;p&gt;&lt;!--more--&gt;&lt;/p&gt;
&lt;h2&gt;How Ansible Works&lt;/h2&gt;
&lt;p&gt;Ansible provides a runtime for executing playbooks. A playbook is simply a group of plays which are essentially steps executed in order. A play is one or more tasks. A task is simply the execution of an Ansible module. An Ansible module is python code that does something, like install a software package, update the system, change a configuration file, check if something is set correctly, etc. There are 1000s of Ansible modules and a huge community around it.&lt;/p&gt;
&lt;p&gt;In order to run a playbook against hosts you need an inventory which is simply a grouping of hosts and host vars (parameters). That is about it for the basics. Below is a diagram showing the Ansible automation engine architecture.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;alignnone  wp-image-14622&quot; src=&quot;/assets/2020/05/ansible_arch.png&quot; alt=&quot;ansible_arch&quot; width=&quot;1059&quot; height=&quot;740&quot; /&gt;&lt;/p&gt;
&lt;p&gt;This guide assumes you know some Ansible or are familar with the basics. If not I created an Ansible getting started guide &lt;a href=&quot;https://keithtenzer.com/2017/11/09/ansible-getting-started-guide/&quot;&gt;here&lt;/a&gt;. I highly recommend giving it a look before proceeding.&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;In order for a windows host to be managed by Ansible there are a few prerequisites&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Powershell version 3.0 or higher (Should be fine with Windows 2012 or higher)&lt;/li&gt;
&lt;li&gt;.NET Framework 4.0 or higher (should be fine with Windows 2012 or higher)&lt;/li&gt;
&lt;li&gt;Windows Remote Management Listener or SSH (cygwin)&lt;/li&gt;
&lt;li&gt;Windows 7, 8.1, and 10, and server OSs including Windows Server 2008, 2008 R2, 2012, 2012 R2, 2016, and 2019&lt;/li&gt;
&lt;li&gt;Chocolatey for installing 3rd party software&lt;/li&gt;
&lt;li&gt;WSUS for updating OS packages and patching&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Install Chocalatey and WSUS&lt;/h2&gt;
&lt;p&gt;There are various tools that can be used. I recommend using Chocolatey for installing packages and WSUS for OS updates/patching.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Install Chocalately&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using powershell we can install chocalately.&lt;/p&gt;
&lt;pre&gt;PS C:\windows\system32&amp;gt; Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString(&apos;https://chocolatey.org/install.ps1&apos;))&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Install WSUS&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Open server manager. In the top right under manage you can add or change roles.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;alignnone  wp-image-14632&quot; src=&quot;/assets/2020/05/server_manager.png&quot; alt=&quot;server_manager&quot; width=&quot;814&quot; height=&quot;429&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Under server roles select Windows Service Update Services. This will install WSUS. Once installed there will be a WSUS option in server manager. You can select it and configure what packages should be updated, etc.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;alignnone  wp-image-14633&quot; src=&quot;/assets/2020/05/wsus.png&quot; alt=&quot;wsus&quot; width=&quot;844&quot; height=&quot;409&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Open WSUS and check that the computer is showing up under &apos;All Computers&apos;. If not and you are running outside of domain you may need the following fix.&lt;/p&gt;
&lt;pre&gt;c:\&amp;gt; net stop wuauserv
c:\&amp;gt; regsvr32 /s wuapi.dll
c:\&amp;gt; regsvr32 /s wups.dll
c:\&amp;gt; regsvr32 /s wuaueng.dll
c:\&amp;gt; regsvr32 /s wucltui.dll
c:\&amp;gt; regsvr32 /s msxml3.dll
c:\&amp;gt; cd %windir%\SoftwareDistribution
c:\&amp;gt; rd /s/q DataStore
c:\&amp;gt; mkdir DataStore
c:\&amp;gt; rd /s/q Download
c:\&amp;gt; mkdir Download
c:\&amp;gt; net start wuauserv
c:\&amp;gt; reg delete HKLM\SOFTWARE\Microsoft\Windows\CurrentVersion\WindowsUpdate /v AccountDomainSid /f
c:\&amp;gt; reg delete HKLM\SOFTWARE\Microsoft\Windows\CurrentVersion\WindowsUpdate /v PingID /f
c:\&amp;gt; reg delete HKLM\SOFTWARE\Microsoft\Windows\CurrentVersion\WindowsUpdate /v SusClientId /f
c:\&amp;gt; wuauclt /resetauthorization
c:\&amp;gt; wuauclt /detectnow
c:\&amp;gt; wuauclt /reportnow&lt;/pre&gt;
&lt;h2&gt;Configuring Windows Remote Management for Ansible&lt;/h2&gt;
&lt;p&gt;Ansible requires no agents. It uses what the OS provides for communication. In Windows you can use SSH or Windows Remote Management (WinRM). Since SSH is more or less bolted on, WinRM is usually the preferred choice.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Test to ensure WinRM is working&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First install WinRM if it isn&apos;t installed. Execute the hostname command through WinRM.&lt;/p&gt;
&lt;pre&gt;PS C:\windows\system32&amp;gt; winrs -r:http://:5985/wsman -u: -p: hostname
win2012&lt;/pre&gt;
&lt;p&gt;Optionally you can also test WinRM by making a remote desktop connection from another windows host.&lt;/p&gt;
&lt;p&gt;Note: you may run into an issue where you get an authentication error and a CredSSL issue if you have an older windows version. This can also happen if you don&apos;t have a valid SSL certificate. If you run into this issue, update your registry.&lt;/p&gt;
&lt;pre&gt;c:\&amp;gt;reg add &quot;HKLM\Software\Microsoft\Windows\CurrentVersion\Policies\System\CredSSP\Parameters&quot; /f /v AllowEncryptionOracle /t REG_DWORD /d 2&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Enable basic auth&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There are several authentication methods. In this example we will use basic authentication which is username/password. This is of course the least secure method.&lt;/p&gt;
&lt;pre&gt;&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;PS C:\&amp;gt; Set-Item -Path WSMan:\localhost\Service\Auth\Basic -Value $true&lt;/span&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Update WinRM &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A script is provided by Ansible community to check WinRM and make necessary changes to allow Ansible to connect. In this case we are using basic authentication but likely you will want to use something more secure. The script can be found &lt;a href=&quot;https://raw.githubusercontent.com/ansible/ansible/devel/examples/scripts/ConfigureRemotingForAnsible.ps1&quot;&gt;here&lt;/a&gt; and other authentication options are documented in the script header.&lt;/p&gt;
&lt;p&gt;Open a powershell command prompt.&lt;/p&gt;
&lt;p&gt;Store URL path to script.&lt;/p&gt;
&lt;pre&gt;PS C:\&amp;gt; $url = &quot;https://raw.githubusercontent.com/ansible/ansible/devel/examples/scripts/ConfigureRemotingForAnsible.ps1&quot;&lt;/pre&gt;
&lt;p&gt;Store location for the script&lt;/p&gt;
&lt;pre&gt;PS C:\&amp;gt; $file = &quot;$env:temp\ConfigureRemotingForAnsible.ps1&quot;&lt;/pre&gt;
&lt;p&gt;Download script and output to file locally.&lt;/p&gt;
&lt;pre&gt;PS C:\&amp;gt; (New-Object -TypeName System.Net.WebClient).DownloadFile($url, $file)&lt;/pre&gt;
&lt;p&gt;Execute the script.&lt;/p&gt;
&lt;pre&gt;PS C:\&amp;gt; powershell.exe -ExecutionPolicy ByPass -File $file
Ok.&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Check WinRM connection&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;PS C:\windows\system32&amp;gt; winrm enumerate winrm/config/Listener
Listener
Address = *
Transport = HTTP
Port = 5985
Hostname
Enabled = true
URLPrefix = wsman
CertificateThumbprint
ListeningOn = 10.10.1.139, 127.0.0.1, ::1, fe80::5efe:10.10.1.139%22, fe80::8155:327a:aeaf:365a%12

Listener
Address = *
Transport = HTTPS
Port = 5986
Hostname
Enabled = true
URLPrefix = wsman
CertificateThumbprint = 7a38de2c212764a54de106dc756f7cbc275156a3
ListeningOn = 10.10.1.139, 127.0.0.1, ::1, fe80::5efe:10.10.1.139%22, fe80::8155:327a:aeaf:365a%12&lt;/pre&gt;
&lt;p&gt;Ensure the HTTP/HTTPS ports are open.&lt;/p&gt;
&lt;p&gt;More details about WinRM setup and how to setup a listener manually are documented &lt;a href=&quot;https://docs.ansible.com/ansible/latest/user_guide/windows_setup.html#winrm-setup&quot;&gt;here&lt;/a&gt;. In this case I used the default listener configured by WinRM.&lt;/p&gt;
&lt;h2&gt;Create Inventory File&lt;/h2&gt;
&lt;p&gt;We will create an inventory file with just a single host in a group called windows. From here on out we will be working on a Linux server where we have Ansible installed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Create Ansible inventory file&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;$ vi inventory
[windows]
138.204.12.111&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Test ansible connection&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using the inventory file we can test if Ansible can communicate with our windows server.&lt;/p&gt;
&lt;pre&gt;$ ansible -i ../inventory windows -m win_ping -e ansible_connection=winrm \
-e ansible_user=Admin -e ansible_password=&amp;lt;password&amp;gt; \
-e ansible_winrm_transport=basic \
-e ansible_winrm_server_cert_validation=ignore

138.201.147.202 | SUCCESS =&amp;gt; {
&quot;changed&quot;: false,
&quot;ping&quot;: &quot;pong&quot;
}&lt;/pre&gt;
&lt;h2&gt;Windows Patch Management&lt;/h2&gt;
&lt;p&gt;Now that Ansible is working with WinRM we can automate. In this case we will automate software package installation and updates.&lt;/p&gt;
&lt;p&gt;Playbook and roles are available &lt;a href=&quot;https://github.com/ktenzer/ansible-windows&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Create Playbook&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We haven&apos;t talked about roles. In Ansible roles are how we make playbooks reusable. It is always good practice to create roles. A role essentially allows you to organize Ansible plays and their dependencies together allowing them to be consumed easily. In order to use roles you need to create a certain directory structure and hierarchy. Create a playbook that imports our roles.&lt;/p&gt;
&lt;pre&gt;$ vi windows_baseline.yaml
---
- name: Windows Baseline
  hosts: &quot;&quot;
  connection: winrm
  gather_facts: true
  vars:
    ansible_user: &quot;&quot;
    ansible_password: &quot;&quot;
    ansible_connection: winrm 
    ansible_winrm_transport: basic 
    ansible_winrm_server_cert_validation: ignore

  tasks:
    - name: Install Baseline Packages
      include_role:
        name: install

    - name: Perform Updates
      include_role:
        name: updates
&lt;/pre&gt;
&lt;p&gt;Here we are setting hosts, ansible user and password as variables. These inputs must be provided when executing the playbook. Hosts should be the name of our host group from our inventory file. This playbook has two tasks, a role to install packages and a role to do an OS update.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Create Ansible install role&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;At a minimum your role will need a tasks directory&lt;/p&gt;
&lt;pre&gt;$ mkdir -p roles/install/tasks&lt;/pre&gt;
&lt;p&gt;Create a task to install git using the chocalatey module.&lt;/p&gt;
&lt;pre&gt;$ vi roles/install/tasks/main.yaml
---
- name: Install Git
  win_chocolatey:
    name: git
    state: present
&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Create Ansible patch update role&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;At a minimum your role will need a tasks directory&lt;/p&gt;
&lt;pre&gt;$ mkdir -p roles/updates/tasks&lt;/pre&gt;
&lt;p&gt;Create a task to perform an OS update.&lt;/p&gt;
&lt;pre&gt;vi roles/updates/tasks/main.yaml
---
- name: Update windows packages
  win_updates:
    category_names:
      - CriticalUpdates
      - SecurityUpdates
    reboot: yes
    reboot_timeout: 500
&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Run playbook using inventory&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The &apos;-vvvvv&apos; allows the playbook to run in debug mode for maximum verbosity.&lt;/p&gt;
&lt;pre&gt;$ ansible-playbook -i ./inventory -e target=windows -e user=Admin -e password= windows_baseline.yaml
PLAY [Windows Baseline] ****************************************************************************

TASK [Gathering Facts] *****************************************************************************
ok: [138.201.147.202]

TASK [Install Baseline Packages] *******************************************************************

TASK [install : Install Git] ***********************************************************************
ok: [138.201.147.202]

TASK [Perform Updates] *****************************************************************************

TASK [updates : Update windows packages] ***********************************************************
changed: [138.201.147.202]

PLAY RECAP *****************************************************************************************
138.201.147.202            : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=
&lt;/pre&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ansible/ansible-examples/tree/master/windows&quot;&gt;Here&lt;/a&gt; are some additional examples of windows playbooks that may be of interest on your journey.&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this article we discussed the value of automation and why it is just a game changer. We provided a step-by-step on preparing a windows host for Ansible. Finally using the Ansible automation language, showed how to use native windows tooling to install and update OS patches. Hopefully this will provide a good starting point for a journey into windows automation with Ansible.&lt;/p&gt;
&lt;p&gt;(c) 2020 Keith Tenzer&lt;/p&gt;</content><author><name>Keith Tenzer</name></author><category term="Ansible" /><category term="Automation" /><category term="Windows" /><summary type="html">Overview In this article we will focus on how to get started with automation of windows using Ansible. Specifically we will look at installing 3rd party software and OS updates. Automation is the basis for cloud-computing or cloud-native patterns and breeds a culture of innovation. Let&apos;s face it, we cannot innovate, if we are stuck doing mundane tasks and manual labor. Ansible has revolutionized automation because until Ansible, automating was rather complicated and required lots of domain knowledge. Ansible provides an automation language that the entire organization can use because it is so easy and so flexible. The best thing about Ansible though it it brings teams together and fosters a devops culture. It brings the Linux and Windows world together! How Ansible Works Ansible provides a runtime for executing playbooks. A playbook is simply a group of plays which are essentially steps executed in order. A play is one or more tasks. A task is simply the execution of an Ansible module. An Ansible module is python code that does something, like install a software package, update the system, change a configuration file, check if something is set correctly, etc. There are 1000s of Ansible modules and a huge community around it. In order to run a playbook against hosts you need an inventory which is simply a grouping of hosts and host vars (parameters). That is about it for the basics. Below is a diagram showing the Ansible automation engine architecture. This guide assumes you know some Ansible or are familar with the basics. If not I created an Ansible getting started guide here. I highly recommend giving it a look before proceeding. Prerequisites In order for a windows host to be managed by Ansible there are a few prerequisites Powershell version 3.0 or higher (Should be fine with Windows 2012 or higher) .NET Framework 4.0 or higher (should be fine with Windows 2012 or higher) Windows Remote Management Listener or SSH (cygwin) Windows 7, 8.1, and 10, and server OSs including Windows Server 2008, 2008 R2, 2012, 2012 R2, 2016, and 2019 Chocolatey for installing 3rd party software WSUS for updating OS packages and patching Install Chocalatey and WSUS There are various tools that can be used. I recommend using Chocolatey for installing packages and WSUS for OS updates/patching. Install Chocalately Using powershell we can install chocalately. PS C:\windows\system32&amp;gt; Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString(&apos;https://chocolatey.org/install.ps1&apos;)) Install WSUS Open server manager. In the top right under manage you can add or change roles. Under server roles select Windows Service Update Services. This will install WSUS. Once installed there will be a WSUS option in server manager. You can select it and configure what packages should be updated, etc. Open WSUS and check that the computer is showing up under &apos;All Computers&apos;. If not and you are running outside of domain you may need the following fix. c:\&amp;gt; net stop wuauserv c:\&amp;gt; regsvr32 /s wuapi.dll c:\&amp;gt; regsvr32 /s wups.dll c:\&amp;gt; regsvr32 /s wuaueng.dll c:\&amp;gt; regsvr32 /s wucltui.dll c:\&amp;gt; regsvr32 /s msxml3.dll c:\&amp;gt; cd %windir%\SoftwareDistribution c:\&amp;gt; rd /s/q DataStore c:\&amp;gt; mkdir DataStore c:\&amp;gt; rd /s/q Download c:\&amp;gt; mkdir Download c:\&amp;gt; net start wuauserv c:\&amp;gt; reg delete HKLM\SOFTWARE\Microsoft\Windows\CurrentVersion\WindowsUpdate /v AccountDomainSid /f c:\&amp;gt; reg delete HKLM\SOFTWARE\Microsoft\Windows\CurrentVersion\WindowsUpdate /v PingID /f c:\&amp;gt; reg delete HKLM\SOFTWARE\Microsoft\Windows\CurrentVersion\WindowsUpdate /v SusClientId /f c:\&amp;gt; wuauclt /resetauthorization c:\&amp;gt; wuauclt /detectnow c:\&amp;gt; wuauclt /reportnow Configuring Windows Remote Management for Ansible Ansible requires no agents. It uses what the OS provides for communication. In Windows you can use SSH or Windows Remote Management (WinRM). Since SSH is more or less bolted on, WinRM is usually the preferred choice. Test to ensure WinRM is working First install WinRM if it isn&apos;t installed. Execute the hostname command through WinRM. PS C:\windows\system32&amp;gt; winrs -r:http://:5985/wsman -u: -p: hostname win2012 Optionally you can also test WinRM by making a remote desktop connection from another windows host. Note: you may run into an issue where you get an authentication error and a CredSSL issue if you have an older windows version. This can also happen if you don&apos;t have a valid SSL certificate. If you run into this issue, update your registry. c:\&amp;gt;reg add &quot;HKLM\Software\Microsoft\Windows\CurrentVersion\Policies\System\CredSSP\Parameters&quot; /f /v AllowEncryptionOracle /t REG_DWORD /d 2 Enable basic auth There are several authentication methods. In this example we will use basic authentication which is username/password. This is of course the least secure method. PS C:\&amp;gt; Set-Item -Path WSMan:\localhost\Service\Auth\Basic -Value $true Update WinRM  A script is provided by Ansible community to check WinRM and make necessary changes to allow Ansible to connect. In this case we are using basic authentication but likely you will want to use something more secure. The script can be found here and other authentication options are documented in the script header. Open a powershell command prompt. Store URL path to script. PS C:\&amp;gt; $url = &quot;https://raw.githubusercontent.com/ansible/ansible/devel/examples/scripts/ConfigureRemotingForAnsible.ps1&quot; Store location for the script PS C:\&amp;gt; $file = &quot;$env:temp\ConfigureRemotingForAnsible.ps1&quot; Download script and output to file locally. PS C:\&amp;gt; (New-Object -TypeName System.Net.WebClient).DownloadFile($url, $file) Execute the script. PS C:\&amp;gt; powershell.exe -ExecutionPolicy ByPass -File $file Ok. Check WinRM connection PS C:\windows\system32&amp;gt; winrm enumerate winrm/config/Listener Listener Address = * Transport = HTTP Port = 5985 Hostname Enabled = true URLPrefix = wsman CertificateThumbprint ListeningOn = 10.10.1.139, 127.0.0.1, ::1, fe80::5efe:10.10.1.139%22, fe80::8155:327a:aeaf:365a%12</summary></entry><entry><title type="html">Red Hat Subscription Reporting Guide</title><link href="https://keithtenzer.com/general/red-hat-subscription-reporting-guide/" rel="alternate" type="text/html" title="Red Hat Subscription Reporting Guide" /><published>2020-05-13T00:00:00-07:00</published><updated>2020-05-13T00:00:00-07:00</updated><id>https://keithtenzer.com/general/red-hat-subscription-reporting-guide</id><content type="html" xml:base="https://keithtenzer.com/general/red-hat-subscription-reporting-guide/">&lt;h2&gt;&lt;img class=&quot;alignnone  wp-image-14584&quot; src=&quot;/assets/2020/05/ecommerce-subscription-ts-100621375-large.jpg&quot; alt=&quot;ecommerce-subscription-ts-100621375-large&quot; width=&quot;362&quot; height=&quot;240&quot; /&gt;&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;source: https://www.cio.com/article/3199910/zuora-blazes-a-new-trail-to-the-subscription-economy-and-a-post-erp-world.html&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;This article will look at the various options available, to do subscription reporting for Red Hat products. Many large organizations sometimes struggle to keep track of what subscriptions are being used, often maintaining their own spreadsheets. This can be very error prone and time consuming. Systems can even be subscribed to the wrong subscriptions, for example a virtual machine running RHEL using a physical subscription. Many Red Hat customers have several different products, not just RHEL and being able to actively inventory the entire subscription landscape is critical.&lt;/p&gt;
&lt;p&gt;&lt;!--more--&gt;&lt;/p&gt;
&lt;h2&gt;Options for Subscription Reporting&lt;/h2&gt;
&lt;p&gt;There are various options for subscription reporting depending on if you have Satellite (Red Hat Systems Management) or not.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Subscription Entitlement Report &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Starting with Satellite 6.7 there is now a canned report that is very customizable. This is the recommended way of reporting subscriptions in Satellite 6.7+. Provides granular host level subscription reporting.&lt;/p&gt;
&lt;p&gt;Example of the out-of-the-box entitlement report is &lt;a href=&quot;https://gist.github.com/ktenzer/7632ad9f75ba4424b1b59a9e0ba229ea&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Red Hat Discovery Tool&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The discovery tool can do reporting on subscriptions in Satellite and non-Satellite environments. Provides granular host level subscription reporting. This is the recommendation if you don&apos;t have Satellite 6.7+ or have systems not subscribing through Satellite. The discovery tool can be installed in a connected or disconnected environment.&lt;/p&gt;
&lt;p&gt;The discovery tool also has an upstream project and is available via the community: &lt;a href=&quot;https://github.com/quipucords/quipucords&quot;&gt;https://github.com/quipucords/quipucords&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Red Hat Subscription Watch&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is a cloud service offering that can aggregate subscription reporting across various infrastructure or even organizations. Data is sent either from Red Hat Insights or Satellite to a customers portal on cloud.redhat.com. This is a recommended addition to provide additional visibility but does not provide granular information at the host level.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Subscription Inventory Script&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Provides granular host level subscription reporting for Satellite environments. This is a script not a product so use at your own risk. However this is recommended if you have Satellite 6.6 or below and cannot for whatever reason leverage the Red Hat Discovery tool.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/RedHatSatellite/sat6Inventory/tree/product-cert&quot;&gt;https://github.com/RedHatSatellite/sat6Inventory/tree/product-cert&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Using the Satellite Subscription Inventory Script&lt;/h2&gt;
&lt;p&gt;Since the other options are products and well documented, I will focus on showing how to do subscription reporting using the inventory script on RHEL 7. First you will need to deploy a RHEL 7 system.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Enable Correct RHEL Repos&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First ensure all repositories are disabled and then just enable the rhel-7-server-rpms.&lt;/p&gt;
&lt;pre&gt;$ sudo subscription-manager repos --disable=*
$ sudo subscription-manager repos --enable=rhel-7-server-rpms&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Install python 2.6+&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;$ sudo yum install -y python&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Install Git (Only for connected setup)&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;$ sudo yum install -y git&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Clone git repository (Only for connected setup)&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;$ git clone https://github.com/RedHatSatellite/sat6Inventory.git&lt;/pre&gt;
&lt;p&gt;If the system isn&apos;t connected to the internet then you need to download from the git repository. You can download the script from &lt;a href=&quot;https://github.com/RedHatSatellite/sat6Inventory/tree/product-cert&quot;&gt;https://github.com/RedHatSatellite/sat6Inventory/tree/product-cert&lt;/a&gt; by selecting download. You can then copy the tarball to your system and extract it.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;alignnone  wp-image-14605&quot; src=&quot;/assets/2020/05/download.png&quot; alt=&quot;download&quot; width=&quot;1828&quot; height=&quot;484&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Change directory&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;cd sat6Inventory&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Run the inventory script&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You need to pass the admin user, password and the Satellite organization.&lt;/p&gt;
&lt;pre&gt;./sat6Inventory.py -s sat-bc87.rhpds.opentlc.com -l admin -p  \
-o &apos;EXAMPLE.COM&apos;&lt;/pre&gt;
&lt;p&gt;The script will output a CSV.&lt;/p&gt;
&lt;pre&gt;$ ls
&lt;strong&gt;EXAMPLE.COM_inventory_report.csv&lt;/strong&gt; LICENSE README.md Sample Report sat6Inventory.py&lt;/pre&gt;
&lt;p&gt;You can now import the script output CSV into a spreadsheet program and manipulate as desired.&lt;/p&gt;
&lt;p&gt;A sample report is also provided &lt;a href=&quot;https://github.com/RedHatSatellite/sat6Inventory/blob/product-cert/Sample%20Report/Example_inventory_report.csv&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Reconciling mismatched entitlements&lt;/h2&gt;
&lt;p&gt;If you have a Satellite environment with RHEL physical and VDC (virtual subscriptions) it is possible that a virtual RHEL host is possibly using a physical instead of virtual subscription. To identify such hosts you can run the below command from your Satellite server.&lt;/p&gt;
&lt;pre&gt;$ sudo foreman-rake katello:virt_who_report&lt;/pre&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this article we covered the various options for doing subscription reporting of Red Hat products. We also showed a short hands-on guide for using the inventory script. Hopefully this provided guidance to choose the most appropriate approach to subscription reporting.&lt;/p&gt;
&lt;p&gt;(c) 2020 Keith Tenzer&lt;/p&gt;</content><author><name>Keith Tenzer</name></author><category term="General" /><category term="Red Hat" /><category term="Reporting" /><category term="RHEL" /><category term="Satellite" /><category term="Subscription" /><summary type="html">source: https://www.cio.com/article/3199910/zuora-blazes-a-new-trail-to-the-subscription-economy-and-a-post-erp-world.html Overview This article will look at the various options available, to do subscription reporting for Red Hat products. Many large organizations sometimes struggle to keep track of what subscriptions are being used, often maintaining their own spreadsheets. This can be very error prone and time consuming. Systems can even be subscribed to the wrong subscriptions, for example a virtual machine running RHEL using a physical subscription. Many Red Hat customers have several different products, not just RHEL and being able to actively inventory the entire subscription landscape is critical. Options for Subscription Reporting There are various options for subscription reporting depending on if you have Satellite (Red Hat Systems Management) or not. Subscription Entitlement Report  Starting with Satellite 6.7 there is now a canned report that is very customizable. This is the recommended way of reporting subscriptions in Satellite 6.7+. Provides granular host level subscription reporting. Example of the out-of-the-box entitlement report is here. Red Hat Discovery Tool The discovery tool can do reporting on subscriptions in Satellite and non-Satellite environments. Provides granular host level subscription reporting. This is the recommendation if you don&apos;t have Satellite 6.7+ or have systems not subscribing through Satellite. The discovery tool can be installed in a connected or disconnected environment. The discovery tool also has an upstream project and is available via the community: https://github.com/quipucords/quipucords Red Hat Subscription Watch This is a cloud service offering that can aggregate subscription reporting across various infrastructure or even organizations. Data is sent either from Red Hat Insights or Satellite to a customers portal on cloud.redhat.com. This is a recommended addition to provide additional visibility but does not provide granular information at the host level. Subscription Inventory Script Provides granular host level subscription reporting for Satellite environments. This is a script not a product so use at your own risk. However this is recommended if you have Satellite 6.6 or below and cannot for whatever reason leverage the Red Hat Discovery tool. https://github.com/RedHatSatellite/sat6Inventory/tree/product-cert Using the Satellite Subscription Inventory Script Since the other options are products and well documented, I will focus on showing how to do subscription reporting using the inventory script on RHEL 7. First you will need to deploy a RHEL 7 system. Enable Correct RHEL Repos First ensure all repositories are disabled and then just enable the rhel-7-server-rpms. $ sudo subscription-manager repos --disable=* $ sudo subscription-manager repos --enable=rhel-7-server-rpms Install python 2.6+ $ sudo yum install -y python Install Git (Only for connected setup) $ sudo yum install -y git Clone git repository (Only for connected setup) $ git clone https://github.com/RedHatSatellite/sat6Inventory.git If the system isn&apos;t connected to the internet then you need to download from the git repository. You can download the script from https://github.com/RedHatSatellite/sat6Inventory/tree/product-cert by selecting download. You can then copy the tarball to your system and extract it. Change directory cd sat6Inventory Run the inventory script You need to pass the admin user, password and the Satellite organization. ./sat6Inventory.py -s sat-bc87.rhpds.opentlc.com -l admin -p \ -o &apos;EXAMPLE.COM&apos; The script will output a CSV. $ ls EXAMPLE.COM_inventory_report.csv LICENSE README.md Sample Report sat6Inventory.py You can now import the script output CSV into a spreadsheet program and manipulate as desired. A sample report is also provided here. Reconciling mismatched entitlements If you have a Satellite environment with RHEL physical and VDC (virtual subscriptions) it is possible that a virtual RHEL host is possibly using a physical instead of virtual subscription. To identify such hosts you can run the below command from your Satellite server. $ sudo foreman-rake katello:virt_who_report Summary In this article we covered the various options for doing subscription reporting of Red Hat products. We also showed a short hands-on guide for using the inventory script. Hopefully this provided guidance to choose the most appropriate approach to subscription reporting. (c) 2020 Keith Tenzer</summary></entry><entry><title type="html">OpenShift Operator Getting Started Part I</title><link href="https://keithtenzer.com/openshift/openshift-operator-getting-started-part-i/" rel="alternate" type="text/html" title="OpenShift Operator Getting Started Part I" /><published>2020-04-23T00:00:00-07:00</published><updated>2020-04-23T00:00:00-07:00</updated><id>https://keithtenzer.com/openshift/openshift-operator-getting-started-part-i</id><content type="html" xml:base="https://keithtenzer.com/openshift/openshift-operator-getting-started-part-i/">&lt;h2&gt;&lt;img class=&quot;alignnone  wp-image-14443&quot; src=&quot;/assets/2020/04/38202270.png?w=173&amp;amp;h=173&quot; alt=&quot;38202270&quot; width=&quot;173&quot; height=&quot;173&quot; /&gt;&lt;/h2&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;In this article we will introduce the concept of Operators, the Operator Framework and Operator Lifecycle Management. This article is part of a series that will walk you through understanding and building operators in Go or Ansible end-to-end.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://keithtenzer.com/2020/04/23/openshift-operator-getting-started-part-i/&quot;&gt;OpenShift Operator Getting Started Part I&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://keithtenzer.com/2020/01/27/openshift-operator-sdk-go-getting-started-guide-part-ii/&quot;&gt;OpenShift Operator SDK: Go Getting Started Guide Part II&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://keithtenzer.com/2020/04/23/openshift-operator-sdk-ansible-getting-started-guide-part-iii/&quot;&gt;OpenShift Operator SDK: Ansible Getting Started Guide Part III&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://keithtenzer.com/2020/04/23/openshift-operator-lifecycle-management-guide-integrating-operators-in-olm-part-iv/&quot;&gt;OpenShift Operator Lifecycle Management Guide: Integrating Operators in OLM Part IV&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First, what exactly is an Operator?&lt;/p&gt;
&lt;p&gt;&lt;em&gt;An Operator is a method of packaging, deploying and managing a Kubernetes application.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;source: &lt;a href=&quot;https://coreos.com/operators/&quot;&gt;https://coreos.com/operators/&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Or simply, it is the application that can deploy itself, manage itself and update itself. Welcome to the brave new world, where we don’t spend time doing repetitive manually tasks, but rather put our knowledge into software so it can do it for us, better.&lt;/p&gt;
&lt;p&gt;&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;The Kubernetes Operator pattern was introduced by CoreOS engineers in 2016 as again, a way to implement and inject application specific domain knowledge into the running of containerized applications. The idea was to couple Service Reliability Engineering (SRE) output with the application and provide a standard programmable path to do so. SRE teams as you likely know, operate applications by creating software. Most organizations unfortunately don’t have the luxury of having highly paid software engineers to run their applications, nor the time to build such software. Now, with Operators and the Operator Framework, there is a way for vendors or customers to provide that domain knowledge using a standardized reusable approach. The result is applications that run themselves or come with their own pre-programmed built-in SRE team. This is obviously a huge game-changer and differentiator for operating applications. In my opinion it is the only way to deal with the increased complexity we see today. This is the reason eventually, that every application will likely be deployed in containers on Kubernetes. It is simply no longer, with today’s complexity, possible for application domain knowledge to exist in a few people’s heads, that in turn need to operate application lifecycle manually. Think of an operator as the new Linux “Package Manager” and Kubernetes as the new “Linux”.&lt;/p&gt;
&lt;h2&gt;Introducing the Operator Framework&lt;/h2&gt;
&lt;p&gt;The Operator Framework is a toolkit to make it easy to build, scale and run operators. It includes the SDK, Operator Lifecycle Manager and Metering.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Operator SDK&lt;/strong&gt; – Provides tooling to build and package operators. It abstracts Kubernetes API.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;alignnone  wp-image-14437&quot; src=&quot;/assets/2020/04/screenshot-from-2020-01-25-16-02-08.png?w=1248&amp;amp;h=369&quot; alt=&quot;Screenshot from 2020-01-25 16-02-08&quot; width=&quot;1248&quot; height=&quot;369&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Operator Lifecycle Manager&lt;/strong&gt; – Management control plane for operators. It governs who can access/deploy a given operator, namespaces where operators can run and lifecycle management, such as updates to an operator.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;alignnone  wp-image-14438&quot; src=&quot;/assets/2020/04/screenshot-from-2020-01-25-16-02-28.png?w=1236&amp;amp;h=340&quot; alt=&quot;Screenshot from 2020-01-25 16-02-28&quot; width=&quot;1236&quot; height=&quot;340&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Metering&lt;/strong&gt; – Provides ability to record historical usage which in turn can be used for operator reporting.&lt;/p&gt;
&lt;h2&gt;Understanding How It All Works&lt;/h2&gt;
&lt;p&gt;Using the Operator SDK we create operators. An operator can be written in Go or Ansible. An operator provides one or more custom resource definitions (CRDs) to allow users to interact with the application using the standard kubernetes API. A CRD is simply an extention to the kubernetes API. It also provides a custom resource (CR) for how a user interacts with the CRD. In addition the operator provides methods for how to install, delete and update itself. The operator has the ability to watch CRDs. If a user creates, deletes or updates a CR, the operator will see that and call the necessary functions. Those functions can be implemented in Go or Ansible. Yep all you need to build an operator end-to-end is Ansible knowledge!&lt;/p&gt;
&lt;p&gt;Once you have an operator that can be deployed, it is time to look into Operator Lifecycle Management. OpenShift comes with OLM already built-in but it can also be added to any standard kubernetes cluster. OLM has the concept of a catalog. This is essentially an operator that provides one or more application bundles. OpenShift has several built-in catalogs.&lt;/p&gt;
&lt;pre&gt;$ oc get catalogsource -n openshift-marketplace
NAME                      DISPLAY                   TYPE   PUBLISHER          AGE
certified-operators       Certified Operators       grpc   Red Hat            18d
community-operators       Community Operators       grpc   Red Hat            18d 
redhat-operators          Red Hat Operators         grpc   Red Hat            18d&lt;/pre&gt;
&lt;p&gt;Red Hat, ISVs, Partners, anyone can bundle their application operators into a catalog. Operator Hub simply lists the applications exposed by a catalog via OLM. Each application bundle is composed of the operator, it&apos;s own k8s objects and a manifest. The manifest is exposed via the cluster server version (CSV) CRD and consumed in OLM. This allows for flexibility to version operators and their manifests separately.&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this article we discussed the Operator Framework for Kubernetes and OpenShift. A powerful framework for building, packaging and deploying kubernetes-native applications. The Operator Framework consists of the Operator SDK, Operator Lifecycle Management and Metering. Through Operators and the Operator Lifecycle Management the full power of kubernetes can be utilized, allowing application operators to have a built-in SRE approach which in turn increases application availability, programability and efficiency.&lt;/p&gt;
&lt;p&gt;Happy Operatoring!&lt;/p&gt;
&lt;p&gt;(c) 2020 Keith Tenzer&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;</content><author><name>Keith Tenzer</name></author><category term="OpenShift" /><category term="cloud-native" /><category term="Containers" /><category term="Kubernetes" /><category term="olm" /><category term="OpenShift" /><category term="Operator" /><category term="Operator Framework" /><category term="Operator Lifecycle Management" /><category term="operator-sdk" /><summary type="html">Overview In this article we will introduce the concept of Operators, the Operator Framework and Operator Lifecycle Management. This article is part of a series that will walk you through understanding and building operators in Go or Ansible end-to-end. OpenShift Operator Getting Started Part I OpenShift Operator SDK: Go Getting Started Guide Part II OpenShift Operator SDK: Ansible Getting Started Guide Part III OpenShift Operator Lifecycle Management Guide: Integrating Operators in OLM Part IV First, what exactly is an Operator? An Operator is a method of packaging, deploying and managing a Kubernetes application. source: https://coreos.com/operators/ Or simply, it is the application that can deploy itself, manage itself and update itself. Welcome to the brave new world, where we don’t spend time doing repetitive manually tasks, but rather put our knowledge into software so it can do it for us, better. The Kubernetes Operator pattern was introduced by CoreOS engineers in 2016 as again, a way to implement and inject application specific domain knowledge into the running of containerized applications. The idea was to couple Service Reliability Engineering (SRE) output with the application and provide a standard programmable path to do so. SRE teams as you likely know, operate applications by creating software. Most organizations unfortunately don’t have the luxury of having highly paid software engineers to run their applications, nor the time to build such software. Now, with Operators and the Operator Framework, there is a way for vendors or customers to provide that domain knowledge using a standardized reusable approach. The result is applications that run themselves or come with their own pre-programmed built-in SRE team. This is obviously a huge game-changer and differentiator for operating applications. In my opinion it is the only way to deal with the increased complexity we see today. This is the reason eventually, that every application will likely be deployed in containers on Kubernetes. It is simply no longer, with today’s complexity, possible for application domain knowledge to exist in a few people’s heads, that in turn need to operate application lifecycle manually. Think of an operator as the new Linux “Package Manager” and Kubernetes as the new “Linux”. Introducing the Operator Framework The Operator Framework is a toolkit to make it easy to build, scale and run operators. It includes the SDK, Operator Lifecycle Manager and Metering. Operator SDK – Provides tooling to build and package operators. It abstracts Kubernetes API. Operator Lifecycle Manager – Management control plane for operators. It governs who can access/deploy a given operator, namespaces where operators can run and lifecycle management, such as updates to an operator. Metering – Provides ability to record historical usage which in turn can be used for operator reporting. Understanding How It All Works Using the Operator SDK we create operators. An operator can be written in Go or Ansible. An operator provides one or more custom resource definitions (CRDs) to allow users to interact with the application using the standard kubernetes API. A CRD is simply an extention to the kubernetes API. It also provides a custom resource (CR) for how a user interacts with the CRD. In addition the operator provides methods for how to install, delete and update itself. The operator has the ability to watch CRDs. If a user creates, deletes or updates a CR, the operator will see that and call the necessary functions. Those functions can be implemented in Go or Ansible. Yep all you need to build an operator end-to-end is Ansible knowledge! Once you have an operator that can be deployed, it is time to look into Operator Lifecycle Management. OpenShift comes with OLM already built-in but it can also be added to any standard kubernetes cluster. OLM has the concept of a catalog. This is essentially an operator that provides one or more application bundles. OpenShift has several built-in catalogs. $ oc get catalogsource -n openshift-marketplace NAME DISPLAY TYPE PUBLISHER AGE certified-operators Certified Operators grpc Red Hat 18d community-operators Community Operators grpc Red Hat 18d redhat-operators Red Hat Operators grpc Red Hat 18d Red Hat, ISVs, Partners, anyone can bundle their application operators into a catalog. Operator Hub simply lists the applications exposed by a catalog via OLM. Each application bundle is composed of the operator, it&apos;s own k8s objects and a manifest. The manifest is exposed via the cluster server version (CSV) CRD and consumed in OLM. This allows for flexibility to version operators and their manifests separately. Summary In this article we discussed the Operator Framework for Kubernetes and OpenShift. A powerful framework for building, packaging and deploying kubernetes-native applications. The Operator Framework consists of the Operator SDK, Operator Lifecycle Management and Metering. Through Operators and the Operator Lifecycle Management the full power of kubernetes can be utilized, allowing application operators to have a built-in SRE approach which in turn increases application availability, programability and efficiency. Happy Operatoring! (c) 2020 Keith Tenzer &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;</summary></entry><entry><title type="html">OpenShift Operator Lifecycle Management Guide: Integrating Operators in OLM Part IV</title><link href="https://keithtenzer.com/openshift/openshift-operator-lifecycle-management-guide-integrating-operators-in-olm-part-iv/" rel="alternate" type="text/html" title="OpenShift Operator Lifecycle Management Guide: Integrating Operators in OLM Part IV" /><published>2020-04-23T00:00:00-07:00</published><updated>2020-04-23T00:00:00-07:00</updated><id>https://keithtenzer.com/openshift/openshift-operator-lifecycle-management-guide-integrating-operators-in-olm-part-iv</id><content type="html" xml:base="https://keithtenzer.com/openshift/openshift-operator-lifecycle-management-guide-integrating-operators-in-olm-part-iv/">&lt;h2&gt;&lt;img class=&quot;alignnone  wp-image-14443&quot; src=&quot;/assets/2020/04/38202270.png?w=173&amp;amp;h=173&quot; alt=&quot;38202270&quot; width=&quot;173&quot; height=&quot;173&quot; /&gt;&lt;/h2&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;In this article we will provide a hands-on guide to integrating your already built Operator with the Operator Lifecycle Manager (OLM). Using the Operator SDK and OPM tool we will create the application manifests and images so your application Operator can be managed through OLM.&lt;/p&gt;
&lt;p&gt;This article is part of a series that will walk you through understanding and building operators in Go or Ansible end-to-end.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://keithtenzer.com/2020/04/23/openshift-operator-getting-started-part-i/&quot;&gt;OpenShift Operator Getting Started Part I&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://keithtenzer.com/2020/01/27/openshift-operator-sdk-go-getting-started-guide-part-ii/&quot;&gt;OpenShift Operator SDK: Go Getting Started Guide Part II&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://keithtenzer.com/2020/04/23/openshift-operator-sdk-ansible-getting-started-guide-part-iii/&quot;&gt;OpenShift Operator SDK: Ansible Getting Started Guide Part III&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://keithtenzer.com/2020/04/23/openshift-operator-lifecycle-management-guide-integrating-operators-in-olm-part-iv/&quot;&gt;OpenShift Operator Lifecycle Management Guide: Integrating Operators in OLM Part IV&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;In order to add your Operator to OLM you will need two tools: the &lt;a href=&quot;https://github.com/operator-framework/operator-sdk/releases&quot;&gt;operator-sdk&lt;/a&gt; and &lt;a href=&quot;https://github.com/operator-framework/operator-registry/releases&quot;&gt;opm&lt;/a&gt;. Download the binaries for operator-sdk and opm. Install them under /usr/local/bin.&lt;/p&gt;
&lt;h2&gt;Create Cluster Service Version&lt;/h2&gt;
&lt;p&gt;The Cluster Service Version (CSV) is the manifest that allows your application operator to be bundled and exposed to the OLM API. The CSV is itself a CRD in kubernetes. Before creating a the CSV manifest you need the deployment scafolding. These are the objects and CRDs that your Operator require. These are created automatically when using the operator-sdk to create a application under the deploy directory.&lt;/p&gt;
&lt;pre&gt;$ operator-sdk new cars-operator --repo github.com/cars-operator&lt;/pre&gt;
&lt;p&gt;When generating an CSV you will provide the path to the deploy directory where those objects can be found.&lt;/p&gt;
&lt;pre&gt;$ operator-sdk generate csv --csv-version 1.0.0 --deploy-dir deploy&lt;/pre&gt;
&lt;p&gt;The generate csv command will create an olm_catalog/cars-operator/manifests directory. There you will find the &apos;clusterserviceversion&apos; yaml. You will however want to edit and add a lot. As such I have provided an &lt;a href=&quot;https://github.com/sa-mw-dach/podium/blob/master/podium-operator/deploy/olm-catalog/podium-operator/manifests/podium-operator.clusterserviceversion.yaml&quot;&gt;example&lt;/a&gt; csv for an Operator I built.&lt;/p&gt;
&lt;p&gt;Note: If you want to add an icon, so it shows up nicely in Operator Hub, the image needs to be a base64 bit stream inside the csv.&lt;/p&gt;
&lt;pre&gt;$ cat myimage.png | base64&lt;/pre&gt;
&lt;h2&gt;Create Application Operator Catalog Bundle&lt;/h2&gt;
&lt;p&gt;Now that we have generated and updated our CSV we can create an application bundle.&lt;/p&gt;
&lt;pre&gt;$ sudo operator-sdk bundle create quay.io/ktenzer/podium-operator-catalog:latest \
--channels beta --package podium-operator-catalog --directory \
deploy/olm-catalog/podium-operator/manifests&lt;/pre&gt;
&lt;p&gt;This will package the application manifest into a container image. Push the image to a public or private repository.&lt;/p&gt;
&lt;pre&gt;$ sudo docker push quay.io/ktenzer/podium-operator-catalog:latest&lt;/pre&gt;
&lt;h2&gt;Create Application Operator Catalog Index&lt;/h2&gt;
&lt;p&gt;Once we have bundled an application operator we can add it to a catalog index. The catalog index provides one or more application bundles to OLM. This is also what the Operator Hub directly interfaces with.&lt;/p&gt;
&lt;pre&gt;$ sudo opm index add -c docker --bundles \
quay.io/ktenzer/podium-operator-catalog:latest \
--tag quay.io/ktenzer/podium-operator-index:latest&lt;/pre&gt;
&lt;p&gt;Again an image is created that needs to be pushed to our private or public repository.&lt;/p&gt;
&lt;pre&gt;$ sudo docker push quay.io/ktenzer/podium-operator-index:latest&lt;/pre&gt;
&lt;h2&gt;Deploy Catalog Source&lt;/h2&gt;
&lt;p&gt;A catalog source is a CRD that defines an OLM catalog and points to a catalog index image, for example the one we just created.&lt;/p&gt;
&lt;p&gt;Create a catalog source yaml file that points to the catalog index image.&lt;/p&gt;
&lt;pre&gt;$ vi catalog_source.yaml
apiVersion: operators.coreos.com/v1alpha1
kind: CatalogSource
metadata:
  name: podium-operator-catalog
spec:
  sourceType: grpc
  &lt;strong&gt;image: quay.io/ktenzer/podium-operator-index:latest&lt;/strong&gt;
  displayName: Podium Operator Catalog
  publisher: Podium Community&lt;/pre&gt;
&lt;p&gt;Deploy the catalog source under the openshift-marketplace namespace or where you have deployed OLM.&lt;/p&gt;
&lt;pre&gt;$ oc create -f catalog_source.yaml -n openshift-marketplace&lt;/pre&gt;
&lt;p&gt;Below we can see a list of catalogs. Again each catalog can contain one or more applications. All are default in OpenShift, except the one in bold which I added.&lt;/p&gt;
&lt;pre&gt;$ oc get catalogsource -n openshift-marketplace
NAME                     DISPLAY                 TYPE PUBLISHER        AGE
certified-operators      Certified Operators     grpc Red Hat          18d
community-operators      Community Operators     grpc Red Hat          18d
&lt;strong&gt;podium-operator-catalog  Podium Operator Catalog grpc Podium Community 31m&lt;/strong&gt;
redhat-operators         Red Hat Operators       grpc Red Hat          18d&lt;/pre&gt;
&lt;p&gt;Each catalog has it&apos;s own operator / pod.&lt;/p&gt;
&lt;pre&gt;$ oc get pods -n openshift-marketplace
NAME                                    READY   STATUS    RESTARTS   AGE
certified-operators-76d9d8b886-bnsz7    1/1     Running   0          14h
community-operators-74d675f545-g7d2t    1/1     Running   0          18h
&lt;strong&gt;podium-operator-catalog-67gsk           1/1     Running   0          31m&lt;/strong&gt;
marketplace-operator-75f49679d7-n7v2r   1/1     Running   0          17d
redhat-operators-87d549bf4-mxf7w        1/1     Running   0          13h&lt;/pre&gt;
&lt;p&gt;Once a catalog operator exists, the applications it offers will show up in Operator Hub and can be installed.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;alignnone size-full wp-image-14528&quot; src=&quot;/assets/2020/04/podium_install.png&quot; alt=&quot;podium_install&quot; width=&quot;1094&quot; height=&quot;828&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Operators can be cluster-wide or namespace scoped. The Operator Lifecycle Management also provides a subscription, allowing updates to be pulled from various channels. This is similar to the relationship between an RPM and Repository.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;alignnone size-full wp-image-14529&quot; src=&quot;/assets/2020/04/podium_install2.png&quot; alt=&quot;podium_install2&quot; width=&quot;792&quot; height=&quot;743&quot; /&gt;&lt;/p&gt;
&lt;p&gt;After you deploy application via Operator Hub it will launch the operator. Cluster-wide operators will run in the openshift-operators namespace while namespaced operators will only run in a user defined namespace.&lt;/p&gt;
&lt;pre&gt;$ oc get pods -n openshift-operators
NAME                               READY   STATUS    RESTARTS   AGE
podium-operator-6855dc9478-q65bt   1/1     Running   0          38m&lt;/pre&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this article we stepped through the process of integrating an already existing operator with the Operator Lifecycle Manager. We saw how to generate an application manifest, build an application catalog bundle, add the bundle to an application index and finally deploy the application catalog, seamlessly integrating with Operator Hub. Using OLM provides not only a way to install applications but manage their lifecycle, releases and updates.&lt;/p&gt;
&lt;p&gt;Happy Operatoring!&lt;/p&gt;
&lt;p&gt;(c) 2020 Keith Tenzer&lt;/p&gt;</content><author><name>Keith Tenzer</name></author><category term="OpenShift" /><category term="Containers" /><category term="Kubernetes" /><category term="olm" /><category term="OpenShift" /><category term="Operator" /><category term="Operator Lifecycle Management" /><category term="operator-sdk" /><summary type="html">Overview In this article we will provide a hands-on guide to integrating your already built Operator with the Operator Lifecycle Manager (OLM). Using the Operator SDK and OPM tool we will create the application manifests and images so your application Operator can be managed through OLM. This article is part of a series that will walk you through understanding and building operators in Go or Ansible end-to-end. OpenShift Operator Getting Started Part I OpenShift Operator SDK: Go Getting Started Guide Part II OpenShift Operator SDK: Ansible Getting Started Guide Part III OpenShift Operator Lifecycle Management Guide: Integrating Operators in OLM Part IV In order to add your Operator to OLM you will need two tools: the operator-sdk and opm. Download the binaries for operator-sdk and opm. Install them under /usr/local/bin. Create Cluster Service Version The Cluster Service Version (CSV) is the manifest that allows your application operator to be bundled and exposed to the OLM API. The CSV is itself a CRD in kubernetes. Before creating a the CSV manifest you need the deployment scafolding. These are the objects and CRDs that your Operator require. These are created automatically when using the operator-sdk to create a application under the deploy directory. $ operator-sdk new cars-operator --repo github.com/cars-operator When generating an CSV you will provide the path to the deploy directory where those objects can be found. $ operator-sdk generate csv --csv-version 1.0.0 --deploy-dir deploy The generate csv command will create an olm_catalog/cars-operator/manifests directory. There you will find the &apos;clusterserviceversion&apos; yaml. You will however want to edit and add a lot. As such I have provided an example csv for an Operator I built. Note: If you want to add an icon, so it shows up nicely in Operator Hub, the image needs to be a base64 bit stream inside the csv. $ cat myimage.png | base64 Create Application Operator Catalog Bundle Now that we have generated and updated our CSV we can create an application bundle. $ sudo operator-sdk bundle create quay.io/ktenzer/podium-operator-catalog:latest \ --channels beta --package podium-operator-catalog --directory \ deploy/olm-catalog/podium-operator/manifests This will package the application manifest into a container image. Push the image to a public or private repository. $ sudo docker push quay.io/ktenzer/podium-operator-catalog:latest Create Application Operator Catalog Index Once we have bundled an application operator we can add it to a catalog index. The catalog index provides one or more application bundles to OLM. This is also what the Operator Hub directly interfaces with. $ sudo opm index add -c docker --bundles \ quay.io/ktenzer/podium-operator-catalog:latest \ --tag quay.io/ktenzer/podium-operator-index:latest Again an image is created that needs to be pushed to our private or public repository. $ sudo docker push quay.io/ktenzer/podium-operator-index:latest Deploy Catalog Source A catalog source is a CRD that defines an OLM catalog and points to a catalog index image, for example the one we just created. Create a catalog source yaml file that points to the catalog index image. $ vi catalog_source.yaml apiVersion: operators.coreos.com/v1alpha1 kind: CatalogSource metadata: name: podium-operator-catalog spec: sourceType: grpc image: quay.io/ktenzer/podium-operator-index:latest displayName: Podium Operator Catalog publisher: Podium Community Deploy the catalog source under the openshift-marketplace namespace or where you have deployed OLM. $ oc create -f catalog_source.yaml -n openshift-marketplace Below we can see a list of catalogs. Again each catalog can contain one or more applications. All are default in OpenShift, except the one in bold which I added. $ oc get catalogsource -n openshift-marketplace NAME DISPLAY TYPE PUBLISHER AGE certified-operators Certified Operators grpc Red Hat 18d community-operators Community Operators grpc Red Hat 18d podium-operator-catalog Podium Operator Catalog grpc Podium Community 31m redhat-operators Red Hat Operators grpc Red Hat 18d Each catalog has it&apos;s own operator / pod. $ oc get pods -n openshift-marketplace NAME READY STATUS RESTARTS AGE certified-operators-76d9d8b886-bnsz7 1/1 Running 0 14h community-operators-74d675f545-g7d2t 1/1 Running 0 18h podium-operator-catalog-67gsk 1/1 Running 0 31m marketplace-operator-75f49679d7-n7v2r 1/1 Running 0 17d redhat-operators-87d549bf4-mxf7w 1/1 Running 0 13h Once a catalog operator exists, the applications it offers will show up in Operator Hub and can be installed. Operators can be cluster-wide or namespace scoped. The Operator Lifecycle Management also provides a subscription, allowing updates to be pulled from various channels. This is similar to the relationship between an RPM and Repository. After you deploy application via Operator Hub it will launch the operator. Cluster-wide operators will run in the openshift-operators namespace while namespaced operators will only run in a user defined namespace. $ oc get pods -n openshift-operators NAME READY STATUS RESTARTS AGE podium-operator-6855dc9478-q65bt 1/1 Running 0 38m Summary In this article we stepped through the process of integrating an already existing operator with the Operator Lifecycle Manager. We saw how to generate an application manifest, build an application catalog bundle, add the bundle to an application index and finally deploy the application catalog, seamlessly integrating with Operator Hub. Using OLM provides not only a way to install applications but manage their lifecycle, releases and updates. Happy Operatoring! (c) 2020 Keith Tenzer</summary></entry><entry><title type="html">OpenShift Operator SDK: Ansible Getting Started Guide Part III</title><link href="https://keithtenzer.com/openshift/openshift-operator-sdk-ansible-getting-started-guide-part-iii/" rel="alternate" type="text/html" title="OpenShift Operator SDK: Ansible Getting Started Guide Part III" /><published>2020-04-23T00:00:00-07:00</published><updated>2020-04-23T00:00:00-07:00</updated><id>https://keithtenzer.com/openshift/openshift-operator-sdk-ansible-getting-started-guide-part-iii</id><content type="html" xml:base="https://keithtenzer.com/openshift/openshift-operator-sdk-ansible-getting-started-guide-part-iii/">&lt;h2&gt;&lt;img class=&quot;alignnone  wp-image-14443&quot; src=&quot;/assets/2020/04/38202270.png?w=173&amp;amp;h=173&quot; alt=&quot;38202270&quot; width=&quot;173&quot; height=&quot;173&quot; /&gt;&lt;/h2&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;In this article we will provide a hands-on guide to building your very first operator in Ansible. Using the Operator SDK we will learn how to create boilerplate code, build and deploy an operator.&lt;/p&gt;
&lt;p&gt;This article is part of a series that will walk you through understanding and building operators in Go or Ansible end-to-end.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://keithtenzer.com/2020/04/23/openshift-operator-getting-started-part-i/&quot;&gt;OpenShift Operator Getting Started Part I&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://keithtenzer.com/2020/01/27/openshift-operator-sdk-go-getting-started-guide-part-ii/&quot;&gt;OpenShift Operator SDK: Go Getting Started Guide Part II&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://keithtenzer.com/2020/04/23/openshift-operator-sdk-ansible-getting-started-guide-part-iii/&quot;&gt;OpenShift Operator SDK: Ansible Getting Started Guide Part III&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://keithtenzer.com/2020/04/23/openshift-operator-lifecycle-management-guide-integrating-operators-in-olm-part-iv/&quot;&gt;OpenShift Operator Lifecycle Management Guide: Integrating Operators in OLM Part IV&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;!--more--&gt;&lt;/p&gt;
&lt;h2&gt;Setup Development Environment&lt;/h2&gt;
&lt;p&gt;There are some prerequisites needed to develop and build an operator using Ansible. Also this guide and the operator-sdk assume you know Ansible roles. If you aren not yet up to speed please read about Ansible roles before proceeding.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Install Docker 17.03+&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Add the docker ce repositories. Note: you can also use podman and buildah instead of Docker for those that want a complete and clean divorce.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;$ sudo &lt;/span&gt;dnf &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; install dnf-plugins-core&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&quot;highlight&quot;&gt;$ &lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;dnf config-manager &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--add-repo&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    https://download.docker.com/linux/fedora/docker-ce.repo&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Install docker-ce&lt;/p&gt;
&lt;pre&gt;$ sudo dnf -y install docker-ce docker-ce-cli&lt;/pre&gt;
&lt;pre&gt;$ sudo systemctl start docker
$ sudo systemctl enable docker&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Install Ansible and Module Dependencies&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Install ansible&lt;/p&gt;
&lt;pre&gt;$ dnf install ansible&lt;/pre&gt;
&lt;p&gt;The Ansible runner and http runner is used to run a local version of the operator. This is very useful for development and testing.&lt;/p&gt;
&lt;pre&gt;$ pip3 install --user ansible-runner
$ pip3 install --user ansible-runner-http&lt;/pre&gt;
&lt;p&gt;Install required python modules&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ pip3 install --user requests&lt;/code&gt; &lt;code&gt;$ pip3 install --user openshift&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Install Operator Framework SDK&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You can simply &lt;a href=&quot;https://github.com/operator-framework/operator-sdk/releases&quot;&gt;download&lt;/a&gt;a pre-built release and install it under /usr/local/bin.&lt;/p&gt;
&lt;h2&gt;Building Your First Operator&lt;/h2&gt;
&lt;p&gt;The SDK is able to generate not only boilerplate code but also the CRDs, controller and API endpoints. In this example we will create a cars operator. It will simply provide an endpoint allowing us to do CRUD on car objects. Each car object will spawn a pod with a base image.&lt;/p&gt;
&lt;p&gt;Using operatore-sdk cli create boilerplate code.&lt;/p&gt;
&lt;pre&gt;$ operator-sdk new cars-operator --api-version=cars.example.com/v1alpha1 --kind=Car --type=ansible&lt;/pre&gt;
&lt;p&gt;The operator sdk creates boilerplate roles.&lt;/p&gt;
&lt;pre&gt;$ ls cars-operator
build  deploy  molecule  requirements.yml  roles  watches.yaml&lt;/pre&gt;
&lt;p&gt;The build directory containers Dockerfile, deploy is where the yaml files are for creating the k8s operator objects (including CRD/CR), the roles directory contains a role called car and finally watches.yaml is the mapping from a k8s object / CRD to a roll.&lt;/p&gt;
&lt;p&gt;By default when a car instance is created the operator will execute the role car. You can of course change the behavior, even configure finalizers for dealing with deletion of components not under the control of k8s (see the sdk &lt;a href=&quot;https://github.com/operator-framework/operator-sdk/blob/master/doc/ansible/user-guide.md&quot;&gt;guide&lt;/a&gt;)&lt;/p&gt;
&lt;h2&gt;Run Operator locally&lt;/h2&gt;
&lt;p&gt;The Ansible operator can be run locally. Simply deploy the CRD, service account and role.&lt;/p&gt;
&lt;p&gt;Create new project for our operator&lt;/p&gt;
&lt;pre&gt;$ oc new-project cars-operator&lt;/pre&gt;
&lt;p&gt;Setup service accounts and role bindings&lt;/p&gt;
&lt;pre&gt;$ oc create -f deploy/service_account.yaml&lt;/pre&gt;
&lt;pre&gt;$ oc create -f deploy/role.yaml&lt;/pre&gt;
&lt;pre&gt;$ oc create -f deploy/role_binding.yaml&lt;/pre&gt;
&lt;p&gt;Create the CRD for our operator&lt;/p&gt;
&lt;pre&gt;$ oc create -f deploy/crds/cars.example.com_cars_crd.yaml&lt;/pre&gt;
&lt;p&gt;Run Operator locally&lt;/p&gt;
&lt;pre&gt;$ operator-sdk run --local&lt;/pre&gt;
&lt;p&gt;Create a car&lt;/p&gt;
&lt;pre&gt;$ vi car.yaml
apiVersion: cars.example.com/v1alpha1
kind: Car
metadata:
  name: example-car
spec:
  size: 3
  bmw:
    model: m3
  audi:
    model: rs4&lt;/pre&gt;
&lt;pre&gt;$ oc create -f car.yaml&lt;/pre&gt;
&lt;p&gt;Anything under &apos;spec&apos; will be passed to Ansible as a variable. If you want to reference the size in the ansible role you simply use &apos;&apos;. You can also access nested variables such as the car model by using &apos;&apos;.&lt;/p&gt;
&lt;h2&gt;Build Ansible Operator&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Create User on Quay.io&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We will be using quay to store operator images. Authenticate using Github or Gmail to &lt;a href=&quot;https://quay.io/&quot;&gt;https://quay.io&lt;/a&gt;. Once authenticated go to account settings and set a password.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Test Quay.io Credentials&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;$ sudo docker login quay.io&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Build Operator&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using operator-sdk cli build the operator which will push the image to your local Docker registry. Make sure you are in the cars-operator directory.&lt;/p&gt;
&lt;pre&gt;$ sudo operator-sdk build quay.io/ktenzer/cars-operator&lt;/pre&gt;
&lt;p&gt;Note: Substitute ktenzer for your username.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Push Operator to your Quay.io Account&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;$ sudo docker push quay.io/ktenzer/cars-operator:latest&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Make Quay Repository Public&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;By default your Quay repository will be private. If we want to access it from OpenShift we need to make it public. Login to quay.io with your username/password&lt;/p&gt;
&lt;p&gt;Select the cars-operator repository&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;alignnone  wp-image-14432&quot; src=&quot;/assets/2020/04/screenshot-from-2020-01-25-13-04-42.png?w=1296&amp;amp;h=347&quot; alt=&quot;Screenshot from 2020-01-25 13-04-42&quot; width=&quot;1296&quot; height=&quot;347&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Under Repository Settings enable visibility to be public&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;alignnone  wp-image-14433&quot; src=&quot;/assets/2020/04/screenshot-from-2020-01-25-13-05-07.png?w=1649&amp;amp;h=405&quot; alt=&quot;Screenshot from 2020-01-25 13-05-07&quot; width=&quot;1649&quot; height=&quot;405&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update Image in operator.yaml&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We need to point the image to the location in our Quay repository.&lt;/p&gt;
&lt;pre&gt;vi deploy/operator.yaml
---
# Replace this with the built image name
image: &lt;strong&gt;quay.io/ktenzer/cars-operator&lt;/strong&gt;
command:
---&lt;/pre&gt;
&lt;h2&gt;Deploy Cars Operator on OpenShift&lt;/h2&gt;
&lt;p&gt;Now that the operator is built and pushed to our Quay repository we can deploy it on an OpenShift cluster.&lt;/p&gt;
&lt;p&gt;Authenticate to OpenShift Cluster&lt;/p&gt;
&lt;pre&gt;$oc login https://api.ocp4.keithtenzer.com:6443&lt;/pre&gt;
&lt;p&gt;Create new project for our operator&lt;/p&gt;
&lt;pre&gt;$ oc new-project cars-operator&lt;/pre&gt;
&lt;p&gt;Setup service accounts and role bindings&lt;/p&gt;
&lt;pre&gt;$ oc create -f deploy/service_account.yaml&lt;/pre&gt;
&lt;pre&gt;$ oc create -f deploy/role.yaml&lt;/pre&gt;
&lt;pre&gt;$ oc create -f deploy/role_binding.yaml&lt;/pre&gt;
&lt;p&gt;Create the CRD for our operator&lt;/p&gt;
&lt;pre&gt;$ oc create -f deploy/crds/cars.example.com_cars_crd.yaml&lt;/pre&gt;
&lt;p&gt;Deploy our operator&lt;/p&gt;
&lt;pre&gt;$ oc create -f deploy/operator.yaml&lt;/pre&gt;
&lt;p&gt;Create the CR for our operator&lt;/p&gt;
&lt;pre&gt;$ oc create -f deploy/crds/cars.example.com_v1alpha1_car_cr.yaml&lt;/pre&gt;
&lt;h2&gt;Using Cars Operator&lt;/h2&gt;
&lt;p&gt;The cars operator will automatically deploy an example-car. Whe can query our car object just like any other Kubernetes object. This is the beauty of CR/CRDs and operators. We can easily extend the Kubernetes API without needing to understand it’s complexity.&lt;/p&gt;
&lt;pre&gt;$ oc get car
NAME AGE
example-car 31m&lt;/pre&gt;
&lt;p&gt;Next we can get information about our example-car.&lt;/p&gt;
&lt;pre&gt;$ oc get car example-car -o yaml
apiVersion: cars.example.com/v1alpha1
kind: Car
metadata:
  creationTimestamp: &quot;2020-01-25T12:15:45Z&quot;
  generation: 1
  name: example-car
  namespace: cars-operator
  resourceVersion: &quot;2635723&quot;
  selfLink: /apis/cars.example.com/v1alpha1/namespaces/cars-operator/cars/example-car
  uid: 6a424ef9-3f6c-11ea-a391-fa163e9f184b
spec:
  size: 3

&lt;/pre&gt;
&lt;p&gt;Looking at the running pods in our cars-operator project we see the operator and our example-car.&lt;/p&gt;
&lt;pre&gt;$ oc get pods -n cars-operator
NAME                            READY   STATUS    RESTARTS   AGE
cars-operator-b98bff54d-t2465   1/1     Running   0          5m
example-car-pod                 1/1     Running   0          5m&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Create a new Car&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Lets now create a BMW car.&lt;/p&gt;
&lt;pre&gt;$ vi bmw.yaml
kind: Car
metadata:
  name: bmw
spec:
  size: 3
  bmw:
    model: m3&lt;/pre&gt;
&lt;pre&gt;$ oc create -f bmw.yaml&lt;/pre&gt;
&lt;p&gt;Here we can see we now have a BMW car.&lt;/p&gt;
&lt;pre&gt;$ oc get car
NAME          AGE
bmw           11m
example-car   31m&lt;/pre&gt;
&lt;p&gt;Of course we can get information about our BMW car.&lt;/p&gt;
&lt;pre&gt;$ oc get car bmw -o yaml
apiVersion: cars.example.com/v1alpha1
kind: Car
metadata:
  creationTimestamp: &quot;2020-01-25T12:35:25Z&quot;
  generation: 1
  name: bmw
  namespace: cars-operator
  resourceVersion: &quot;2644044&quot;
  selfLink: /apis/cars.example.com/v1alpha1/namespaces/cars-operator/cars/bmw
  uid: 294bc47f-3f6f-11ea-b32c-fa163e3e8e24
spec:
  size: 1&lt;/pre&gt;
&lt;p&gt;Finally as with the example-car, the operator will start a new pod when the BMW car is created.&lt;/p&gt;
&lt;pre&gt;$ oc get pods -n cars-operator
NAME                            READY   STATUS    RESTARTS   AGE
bmw-pod                         1/1     Running   0          10m
cars-operator-b98bff54d-t2465   1/1     Running   0          14m
example-car-pod                 1/1     Running   0          14m&lt;/pre&gt;
&lt;h2&gt;Cleanup&lt;/h2&gt;
&lt;p&gt;Follow these steps to remove the operator cleanly.&lt;/p&gt;
&lt;pre&gt;$ oc delete -f deploy/crds/cars.example.com_v1alpha1_car_cr.yaml&lt;/pre&gt;
&lt;pre&gt;$ oc delete -f deploy/operator.yaml&lt;/pre&gt;
&lt;pre&gt;$ oc delete -f deploy/role.yaml&lt;/pre&gt;
&lt;pre&gt;$ oc delete -f deploy/role_binding.yaml&lt;/pre&gt;
&lt;pre&gt;$ oc delete -f deploy/service_account.yaml&lt;/pre&gt;
&lt;pre&gt;$ oc delete -f deploy/crds/cars.example.com_cars_crd.yaml&lt;/pre&gt;
&lt;pre&gt;$ oc delete project cars-operator&lt;/pre&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this article a step-by-step guide was provided to setup a development environment, generate boilerplate code and deploy our custom cars operator using Ansible on OpenShift with the Operator Framework.&lt;/p&gt;
&lt;p&gt;Happy Operatoring!&lt;/p&gt;
&lt;p&gt;(c) 2020 Keith Tenzer&lt;/p&gt;</content><author><name>Keith Tenzer</name></author><category term="OpenShift" /><category term="Ansible" /><category term="Containers" /><category term="Kubernetes" /><category term="OpenShift" /><category term="Operator" /><category term="Operator Framework" /><category term="operator-sdk" /><summary type="html">Overview In this article we will provide a hands-on guide to building your very first operator in Ansible. Using the Operator SDK we will learn how to create boilerplate code, build and deploy an operator. This article is part of a series that will walk you through understanding and building operators in Go or Ansible end-to-end. OpenShift Operator Getting Started Part I OpenShift Operator SDK: Go Getting Started Guide Part II OpenShift Operator SDK: Ansible Getting Started Guide Part III OpenShift Operator Lifecycle Management Guide: Integrating Operators in OLM Part IV Setup Development Environment There are some prerequisites needed to develop and build an operator using Ansible. Also this guide and the operator-sdk assume you know Ansible roles. If you aren not yet up to speed please read about Ansible roles before proceeding. Install Docker 17.03+ Add the docker ce repositories. Note: you can also use podman and buildah instead of Docker for those that want a complete and clean divorce. $ sudo dnf -y install dnf-plugins-core $ sudo dnf config-manager \ --add-repo \ https://download.docker.com/linux/fedora/docker-ce.repo Install docker-ce $ sudo dnf -y install docker-ce docker-ce-cli $ sudo systemctl start docker $ sudo systemctl enable docker Install Ansible and Module Dependencies Install ansible $ dnf install ansible The Ansible runner and http runner is used to run a local version of the operator. This is very useful for development and testing. $ pip3 install --user ansible-runner $ pip3 install --user ansible-runner-http Install required python modules $ pip3 install --user requests $ pip3 install --user openshift Install Operator Framework SDK You can simply downloada pre-built release and install it under /usr/local/bin. Building Your First Operator The SDK is able to generate not only boilerplate code but also the CRDs, controller and API endpoints. In this example we will create a cars operator. It will simply provide an endpoint allowing us to do CRUD on car objects. Each car object will spawn a pod with a base image. Using operatore-sdk cli create boilerplate code. $ operator-sdk new cars-operator --api-version=cars.example.com/v1alpha1 --kind=Car --type=ansible The operator sdk creates boilerplate roles. $ ls cars-operator build deploy molecule requirements.yml roles watches.yaml The build directory containers Dockerfile, deploy is where the yaml files are for creating the k8s operator objects (including CRD/CR), the roles directory contains a role called car and finally watches.yaml is the mapping from a k8s object / CRD to a roll. By default when a car instance is created the operator will execute the role car. You can of course change the behavior, even configure finalizers for dealing with deletion of components not under the control of k8s (see the sdk guide) Run Operator locally The Ansible operator can be run locally. Simply deploy the CRD, service account and role. Create new project for our operator $ oc new-project cars-operator Setup service accounts and role bindings $ oc create -f deploy/service_account.yaml $ oc create -f deploy/role.yaml $ oc create -f deploy/role_binding.yaml Create the CRD for our operator $ oc create -f deploy/crds/cars.example.com_cars_crd.yaml Run Operator locally $ operator-sdk run --local Create a car $ vi car.yaml apiVersion: cars.example.com/v1alpha1 kind: Car metadata: name: example-car spec: size: 3 bmw: model: m3 audi: model: rs4 $ oc create -f car.yaml Anything under &apos;spec&apos; will be passed to Ansible as a variable. If you want to reference the size in the ansible role you simply use &apos;&apos;. You can also access nested variables such as the car model by using &apos;&apos;. Build Ansible Operator Create User on Quay.io We will be using quay to store operator images. Authenticate using Github or Gmail to https://quay.io. Once authenticated go to account settings and set a password. Test Quay.io Credentials $ sudo docker login quay.io Build Operator Using operator-sdk cli build the operator which will push the image to your local Docker registry. Make sure you are in the cars-operator directory. $ sudo operator-sdk build quay.io/ktenzer/cars-operator Note: Substitute ktenzer for your username. Push Operator to your Quay.io Account $ sudo docker push quay.io/ktenzer/cars-operator:latest Make Quay Repository Public By default your Quay repository will be private. If we want to access it from OpenShift we need to make it public. Login to quay.io with your username/password Select the cars-operator repository Under Repository Settings enable visibility to be public Update Image in operator.yaml We need to point the image to the location in our Quay repository. vi deploy/operator.yaml --- # Replace this with the built image name image: quay.io/ktenzer/cars-operator command: --- Deploy Cars Operator on OpenShift Now that the operator is built and pushed to our Quay repository we can deploy it on an OpenShift cluster. Authenticate to OpenShift Cluster $oc login https://api.ocp4.keithtenzer.com:6443 Create new project for our operator $ oc new-project cars-operator Setup service accounts and role bindings $ oc create -f deploy/service_account.yaml $ oc create -f deploy/role.yaml $ oc create -f deploy/role_binding.yaml Create the CRD for our operator $ oc create -f deploy/crds/cars.example.com_cars_crd.yaml Deploy our operator $ oc create -f deploy/operator.yaml Create the CR for our operator $ oc create -f deploy/crds/cars.example.com_v1alpha1_car_cr.yaml Using Cars Operator The cars operator will automatically deploy an example-car. Whe can query our car object just like any other Kubernetes object. This is the beauty of CR/CRDs and operators. We can easily extend the Kubernetes API without needing to understand it’s complexity. $ oc get car NAME AGE example-car 31m Next we can get information about our example-car. $ oc get car example-car -o yaml apiVersion: cars.example.com/v1alpha1 kind: Car metadata: creationTimestamp: &quot;2020-01-25T12:15:45Z&quot; generation: 1 name: example-car namespace: cars-operator resourceVersion: &quot;2635723&quot; selfLink: /apis/cars.example.com/v1alpha1/namespaces/cars-operator/cars/example-car uid: 6a424ef9-3f6c-11ea-a391-fa163e9f184b spec: size: 3</summary></entry><entry><title type="html">OpenShift Application Certificate Management with Let’s Encrypt</title><link href="https://keithtenzer.com/openshift/openshift-application-certificate-management-with-lets-encrypt/" rel="alternate" type="text/html" title="OpenShift Application Certificate Management with Let’s Encrypt" /><published>2020-04-03T00:00:00-07:00</published><updated>2020-04-03T00:00:00-07:00</updated><id>https://keithtenzer.com/openshift/openshift-application-certificate-management-with-lets-encrypt</id><content type="html" xml:base="https://keithtenzer.com/openshift/openshift-application-certificate-management-with-lets-encrypt/">&lt;h2&gt;&lt;img class=&quot;alignnone  wp-image-12890&quot; src=&quot;/assets/2020/04/OpenShift-LogoType.svg_.png&quot; alt=&quot;OpenShift-LogoType.svg&quot; width=&quot;111&quot; height=&quot;118&quot; /&gt;&lt;img class=&quot;alignnone  wp-image-5506&quot; src=&quot;/assets/2020/04/plus_sign.gif&quot; alt=&quot;plus_sign&quot; width=&quot;142&quot; height=&quot;142&quot; /&gt;&lt;img class=&quot;alignnone  wp-image-14504&quot; src=&quot;/assets/2020/04/le-logo-twitter-noalpha.png&quot; alt=&quot;le-logo-twitter-noalpha&quot; width=&quot;161&quot; height=&quot;161&quot; /&gt;&lt;/h2&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;In this short article we will look at a solution for application certificates in OpenShift. Let&apos;s Encrypt is a non-profit certificate authority and provides easy on-demand TLS certificates. Each application you create that you want to expose to users will of course have it&apos;s own URL and require a TLS certificate. It can be quite tedious to manage these certificates and deploy them manually. Kubernetes platforms also require an innovative, built-in native approach to properly mitigate complexity.&lt;/p&gt;
&lt;p&gt;Thankfully a fellow RHatter (Tomáš Nožička) has created a k8s admission controller that integrates with let&apos;s encrypt. A k8s admission controller is a pattern for extending kubernetes platform capabilities by reacting to API events in real-time.  In this case the admission controller watches the route APIs. If a new route is added, plus has the right annotation, the admission controller will automatically register the route with Let&apos;s Encrypt, wait for the certificate and finally configure the certificate automatically in the route.&lt;/p&gt;
&lt;p&gt;Tomáš has provided the code and yaml for an easy deployment in the following Github repository: &lt;a href=&quot;https://github.com/tnozicka/openshift-acme&quot;&gt;https://github.com/tnozicka/openshift-acme&lt;/a&gt;. While hee does provide documentation there are a few additional steps that need explanation when creating a route. I decided to as such put it all together in a simple concise post.&lt;/p&gt;
&lt;p&gt;&lt;!--more--&gt;&lt;/p&gt;
&lt;h2&gt;Configure Let&apos;s Encrypt OpenShift Admission Controller&lt;/h2&gt;
&lt;p&gt;Here we will enable the k8s admission controller on the entire cluster but it is possible to limit the scope to individual namespaces. See Github repository for more information.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Create new project&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;$ oc new-project acme&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Deploy Controller&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;$ oc apply -fhttps://raw.githubusercontent.com/tnozicka/openshift-acme/master/deploy/cluster-wide/{clusterrole,serviceaccount,issuer-letsencrypt-live,deployment}.yaml&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Create Service Account and Role Binding&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;$ oc create clusterrolebinding openshift-acme --clusterrole=openshift-acme \
--serviceaccount=&quot;$( oc project -q ):openshift-acme&quot; --dry-run -o yaml | \
oc apply -f -&lt;/pre&gt;
&lt;h2&gt;Add Certificate to Application Route&lt;/h2&gt;
&lt;p&gt;Now that the k8s admission controller is deployed we will create a route. In OpenShift a route is how we expose a service to the outside world. Since we will provide a custom TLS certificate, route termination should be on the edge. As such we can use http from the router edge to the service.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Create Route&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;$ vi letsencrypt_route.yaml
kind: Route
apiVersion: route.openshift.io/v1
metadata:
  name: meet
  labels:
    app: jitsi
  annotations:
    openshift.io/host.generated: &apos;true&apos;
    &lt;strong&gt;kubernetes.io/tls-acme: &quot;true&quot;&lt;/strong&gt;
spec:
  host: meet-jitsi.apps.ocp4.keithtenzer.com
  to:
    kind: Service
    name: web
    weight: 100
  port:
    targetPort: http
  tls:
    termination: edge
  wildcardPolicy: None
&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Validate Route&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As soon as the ACME admission controller sees a route with the annotation kubernetes.io/tls-acme: &quot;true&quot; it will register URL with Let&apos;s Encrypt. A temporary route will be created so once the certificate is accepted it can be pulled down and added to route.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;alignnone  wp-image-14500&quot; src=&quot;/assets/2020/04/exposer.png&quot; alt=&quot;exposer&quot; width=&quot;1256&quot; height=&quot;327&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Once process completes the temporary exposer route is removed and the certificate and private key are added to the original route.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;alignnone  wp-image-14501&quot; src=&quot;/assets/2020/04/exposer_2.png&quot; alt=&quot;exposer_2&quot; width=&quot;2040&quot; height=&quot;396&quot; /&gt;&lt;/p&gt;
&lt;p&gt;It&apos;s that easy and you now have a valid certificate for your application in OpenShift.&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this article we showed how to setup application certificates in an OpenShift enviornment using a k8s admission controller. This solution also shows the value of a k8s admission controller being able to customize and adapt the environment to an organizations governance model.&lt;/p&gt;
&lt;p&gt;(c) 2020 Keith Tenzer&lt;/p&gt;</content><author><name>Keith Tenzer</name></author><category term="OpenShift" /><category term="LetsEncypt" /><category term="Certificates" /><summary type="html">Overview In this short article we will look at a solution for application certificates in OpenShift. Let&apos;s Encrypt is a non-profit certificate authority and provides easy on-demand TLS certificates. Each application you create that you want to expose to users will of course have it&apos;s own URL and require a TLS certificate. It can be quite tedious to manage these certificates and deploy them manually. Kubernetes platforms also require an innovative, built-in native approach to properly mitigate complexity. Thankfully a fellow RHatter (Tomáš Nožička) has created a k8s admission controller that integrates with let&apos;s encrypt. A k8s admission controller is a pattern for extending kubernetes platform capabilities by reacting to API events in real-time.  In this case the admission controller watches the route APIs. If a new route is added, plus has the right annotation, the admission controller will automatically register the route with Let&apos;s Encrypt, wait for the certificate and finally configure the certificate automatically in the route. Tomáš has provided the code and yaml for an easy deployment in the following Github repository: https://github.com/tnozicka/openshift-acme. While hee does provide documentation there are a few additional steps that need explanation when creating a route. I decided to as such put it all together in a simple concise post. Configure Let&apos;s Encrypt OpenShift Admission Controller Here we will enable the k8s admission controller on the entire cluster but it is possible to limit the scope to individual namespaces. See Github repository for more information. Create new project $ oc new-project acme Deploy Controller $ oc apply -fhttps://raw.githubusercontent.com/tnozicka/openshift-acme/master/deploy/cluster-wide/{clusterrole,serviceaccount,issuer-letsencrypt-live,deployment}.yaml Create Service Account and Role Binding $ oc create clusterrolebinding openshift-acme --clusterrole=openshift-acme \ --serviceaccount=&quot;$( oc project -q ):openshift-acme&quot; --dry-run -o yaml | \ oc apply -f - Add Certificate to Application Route Now that the k8s admission controller is deployed we will create a route. In OpenShift a route is how we expose a service to the outside world. Since we will provide a custom TLS certificate, route termination should be on the edge. As such we can use http from the router edge to the service. Create Route $ vi letsencrypt_route.yaml kind: Route apiVersion: route.openshift.io/v1 metadata: name: meet labels: app: jitsi annotations: openshift.io/host.generated: &apos;true&apos; kubernetes.io/tls-acme: &quot;true&quot; spec: host: meet-jitsi.apps.ocp4.keithtenzer.com to: kind: Service name: web weight: 100 port: targetPort: http tls: termination: edge wildcardPolicy: None Validate Route As soon as the ACME admission controller sees a route with the annotation kubernetes.io/tls-acme: &quot;true&quot; it will register URL with Let&apos;s Encrypt. A temporary route will be created so once the certificate is accepted it can be pulled down and added to route. Once process completes the temporary exposer route is removed and the certificate and private key are added to the original route. It&apos;s that easy and you now have a valid certificate for your application in OpenShift. Summary In this article we showed how to setup application certificates in an OpenShift enviornment using a k8s admission controller. This solution also shows the value of a k8s admission controller being able to customize and adapt the environment to an organizations governance model. (c) 2020 Keith Tenzer</summary></entry><entry><title type="html">The Great Human Experiment: Global Disconnection from Coronavirus</title><link href="https://keithtenzer.com/general/the-great-human-experiment-global-disconnection-from-coronavirus/" rel="alternate" type="text/html" title="The Great Human Experiment: Global Disconnection from Coronavirus" /><published>2020-03-05T00:00:00-08:00</published><updated>2020-03-05T00:00:00-08:00</updated><id>https://keithtenzer.com/general/the-great-human-experiment-global-disconnection-from-coronavirus</id><content type="html" xml:base="https://keithtenzer.com/general/the-great-human-experiment-global-disconnection-from-coronavirus/">&lt;p&gt;&lt;img class=&quot;alignnone size-full wp-image-14485&quot; src=&quot;/assets/2020/03/jedicouncil-conspiracy.png&quot; alt=&quot;JediCouncil-Conspiracy&quot; width=&quot;1920&quot; height=&quot;816&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;source: &lt;a href=&quot;https://starwars.fandom.com/wiki/Shaak_Ti&quot;&gt;https://starwars.fandom.com/wiki/Shaak_Ti&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Cornavirus has arrived at the global level, it is likely only a matter of time before it is declared a world pandemic. Most are wondering how long it will stay and when will things go back to normal? Personally I think we reached the point, there will be no going back and that is not necessarily a bad thing. In this article we will discuss why the cornavirus is just as much an opportunity for the human race as it is a threat. We will discuss how technology can help and how a global world can function, disconnected.&lt;/p&gt;
&lt;p&gt;&lt;!--more--&gt;&lt;/p&gt;
&lt;h2&gt;The Human Interaction&lt;/h2&gt;
&lt;p&gt;It is of course in our nature to have personal face-to-face contact with one another. We require a personal relationship in order to build trust. Without trust, a relationship cannot exist and nothing meaningful will develop. Coronavirus has managed to cripple the world economies in a matter of weeks because of our human nature. Our world, globalization as it exists today does not function without face-to-face human interactions. At this point there are only two solutions&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ignore coronavirus and just keep on going and hope for the best with good hygiene&lt;/li&gt;
&lt;li&gt;Change the way we work, live and travel&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I think we can consider ourselves lucky as a whole actually, this virus could have had a much higher fatality rate and yet it showed us our greatest strength, the human connection, is also our greatest weakness.&lt;/p&gt;
&lt;h2&gt;The Cost of Human Interaction&lt;/h2&gt;
&lt;p&gt;As our world has become more global, human interaction means a lot more travel. While this has provided an economic boom around the globe, it is also I am afraid at a very, very high cost. The below diagram shows the explosion of world travel.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;alignnone  wp-image-14470&quot; src=&quot;/assets/2020/03/world_travel.png&quot; alt=&quot;world_travel&quot; width=&quot;711&quot; height=&quot;512&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;source: &lt;a href=&quot;https://ourworldindata.org/tourism&quot;&gt;https://ourworldindata.org/tourism&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Below is a graph showing CO2 increases and it&apos;s effect on global temperatures. It is obvious to see the likely correlation between global travel and climate change. Of course, there are many other impacts but global travel is a big one and it has been increasing dramatically.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;alignnone  wp-image-14479&quot; src=&quot;/assets/2020/03/co2-levels.png&quot; alt=&quot;co2-levels&quot; width=&quot;753&quot; height=&quot;579&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em style=&quot;color:var(--color-text);&quot;&gt;source: &lt;a href=&quot;https://www.globalchange.gov/browse/multimedia/global-temperature-and-carbon-dioxide&quot;&gt;https://www.globalchange.gov/browse/multimedia/global-temperature-and-carbon-dioxide&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Now I am not advocating that we stay at home and stop all travel but rather that the balance is simply out-of-sync. Ironically climate change was supposed to bring us back to equilibrium but it may indirectly be the coronavirus that moves the needle first, at least slightly by reducing travel and forcing alternatives.&lt;/p&gt;
&lt;h2&gt;A New Normal and a lot of Disruption&lt;/h2&gt;
&lt;p&gt;I think it is clear that we will not be ignoring the coronavirus and that it likely won&apos;t just go away in the near-term. Based on that assumption, I think we will be changing the way we work and live, not just temporarily but maybe, permanently. There will be an adjustment time but likely once we adjust, we won&apos;t go back. For many it simply means instead of going to the office, you work at home or have a remote meeting. Those that are involved in producing goods, work in factories and such will have to adapt further. Regular screening, testing and more isolation will likely be needed in the place of work which require significant investment, depending on how long the coronavirus is a threat. All of this will create a lot of new opportunities, areas of investment and of course bring traditional ones (like the travel industry) to the brink. The longer the coronavirus lasts the more disruptive change will occur. It is impossible to say where this all leads other than at least a temporary new normal involving a lot less travel and consumption.&lt;/p&gt;
&lt;h2&gt;Technology, Life, Work and Culture&lt;/h2&gt;
&lt;p&gt;In order to truly experience something, you have to feel it, you have to smell it and you certainly have to taste it. Our senses and the ability to forge experiences from them is the human experience. However, do you really need to be there in person at ground zero? I don&apos;t think so at all. With technology, virtual reality and how things are evolving, maybe we will have less of a need to physically go places in the future to get those experiences.&lt;/p&gt;
&lt;p&gt;At work we also need experiences and interactions. Most companies are under invested compared to technology capabilities. Organizations just never evolved from the days of where you had to go to the office or the client to get anything done. Now though companies will have to adapt, cultures will change. I am convinced with the right technology, the experiences will be just as good.&lt;/p&gt;
&lt;p&gt;Events around the world are cancelled but do we really need to visit an exhibition hall, in some city, half way across the world, to have an experience? Companies are already adapting, Red Hat for example just announced, the largest opensource event in the world, Red Hat Summit, will be virtual (&lt;a href=&quot;https://www.redhat.com/en/blog/moving-red-hat-summit-2020-virtual-experience&quot;&gt;https://www.redhat.com/en/blog/moving-red-hat-summit-2020-virtual-experience&lt;/a&gt;).  I am convinced it will not only succeed but excel, other companies will follow. Who would have thought of a virtual conference at that scale just a few weeks ago?&lt;/p&gt;
&lt;p&gt;I work at Red Hat, a company I am so proud to be a part of because of the culture. At Red Hat most people work from wherever, they are all over the world. We look for the best people and worry about logistics second. Red Hat has the strongest company culture I have ever experienced, yet there is limited direct connection points. How is it possible to have a culture you can breath, taste and feel without a central point of reference or focus? It&apos;s because culture is not about a place, it is about the people that take part in it. Culture can exist and thrive through any medium, physical or virtual.&lt;/p&gt;
&lt;p&gt;The opensource community has long adapted to this new world because people form all over the world, different organizations, countries came together to work on real problems, effectively. They did it mostly, without any face-to-face meetings. A way of working evolved, a culture evolved and it is successful not only in a limited travel world but even a zero travel world. The rest of the world will figure this out and follow because now, unfortunately there is no choice.&lt;/p&gt;
&lt;p&gt;There will be many business casualties and sadly also lost life. However, there is opportunity to learn and make the world a better, more robust, sustainable place. Let us not miss this opportunity!&lt;/p&gt;
&lt;p&gt;(c) 2020 Keith Tenzer&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;</content><author><name>Keith Tenzer</name></author><category term="General" /><category term="coronavirus" /><category term="culture" /><category term="globalization" /><category term="opensource" /><category term="remote" /><summary type="html">source: https://starwars.fandom.com/wiki/Shaak_Ti Cornavirus has arrived at the global level, it is likely only a matter of time before it is declared a world pandemic. Most are wondering how long it will stay and when will things go back to normal? Personally I think we reached the point, there will be no going back and that is not necessarily a bad thing. In this article we will discuss why the cornavirus is just as much an opportunity for the human race as it is a threat. We will discuss how technology can help and how a global world can function, disconnected. The Human Interaction It is of course in our nature to have personal face-to-face contact with one another. We require a personal relationship in order to build trust. Without trust, a relationship cannot exist and nothing meaningful will develop. Coronavirus has managed to cripple the world economies in a matter of weeks because of our human nature. Our world, globalization as it exists today does not function without face-to-face human interactions. At this point there are only two solutions Ignore coronavirus and just keep on going and hope for the best with good hygiene Change the way we work, live and travel I think we can consider ourselves lucky as a whole actually, this virus could have had a much higher fatality rate and yet it showed us our greatest strength, the human connection, is also our greatest weakness. The Cost of Human Interaction As our world has become more global, human interaction means a lot more travel. While this has provided an economic boom around the globe, it is also I am afraid at a very, very high cost. The below diagram shows the explosion of world travel. source: https://ourworldindata.org/tourism Below is a graph showing CO2 increases and it&apos;s effect on global temperatures. It is obvious to see the likely correlation between global travel and climate change. Of course, there are many other impacts but global travel is a big one and it has been increasing dramatically. source: https://www.globalchange.gov/browse/multimedia/global-temperature-and-carbon-dioxide Now I am not advocating that we stay at home and stop all travel but rather that the balance is simply out-of-sync. Ironically climate change was supposed to bring us back to equilibrium but it may indirectly be the coronavirus that moves the needle first, at least slightly by reducing travel and forcing alternatives. A New Normal and a lot of Disruption I think it is clear that we will not be ignoring the coronavirus and that it likely won&apos;t just go away in the near-term. Based on that assumption, I think we will be changing the way we work and live, not just temporarily but maybe, permanently. There will be an adjustment time but likely once we adjust, we won&apos;t go back. For many it simply means instead of going to the office, you work at home or have a remote meeting. Those that are involved in producing goods, work in factories and such will have to adapt further. Regular screening, testing and more isolation will likely be needed in the place of work which require significant investment, depending on how long the coronavirus is a threat. All of this will create a lot of new opportunities, areas of investment and of course bring traditional ones (like the travel industry) to the brink. The longer the coronavirus lasts the more disruptive change will occur. It is impossible to say where this all leads other than at least a temporary new normal involving a lot less travel and consumption. Technology, Life, Work and Culture In order to truly experience something, you have to feel it, you have to smell it and you certainly have to taste it. Our senses and the ability to forge experiences from them is the human experience. However, do you really need to be there in person at ground zero? I don&apos;t think so at all. With technology, virtual reality and how things are evolving, maybe we will have less of a need to physically go places in the future to get those experiences. At work we also need experiences and interactions. Most companies are under invested compared to technology capabilities. Organizations just never evolved from the days of where you had to go to the office or the client to get anything done. Now though companies will have to adapt, cultures will change. I am convinced with the right technology, the experiences will be just as good. Events around the world are cancelled but do we really need to visit an exhibition hall, in some city, half way across the world, to have an experience? Companies are already adapting, Red Hat for example just announced, the largest opensource event in the world, Red Hat Summit, will be virtual (https://www.redhat.com/en/blog/moving-red-hat-summit-2020-virtual-experience).  I am convinced it will not only succeed but excel, other companies will follow. Who would have thought of a virtual conference at that scale just a few weeks ago? I work at Red Hat, a company I am so proud to be a part of because of the culture. At Red Hat most people work from wherever, they are all over the world. We look for the best people and worry about logistics second. Red Hat has the strongest company culture I have ever experienced, yet there is limited direct connection points. How is it possible to have a culture you can breath, taste and feel without a central point of reference or focus? It&apos;s because culture is not about a place, it is about the people that take part in it. Culture can exist and thrive through any medium, physical or virtual. The opensource community has long adapted to this new world because people form all over the world, different organizations, countries came together to work on real problems, effectively. They did it mostly, without any face-to-face meetings. A way of working evolved, a culture evolved and it is successful not only in a limited travel world but even a zero travel world. The rest of the world will figure this out and follow because now, unfortunately there is no choice. There will be many business casualties and sadly also lost life. However, there is opportunity to learn and make the world a better, more robust, sustainable place. Let us not miss this opportunity! (c) 2020 Keith Tenzer &amp;nbsp; &amp;nbsp;</summary></entry></feed>