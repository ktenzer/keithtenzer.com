<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-08-15T19:59:28-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Keith Tenzer’s Blog</title><subtitle></subtitle><author><name>Keith Tenzer</name></author><entry><title type="html">My First Day at Temporal</title><link href="http://localhost:4000/temporal/my-first-day-at-temporal/" rel="alternate" type="text/html" title="My First Day at Temporal" /><published>2022-08-15T00:00:00-07:00</published><updated>2022-08-15T00:00:00-07:00</updated><id>http://localhost:4000/temporal/my-first-day-at-temporal</id><content type="html" xml:base="http://localhost:4000/temporal/my-first-day-at-temporal/">&lt;p&gt;&lt;img src=&quot;/assets/2022-08-15/logo-temporal-with-copy.svg&quot; alt=&quot;Temporal&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Today is my first day at temporal and with that I wanted to share some thoughts around my decision, why Temporal and my experience thus far. As you can imagine, making any career change is always a very careful thought process and as we get older, have more responsibilities, the gravity around those decisions becomes stronger.&lt;/p&gt;

&lt;h2 id=&quot;why-temporal&quot;&gt;Why Temporal?&lt;/h2&gt;
&lt;p&gt;After IBM acquired Red Hat in 2018, I had the opportunity to visit the IBM research and development lab in Boeblingen, Germany. The idea was so RHatters could learn about some of the IBM history and why mainframes even today are great, at least with Linux. As you can imagine, I was more than skeptical. We spent years modernizing monolithic applications and moving workloads to Linux. Mainframe technology was a dinosaur that predated anything I ever worked on and as such I dismissed it, what could I possibly learn? Even once we arrived at the facility; the gray, lifeless, government looking buildings exhumed anything but innovation or creativity.&lt;/p&gt;

&lt;p&gt;Of course as the cliche goes, Looks can and are deceiving. Our minds judge everything and blind us if we let them. So what did I learn on that day? Reliability, you can shoot an IBM mainframe with a high-powered weapon and it will continue to run. That is why still today, well into the era of cloud some of the most critical computing workflows (government, banks, aviation, insurance and healthcare) are still done on mainframes. One of the challenges with cloud-native is of course, reliability across many disparate services. Monolithic environments are simply much easier to control and anticipate. Developers are left to solve reliability on their own with 10s or even 100s of microservices potentially involved in a workflow. Of course, this ends up being a lot of code, technical debt and reliability is never quite certain, certainly not to the level of a mainframe.&lt;/p&gt;

&lt;h3 id=&quot;opportunity&quot;&gt;Opportunity&lt;/h3&gt;
&lt;p&gt;Temporal is solving this problem through a workflow-as-code approach. Handling workflow state, execution, retries and allowing developers to orchestrate microservices into a workflow that is reliable as running water. It is a simple purpose with enormous potential. What could we accomplish if we further increased reliability of cloud-native applications and even started to modernize legacy applications shackled in their moorings? That has me extremely excited!&lt;/p&gt;

&lt;h3 id=&quot;execution&quot;&gt;Execution&lt;/h3&gt;
&lt;p&gt;Still it is one thing to have great potential but it isn’t worth that much if you can’t execute. It is far better to have great execution of a poor plan, than poor execution of a great plan. At the end of the day, execution comes down to people and the will to overcome adversity or whatever else gets thrown in the way. My interview process at Temporal was not an interview at all. It was a conversation involving sharing ideas and thoughts bi-directionally. Everyone I met was passionate, focused, professional and curious. All seemed united, being part of the same cause but each with their own role to play, led by their passion.&lt;/p&gt;

&lt;h3 id=&quot;opensource&quot;&gt;Opensource&lt;/h3&gt;
&lt;p&gt;While I needed to understand Temporal’s purpose, believe the vision could be executed, there is still something more. For me, especially coming from Red Hat, the technology needed to be opensource. I saw a great interview with Maxim Fateev (CEO of Temporal) where he explains the reasoning why Temporal needs to be opensource? He says that anything developer orientated today, needs to be opensource or it simply won’t be seriously considered. This was not only reassuring but also gave me great satisfaction in just how far opensource adoption has come. The problems of today are simply bigger than any one company and opensource is the only way to collaborate together across organizations, to solve them.&lt;/p&gt;

&lt;h3 id=&quot;the-right-fit&quot;&gt;The Right Fit&lt;/h3&gt;
&lt;p&gt;Finally the role also had to be the right fit. I had a desire to get closer to developers after being mostly on the platform side the last 7+ years. I am in fact a closet software hacker, it is one of the things I do for fun. The solution architect role at Temporal required someone of course with pre-sales skills but also software development skills (a less common combination). Temporal is consumed through an SDK by developers and as such being able to provide recommended practice or guidance to help customers also means understanding the code. I have long waited for a pre-sales role where my software development skills and passion were not just an asset but a requirement.&lt;/p&gt;

&lt;h2 id=&quot;my-experience-so-far&quot;&gt;My Experience So Far&lt;/h2&gt;
&lt;p&gt;It might seem odd to talk about experience when this is literally day one but there is actually a lot to talk about. I have been on Temporal’s community Slack for several weeks. Following some of the discussions and after I accepted an offer folks started to reach out. Even asking for my feedback or ideas about certain things. I attended a pre-screen of a 101 training session that will be presented at Replay, Temporal’s first user conference August 25-26 in Seattle. I even got to provide feedback all before my actual start date. I have had my laptop now for about two weeks which I am proud to say runs Fedora 36 (Gnome 42) and the Friday before my start date I had all my accounts setup. It is day 1 and I am ready to rock!&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this article I discussed thoughts around my decision to make a career change and pursue a new opportunity at Temporal. I am extremely excited about taking my first steps on this new journey and cannot wait to discover what is around the corner. I hope that my perspective may be of help, in your career or life journey.&lt;/p&gt;

&lt;p&gt;(c) 2022 Keith Tenzer&lt;/p&gt;</content><author><name>Keith Tenzer</name></author><category term="Temporal" /><category term="Temporal" /><category term="Opensource" /><category term="Workflows" /><category term="Activities" /><summary type="html">Overview Today is my first day at temporal and with that I wanted to share some thoughts around my decision, why Temporal and my experience thus far. As you can imagine, making any career change is always a very careful thought process and as we get older, have more responsibilities, the gravity around those decisions becomes stronger.</summary></entry><entry><title type="html">Blog with Gitops Practices and GitHub</title><link href="http://localhost:4000/linux/blog-with-gitops-practices-and-github/" rel="alternate" type="text/html" title="Blog with Gitops Practices and GitHub" /><published>2022-02-10T00:00:00-08:00</published><updated>2022-02-10T00:00:00-08:00</updated><id>http://localhost:4000/linux/blog-with-gitops-practices-and-github</id><content type="html" xml:base="http://localhost:4000/linux/blog-with-gitops-practices-and-github/">&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Want to build your brand, while living the gitops revolution and not paying anything for it? That is exactly what this article will walk you through. I will show you how to setup your customized blog hosted in Github and create blog posts, doing it the way developers do it, one commit at a time.&lt;/p&gt;

&lt;p&gt;Now most people go to wordpress or some blog platform and pay money for it. I think that is a shame because it is a missed opportunity to live gitops which is at the center of automating everything. Github has long provided the ability to create a basic blogs using gh-pages. Github uses a framework written in ruby called Jekyll which is available outside of Github, it’s opensource. Jekyll provides a templating framework and handles all the html, css stuff, allowing you to just write your blog in markdown (which anyone can do). The Jekyll themes Github provides are seriously limited and no fun. Most people don’t know this but with a little extra effort you can actually use any Jekyll themes and there are 100’s if not 1000’s so you can create your brand just the way you like it.&lt;/p&gt;

&lt;h2 id=&quot;setup-github-repository&quot;&gt;Setup Github Repository&lt;/h2&gt;
&lt;p&gt;First if you don’t have a &lt;a href=&quot;https://github.com/&quot;&gt;github&lt;/a&gt; account get one. Next once you have an account create a new repository. This should be the name of the blog. In my case my blog is keithtenzer.com so that is also the name of the &lt;a href=&quot;https://github.com/ktenzer/keithtenzer.com&quot;&gt;repository&lt;/a&gt;. Next decide if you are okay with a github domain name or if you want your own custom domain. I strongly recommend you pay the $10 a year and get your name. It is your brand after all and using your name makes it that way and also allows you to blog about anything.&lt;/p&gt;

&lt;h2 id=&quot;create-your-domain-optional&quot;&gt;Create Your Domain (optional)&lt;/h2&gt;
&lt;p&gt;If you aren’t interested in your own domain you can skip this part.
Decide where you want your domain to be hosted. I strongly recommend &lt;a href=&quot;https://www.cloudflare.com/&quot;&gt;cloudflare&lt;/a&gt;. Purchase your domain by selecting buy domain.
&lt;img src=&quot;/assets/2022-02-10/buy_domain.png&quot; alt=&quot;Buy Domain&quot; /&gt;
Once you have a domain you need to point it at Github. Under websites, click DNS and configure DNS as follows. Simply replace &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;keithtenzer.com&lt;/code&gt; with your domain. Also make sure you add a CNAME for www to &amp;lt;pre&amp;gt;username.github.io&amp;lt;/pre&amp;gt;.
&lt;img src=&quot;/assets/2022-02-10/buy_domain.png&quot; alt=&quot;Buy Domain&quot; /&gt;
Once DNS changes are done it can take 24 hours to propagate so I recommend waiting. You can check by querying DNS using nslookup or dig and seeing when your domain shows the IPs you entered for Github.&lt;/p&gt;

&lt;h2 id=&quot;initialize-blog-in-github&quot;&gt;Initialize Blog in Github&lt;/h2&gt;
&lt;p&gt;Log in to Github and under repositories and create a new one.
&lt;img src=&quot;/assets/2022-02-10/blog_repo.png&quot; alt=&quot;Blog Repository&quot; /&gt;
Clone your repository by copying the code link and running it in a CLI terminal.&lt;/p&gt;
&lt;pre&gt;$ git clone https://github.com/ktenzer/helloblog.git&lt;/pre&gt;
&lt;p&gt;You may need to install &lt;a href=&quot;https://github.com/git-guides/install-git&quot;&gt;git&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;enable-blog&quot;&gt;Enable Blog&lt;/h2&gt;
&lt;p&gt;Once the repository is initialized we can enable the blog by going to settings. Under pages you simply need to select the branch (main) and fill out your custom domain if you have one.
&lt;img src=&quot;/assets/2022-02-10/enable_blog.png&quot; alt=&quot;Enable Blog&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;select-a-jekyll-themes&quot;&gt;Select a Jekyll Themes&lt;/h2&gt;
&lt;p&gt;As I mentioned there are many Jekyll themes. You can search for curated ones &lt;a href=&quot;https://jekyllthemes.io/&quot;&gt;here&lt;/a&gt;. For my blog I use &lt;a href=&quot;https://mmistakes.github.io/minimal-mistakes/&quot;&gt;minimal-mistakes&lt;/a&gt;. Once you have chosen a theme simply clone it from Github in a separate directory from where you cloned your blog following same steps as above.&lt;/p&gt;
&lt;pre&gt;$ git clone https://github.com/mmistakes/minimal-mistakes.git&lt;/pre&gt;
&lt;p&gt;Next copy the contents into your blog repository.&lt;/p&gt;
&lt;pre&gt;cp -r minimal-mistakes/* helloblog/&lt;/pre&gt;

&lt;h2 id=&quot;customize-blog&quot;&gt;Customize Blog&lt;/h2&gt;
&lt;p&gt;Now that you have a theme it is time to customize. Under your blog directory edit the _config.yml. You need to uncomment the remote_theme. In addition you can add title, author information and whatever else.&lt;/p&gt;
&lt;pre&gt;$ vi _config.yml
remote_theme           : &quot;mmistakes/minimal-mistakes&quot;
&lt;/pre&gt;

&lt;h2 id=&quot;setup-jekyll&quot;&gt;Setup Jekyll&lt;/h2&gt;
&lt;p&gt;Now that you have a blog, you are dying to publish it right? Not so fast, now it is time to test it locally. After all you wouldn’t want to publish anything you didn’t review first, right? In order to test locally you need to install and setup jekyll. This part is much easier if you are running Linux like me. If not maybe you should also consider &lt;a href=&quot;https://keithtenzer.com/linux/The-Fedora-Workstation-Experience/&quot;&gt;switching&lt;/a&gt; to Linux, your Macbook is holding you back from greatness (trust me). You could also run Fedora as a virtual machine, well as long as you don’t have one of those shiny M1 Macbook’s (maybe someone will make that work in future though).
Either way below are the steps for Fedora Linux.&lt;/p&gt;
&lt;pre&gt;$ cd helloblog
$ sudo dnf install ruby
$ sudo dnf install -y ruby-devel
$ sudo dnf install -y g++
$ bundler install
$ gem install eventmachine -v &apos;1.2.7&apos; -- --with-cppflags=-I/usr/local/opt/openssl/include
$ gem install jekyll bundler
$ sudo gem update --system 3.2.30 --install-dir=/usr/share/gems --bindir /usr/local/bin
$ sudo gem pristine ffi --version 1.15.4
$ sudo gem pristine http_parser.rb --version 0.8.0
$ sudo gem pristine sassc --version 2.4.0
$ bundle install
$ bundle add webrick
&lt;/pre&gt;

&lt;p&gt;It may want you to pristine different version of gems. If that is the case just change the version to what it wants.&lt;/p&gt;

&lt;h2 id=&quot;test-blog&quot;&gt;Test Blog&lt;/h2&gt;
&lt;p&gt;Now that Jekyll is setup we can test our new blog.&lt;/p&gt;
&lt;pre&gt;$ bundle exec jekyll serve

Configuration file: /home/ktenzer/helloblog/_config.yml
            Source: /home/ktenzer/helloblog
       Destination: /home/ktenzer/helloblog/_site
 Incremental build: disabled. Enable with --incremental
      Generating... 
       Jekyll Feed: Generating feed for posts
                    done in 0.192 seconds.
 Auto-regeneration: enabled for &apos;/home/ktenzer/helloblog&apos;
    Server address: http://127.0.0.1:4000
  Server running... press ctrl-c to stop.
&lt;/pre&gt;
&lt;p&gt;Once Jekyll is running simply open your web browser of choice and go to the server address link.
&lt;img src=&quot;/assets/2022-02-10/hello_blog.png&quot; alt=&quot;Hello World Blog&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;push-blog-to-github&quot;&gt;Push Blog to Github&lt;/h1&gt;
&lt;p&gt;Now that we have tested our blog and are happy lets commit the code! If you want to brush up on Github I recommend the &lt;a href=&quot;https://docs.github.com/en/get-started&quot;&gt;tutorials&lt;/a&gt;.
First lets add our files locally from within our blog directory.&lt;/p&gt;
&lt;pre&gt;$ git add .&lt;/pre&gt;
&lt;p&gt;To see the files that will be commited (optional).&lt;/p&gt;
&lt;pre&gt;$ git status -s&lt;/pre&gt;
&lt;p&gt;Commit files locally.&lt;/p&gt;
&lt;pre&gt;$ git commit -a -m &quot;first blog entry&quot;&lt;/pre&gt;
&lt;p&gt;Push changes into Github.&lt;/p&gt;
&lt;pre&gt;$ git push origin&lt;/pre&gt;
&lt;p&gt;In order to push changes you will need a token. You can create a token under account settings-&amp;gt;developer settings-&amp;gt;personal access tokens in Github.
Once changes are pushed it goes through CI/CD (Github Actions) and then is published. It usually takes just few minutes. You can check the status by looking at workflows under actions. Once the action which does CI/CD is complete check out your blog in production &lt;a href=&quot;https://ktenzer.github.io/helloblog/&quot;&gt;hello blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2022-02-10/actions.png&quot; alt=&quot;Github Actions&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;add-post-to-blog&quot;&gt;Add Post to Blog&lt;/h2&gt;
&lt;p&gt;Now that the blog is up and running let’s create a blog entry. Create the _posts directory and then create a new file with the format YYYY-MM-DD-post-name.md. Add the header information that defines title, layout, categories and tags. Layouts are located in the _layouts directory and provided by the Jekyll theme.&lt;/p&gt;
&lt;pre&gt;$ mkdir _posts&lt;/pre&gt;
&lt;pre&gt;$ vi _posts/2022-02-10-hello-world.md&lt;/pre&gt;
&lt;pre&gt;--- 
layout: single
title:  &quot;Hello World&quot;
categories:
- Linux
tags:
- Linux
- Fedora
- Opensource
---
![Hello World Image](/assets/2022-02-10/hello_world_md.png)
## Overview
This is a hellow world blog
## Hello
## World
## Summary
This was a hello world blog
(c) 2022 Keith Tenzer&lt;/pre&gt;

&lt;h3 id=&quot;add-image-to-post&quot;&gt;Add Image to Post&lt;/h3&gt;
&lt;p&gt;Images, videos and other artifacts go in the assets directory. It is best to organize under the date of your post so you can find stuff.
First create a directory.&lt;/p&gt;
&lt;pre&gt;$ mkdir assets/2022-02-10&lt;/pre&gt;
&lt;p&gt;Next copy image to that directory and then you can link it in your post using the /assets/2022-02-10 path.&lt;/p&gt;
&lt;pre&gt;$ cp /home/ktenzer/Pictures/hello_world_md.png assets/2022-02-10/ &lt;/pre&gt;

&lt;h2 id=&quot;test-post&quot;&gt;Test Post&lt;/h2&gt;
&lt;p&gt;Simply run Jekyll again to test locally.&lt;/p&gt;
&lt;pre&gt;$ bundle exec jekyll serve&lt;/pre&gt;
&lt;h2 id=&quot;push-blog-post-to-github&quot;&gt;Push Blog Post to Github&lt;/h2&gt;
&lt;p&gt;Repeat same steps above when we originally pushed our first commit. 
First lets add our files locally from without our blog directory.&lt;/p&gt;
&lt;pre&gt;$ git add .&lt;/pre&gt;
&lt;p&gt;To see the files that will be commited (optional).&lt;/p&gt;
&lt;pre&gt;$ git status -s&lt;/pre&gt;
&lt;p&gt;Commit files locally.&lt;/p&gt;
&lt;pre&gt;$ git commit -a -m &quot;first blog entry&quot;&lt;/pre&gt;
&lt;p&gt;Push changes into Github.&lt;/p&gt;
&lt;pre&gt;$ git push origin&lt;/pre&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;Blogging is a great way to build your own brand and be an ambassador for what is important to you. It allows you to share your experiences and knowledge with the rest of the world. In this article we went through the steps to create your own unique blog platform using Jekyll and hosting it in Github (for free). One thing I learned that I will always take with me, sharing is caring.&lt;/p&gt;

&lt;p&gt;(c) 2022 Keith Tenzer&lt;/p&gt;</content><author><name>Keith Tenzer</name></author><category term="Linux" /><category term="Linux" /><category term="Fedora" /><category term="Opensource" /><summary type="html">Overview Want to build your brand, while living the gitops revolution and not paying anything for it? That is exactly what this article will walk you through. I will show you how to setup your customized blog hosted in Github and create blog posts, doing it the way developers do it, one commit at a time.</summary></entry><entry><title type="html">The Fedora Workstation Experience</title><link href="http://localhost:4000/linux/The-Fedora-Workstation-Experience/" rel="alternate" type="text/html" title="The Fedora Workstation Experience" /><published>2022-01-10T00:00:00-08:00</published><updated>2022-01-10T00:00:00-08:00</updated><id>http://localhost:4000/linux/The-Fedora-Workstation-Experience</id><content type="html" xml:base="http://localhost:4000/linux/The-Fedora-Workstation-Experience/">&lt;p&gt;&lt;img src=&quot;/assets/2022-01-10/desktop.png&quot; alt=&quot;Fedora&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;A lot of people always ask me what is the best way to contribute to opensource? Of course contributing code, documentation, spreading the gospel or even talking about opensource are great. However the easiest and seemingly least obvious method is to actually use opensource. The best way to do that is by running Linux on your workstation. There are a lot of great Linux distro’s to choose from and me being a RHatter, well let’s just say I have an impartial view.&lt;/p&gt;

&lt;p&gt;Being an ex-macbooker, I love the simplicity and minimalist approach of Apple’s UI design and experience but I dislike their proprietary, closed system approach. As such I aim to find the best of both worlds: great UI experience without compromising on freedom, choice and promoting the power of opensource!&lt;/p&gt;

&lt;h2 id=&quot;hardware&quot;&gt;Hardware&lt;/h2&gt;
&lt;p&gt;Currently I have a Lenovo Thinkpad X1 but great thing here is you have choice. I get that Apple’s Macbook hardware has a great look and feel (touchpad works great) but those are pretty minor things and your going to cover your laptop in stickers anyway, right?&lt;/p&gt;

&lt;h2 id=&quot;installing-fedora-latest&quot;&gt;Installing Fedora Latest&lt;/h2&gt;
&lt;p&gt;Fedora has a &lt;a href=&quot;https://getfedora.org/en/workstation/download/&quot;&gt;media writer&lt;/a&gt; and you can download this to create a USB Image to install Fedora. You can provide your own ISO or just let it create an image using the latest Fedora. Once you have USB image simply reboot your laptop/desktop press F12 or F8 to enter boot menu and boot from your USB. The rest is just accepting defaults. If your more advanced user you may want to also setup your own filesystem partitions and I always recommend turning on disk encryption with LUKS.&lt;/p&gt;

&lt;h2 id=&quot;customizing-fedora-workstation&quot;&gt;Customizing Fedora Workstation&lt;/h2&gt;
&lt;p&gt;Once you get booted into Fedora it’s time to customize. Unlike MacOS or Windows you can truly customize the desktop environment and while that is powerful and rewarding it also can be time consuming as well as turn most people off. The point here is to get something great with limited effort.&lt;/p&gt;

&lt;h3 id=&quot;update-fedora&quot;&gt;Update Fedora&lt;/h3&gt;
&lt;p&gt;Just like any OS clicking the update button is usually the first step. You can of course click on Applications-&amp;gt;System Tools-&amp;gt;Software which launches the UI software package manager but this is Linux right?&lt;/p&gt;

&lt;pre&gt;$ sudo dnf update -y&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2022-01-10/update.png&quot; alt=&quot;Fedora&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;install-gnome-extentions&quot;&gt;Install GNOME Extentions&lt;/h3&gt;
&lt;p&gt;GNOME is the UI operating environment and has a modular plugin framework for extending it. There have been huge advances in GNOME performance and while developers do all the hard work, users make those contributions meaningful which in turns leads to more development resources.&lt;/p&gt;
&lt;pre&gt;$ sudo dnf install gnome-extensions-app&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2022-01-10/extentions.png&quot; alt=&quot;GNOME Extentions&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;install-dock-from-dash-extensions&quot;&gt;Install Dock from Dash Extensions&lt;/h4&gt;
&lt;p&gt;This extension will add a docking bar in center for favorite applications just like MacOS.
Navigate to &lt;a href=&quot;https://extensions.gnome.org/&quot;&gt;https://extensions.gnome.org/&lt;/a&gt; and in search filled enter “dock from dash”. Click on the extension and in upper right there is a slider to enable the extension.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2022-01-10/dock_and_dash.png&quot; alt=&quot;Dock and Dash&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;install-gnome-tweaks&quot;&gt;Install GNOME Tweaks&lt;/h4&gt;
&lt;p&gt;Using GNOME tweaks you can configure many aspects of the UI we will be using later&lt;/p&gt;

&lt;pre&gt;$ sudo dnf install -y gnome-tweaks&lt;/pre&gt;

&lt;h3 id=&quot;install-google-chrome&quot;&gt;Install Google Chrome&lt;/h3&gt;

&lt;pre&gt;$ sudo dnf install google-chrome&lt;/pre&gt;

&lt;h3 id=&quot;disable-wayland&quot;&gt;Disable Wayland&lt;/h3&gt;
&lt;p&gt;Overall wayland works but one area there is still issues is in screen sharing. If you care about that I would recommend disabling it.&lt;/p&gt;

&lt;pre&gt;
$ sudo vi /etc/gdm/custom.conf
WaylandEnable=false
&lt;/pre&gt;

&lt;h3 id=&quot;configure-macos-theme&quot;&gt;Configure MacOS Theme&lt;/h3&gt;
&lt;p&gt;These steps will enable MacOS icons and desktop theme.&lt;/p&gt;

&lt;pre&gt;$ sudo dnf install la-capitaine-icon-theme&lt;/pre&gt;

&lt;pre&gt;$ git clone https://github.com/paullinuxthemer/Mc-OS-themes.git&lt;/pre&gt;

&lt;pre&gt;$ mkdir ~/.themes&lt;/pre&gt;

&lt;pre&gt;$ cp -r Mc-OS-themes/McOS-MJV ~/.themes&lt;/pre&gt;

&lt;p&gt;Open GNOME Tweak tool Applications-&amp;gt;Utilities-&amp;gt;Tweak and navigate to appearance section. Set application theme to McOS-MJV and icons to La-Capitane. Navigate to window titlebars section and enable maximize/minimize under titlebar buttons. This adds buttons to all windows that let you maximize or minimize them.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2022-01-10/themes.png&quot; alt=&quot;Themes&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2022-01-10/appearance.png&quot; alt=&quot;Appearance&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2022-01-10/minimize_buttons.png&quot; alt=&quot;Window Buttons&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;install-rpm-fusion&quot;&gt;Install RPM Fusion&lt;/h3&gt;
&lt;p&gt;This is a tool that gives you access to a lot of community developed tools and is very useful for workstations.&lt;/p&gt;

&lt;pre&gt;$ sudo dnf install https://download1.rpmfusion.org/free/fedora/rpmfusion-free-release-$(rpm -E %fedora).noarch.rpm&lt;/pre&gt;

&lt;h3 id=&quot;optimize-battery-usage&quot;&gt;Optimize Battery Usage&lt;/h3&gt;
&lt;p&gt;There are additional drivers needed to ensure your battery is used efficiently. Not installing these leads to quicker battery drain.&lt;/p&gt;

&lt;pre&gt;$ sudo dnf install tlp tlp-rdw&lt;/pre&gt;

&lt;p&gt;For thinkpads add additional driver from RPM fusion.&lt;/p&gt;
&lt;pre&gt;$ dnf install https://download1.rpmfusion.org/free/fedora/rpmfusion-free-release-$(rpm -E %fedora).noarch.rpm
&lt;/pre&gt;

&lt;h3 id=&quot;install-multimedia-codecs&quot;&gt;Install Multimedia Codecs&lt;/h3&gt;
&lt;pre&gt;$ sudo dnf groupupdate multimedia --setop=&quot;install_weak_deps=False&quot; --exclude=PackageKit-gstreamer-plugin&lt;/pre&gt;
&lt;pre&gt;$ sudo dnf groupupdate sound-and-video&lt;/pre&gt;

&lt;h3 id=&quot;setup-solarized&quot;&gt;Setup Solarized&lt;/h3&gt;
&lt;p&gt;Solarized is a color scheme for terminal sessions. Let’s face it staring at a black screen with green text is bad and tiring for your eyes. These color patters are soothing and will let you enjoy starring at CLI terminals.&lt;/p&gt;
&lt;pre&gt;$ sudo dnf install -y p7zip&lt;/pre&gt;
&lt;pre&gt;$ mkdir ~/solarized&lt;/pre&gt;
&lt;pre&gt;$ cd ~/solarized&lt;/pre&gt;
&lt;pre&gt;$ git clone https://github.com/sigurdga/gnome-terminal-colors-solarized.git&lt;/pre&gt;
&lt;pre&gt;$ gnome-terminal-colors-solarized/install.sh&lt;/pre&gt;

&lt;p&gt;Setup vim and install solarized colors for vim.&lt;/p&gt;
&lt;pre&gt;$ sudo dnf install -y vim&lt;/pre&gt;
&lt;pre&gt;$ git clone https://github.com/altercation/vim-colors-solarized.git&lt;/pre&gt;
&lt;pre&gt;$ mkdir -p ~/.vim/colors&lt;/pre&gt;
&lt;pre&gt;$ cp vim-colors-solarized/colors/solarized.vim ~/.vim/colors/&lt;/pre&gt;

&lt;h3 id=&quot;install-vscode&quot;&gt;Install VSCODE&lt;/h3&gt;
&lt;p&gt;Pretty much the standard IDE for software development, infrastructure-as-code, blogging or anything that requires editing text.&lt;/p&gt;
&lt;pre&gt;$ sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc&lt;/pre&gt;

&lt;pre&gt;$ cat &amp;lt;&amp;lt;EOF | sudo tee /etc/yum.repos.d/vscode.repo
[code]
name=Visual Studio Code
baseurl=https://packages.microsoft.com/yumrepos/vscode
enabled=1
gpgcheck=1
gpgkey=https://packages.microsoft.com/keys/microsoft.asc
EOF
&lt;/pre&gt;

&lt;pre&gt;$ sudo dnf check-update&lt;/pre&gt;
&lt;pre&gt;$ sudo dnf install -y code&lt;/pre&gt;

&lt;h3 id=&quot;install-ansible-optional&quot;&gt;Install Ansible (Optional)&lt;/h3&gt;
&lt;p&gt;If you aren’t automating stuff with Ansible it is never to late to change your life.&lt;/p&gt;
&lt;pre&gt;$ sudo dnf install -y ansible&lt;/pre&gt;
&lt;pre&gt;$ sudo dnf install -y python-ansible-runner&lt;/pre&gt;
&lt;pre&gt;$ pip install ansible-runner-http&lt;/pre&gt;
&lt;pre&gt;$ pip install openshift&lt;/pre&gt;
&lt;pre&gt;$ ansible-galaxy collection install kubernetes.core&lt;/pre&gt;

&lt;h3 id=&quot;install-go-optional&quot;&gt;Install Go (Optional)&lt;/h3&gt;
&lt;p&gt;Golang is by far my language of choice for it’s simplicity and elegance.&lt;/p&gt;
&lt;pre&gt;$ mkdir -p ~/go/src/github.com&lt;/pre&gt;
&lt;pre&gt;$ sudo dnf install -y go&lt;/pre&gt;
&lt;pre&gt;$ vi ~/.bash_profile
export GOBIN=/home/username
export GOPATH=/home/username/src
export EDITOR=vim
&lt;/pre&gt;

&lt;h3 id=&quot;install-openshift-and-kubectl-optional&quot;&gt;Install OpenShift and Kubectl (Optional)&lt;/h3&gt;
&lt;p&gt;Likely there are newer releases so grab latest.
&lt;a href=&quot;https://access.redhat.com/downloads/content/290/ver=4.9/rhel---8/4.9.13/x86_64/product-software&quot;&gt;OpenShift and Kubectl 4.9&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;install-operator-sdk-optional&quot;&gt;Install Operator SDK (Optional)&lt;/h3&gt;
&lt;p&gt;If you want to put all those Ansible skills to use in cloud-native world start writing Operators.&lt;/p&gt;

&lt;p&gt;Likely there are newer releases so grab latest or release for your OCP release.
&lt;a href=&quot;https://docs.openshift.com/container-platform/4.9/operators/operator_sdk/osdk-installing-cli.html#osdk-installing-cli-linux-macos_osdk-installing-cli&quot;&gt;Operator Framework 4.9&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this article we discussed the importance of choice and at least why you might consider Linux for your next workstation operating system. We also go through a step-by-step guide in configuring your Fedora workstation and give it similar look and feel to MacOS. If I only convinced one person to give Linux a try then it was all worth it!&lt;/p&gt;

&lt;p&gt;(c) 2022 Keith Tenzer&lt;/p&gt;</content><author><name>Keith Tenzer</name></author><category term="Linux" /><category term="Linux" /><category term="Fedora" /><category term="Opensource" /><summary type="html">Overview A lot of people always ask me what is the best way to contribute to opensource? Of course contributing code, documentation, spreading the gospel or even talking about opensource are great. However the easiest and seemingly least obvious method is to actually use opensource. The best way to do that is by running Linux on your workstation. There are a lot of great Linux distro’s to choose from and me being a RHatter, well let’s just say I have an impartial view.</summary></entry><entry><title type="html">Building Ansible Operators 1-2-3</title><link href="http://localhost:4000/openshift/building-ansible-operators-1-2-3/" rel="alternate" type="text/html" title="Building Ansible Operators 1-2-3" /><published>2021-12-03T00:00:00-08:00</published><updated>2021-12-03T00:00:00-08:00</updated><id>http://localhost:4000/openshift/building-ansible-operators-1-2-3</id><content type="html" xml:base="http://localhost:4000/openshift/building-ansible-operators-1-2-3/">&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;In this article we will go step by step to build a Kubernetes Operator using Ansible and the Operator Framework. Operators provide the ability to not only deploy an application but also manage the entire lifecycle while also baking in higher degrees of intelligence as well as resilience. Before we get to the Operator lets briefly revisit the ways we can deploy and manage applications in k8s.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Deployment&lt;/li&gt;
  &lt;li&gt;Deployment Template&lt;/li&gt;
  &lt;li&gt;Helm&lt;/li&gt;
  &lt;li&gt;Operator&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A k8s deployment is the simplest method but there is no way to parameterize unless you are doing it through Ansible k8s module or something else that handles that. A deployment template does provide parameterization but is only available on OpenShift and also doesn’t handle packaging or management of the application itself. Helm provides parameterization and also packaging but doesn’t provide application lifecycle management or allow for building extended intelligence for added resilience. Operators provide the full package and provide the pattern to improve operability over time. Some suggest just using the simplest tool for the job. If you just need to deploy an app for example and don’t need parameterization, use a k8s deployment. Personally my view is that with Ansible, Operators are just about as easy as a k8s deployment but with so much added benefit that the Operator approach always makes sense. My hope and goal with this article is to maybe influence a few more Operators and show that it isn’t really any additional work.&lt;/p&gt;

&lt;h2 id=&quot;pre-requisites&quot;&gt;Pre-requisites&lt;/h2&gt;
&lt;p&gt;Your starting point should be an application you can deploy using a k8s deployment or deployment config. From there the next thing is to setup a development environment for building Operators using the Operator Framework and Ansible.
&lt;a href=&quot;https://docs.openshift.com/container-platform/4.8/operators/operator_sdk/ansible/osdk-ansible-quickstart.html&quot;&gt;Operator Pre-requisites&lt;/a&gt;
Once you have operator-sdk and openshift or kubectl client you need some Ansible dependencies.&lt;/p&gt;
&lt;pre&gt;$ sudo dnf install -y ansible&lt;/pre&gt;
&lt;pre&gt;$ sudo dnf install -y python-ansible-runner&lt;/pre&gt;
&lt;pre&gt;$ sudo dnf install -y python-ansible-runner&lt;/pre&gt;
&lt;pre&gt;$ sudo pip install ansible-runner-http&lt;/pre&gt;
&lt;pre&gt;$ sudo pip install openshift&lt;/pre&gt;
&lt;pre&gt;$ ansible-galaxy collection install kubernetes.core &lt;/pre&gt;
&lt;pre&gt;$ ansible-galaxy collection install operator_sdk.util &lt;/pre&gt;

&lt;h2 id=&quot;create-operator-scaffolding&quot;&gt;Create Operator Scaffolding&lt;/h2&gt;
&lt;p&gt;Before we can begin coding the Operator we need boiler plate code and thankfully the operator sdk does all that.&lt;/p&gt;
&lt;pre&gt;$ operator-sdk init --plugins=ansible --domain=kubecraft.com&lt;/pre&gt;
&lt;pre&gt;$ operator-sdk create api     --group cache     --version v1     --kind Kubecraft     --generate-role&lt;/pre&gt;

&lt;h2 id=&quot;customizing-operator&quot;&gt;Customizing Operator&lt;/h2&gt;
&lt;p&gt;At this point we need an application. My approach is to first create a k8s deployment and test deploying my application before building the Operator. In this example we will use an app called &lt;a href=&quot;https://github.com/ktenzer/kubecraftadmin&quot;&gt;Kubekraft&lt;/a&gt;. It is a fun app that connects k8s world to minecraft through a minecraft websocket server written in Go. Browse to the yaml folder and you will see the k8s deployment.yaml. This is what we will use to build out our Operator.
Under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;roles/kubecraft/tasks&lt;/code&gt; directory we create tasks in Ansible to deploy what we had in the k8s deployment yaml which is a deployment, service and route. In addition we added a task to get the application domain dynamically so we can build our route properly. This also demonstrates how to query and use other k8s resources in our Ansible code.&lt;/p&gt;

&lt;p&gt;In addition, if your operator creates k8s resources you need to ensure proper permissions. Under the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;config/rbac&lt;/code&gt; directory you can add permissions to the role.yaml. For this operator I added services and routes so that those resources  can be created by the Operator.&lt;/p&gt;

&lt;h2 id=&quot;testing-operator&quot;&gt;Testing Operator&lt;/h2&gt;
&lt;p&gt;The operator sdk provides a simple way to test the operator locally. Once we have our tasks complete under the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;role/project/tasks&lt;/code&gt; directory, we simply need to create a new project, run the operator locally and create a custom resource (CR) which executes our role and tasks we defined.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Create project&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;$ oc new-project kubecraft&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Run operator locally&lt;/strong&gt;
Run these commands from the root directory of your operator project where the Makefile resides.&lt;/p&gt;
&lt;pre&gt;$ make install&lt;/pre&gt;
&lt;pre&gt;$ make run&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Create custom resource&lt;/strong&gt;
In a new terminal create CR. Also note that our operator expects as user input, a comma separated list of namespaces to monitor. User input is parameterized via the custom resource so if you look inside you will see the namespaces parameter set and being consumed within the Ansible role.&lt;/p&gt;
&lt;pre&gt;$ oc create -f config/samples/cache_v1_kubecraft.yaml&lt;/pre&gt;

&lt;h2 id=&quot;building-operator&quot;&gt;Building Operator&lt;/h2&gt;
&lt;p&gt;Once we have tested the operator locally we can build and publish our operator to a registry.
&lt;strong&gt;Authenticate to registry&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;$ sudo docker login https://registry.redhat.io&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Build operator image and push to registry&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;$ sudo make docker-build docker-push IMG=quay.io/ktenzer/kubecraft-operator:latest&lt;/pre&gt;
&lt;p&gt;If you are using quay.io as your registry make sure to login and make the image is public so it can be accessed.&lt;/p&gt;

&lt;h2 id=&quot;running-operator&quot;&gt;Running Operator&lt;/h2&gt;
&lt;p&gt;Now that we have the operator tested and the image built we can simply deploy it, create a CR and rule the world!
 The operator sdk makes this really easy and streamlines everything into a single command.&lt;/p&gt;
&lt;pre&gt;$ make deploy IMG=quay.io/ktenzer/kubecraft-operator:latest&lt;/pre&gt;

&lt;p&gt;BY default the operator will be installed into a project &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;operatorName-system&lt;/code&gt; however you can change that by updating the project name in the PROJECT file under the root of the operator project. In this case we changed it to kubecraft-operator.&lt;/p&gt;

&lt;p&gt;We can remove the operator also using make.&lt;/p&gt;
&lt;pre&gt;$ make undeploy&lt;/pre&gt;

&lt;h2 id=&quot;create-operator-bundle&quot;&gt;Create Operator Bundle&lt;/h2&gt;
&lt;p&gt;The operator bundle allows integration with operator lifecycle manager (olm) which provides a facility for upgrading operator seamlessly as well as integrating with operator hub. First we will generate the bundle boiler plate.&lt;/p&gt;
&lt;pre&gt;$ make bundle&lt;/pre&gt;
&lt;p&gt;If you want to change anything, like add image you can update &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bundle/manifests&lt;/code&gt; clusterserviceversion. When your ready you will build bundle and then push it to your repository. Remember if using quay.io to make the image public.&lt;/p&gt;
&lt;pre&gt;$ sudo make bundle-build BUNDLE_IMG=quay.io/ktenzer/kubecraft-operator-bundle:latest&lt;/pre&gt;
&lt;p&gt;Push bundle to registry.&lt;/p&gt;
&lt;pre&gt;$ sudo docker push quay.io/ktenzer/kubecraft-operator-bundle:latest&lt;/pre&gt;
&lt;h2 id=&quot;run-operator-bundle&quot;&gt;Run operator bundle&lt;/h2&gt;
&lt;p&gt;Now that we have the bundle built and pushed to registry we can deploy it to a cluster.&lt;/p&gt;
&lt;pre&gt;$ operator-sdk run bundle -n kubecraft-operator quay.io/ktenzer/kubecraft-operator-bundle:latest&lt;/pre&gt;
&lt;p&gt;Once our operator bundle is running simply create a new project, a CR and watch the magic happen.&lt;/p&gt;
&lt;pre&gt;$ oc project kubecraft&lt;/pre&gt;
&lt;pre&gt;$ oc create -f config/samples/cache_v1_kubecraft.yaml&lt;/pre&gt;
&lt;pre&gt;$ oc get deployment
NAME        READY   UP-TO-DATE   AVAILABLE   AGE
kubecraft   1/1     1            1           5s
&lt;/pre&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this article we created an operator using the operator sdk and Ansible. We saw how to take a simple k8s deployment and turn it into a fully managed operator. Finally we created a bundle showing the integration with operator lifecycle manager and operator hub. Hopefully this article helps you get started with Ansible operators and next time you deploy an application in k8s you would consider the operator approach.&lt;/p&gt;

&lt;p&gt;(c) 2021 Keith Tenzer&lt;/p&gt;</content><author><name>Keith Tenzer</name></author><category term="OpenShift" /><category term="Containers" /><category term="OpenShift" /><category term="Operators" /><category term="Operator Framework" /><category term="Ansible" /><summary type="html">Overview In this article we will go step by step to build a Kubernetes Operator using Ansible and the Operator Framework. Operators provide the ability to not only deploy an application but also manage the entire lifecycle while also baking in higher degrees of intelligence as well as resilience. Before we get to the Operator lets briefly revisit the ways we can deploy and manage applications in k8s. Deployment Deployment Template Helm Operator</summary></entry><entry><title type="html">OpenShift Service Mesh Getting Started Guide</title><link href="http://localhost:4000/openshift/openshift-service-mesh-getting-started-guide/" rel="alternate" type="text/html" title="OpenShift Service Mesh Getting Started Guide" /><published>2021-04-27T00:00:00-07:00</published><updated>2021-04-27T00:00:00-07:00</updated><id>http://localhost:4000/openshift/openshift-service-mesh-getting-started-guide</id><content type="html" xml:base="http://localhost:4000/openshift/openshift-service-mesh-getting-started-guide/">&lt;h2&gt;&lt;/h2&gt;
&lt;p&gt;&lt;!-- wp:image {&quot;id&quot;:12890,&quot;width&quot;:79,&quot;height&quot;:83,&quot;sizeSlug&quot;:&quot;large&quot;,&quot;linkDestination&quot;:&quot;media&quot;} --&gt;&lt;/p&gt;
&lt;figure class=&quot;wp-block-image size-large is-resized&quot;&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2018/12/OpenShift-LogoType.svg_.png&quot;&gt;&lt;img src=&quot;/assets/2021/04/OpenShift-LogoType.svg_.png?w=959&quot; alt=&quot;&quot; class=&quot;wp-image-12890&quot; width=&quot;79&quot; height=&quot;83&quot; /&gt;&lt;/a&gt;&lt;/figure&gt;
&lt;p&gt;&lt;!-- /wp:image --&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2018/12/OpenShift-LogoType.svg_.png&quot;&gt;&lt;!-- /wp:image --&gt;&lt;/a&gt;&lt;!-- /wp:image --&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/04/istio.png&quot;&gt;&lt;!-- /wp:image --&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;!-- /wp:image --&gt;&lt;/h2&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;In this article we will explore the OpenShift Service Mesh and deploy a demo application to better understand the various concepts. First you might be asking yourself, why do I need a Service Mesh? If you have a microservice architecture then you likely have many services that interact with one another in various ways. If a downstream service breaks, how will you know? How can you trace an error through all services to find out where it originated? How will you manage exposing new APIs/capabilities to users? Service Mesh provides the answer to those questions providing 1) Visibility across all microservices 2) Traceability through all of the microservice interactions and 3) Ruleset governing service versions and capabilities that are introduced into the environment. OpenShift Service Mesh uses Istio as the mesh, Kiali for the dashboard and Jaeger for traceability. &lt;/p&gt;
&lt;p&gt;&lt;!--more--&gt;&lt;/p&gt;
&lt;h2&gt;Install OpenShift Service Mesh&lt;/h2&gt;
&lt;p&gt;OpenShift Service Mesh enables visibility of traffic as is traverses all services within the mesh. In case of an error you can not only see what service is actually the source for the error but also are able to trace the API communications between services. Finally it allows us to implement rules, like balancing traffic between multiple ratings API endpoints or even sending specific users to an API version. You can even create a chaos monkey using the ruleset that will inject various errors so you can observe how failures are handled. A Service Mesh is critical for any complex microservice application and without it you are literally flying blind while adding technical debt unable to manage or monitor service interactions properly.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Create a new project&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First we will create a project for hosting the service mesh control plane.&lt;/p&gt;
&lt;pre&gt;$ oc new-project bookinfo-mesh&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Install OpenShift Service Mesh&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under the project booking-mesh go to Operator Hub and install Red Hat OpenShift Service Mesh, Red Hat OpenShift Jaeger and Kiali Operator.&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:image {&quot;id&quot;:14769,&quot;sizeSlug&quot;:&quot;large&quot;,&quot;linkDestination&quot;:&quot;media&quot;} --&gt;&lt;/p&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/04/operators_servicemesh.png&quot;&gt;&lt;img class=&quot;wp-image-14769&quot; src=&quot;/assets/2021/04/operators_servicemesh.png?w=1024&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/figure&gt;
&lt;p&gt;&lt;!-- /wp:image --&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Configure OpenShift Service Mesh&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Create a service mesh control plane using defaults.&lt;/p&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/04/booking_mesh-1.png&quot;&gt;&lt;img class=&quot;wp-image-14742&quot; src=&quot;/assets/2021/04/booking_mesh-1.png?w=1024&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;Create a service mesh member. Update the yaml and change namespace under controlPlaneRef to bookinfo-mesh.&lt;/figure&gt;
&lt;pre&gt;apiVersion: maistra.io/v1&lt;br /&gt;kind: ServiceMeshMember&lt;br /&gt;metadata:&lt;br /&gt;  namespace: bookinfo-mesh&lt;br /&gt;  name: default&lt;br /&gt;spec:&lt;br /&gt;  controlPlaneRef:&lt;br /&gt;    name: basic&lt;br /&gt;    namespace: bookinfo-mesh&lt;/pre&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/04/mesh_role.png&quot;&gt;&lt;img class=&quot;wp-image-14744&quot; src=&quot;/assets/2021/04/mesh_role.png?w=1024&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;Finally create a service member role adding name of the project that will access the service mesh..&lt;/figure&gt;
&lt;div&gt;
&lt;pre&gt;apiVersion: maistra.io/v1&lt;br /&gt;kind: ServiceMeshMemberRoll&lt;br /&gt;metadata:&lt;br /&gt;  namespace: bookinfo-mesh&lt;br /&gt;  name: default&lt;br /&gt;spec:&lt;br /&gt;  members:&lt;br /&gt;    - bookinfo&lt;/pre&gt;
&lt;/div&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/04/mesh_member_role.png&quot;&gt;&lt;img class=&quot;wp-image-14746&quot; src=&quot;/assets/2021/04/mesh_member_role.png?w=1024&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/figure&gt;
&lt;h2&gt;Deploy Demo Application&lt;/h2&gt;
&lt;p&gt;In order to look into the capabilities provided by the OpenShift Service Mesh we will deploy a simple book review app. The application is static but shows reviews for a book and has several microservices. The product page is the entry point service. It provides information on books and their reviews. It accesses book reviews through the reviews service which is also accesses the ratings service downstream to allow users to give a book a rating. It also gets details about a book via the details service. Like a true polyglot all services are written in a different programming language. The ratings service provides three different API versions, v1 doesn&apos;t display ratings, v2 shows ratings in black and v3 shows ratings in red. The idea is all about innovating quickly and getting real user feedback to take the app in the right direction.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/04/demoapp.png&quot;&gt;&lt;img class=&quot;wp-image-14764&quot; src=&quot;/assets/2021/04/demoapp.png?w=1024&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Create a project for the book app.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Create a project to host the book app.&lt;/p&gt;
&lt;pre&gt;$ oc new-project bookinfo&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Deploy book app.&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;$ oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-2.0/samples/bookinfo/platform/kube/bookinfo.yaml&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Create a service mesh gateway.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Once the app is deployed we need to create a gateway and setup the URI matches.&lt;/p&gt;
&lt;pre&gt;$ oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-2.0/samples/bookinfo/networking/bookinfo-gateway.yaml&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Create service mesh rule set.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Since the ratings service has 3 API versions we need some rules to govern the traffic.&lt;/p&gt;
&lt;pre&gt;$ oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-2.0/samples/bookinfo/networking/destination-rule-all.yaml&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Access the application.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Get the route to the application and add the /productpage to access via web browser.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ export GATEWAY_URL=$(oc -n istio-system get route istio-ingressgateway -o jsonpath=&apos;{.spec.host}&apos;)&lt;br /&gt;$ echo $GATEWAY_URL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;http://istio-ingressgateway-bookinfo-mesh.apps.ocp4.rh-southwest.com/productpage&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:image {&quot;id&quot;:14775,&quot;sizeSlug&quot;:&quot;large&quot;,&quot;linkDestination&quot;:&quot;media&quot;} --&gt;&lt;/p&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/04/bookinfo2.png&quot;&gt;&lt;img class=&quot;wp-image-14775&quot; src=&quot;/assets/2021/04/bookinfo2.png?w=1024&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/figure&gt;
&lt;p&gt;&lt;!-- /wp:image --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:image {&quot;id&quot;:14752,&quot;sizeSlug&quot;:&quot;large&quot;,&quot;linkDestination&quot;:&quot;media&quot;} --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:image --&gt;If you refresh the web page you should see the ratings change between no ratings, black and red. This is because the ruleset is sending traffic to all three API endpoints (v1, v2 and v3).&lt;/p&gt;
&lt;h2&gt;Update Service Mesh Ruleset&lt;/h2&gt;
&lt;p&gt;As mentioned we can create rules that enable us to send certain traffic to a different API endpoint. This could be for a canary deployment where we want to test how users interact with different capabilities (red or black ratings stars), to perform a blue/green deployment upgrading to a newer API version or even sending different users to different APIs.&lt;/p&gt;
&lt;p&gt;In this case we will simply apply a new rule that only sends traffic to v2 (black) and v3 (red) ratings API.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Apply a new ruleset.&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ oc replace -f https://raw.githubusercontent.com/istio/istio/master/samples/bookinfo/networking/virtual-service-reviews-v2-v3.yaml&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now when you access the book app and refresh you should see it switched between red and black ratings.&lt;/p&gt;
&lt;p&gt;Looking at Kiali versioned app graph we can see that traffic is only going to v2 and v3 as we would expect.&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:image {&quot;id&quot;:14760,&quot;sizeSlug&quot;:&quot;large&quot;,&quot;linkDestination&quot;:&quot;media&quot;} --&gt;&lt;/p&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;!-- wp:image {&quot;id&quot;:14784,&quot;sizeSlug&quot;:&quot;large&quot;,&quot;linkDestination&quot;:&quot;media&quot;} --&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/04/kiali_ratings_v2_v3-1.png&quot;&gt;&lt;img class=&quot;wp-image-14784&quot; src=&quot;/assets/2021/04/kiali_ratings_v2_v3-1.png?w=1024&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/figure&gt;
&lt;p&gt;&lt;!-- /wp:image --&gt;&lt;/p&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;!-- /wp:image --&gt;&lt;/p&gt;
&lt;h2&gt;Troubleshooting Errors&lt;/h2&gt;
&lt;p&gt;As mentioned one of the key values of the OpenShift Service Mesh is the visualization and tracing. In order to generate an error we will scale down the ratings v1 deployment to 0.&lt;/p&gt;
&lt;pre&gt;$ oc scale deployment/ratings-v1 -n bookinfo --replicas 0&lt;/pre&gt;
&lt;p&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/04/bookinfo_error.png&quot;&gt;&lt;img class=&quot;wp-image-14752&quot; src=&quot;/assets/2021/04/bookinfo_error.png?w=1024&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You should now see that the ratings service is currently unavailable when you refresh the app in a browser.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Check Kiali dashboard.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You can access Kiali via route under networking in the bookinfo-mesh project. In Kiali we clearly see the issue is the reviews service as we would expect. &lt;/p&gt;
&lt;p&gt;&lt;!-- wp:image {&quot;id&quot;:14751,&quot;sizeSlug&quot;:&quot;large&quot;,&quot;linkDestination&quot;:&quot;media&quot;} --&gt;&lt;/p&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/04/kali_error.png&quot;&gt;&lt;img class=&quot;wp-image-14751&quot; src=&quot;/assets/2021/04/kali_error.png?w=1024&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/figure&gt;
&lt;p&gt;&lt;!-- /wp:image --&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Open Jaeger to trace the calls that are failing.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Next we can dig into the requests by opening distributed tracing (Jaeger) from the Kiali dashboard. We can see the flow of all calls grouped and identify all request to the ratings service that are throwing an error. We see a 503 is returned which means service in unavailable.&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:image {&quot;id&quot;:14753,&quot;sizeSlug&quot;:&quot;large&quot;,&quot;linkDestination&quot;:&quot;media&quot;} --&gt;&lt;/p&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/04/jaeger_error.png&quot;&gt;&lt;img class=&quot;wp-image-14753&quot; src=&quot;/assets/2021/04/jaeger_error.png?w=1024&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/figure&gt;
&lt;p&gt;&lt;!-- /wp:image --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Kiali dashboard also shows request response (latency in ms) for slowest 50%, 95% and 99% of requests. This allows us to not only see the average latency but compare it to the slowest requests which indicates if we have spikes that could cause user slowness that we might not easily see when looking at just the average.&lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:paragraph --&gt; &lt;!-- wp:image {&quot;id&quot;:14779,&quot;sizeSlug&quot;:&quot;large&quot;,&quot;linkDestination&quot;:&quot;media&quot;} --&gt;&lt;/p&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/04/kali_error2.png&quot;&gt;&lt;img class=&quot;wp-image-14779&quot; src=&quot;/assets/2021/04/kali_error2.png?w=337&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/figure&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this article we discussed the capabilities OpenShift Service Mesh provides for microservice applications. We deployed a service mesh and a book demo polyglot application that leverages the OpenShift Service Mesh. Using Kiali we saw how to gain visibility into our microservice application and easily identify problems as well as trends. Through Jaeger and distributed tracing we were able to identify the exact API causing error conditions. Microservice architectures provide a lot of value but they are harder to manage, control and troubleshoot in certain aspects than their monolithic peers which is why OpenShift Service Mesh is so critical.&lt;/p&gt;
&lt;p&gt;(c) 2021 Keith Tenzer&lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:image --&gt;&lt;/p&gt;</content><author><name>Keith Tenzer</name></author><category term="OpenShift" /><category term="Containers" /><category term="istio" /><category term="jaeger" /><category term="kiali" /><category term="microservice" /><category term="OpenShift" /><category term="polyglot" /><category term="service mesh" /><summary type="html">Overview In this article we will explore the OpenShift Service Mesh and deploy a demo application to better understand the various concepts. First you might be asking yourself, why do I need a Service Mesh? If you have a microservice architecture then you likely have many services that interact with one another in various ways. If a downstream service breaks, how will you know? How can you trace an error through all services to find out where it originated? How will you manage exposing new APIs/capabilities to users? Service Mesh provides the answer to those questions providing 1) Visibility across all microservices 2) Traceability through all of the microservice interactions and 3) Ruleset governing service versions and capabilities that are introduced into the environment. OpenShift Service Mesh uses Istio as the mesh, Kiali for the dashboard and Jaeger for traceability.  Install OpenShift Service Mesh OpenShift Service Mesh enables visibility of traffic as is traverses all services within the mesh. In case of an error you can not only see what service is actually the source for the error but also are able to trace the API communications between services. Finally it allows us to implement rules, like balancing traffic between multiple ratings API endpoints or even sending specific users to an API version. You can even create a chaos monkey using the ruleset that will inject various errors so you can observe how failures are handled. A Service Mesh is critical for any complex microservice application and without it you are literally flying blind while adding technical debt unable to manage or monitor service interactions properly. Create a new project First we will create a project for hosting the service mesh control plane. $ oc new-project bookinfo-mesh Install OpenShift Service Mesh Under the project booking-mesh go to Operator Hub and install Red Hat OpenShift Service Mesh, Red Hat OpenShift Jaeger and Kiali Operator. Configure OpenShift Service Mesh Create a service mesh control plane using defaults. Create a service mesh member. Update the yaml and change namespace under controlPlaneRef to bookinfo-mesh. apiVersion: maistra.io/v1kind: ServiceMeshMembermetadata:  namespace: bookinfo-mesh  name: defaultspec:  controlPlaneRef:    name: basic    namespace: bookinfo-mesh Finally create a service member role adding name of the project that will access the service mesh.. apiVersion: maistra.io/v1kind: ServiceMeshMemberRollmetadata:  namespace: bookinfo-mesh  name: defaultspec:  members:    - bookinfo Deploy Demo Application In order to look into the capabilities provided by the OpenShift Service Mesh we will deploy a simple book review app. The application is static but shows reviews for a book and has several microservices. The product page is the entry point service. It provides information on books and their reviews. It accesses book reviews through the reviews service which is also accesses the ratings service downstream to allow users to give a book a rating. It also gets details about a book via the details service. Like a true polyglot all services are written in a different programming language. The ratings service provides three different API versions, v1 doesn&apos;t display ratings, v2 shows ratings in black and v3 shows ratings in red. The idea is all about innovating quickly and getting real user feedback to take the app in the right direction. Create a project for the book app. Create a project to host the book app. $ oc new-project bookinfo Deploy book app. $ oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-2.0/samples/bookinfo/platform/kube/bookinfo.yaml Create a service mesh gateway. Once the app is deployed we need to create a gateway and setup the URI matches. $ oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-2.0/samples/bookinfo/networking/bookinfo-gateway.yaml Create service mesh rule set. Since the ratings service has 3 API versions we need some rules to govern the traffic. $ oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-2.0/samples/bookinfo/networking/destination-rule-all.yaml Access the application. Get the route to the application and add the /productpage to access via web browser. $ export GATEWAY_URL=$(oc -n istio-system get route istio-ingressgateway -o jsonpath=&apos;{.spec.host}&apos;)$ echo $GATEWAY_URL http://istio-ingressgateway-bookinfo-mesh.apps.ocp4.rh-southwest.com/productpage If you refresh the web page you should see the ratings change between no ratings, black and red. This is because the ruleset is sending traffic to all three API endpoints (v1, v2 and v3). Update Service Mesh Ruleset As mentioned we can create rules that enable us to send certain traffic to a different API endpoint. This could be for a canary deployment where we want to test how users interact with different capabilities (red or black ratings stars), to perform a blue/green deployment upgrading to a newer API version or even sending different users to different APIs. In this case we will simply apply a new rule that only sends traffic to v2 (black) and v3 (red) ratings API. Apply a new ruleset. $ oc replace -f https://raw.githubusercontent.com/istio/istio/master/samples/bookinfo/networking/virtual-service-reviews-v2-v3.yaml Now when you access the book app and refresh you should see it switched between red and black ratings. Looking at Kiali versioned app graph we can see that traffic is only going to v2 and v3 as we would expect.   Troubleshooting Errors As mentioned one of the key values of the OpenShift Service Mesh is the visualization and tracing. In order to generate an error we will scale down the ratings v1 deployment to 0. $ oc scale deployment/ratings-v1 -n bookinfo --replicas 0 You should now see that the ratings service is currently unavailable when you refresh the app in a browser. Check Kiali dashboard. You can access Kiali via route under networking in the bookinfo-mesh project. In Kiali we clearly see the issue is the reviews service as we would expect.  Open Jaeger to trace the calls that are failing. Next we can dig into the requests by opening distributed tracing (Jaeger) from the Kiali dashboard. We can see the flow of all calls grouped and identify all request to the ratings service that are throwing an error. We see a 503 is returned which means service in unavailable. Kiali dashboard also shows request response (latency in ms) for slowest 50%, 95% and 99% of requests. This allows us to not only see the average latency but compare it to the slowest requests which indicates if we have spikes that could cause user slowness that we might not easily see when looking at just the average. Summary In this article we discussed the capabilities OpenShift Service Mesh provides for microservice applications. We deployed a service mesh and a book demo polyglot application that leverages the OpenShift Service Mesh. Using Kiali we saw how to gain visibility into our microservice application and easily identify problems as well as trends. Through Jaeger and distributed tracing we were able to identify the exact API causing error conditions. Microservice architectures provide a lot of value but they are harder to manage, control and troubleshoot in certain aspects than their monolithic peers which is why OpenShift Service Mesh is so critical. (c) 2021 Keith Tenzer</summary></entry><entry><title type="html">OpenShift 4 AWS IPI Installation Getting Started Guide</title><link href="http://localhost:4000/openshift/openshift-4-aws-ipi-installation-getting-started-guide/" rel="alternate" type="text/html" title="OpenShift 4 AWS IPI Installation Getting Started Guide" /><published>2021-01-18T00:00:00-08:00</published><updated>2021-01-18T00:00:00-08:00</updated><id>http://localhost:4000/openshift/openshift-4-aws-ipi-installation-getting-started-guide</id><content type="html" xml:base="http://localhost:4000/openshift/openshift-4-aws-ipi-installation-getting-started-guide/">&lt;p&gt;&lt;!-- wp:gallery {&quot;ids&quot;:[5513,5506,14709],&quot;imageCrop&quot;:false,&quot;linkTo&quot;:&quot;file&quot;,&quot;sizeSlug&quot;:&quot;medium&quot;} --&gt;&lt;/p&gt;
&lt;h2&gt;&lt;img class=&quot;alignnone  wp-image-2609&quot; style=&quot;color:var(--color-text);font-size:16px;font-weight:400;&quot; src=&quot;/assets/2021/01/aws-logo-1280x720-2.png?w=300&quot; alt=&quot;ansible_2&quot; width=&quot;100&quot; height=&quot;100&quot; /&gt;&lt;img class=&quot;alignnone size-full wp-image-12472&quot; style=&quot;color:var(--color-text);font-size:16px;font-weight:400;&quot; src=&quot;/assets/2021/01/plus_sign.gif?w=300&quot; alt=&quot;plus_sign&quot; width=&quot;80&quot; height=&quot;80&quot; /&gt;&lt;img class=&quot;alignnone  wp-image-14629&quot; src=&quot;/assets/2021/01/openshiftlogo.png?w=170&quot; alt=&quot;cropped-Windows-logo1&quot; width=&quot;100&quot; height=&quot;100&quot; /&gt;&lt;/h2&gt;
&lt;p&gt;&lt;!-- /wp:gallery --&gt;&lt;/p&gt;
&lt;p&gt;Happy new year as this will be the first post of 2021! 2020 was obviously a challenging year, my hope is I will have more time to devote to blogging in 2021. Please reach out and let me know what topics would be most helpful.&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:heading {&quot;className&quot;:&quot;has-large-font-size&quot;} --&gt;&lt;/p&gt;
&lt;h2 class=&quot;has-large-font-size&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;&lt;!-- /wp:heading --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;In this Article we will walk through an OpenShift deployment using the IPI (Installer Provisioned Infrastructure) method on AWS. OpenShift offers two possible deployment methods: IPI (as mentioned) and UPI (User Provisioned Infrastructure). The difference is the degree of automation and customization. IPI will not only deploy OpenShift but also all infrastructure components and configurations. IPI is supported in various environments including AWS, Azure, GCE, VMware Vsphere and even Baremetal. IPI is tightly coupled with the infrastructure layer whereas UPI is not and will allow the most customization and work anywhere.&lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Ultimately IPI vs UPI is usually dictated by the requirements. My view is unless you have special requirements (like a stretch cluster or specific integrations with infrastructure that require UPI) always default to IPI. It is far better to have the vendor, in this case Red Hat own more of the share of responsibility and ensure proper, tested infrastructure configurations are being deployed as well as maintained throughout the cluster lifecycle.&lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:more --&gt;&lt;br /&gt;
&lt;!--more--&gt;&lt;br /&gt;
&lt;!-- /wp:more --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:heading {&quot;className&quot;:&quot;has-large-font-size&quot;} --&gt;&lt;/p&gt;
&lt;h2 class=&quot;has-large-font-size&quot;&gt;Create Hosted Zone&lt;/h2&gt;
&lt;p&gt;&lt;!-- /wp:heading --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Before a cluster can be deployed we need to access AWS console and add a hosted zone to the route53 service. In the diagram below, we are adding the domain rh-southwest.com.&lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:image {&quot;id&quot;:14686,&quot;linkDestination&quot;:&quot;custom&quot;,&quot;className&quot;:&quot;size-large&quot;} --&gt;&lt;/p&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/01/route53new-2.png&quot;&gt;&lt;img src=&quot;/assets/2021/01/route53new-2.png?w=1024&quot; alt=&quot;&quot; class=&quot;wp-image-14686&quot; /&gt;&lt;/a&gt;&lt;/figure&gt;
&lt;p&gt;&lt;!-- /wp:image --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:heading {&quot;className&quot;:&quot;has-large-font-size&quot;} --&gt;&lt;/p&gt;
&lt;h2 class=&quot;has-large-font-size&quot;&gt;Download and Install CLI Tools&lt;/h2&gt;
&lt;p&gt;&lt;!-- /wp:heading --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Red Hat provides a CLI tools for deploying and managing OpenShift clusters. The CLI tools wrap kubectl, podman (docker image manipulation) and other tools to provide a single easy-to-use interface. You can of course also use kubectl and individual tooling for manipulating images (docker or podman).&lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Download CLI Tools&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;Access&amp;nbsp;&lt;a href=&quot;https://cloud.redhat.com/openshift/install&quot;&gt;https://cloud.redhat.com/openshift/install&lt;/a&gt; using your Red Hat account. Select AWS as infrastructure provider and the installer provisioned option. Download both the CLI and OpenShift install.&lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Copy Pull Secret&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;The pull secret is used to automatically authenticate to Red Hat content repositories for OpenShift during the install. You will need to provide the pull secret as part of a later step.&lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:image {&quot;id&quot;:14696,&quot;sizeSlug&quot;:&quot;large&quot;,&quot;linkDestination&quot;:&quot;media&quot;} --&gt;&lt;/p&gt;
&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;a href=&quot;https://keithtenzer.files.wordpress.com/2021/01/aws_setup-2.png&quot;&gt;&lt;img src=&quot;/assets/2021/01/aws_setup-2.png?w=1024&quot; alt=&quot;&quot; class=&quot;wp-image-14696&quot; /&gt;&lt;/a&gt;&lt;/figure&gt;
&lt;p&gt;&lt;!-- /wp:image --&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Install CLI Tools&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The CLI tools can be simply extracted into /usr/bin.&lt;/p&gt;
&lt;pre class=&quot;wp-block-preformatted&quot;&gt;$ sudo tar xvf openshift-client-linux.tar.gz -C /usr/bin &lt;br /&gt;$ sudo tar xvf openshift-install-linux.tar.gz -C /usr/bin&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Check CLI Version&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&quot;wp-block-preformatted&quot;&gt;$ openshift-install version openshift-install 4.6.9&lt;/pre&gt;
&lt;h2&gt;Install Configuration&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Create Install Config&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When creating an install config you will need to provide the platform, in this case AWS, the domain configured in route53, a name for the OpenShift cluster and of course the pull secret.&lt;/p&gt;
&lt;pre&gt;$ openshift-install create install-config \
--dir=ocp_install
? SSH Public Key /root/.ssh/id_rsa.pub
? Platform aws
? Cloud aws
? Base Domain rh-southwest.com
? Cluster Name ocp4
? Pull Secret [? for help] ************************************************&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Edit Install Config&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&quot;wp-block-preformatted&quot;&gt;$ vi ocp_install/install-config.yaml &lt;br /&gt;... &lt;br /&gt;compute: &lt;br /&gt;  architecture: amd64 &lt;br /&gt;  hyperthreading: Enabled &lt;br /&gt;  name: worker &lt;br /&gt;  platform: &lt;br /&gt;    aws: &lt;br /&gt;      type: m5.large &lt;br /&gt;      userTags: &lt;br /&gt;        Contact: myemail@mydomain.com &lt;br /&gt;        AlwaysUp: True &lt;br /&gt;        DeleteBy: NEVER &lt;br /&gt;  replicas: 3 &lt;br /&gt;controlPlane: &lt;br /&gt;  architecture: amd64 &lt;br /&gt;  hyperthreading: Enabled &lt;br /&gt;  name: master &lt;br /&gt;  platform: &lt;br /&gt;    aws: &lt;br /&gt;      type: m5.xlarge &lt;br /&gt;      userTags: &lt;br /&gt;        Contact: myemail@mydomain.com &lt;br /&gt;        AlwaysUp: True &lt;br /&gt;        DeleteBy: NEVER &lt;br /&gt;  replicas: 3 &lt;br /&gt;...&lt;/pre&gt;
&lt;p&gt;User tags allow you to label or tag the AWS VMs. In this case we have some automation that will shutdown any clusters that are not supposed to be always available or delete any clusters that have an expiration date set.&lt;/p&gt;
&lt;h2&gt;Deploy OpenShift Cluster&lt;/h2&gt;
&lt;p&gt;Now that we have adjusted the configuration we can deploy the cluster and grab a coffee.&lt;/p&gt;
&lt;pre class=&quot;wp-block-preformatted&quot;&gt;$ [ktenzer@localhost ~]$ time openshift-install create cluster --dir=ocp_install --log-level=debug&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Verify Cluster&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Once the deployment has completed successfully we can verify cluster. In order to gain access to cluster we can use the default system account created during deployment by exporting the kubeconfig.&lt;/p&gt;
&lt;pre class=&quot;wp-block-preformatted&quot;&gt;$ export KUBECONFIG=ocp_install/auth/kubeconfig &lt;br /&gt;$ oc get nodes &lt;br /&gt;NAME                                       STATUS ROLES  AGE VERSION &lt;br /&gt;ip-10-0-147-102.us-east-2.compute.internal Ready  worker 27m v1.19.0+7070803 &lt;br /&gt;ip-10-0-159-222.us-east-2.compute.internal Ready  master 33m v1.19.0+7070803 &lt;br /&gt;ip-10-0-161-231.us-east-2.compute.internal Ready  worker 24m v1.19.0+7070803 &lt;br /&gt;ip-10-0-178-131.us-east-2.compute.internal Ready  master 33m v1.19.0+7070803 &lt;br /&gt;ip-10-0-194-232.us-east-2.compute.internal Ready  worker 24m v1.19.0+7070803 &lt;br /&gt;ip-10-0-218-84.us-east-2.compute.internal Ready   master 33m v1.19.0+7070803&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Show Cluster Version&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&quot;wp-block-preformatted&quot;&gt;$ oc get clusterversion &lt;br /&gt;NAME    VERSION AVAILABLE PROGRESSING SINCE STATUS &lt;br /&gt;version 4.6.9   True      False       7m44s Cluster version is 4.6.9&lt;/pre&gt;
&lt;h2&gt;Configure OAUTH&lt;/h2&gt;
&lt;p&gt;OpenShift supports oauth2, there is a lot of choice with the identification provider. Usually you would integrate with your enterprise LDAP provider. In this case or for test environments you can use the htpasswd provider.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Create Htpasswd file&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&quot;wp-block-preformatted&quot;&gt;$ htpasswd -c -B -b ocp_install/ocp.htpasswd admin pasword123&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Create Secret for Htpasswd&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&quot;wp-block-preformatted&quot;&gt;$ oc create secret generic htpass-secret --from-file=ocp.htpasswd -n openshift-config&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Configure Htpasswd in Oauth&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&quot;wp-block-preformatted&quot;&gt;$ vi ocp_install/htpasswd-cr.yaml &lt;/pre&gt;
&lt;pre&gt;apiVersion: config.openshift.io/v1
kind: OAuth
metadata:
  name: cluster
spec:
  identityProviders:
  - name: my_htpasswd_provider
    mappingMethod: claim
    type: HTPasswd
    htpasswd:
      fileData:
        name: htpass-secret&lt;/pre&gt;
&lt;pre class=&quot;wp-block-preformatted&quot;&gt;$ oc apply -f ocp_install/htpasswd-cr.yaml&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Add Cluster Role&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Once we have added htpasswd as a oauth provider we need to add cluster-admin role to the admin user we already created.&lt;/p&gt;
&lt;pre class=&quot;wp-block-preformatted&quot;&gt;$ oc adm policy add-cluster-role-to-user cluster-admin admin&lt;/pre&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;In this article we discussed the IPI and UPI deployment methods provided by OpenShift. Most organizations will want to choose IPI where possible. It not only simplifies the deployment of OpenShift but also improves the supportability likely leading to a higher SLA or quicker resolution of any potential issues. If not possible, UPI provides all the flexibility needed to deploy OpenShift into any environment albeit with a little more oversight and ownership required. Finally we walked through the IPI deployment of OpenShift on AWS including showing how to configure OAUTH htpasswd provider.&lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- wp:paragraph --&gt;&lt;/p&gt;
&lt;p&gt;(c) 2021 Keith Tenzer &lt;/p&gt;
&lt;p&gt;&lt;!-- /wp:paragraph --&gt;&lt;/p&gt;</content><author><name>Keith Tenzer</name></author><category term="OpenShift" /><category term="AWS" /><category term="OpenShift" /><category term="Kubernetes" /><summary type="html">Happy new year as this will be the first post of 2021! 2020 was obviously a challenging year, my hope is I will have more time to devote to blogging in 2021. Please reach out and let me know what topics would be most helpful. Overview In this Article we will walk through an OpenShift deployment using the IPI (Installer Provisioned Infrastructure) method on AWS. OpenShift offers two possible deployment methods: IPI (as mentioned) and UPI (User Provisioned Infrastructure). The difference is the degree of automation and customization. IPI will not only deploy OpenShift but also all infrastructure components and configurations. IPI is supported in various environments including AWS, Azure, GCE, VMware Vsphere and even Baremetal. IPI is tightly coupled with the infrastructure layer whereas UPI is not and will allow the most customization and work anywhere. Ultimately IPI vs UPI is usually dictated by the requirements. My view is unless you have special requirements (like a stretch cluster or specific integrations with infrastructure that require UPI) always default to IPI. It is far better to have the vendor, in this case Red Hat own more of the share of responsibility and ensure proper, tested infrastructure configurations are being deployed as well as maintained throughout the cluster lifecycle. Create Hosted Zone Before a cluster can be deployed we need to access AWS console and add a hosted zone to the route53 service. In the diagram below, we are adding the domain rh-southwest.com. Download and Install CLI Tools Red Hat provides a CLI tools for deploying and managing OpenShift clusters. The CLI tools wrap kubectl, podman (docker image manipulation) and other tools to provide a single easy-to-use interface. You can of course also use kubectl and individual tooling for manipulating images (docker or podman). Download CLI Tools Access&amp;nbsp;https://cloud.redhat.com/openshift/install using your Red Hat account. Select AWS as infrastructure provider and the installer provisioned option. Download both the CLI and OpenShift install. Copy Pull Secret The pull secret is used to automatically authenticate to Red Hat content repositories for OpenShift during the install. You will need to provide the pull secret as part of a later step. Install CLI Tools The CLI tools can be simply extracted into /usr/bin. $ sudo tar xvf openshift-client-linux.tar.gz -C /usr/bin $ sudo tar xvf openshift-install-linux.tar.gz -C /usr/bin Check CLI Version $ openshift-install version openshift-install 4.6.9 Install Configuration Create Install Config When creating an install config you will need to provide the platform, in this case AWS, the domain configured in route53, a name for the OpenShift cluster and of course the pull secret. $ openshift-install create install-config \ --dir=ocp_install ? SSH Public Key /root/.ssh/id_rsa.pub ? Platform aws ? Cloud aws ? Base Domain rh-southwest.com ? Cluster Name ocp4 ? Pull Secret [? for help] ************************************************ Edit Install Config $ vi ocp_install/install-config.yaml ... compute: architecture: amd64 hyperthreading: Enabled name: worker platform: aws: type: m5.large userTags: Contact: myemail@mydomain.com AlwaysUp: True DeleteBy: NEVER replicas: 3 controlPlane: architecture: amd64 hyperthreading: Enabled name: master platform: aws: type: m5.xlarge userTags: Contact: myemail@mydomain.com AlwaysUp: True DeleteBy: NEVER replicas: 3 ... User tags allow you to label or tag the AWS VMs. In this case we have some automation that will shutdown any clusters that are not supposed to be always available or delete any clusters that have an expiration date set. Deploy OpenShift Cluster Now that we have adjusted the configuration we can deploy the cluster and grab a coffee. $ [ktenzer@localhost ~]$ time openshift-install create cluster --dir=ocp_install --log-level=debug Verify Cluster Once the deployment has completed successfully we can verify cluster. In order to gain access to cluster we can use the default system account created during deployment by exporting the kubeconfig. $ export KUBECONFIG=ocp_install/auth/kubeconfig $ oc get nodes NAME STATUS ROLES AGE VERSION ip-10-0-147-102.us-east-2.compute.internal Ready worker 27m v1.19.0+7070803 ip-10-0-159-222.us-east-2.compute.internal Ready master 33m v1.19.0+7070803 ip-10-0-161-231.us-east-2.compute.internal Ready worker 24m v1.19.0+7070803 ip-10-0-178-131.us-east-2.compute.internal Ready master 33m v1.19.0+7070803 ip-10-0-194-232.us-east-2.compute.internal Ready worker 24m v1.19.0+7070803 ip-10-0-218-84.us-east-2.compute.internal Ready master 33m v1.19.0+7070803 Show Cluster Version $ oc get clusterversion NAME VERSION AVAILABLE PROGRESSING SINCE STATUS version 4.6.9 True False 7m44s Cluster version is 4.6.9 Configure OAUTH OpenShift supports oauth2, there is a lot of choice with the identification provider. Usually you would integrate with your enterprise LDAP provider. In this case or for test environments you can use the htpasswd provider. Create Htpasswd file $ htpasswd -c -B -b ocp_install/ocp.htpasswd admin pasword123 Create Secret for Htpasswd $ oc create secret generic htpass-secret --from-file=ocp.htpasswd -n openshift-config Configure Htpasswd in Oauth $ vi ocp_install/htpasswd-cr.yaml apiVersion: config.openshift.io/v1 kind: OAuth metadata: name: cluster spec: identityProviders: - name: my_htpasswd_provider mappingMethod: claim type: HTPasswd htpasswd: fileData: name: htpass-secret $ oc apply -f ocp_install/htpasswd-cr.yaml Add Cluster Role Once we have added htpasswd as a oauth provider we need to add cluster-admin role to the admin user we already created. $ oc adm policy add-cluster-role-to-user cluster-admin admin Summary In this article we discussed the IPI and UPI deployment methods provided by OpenShift. Most organizations will want to choose IPI where possible. It not only simplifies the deployment of OpenShift but also improves the supportability likely leading to a higher SLA or quicker resolution of any potential issues. If not possible, UPI provides all the flexibility needed to deploy OpenShift into any environment albeit with a little more oversight and ownership required. Finally we walked through the IPI deployment of OpenShift on AWS including showing how to configure OAUTH htpasswd provider. (c) 2021 Keith Tenzer</summary></entry><entry><title type="html">Windows Automation with Ansible: Getting Started Guide</title><link href="http://localhost:4000/ansible/windows-automation-with-ansible-getting-started-guide/" rel="alternate" type="text/html" title="Windows Automation with Ansible: Getting Started Guide" /><published>2020-05-19T00:00:00-07:00</published><updated>2020-05-19T00:00:00-07:00</updated><id>http://localhost:4000/ansible/windows-automation-with-ansible-getting-started-guide</id><content type="html" xml:base="http://localhost:4000/ansible/windows-automation-with-ansible-getting-started-guide/">&lt;h2&gt;&lt;img class=&quot;alignnone  wp-image-2609&quot; style=&quot;color:var(--color-text);font-size:16px;font-weight:400;&quot; src=&quot;/assets/2020/05/ansible_2.png&quot; alt=&quot;ansible_2&quot; width=&quot;185&quot; height=&quot;185&quot; /&gt;&lt;img class=&quot;alignnone size-full wp-image-12472&quot; style=&quot;color:var(--color-text);font-size:16px;font-weight:400;&quot; src=&quot;/assets/2020/05/plus_sign.gif&quot; alt=&quot;plus_sign&quot; width=&quot;161&quot; height=&quot;161&quot; /&gt;&lt;img class=&quot;alignnone  wp-image-14629&quot; src=&quot;/assets/2020/05/cropped-windows-logo1.png&quot; alt=&quot;cropped-Windows-logo1&quot; width=&quot;179&quot; height=&quot;179&quot; /&gt;&lt;/h2&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;In this article we will focus on how to get started with automation of windows using Ansible. Specifically we will look at installing 3rd party software and OS updates. Automation is the basis for cloud-computing or cloud-native patterns and breeds a culture of innovation. Let&apos;s face it, we cannot innovate, if we are stuck doing mundane tasks and manual labor. Ansible has revolutionized automation because until Ansible, automating was rather complicated and required lots of domain knowledge. Ansible provides an automation language that the entire organization can use because it is so easy and so flexible. The best thing about Ansible though it it brings teams together and fosters a devops culture. It brings the Linux and Windows world together!&lt;/p&gt;
&lt;p&gt;&lt;!--more--&gt;&lt;/p&gt;
&lt;h2&gt;How Ansible Works&lt;/h2&gt;
&lt;p&gt;Ansible provides a runtime for executing playbooks. A playbook is simply a group of plays which are essentially steps executed in order. A play is one or more tasks. A task is simply the execution of an Ansible module. An Ansible module is python code that does something, like install a software package, update the system, change a configuration file, check if something is set correctly, etc. There are 1000s of Ansible modules and a huge community around it.&lt;/p&gt;
&lt;p&gt;In order to run a playbook against hosts you need an inventory which is simply a grouping of hosts and host vars (parameters). That is about it for the basics. Below is a diagram showing the Ansible automation engine architecture.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;alignnone  wp-image-14622&quot; src=&quot;/assets/2020/05/ansible_arch.png&quot; alt=&quot;ansible_arch&quot; width=&quot;1059&quot; height=&quot;740&quot; /&gt;&lt;/p&gt;
&lt;p&gt;This guide assumes you know some Ansible or are familar with the basics. If not I created an Ansible getting started guide &lt;a href=&quot;https://keithtenzer.com/2017/11/09/ansible-getting-started-guide/&quot;&gt;here&lt;/a&gt;. I highly recommend giving it a look before proceeding.&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;In order for a windows host to be managed by Ansible there are a few prerequisites&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Powershell version 3.0 or higher (Should be fine with Windows 2012 or higher)&lt;/li&gt;
&lt;li&gt;.NET Framework 4.0 or higher (should be fine with Windows 2012 or higher)&lt;/li&gt;
&lt;li&gt;Windows Remote Management Listener or SSH (cygwin)&lt;/li&gt;
&lt;li&gt;Windows 7, 8.1, and 10, and server OSs including Windows Server 2008, 2008 R2, 2012, 2012 R2, 2016, and 2019&lt;/li&gt;
&lt;li&gt;Chocolatey for installing 3rd party software&lt;/li&gt;
&lt;li&gt;WSUS for updating OS packages and patching&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Install Chocalatey and WSUS&lt;/h2&gt;
&lt;p&gt;There are various tools that can be used. I recommend using Chocolatey for installing packages and WSUS for OS updates/patching.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Install Chocalately&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using powershell we can install chocalately.&lt;/p&gt;
&lt;pre&gt;PS C:\windows\system32&amp;gt; Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString(&apos;https://chocolatey.org/install.ps1&apos;))&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Install WSUS&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Open server manager. In the top right under manage you can add or change roles.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;alignnone  wp-image-14632&quot; src=&quot;/assets/2020/05/server_manager.png&quot; alt=&quot;server_manager&quot; width=&quot;814&quot; height=&quot;429&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Under server roles select Windows Service Update Services. This will install WSUS. Once installed there will be a WSUS option in server manager. You can select it and configure what packages should be updated, etc.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;alignnone  wp-image-14633&quot; src=&quot;/assets/2020/05/wsus.png&quot; alt=&quot;wsus&quot; width=&quot;844&quot; height=&quot;409&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Open WSUS and check that the computer is showing up under &apos;All Computers&apos;. If not and you are running outside of domain you may need the following fix.&lt;/p&gt;
&lt;pre&gt;c:\&amp;gt; net stop wuauserv
c:\&amp;gt; regsvr32 /s wuapi.dll
c:\&amp;gt; regsvr32 /s wups.dll
c:\&amp;gt; regsvr32 /s wuaueng.dll
c:\&amp;gt; regsvr32 /s wucltui.dll
c:\&amp;gt; regsvr32 /s msxml3.dll
c:\&amp;gt; cd %windir%\SoftwareDistribution
c:\&amp;gt; rd /s/q DataStore
c:\&amp;gt; mkdir DataStore
c:\&amp;gt; rd /s/q Download
c:\&amp;gt; mkdir Download
c:\&amp;gt; net start wuauserv
c:\&amp;gt; reg delete HKLM\SOFTWARE\Microsoft\Windows\CurrentVersion\WindowsUpdate /v AccountDomainSid /f
c:\&amp;gt; reg delete HKLM\SOFTWARE\Microsoft\Windows\CurrentVersion\WindowsUpdate /v PingID /f
c:\&amp;gt; reg delete HKLM\SOFTWARE\Microsoft\Windows\CurrentVersion\WindowsUpdate /v SusClientId /f
c:\&amp;gt; wuauclt /resetauthorization
c:\&amp;gt; wuauclt /detectnow
c:\&amp;gt; wuauclt /reportnow&lt;/pre&gt;
&lt;h2&gt;Configuring Windows Remote Management for Ansible&lt;/h2&gt;
&lt;p&gt;Ansible requires no agents. It uses what the OS provides for communication. In Windows you can use SSH or Windows Remote Management (WinRM). Since SSH is more or less bolted on, WinRM is usually the preferred choice.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Test to ensure WinRM is working&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First install WinRM if it isn&apos;t installed. Execute the hostname command through WinRM.&lt;/p&gt;
&lt;pre&gt;PS C:\windows\system32&amp;gt; winrs -r:http://:5985/wsman -u: -p: hostname
win2012&lt;/pre&gt;
&lt;p&gt;Optionally you can also test WinRM by making a remote desktop connection from another windows host.&lt;/p&gt;
&lt;p&gt;Note: you may run into an issue where you get an authentication error and a CredSSL issue if you have an older windows version. This can also happen if you don&apos;t have a valid SSL certificate. If you run into this issue, update your registry.&lt;/p&gt;
&lt;pre&gt;c:\&amp;gt;reg add &quot;HKLM\Software\Microsoft\Windows\CurrentVersion\Policies\System\CredSSP\Parameters&quot; /f /v AllowEncryptionOracle /t REG_DWORD /d 2&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Enable basic auth&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There are several authentication methods. In this example we will use basic authentication which is username/password. This is of course the least secure method.&lt;/p&gt;
&lt;pre&gt;&lt;span class=&quot;l l-Scalar l-Scalar-Plain&quot;&gt;PS C:\&amp;gt; Set-Item -Path WSMan:\localhost\Service\Auth\Basic -Value $true&lt;/span&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Update WinRM &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A script is provided by Ansible community to check WinRM and make necessary changes to allow Ansible to connect. In this case we are using basic authentication but likely you will want to use something more secure. The script can be found &lt;a href=&quot;https://raw.githubusercontent.com/ansible/ansible/devel/examples/scripts/ConfigureRemotingForAnsible.ps1&quot;&gt;here&lt;/a&gt; and other authentication options are documented in the script header.&lt;/p&gt;
&lt;p&gt;Open a powershell command prompt.&lt;/p&gt;
&lt;p&gt;Store URL path to script.&lt;/p&gt;
&lt;pre&gt;PS C:\&amp;gt; $url = &quot;https://raw.githubusercontent.com/ansible/ansible/devel/examples/scripts/ConfigureRemotingForAnsible.ps1&quot;&lt;/pre&gt;
&lt;p&gt;Store location for the script&lt;/p&gt;
&lt;pre&gt;PS C:\&amp;gt; $file = &quot;$env:temp\ConfigureRemotingForAnsible.ps1&quot;&lt;/pre&gt;
&lt;p&gt;Download script and output to file locally.&lt;/p&gt;
&lt;pre&gt;PS C:\&amp;gt; (New-Object -TypeName System.Net.WebClient).DownloadFile($url, $file)&lt;/pre&gt;
&lt;p&gt;Execute the script.&lt;/p&gt;
&lt;pre&gt;PS C:\&amp;gt; powershell.exe -ExecutionPolicy ByPass -File $file
Ok.&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Check WinRM connection&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;PS C:\windows\system32&amp;gt; winrm enumerate winrm/config/Listener
Listener
Address = *
Transport = HTTP
Port = 5985
Hostname
Enabled = true
URLPrefix = wsman
CertificateThumbprint
ListeningOn = 10.10.1.139, 127.0.0.1, ::1, fe80::5efe:10.10.1.139%22, fe80::8155:327a:aeaf:365a%12

Listener
Address = *
Transport = HTTPS
Port = 5986
Hostname
Enabled = true
URLPrefix = wsman
CertificateThumbprint = 7a38de2c212764a54de106dc756f7cbc275156a3
ListeningOn = 10.10.1.139, 127.0.0.1, ::1, fe80::5efe:10.10.1.139%22, fe80::8155:327a:aeaf:365a%12&lt;/pre&gt;
&lt;p&gt;Ensure the HTTP/HTTPS ports are open.&lt;/p&gt;
&lt;p&gt;More details about WinRM setup and how to setup a listener manually are documented &lt;a href=&quot;https://docs.ansible.com/ansible/latest/user_guide/windows_setup.html#winrm-setup&quot;&gt;here&lt;/a&gt;. In this case I used the default listener configured by WinRM.&lt;/p&gt;
&lt;h2&gt;Create Inventory File&lt;/h2&gt;
&lt;p&gt;We will create an inventory file with just a single host in a group called windows. From here on out we will be working on a Linux server where we have Ansible installed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Create Ansible inventory file&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;$ vi inventory
[windows]
138.204.12.111&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Test ansible connection&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using the inventory file we can test if Ansible can communicate with our windows server.&lt;/p&gt;
&lt;pre&gt;$ ansible -i ../inventory windows -m win_ping -e ansible_connection=winrm \
-e ansible_user=Admin -e ansible_password=&amp;lt;password&amp;gt; \
-e ansible_winrm_transport=basic \
-e ansible_winrm_server_cert_validation=ignore

138.201.147.202 | SUCCESS =&amp;gt; {
&quot;changed&quot;: false,
&quot;ping&quot;: &quot;pong&quot;
}&lt;/pre&gt;
&lt;h2&gt;Windows Patch Management&lt;/h2&gt;
&lt;p&gt;Now that Ansible is working with WinRM we can automate. In this case we will automate software package installation and updates.&lt;/p&gt;
&lt;p&gt;Playbook and roles are available &lt;a href=&quot;https://github.com/ktenzer/ansible-windows&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Create Playbook&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We haven&apos;t talked about roles. In Ansible roles are how we make playbooks reusable. It is always good practice to create roles. A role essentially allows you to organize Ansible plays and their dependencies together allowing them to be consumed easily. In order to use roles you need to create a certain directory structure and hierarchy. Create a playbook that imports our roles.&lt;/p&gt;
&lt;pre&gt;$ vi windows_baseline.yaml
---
- name: Windows Baseline
  hosts: &quot;&quot;
  connection: winrm
  gather_facts: true
  vars:
    ansible_user: &quot;&quot;
    ansible_password: &quot;&quot;
    ansible_connection: winrm 
    ansible_winrm_transport: basic 
    ansible_winrm_server_cert_validation: ignore

  tasks:
    - name: Install Baseline Packages
      include_role:
        name: install

    - name: Perform Updates
      include_role:
        name: updates
&lt;/pre&gt;
&lt;p&gt;Here we are setting hosts, ansible user and password as variables. These inputs must be provided when executing the playbook. Hosts should be the name of our host group from our inventory file. This playbook has two tasks, a role to install packages and a role to do an OS update.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Create Ansible install role&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;At a minimum your role will need a tasks directory&lt;/p&gt;
&lt;pre&gt;$ mkdir -p roles/install/tasks&lt;/pre&gt;
&lt;p&gt;Create a task to install git using the chocalatey module.&lt;/p&gt;
&lt;pre&gt;$ vi roles/install/tasks/main.yaml
---
- name: Install Git
  win_chocolatey:
    name: git
    state: present
&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Create Ansible patch update role&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;At a minimum your role will need a tasks directory&lt;/p&gt;
&lt;pre&gt;$ mkdir -p roles/updates/tasks&lt;/pre&gt;
&lt;p&gt;Create a task to perform an OS update.&lt;/p&gt;
&lt;pre&gt;vi roles/updates/tasks/main.yaml
---
- name: Update windows packages
  win_updates:
    category_names:
      - CriticalUpdates
      - SecurityUpdates
    reboot: yes
    reboot_timeout: 500
&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Run playbook using inventory&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The &apos;-vvvvv&apos; allows the playbook to run in debug mode for maximum verbosity.&lt;/p&gt;
&lt;pre&gt;$ ansible-playbook -i ./inventory -e target=windows -e user=Admin -e password= windows_baseline.yaml
PLAY [Windows Baseline] ****************************************************************************

TASK [Gathering Facts] *****************************************************************************
ok: [138.201.147.202]

TASK [Install Baseline Packages] *******************************************************************

TASK [install : Install Git] ***********************************************************************
ok: [138.201.147.202]

TASK [Perform Updates] *****************************************************************************

TASK [updates : Update windows packages] ***********************************************************
changed: [138.201.147.202]

PLAY RECAP *****************************************************************************************
138.201.147.202            : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=
&lt;/pre&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ansible/ansible-examples/tree/master/windows&quot;&gt;Here&lt;/a&gt; are some additional examples of windows playbooks that may be of interest on your journey.&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this article we discussed the value of automation and why it is just a game changer. We provided a step-by-step on preparing a windows host for Ansible. Finally using the Ansible automation language, showed how to use native windows tooling to install and update OS patches. Hopefully this will provide a good starting point for a journey into windows automation with Ansible.&lt;/p&gt;
&lt;p&gt;(c) 2020 Keith Tenzer&lt;/p&gt;</content><author><name>Keith Tenzer</name></author><category term="Ansible" /><category term="Automation" /><category term="Windows" /><summary type="html">Overview In this article we will focus on how to get started with automation of windows using Ansible. Specifically we will look at installing 3rd party software and OS updates. Automation is the basis for cloud-computing or cloud-native patterns and breeds a culture of innovation. Let&apos;s face it, we cannot innovate, if we are stuck doing mundane tasks and manual labor. Ansible has revolutionized automation because until Ansible, automating was rather complicated and required lots of domain knowledge. Ansible provides an automation language that the entire organization can use because it is so easy and so flexible. The best thing about Ansible though it it brings teams together and fosters a devops culture. It brings the Linux and Windows world together! How Ansible Works Ansible provides a runtime for executing playbooks. A playbook is simply a group of plays which are essentially steps executed in order. A play is one or more tasks. A task is simply the execution of an Ansible module. An Ansible module is python code that does something, like install a software package, update the system, change a configuration file, check if something is set correctly, etc. There are 1000s of Ansible modules and a huge community around it. In order to run a playbook against hosts you need an inventory which is simply a grouping of hosts and host vars (parameters). That is about it for the basics. Below is a diagram showing the Ansible automation engine architecture. This guide assumes you know some Ansible or are familar with the basics. If not I created an Ansible getting started guide here. I highly recommend giving it a look before proceeding. Prerequisites In order for a windows host to be managed by Ansible there are a few prerequisites Powershell version 3.0 or higher (Should be fine with Windows 2012 or higher) .NET Framework 4.0 or higher (should be fine with Windows 2012 or higher) Windows Remote Management Listener or SSH (cygwin) Windows 7, 8.1, and 10, and server OSs including Windows Server 2008, 2008 R2, 2012, 2012 R2, 2016, and 2019 Chocolatey for installing 3rd party software WSUS for updating OS packages and patching Install Chocalatey and WSUS There are various tools that can be used. I recommend using Chocolatey for installing packages and WSUS for OS updates/patching. Install Chocalately Using powershell we can install chocalately. PS C:\windows\system32&amp;gt; Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString(&apos;https://chocolatey.org/install.ps1&apos;)) Install WSUS Open server manager. In the top right under manage you can add or change roles. Under server roles select Windows Service Update Services. This will install WSUS. Once installed there will be a WSUS option in server manager. You can select it and configure what packages should be updated, etc. Open WSUS and check that the computer is showing up under &apos;All Computers&apos;. If not and you are running outside of domain you may need the following fix. c:\&amp;gt; net stop wuauserv c:\&amp;gt; regsvr32 /s wuapi.dll c:\&amp;gt; regsvr32 /s wups.dll c:\&amp;gt; regsvr32 /s wuaueng.dll c:\&amp;gt; regsvr32 /s wucltui.dll c:\&amp;gt; regsvr32 /s msxml3.dll c:\&amp;gt; cd %windir%\SoftwareDistribution c:\&amp;gt; rd /s/q DataStore c:\&amp;gt; mkdir DataStore c:\&amp;gt; rd /s/q Download c:\&amp;gt; mkdir Download c:\&amp;gt; net start wuauserv c:\&amp;gt; reg delete HKLM\SOFTWARE\Microsoft\Windows\CurrentVersion\WindowsUpdate /v AccountDomainSid /f c:\&amp;gt; reg delete HKLM\SOFTWARE\Microsoft\Windows\CurrentVersion\WindowsUpdate /v PingID /f c:\&amp;gt; reg delete HKLM\SOFTWARE\Microsoft\Windows\CurrentVersion\WindowsUpdate /v SusClientId /f c:\&amp;gt; wuauclt /resetauthorization c:\&amp;gt; wuauclt /detectnow c:\&amp;gt; wuauclt /reportnow Configuring Windows Remote Management for Ansible Ansible requires no agents. It uses what the OS provides for communication. In Windows you can use SSH or Windows Remote Management (WinRM). Since SSH is more or less bolted on, WinRM is usually the preferred choice. Test to ensure WinRM is working First install WinRM if it isn&apos;t installed. Execute the hostname command through WinRM. PS C:\windows\system32&amp;gt; winrs -r:http://:5985/wsman -u: -p: hostname win2012 Optionally you can also test WinRM by making a remote desktop connection from another windows host. Note: you may run into an issue where you get an authentication error and a CredSSL issue if you have an older windows version. This can also happen if you don&apos;t have a valid SSL certificate. If you run into this issue, update your registry. c:\&amp;gt;reg add &quot;HKLM\Software\Microsoft\Windows\CurrentVersion\Policies\System\CredSSP\Parameters&quot; /f /v AllowEncryptionOracle /t REG_DWORD /d 2 Enable basic auth There are several authentication methods. In this example we will use basic authentication which is username/password. This is of course the least secure method. PS C:\&amp;gt; Set-Item -Path WSMan:\localhost\Service\Auth\Basic -Value $true Update WinRM  A script is provided by Ansible community to check WinRM and make necessary changes to allow Ansible to connect. In this case we are using basic authentication but likely you will want to use something more secure. The script can be found here and other authentication options are documented in the script header. Open a powershell command prompt. Store URL path to script. PS C:\&amp;gt; $url = &quot;https://raw.githubusercontent.com/ansible/ansible/devel/examples/scripts/ConfigureRemotingForAnsible.ps1&quot; Store location for the script PS C:\&amp;gt; $file = &quot;$env:temp\ConfigureRemotingForAnsible.ps1&quot; Download script and output to file locally. PS C:\&amp;gt; (New-Object -TypeName System.Net.WebClient).DownloadFile($url, $file) Execute the script. PS C:\&amp;gt; powershell.exe -ExecutionPolicy ByPass -File $file Ok. Check WinRM connection PS C:\windows\system32&amp;gt; winrm enumerate winrm/config/Listener Listener Address = * Transport = HTTP Port = 5985 Hostname Enabled = true URLPrefix = wsman CertificateThumbprint ListeningOn = 10.10.1.139, 127.0.0.1, ::1, fe80::5efe:10.10.1.139%22, fe80::8155:327a:aeaf:365a%12</summary></entry><entry><title type="html">Red Hat Subscription Reporting Guide</title><link href="http://localhost:4000/general/red-hat-subscription-reporting-guide/" rel="alternate" type="text/html" title="Red Hat Subscription Reporting Guide" /><published>2020-05-13T00:00:00-07:00</published><updated>2020-05-13T00:00:00-07:00</updated><id>http://localhost:4000/general/red-hat-subscription-reporting-guide</id><content type="html" xml:base="http://localhost:4000/general/red-hat-subscription-reporting-guide/">&lt;h2&gt;&lt;img class=&quot;alignnone  wp-image-14584&quot; src=&quot;/assets/2020/05/ecommerce-subscription-ts-100621375-large.jpg&quot; alt=&quot;ecommerce-subscription-ts-100621375-large&quot; width=&quot;362&quot; height=&quot;240&quot; /&gt;&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;source: https://www.cio.com/article/3199910/zuora-blazes-a-new-trail-to-the-subscription-economy-and-a-post-erp-world.html&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;This article will look at the various options available, to do subscription reporting for Red Hat products. Many large organizations sometimes struggle to keep track of what subscriptions are being used, often maintaining their own spreadsheets. This can be very error prone and time consuming. Systems can even be subscribed to the wrong subscriptions, for example a virtual machine running RHEL using a physical subscription. Many Red Hat customers have several different products, not just RHEL and being able to actively inventory the entire subscription landscape is critical.&lt;/p&gt;
&lt;p&gt;&lt;!--more--&gt;&lt;/p&gt;
&lt;h2&gt;Options for Subscription Reporting&lt;/h2&gt;
&lt;p&gt;There are various options for subscription reporting depending on if you have Satellite (Red Hat Systems Management) or not.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Subscription Entitlement Report &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Starting with Satellite 6.7 there is now a canned report that is very customizable. This is the recommended way of reporting subscriptions in Satellite 6.7+. Provides granular host level subscription reporting.&lt;/p&gt;
&lt;p&gt;Example of the out-of-the-box entitlement report is &lt;a href=&quot;https://gist.github.com/ktenzer/7632ad9f75ba4424b1b59a9e0ba229ea&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Red Hat Discovery Tool&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The discovery tool can do reporting on subscriptions in Satellite and non-Satellite environments. Provides granular host level subscription reporting. This is the recommendation if you don&apos;t have Satellite 6.7+ or have systems not subscribing through Satellite. The discovery tool can be installed in a connected or disconnected environment.&lt;/p&gt;
&lt;p&gt;The discovery tool also has an upstream project and is available via the community: &lt;a href=&quot;https://github.com/quipucords/quipucords&quot;&gt;https://github.com/quipucords/quipucords&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Red Hat Subscription Watch&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is a cloud service offering that can aggregate subscription reporting across various infrastructure or even organizations. Data is sent either from Red Hat Insights or Satellite to a customers portal on cloud.redhat.com. This is a recommended addition to provide additional visibility but does not provide granular information at the host level.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Subscription Inventory Script&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Provides granular host level subscription reporting for Satellite environments. This is a script not a product so use at your own risk. However this is recommended if you have Satellite 6.6 or below and cannot for whatever reason leverage the Red Hat Discovery tool.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/RedHatSatellite/sat6Inventory/tree/product-cert&quot;&gt;https://github.com/RedHatSatellite/sat6Inventory/tree/product-cert&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Using the Satellite Subscription Inventory Script&lt;/h2&gt;
&lt;p&gt;Since the other options are products and well documented, I will focus on showing how to do subscription reporting using the inventory script on RHEL 7. First you will need to deploy a RHEL 7 system.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Enable Correct RHEL Repos&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First ensure all repositories are disabled and then just enable the rhel-7-server-rpms.&lt;/p&gt;
&lt;pre&gt;$ sudo subscription-manager repos --disable=*
$ sudo subscription-manager repos --enable=rhel-7-server-rpms&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Install python 2.6+&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;$ sudo yum install -y python&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Install Git (Only for connected setup)&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;$ sudo yum install -y git&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Clone git repository (Only for connected setup)&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;$ git clone https://github.com/RedHatSatellite/sat6Inventory.git&lt;/pre&gt;
&lt;p&gt;If the system isn&apos;t connected to the internet then you need to download from the git repository. You can download the script from &lt;a href=&quot;https://github.com/RedHatSatellite/sat6Inventory/tree/product-cert&quot;&gt;https://github.com/RedHatSatellite/sat6Inventory/tree/product-cert&lt;/a&gt; by selecting download. You can then copy the tarball to your system and extract it.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;alignnone  wp-image-14605&quot; src=&quot;/assets/2020/05/download.png&quot; alt=&quot;download&quot; width=&quot;1828&quot; height=&quot;484&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Change directory&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;cd sat6Inventory&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Run the inventory script&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You need to pass the admin user, password and the Satellite organization.&lt;/p&gt;
&lt;pre&gt;./sat6Inventory.py -s sat-bc87.rhpds.opentlc.com -l admin -p  \
-o &apos;EXAMPLE.COM&apos;&lt;/pre&gt;
&lt;p&gt;The script will output a CSV.&lt;/p&gt;
&lt;pre&gt;$ ls
&lt;strong&gt;EXAMPLE.COM_inventory_report.csv&lt;/strong&gt; LICENSE README.md Sample Report sat6Inventory.py&lt;/pre&gt;
&lt;p&gt;You can now import the script output CSV into a spreadsheet program and manipulate as desired.&lt;/p&gt;
&lt;p&gt;A sample report is also provided &lt;a href=&quot;https://github.com/RedHatSatellite/sat6Inventory/blob/product-cert/Sample%20Report/Example_inventory_report.csv&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Reconciling mismatched entitlements&lt;/h2&gt;
&lt;p&gt;If you have a Satellite environment with RHEL physical and VDC (virtual subscriptions) it is possible that a virtual RHEL host is possibly using a physical instead of virtual subscription. To identify such hosts you can run the below command from your Satellite server.&lt;/p&gt;
&lt;pre&gt;$ sudo foreman-rake katello:virt_who_report&lt;/pre&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this article we covered the various options for doing subscription reporting of Red Hat products. We also showed a short hands-on guide for using the inventory script. Hopefully this provided guidance to choose the most appropriate approach to subscription reporting.&lt;/p&gt;
&lt;p&gt;(c) 2020 Keith Tenzer&lt;/p&gt;</content><author><name>Keith Tenzer</name></author><category term="General" /><category term="Red Hat" /><category term="Reporting" /><category term="RHEL" /><category term="Satellite" /><category term="Subscription" /><summary type="html">source: https://www.cio.com/article/3199910/zuora-blazes-a-new-trail-to-the-subscription-economy-and-a-post-erp-world.html Overview This article will look at the various options available, to do subscription reporting for Red Hat products. Many large organizations sometimes struggle to keep track of what subscriptions are being used, often maintaining their own spreadsheets. This can be very error prone and time consuming. Systems can even be subscribed to the wrong subscriptions, for example a virtual machine running RHEL using a physical subscription. Many Red Hat customers have several different products, not just RHEL and being able to actively inventory the entire subscription landscape is critical. Options for Subscription Reporting There are various options for subscription reporting depending on if you have Satellite (Red Hat Systems Management) or not. Subscription Entitlement Report  Starting with Satellite 6.7 there is now a canned report that is very customizable. This is the recommended way of reporting subscriptions in Satellite 6.7+. Provides granular host level subscription reporting. Example of the out-of-the-box entitlement report is here. Red Hat Discovery Tool The discovery tool can do reporting on subscriptions in Satellite and non-Satellite environments. Provides granular host level subscription reporting. This is the recommendation if you don&apos;t have Satellite 6.7+ or have systems not subscribing through Satellite. The discovery tool can be installed in a connected or disconnected environment. The discovery tool also has an upstream project and is available via the community: https://github.com/quipucords/quipucords Red Hat Subscription Watch This is a cloud service offering that can aggregate subscription reporting across various infrastructure or even organizations. Data is sent either from Red Hat Insights or Satellite to a customers portal on cloud.redhat.com. This is a recommended addition to provide additional visibility but does not provide granular information at the host level. Subscription Inventory Script Provides granular host level subscription reporting for Satellite environments. This is a script not a product so use at your own risk. However this is recommended if you have Satellite 6.6 or below and cannot for whatever reason leverage the Red Hat Discovery tool. https://github.com/RedHatSatellite/sat6Inventory/tree/product-cert Using the Satellite Subscription Inventory Script Since the other options are products and well documented, I will focus on showing how to do subscription reporting using the inventory script on RHEL 7. First you will need to deploy a RHEL 7 system. Enable Correct RHEL Repos First ensure all repositories are disabled and then just enable the rhel-7-server-rpms. $ sudo subscription-manager repos --disable=* $ sudo subscription-manager repos --enable=rhel-7-server-rpms Install python 2.6+ $ sudo yum install -y python Install Git (Only for connected setup) $ sudo yum install -y git Clone git repository (Only for connected setup) $ git clone https://github.com/RedHatSatellite/sat6Inventory.git If the system isn&apos;t connected to the internet then you need to download from the git repository. You can download the script from https://github.com/RedHatSatellite/sat6Inventory/tree/product-cert by selecting download. You can then copy the tarball to your system and extract it. Change directory cd sat6Inventory Run the inventory script You need to pass the admin user, password and the Satellite organization. ./sat6Inventory.py -s sat-bc87.rhpds.opentlc.com -l admin -p \ -o &apos;EXAMPLE.COM&apos; The script will output a CSV. $ ls EXAMPLE.COM_inventory_report.csv LICENSE README.md Sample Report sat6Inventory.py You can now import the script output CSV into a spreadsheet program and manipulate as desired. A sample report is also provided here. Reconciling mismatched entitlements If you have a Satellite environment with RHEL physical and VDC (virtual subscriptions) it is possible that a virtual RHEL host is possibly using a physical instead of virtual subscription. To identify such hosts you can run the below command from your Satellite server. $ sudo foreman-rake katello:virt_who_report Summary In this article we covered the various options for doing subscription reporting of Red Hat products. We also showed a short hands-on guide for using the inventory script. Hopefully this provided guidance to choose the most appropriate approach to subscription reporting. (c) 2020 Keith Tenzer</summary></entry><entry><title type="html">OpenShift Operator Getting Started Part I</title><link href="http://localhost:4000/openshift/openshift-operator-getting-started-part-i/" rel="alternate" type="text/html" title="OpenShift Operator Getting Started Part I" /><published>2020-04-23T00:00:00-07:00</published><updated>2020-04-23T00:00:00-07:00</updated><id>http://localhost:4000/openshift/openshift-operator-getting-started-part-i</id><content type="html" xml:base="http://localhost:4000/openshift/openshift-operator-getting-started-part-i/">&lt;h2&gt;&lt;img class=&quot;alignnone  wp-image-14443&quot; src=&quot;/assets/2020/04/38202270.png?w=173&amp;amp;h=173&quot; alt=&quot;38202270&quot; width=&quot;173&quot; height=&quot;173&quot; /&gt;&lt;/h2&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;In this article we will introduce the concept of Operators, the Operator Framework and Operator Lifecycle Management. This article is part of a series that will walk you through understanding and building operators in Go or Ansible end-to-end.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://keithtenzer.com/2020/04/23/openshift-operator-getting-started-part-i/&quot;&gt;OpenShift Operator Getting Started Part I&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://keithtenzer.com/2020/01/27/openshift-operator-sdk-go-getting-started-guide-part-ii/&quot;&gt;OpenShift Operator SDK: Go Getting Started Guide Part II&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://keithtenzer.com/2020/04/23/openshift-operator-sdk-ansible-getting-started-guide-part-iii/&quot;&gt;OpenShift Operator SDK: Ansible Getting Started Guide Part III&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://keithtenzer.com/2020/04/23/openshift-operator-lifecycle-management-guide-integrating-operators-in-olm-part-iv/&quot;&gt;OpenShift Operator Lifecycle Management Guide: Integrating Operators in OLM Part IV&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First, what exactly is an Operator?&lt;/p&gt;
&lt;p&gt;&lt;em&gt;An Operator is a method of packaging, deploying and managing a Kubernetes application.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;source: &lt;a href=&quot;https://coreos.com/operators/&quot;&gt;https://coreos.com/operators/&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Or simply, it is the application that can deploy itself, manage itself and update itself. Welcome to the brave new world, where we don’t spend time doing repetitive manually tasks, but rather put our knowledge into software so it can do it for us, better.&lt;/p&gt;
&lt;p&gt;&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;The Kubernetes Operator pattern was introduced by CoreOS engineers in 2016 as again, a way to implement and inject application specific domain knowledge into the running of containerized applications. The idea was to couple Service Reliability Engineering (SRE) output with the application and provide a standard programmable path to do so. SRE teams as you likely know, operate applications by creating software. Most organizations unfortunately don’t have the luxury of having highly paid software engineers to run their applications, nor the time to build such software. Now, with Operators and the Operator Framework, there is a way for vendors or customers to provide that domain knowledge using a standardized reusable approach. The result is applications that run themselves or come with their own pre-programmed built-in SRE team. This is obviously a huge game-changer and differentiator for operating applications. In my opinion it is the only way to deal with the increased complexity we see today. This is the reason eventually, that every application will likely be deployed in containers on Kubernetes. It is simply no longer, with today’s complexity, possible for application domain knowledge to exist in a few people’s heads, that in turn need to operate application lifecycle manually. Think of an operator as the new Linux “Package Manager” and Kubernetes as the new “Linux”.&lt;/p&gt;
&lt;h2&gt;Introducing the Operator Framework&lt;/h2&gt;
&lt;p&gt;The Operator Framework is a toolkit to make it easy to build, scale and run operators. It includes the SDK, Operator Lifecycle Manager and Metering.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Operator SDK&lt;/strong&gt; – Provides tooling to build and package operators. It abstracts Kubernetes API.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;alignnone  wp-image-14437&quot; src=&quot;/assets/2020/04/screenshot-from-2020-01-25-16-02-08.png?w=1248&amp;amp;h=369&quot; alt=&quot;Screenshot from 2020-01-25 16-02-08&quot; width=&quot;1248&quot; height=&quot;369&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Operator Lifecycle Manager&lt;/strong&gt; – Management control plane for operators. It governs who can access/deploy a given operator, namespaces where operators can run and lifecycle management, such as updates to an operator.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;alignnone  wp-image-14438&quot; src=&quot;/assets/2020/04/screenshot-from-2020-01-25-16-02-28.png?w=1236&amp;amp;h=340&quot; alt=&quot;Screenshot from 2020-01-25 16-02-28&quot; width=&quot;1236&quot; height=&quot;340&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Metering&lt;/strong&gt; – Provides ability to record historical usage which in turn can be used for operator reporting.&lt;/p&gt;
&lt;h2&gt;Understanding How It All Works&lt;/h2&gt;
&lt;p&gt;Using the Operator SDK we create operators. An operator can be written in Go or Ansible. An operator provides one or more custom resource definitions (CRDs) to allow users to interact with the application using the standard kubernetes API. A CRD is simply an extention to the kubernetes API. It also provides a custom resource (CR) for how a user interacts with the CRD. In addition the operator provides methods for how to install, delete and update itself. The operator has the ability to watch CRDs. If a user creates, deletes or updates a CR, the operator will see that and call the necessary functions. Those functions can be implemented in Go or Ansible. Yep all you need to build an operator end-to-end is Ansible knowledge!&lt;/p&gt;
&lt;p&gt;Once you have an operator that can be deployed, it is time to look into Operator Lifecycle Management. OpenShift comes with OLM already built-in but it can also be added to any standard kubernetes cluster. OLM has the concept of a catalog. This is essentially an operator that provides one or more application bundles. OpenShift has several built-in catalogs.&lt;/p&gt;
&lt;pre&gt;$ oc get catalogsource -n openshift-marketplace
NAME                      DISPLAY                   TYPE   PUBLISHER          AGE
certified-operators       Certified Operators       grpc   Red Hat            18d
community-operators       Community Operators       grpc   Red Hat            18d 
redhat-operators          Red Hat Operators         grpc   Red Hat            18d&lt;/pre&gt;
&lt;p&gt;Red Hat, ISVs, Partners, anyone can bundle their application operators into a catalog. Operator Hub simply lists the applications exposed by a catalog via OLM. Each application bundle is composed of the operator, it&apos;s own k8s objects and a manifest. The manifest is exposed via the cluster server version (CSV) CRD and consumed in OLM. This allows for flexibility to version operators and their manifests separately.&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this article we discussed the Operator Framework for Kubernetes and OpenShift. A powerful framework for building, packaging and deploying kubernetes-native applications. The Operator Framework consists of the Operator SDK, Operator Lifecycle Management and Metering. Through Operators and the Operator Lifecycle Management the full power of kubernetes can be utilized, allowing application operators to have a built-in SRE approach which in turn increases application availability, programability and efficiency.&lt;/p&gt;
&lt;p&gt;Happy Operatoring!&lt;/p&gt;
&lt;p&gt;(c) 2020 Keith Tenzer&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;</content><author><name>Keith Tenzer</name></author><category term="OpenShift" /><category term="cloud-native" /><category term="Containers" /><category term="Kubernetes" /><category term="olm" /><category term="OpenShift" /><category term="Operator" /><category term="Operator Framework" /><category term="Operator Lifecycle Management" /><category term="operator-sdk" /><summary type="html">Overview In this article we will introduce the concept of Operators, the Operator Framework and Operator Lifecycle Management. This article is part of a series that will walk you through understanding and building operators in Go or Ansible end-to-end. OpenShift Operator Getting Started Part I OpenShift Operator SDK: Go Getting Started Guide Part II OpenShift Operator SDK: Ansible Getting Started Guide Part III OpenShift Operator Lifecycle Management Guide: Integrating Operators in OLM Part IV First, what exactly is an Operator? An Operator is a method of packaging, deploying and managing a Kubernetes application. source: https://coreos.com/operators/ Or simply, it is the application that can deploy itself, manage itself and update itself. Welcome to the brave new world, where we don’t spend time doing repetitive manually tasks, but rather put our knowledge into software so it can do it for us, better. The Kubernetes Operator pattern was introduced by CoreOS engineers in 2016 as again, a way to implement and inject application specific domain knowledge into the running of containerized applications. The idea was to couple Service Reliability Engineering (SRE) output with the application and provide a standard programmable path to do so. SRE teams as you likely know, operate applications by creating software. Most organizations unfortunately don’t have the luxury of having highly paid software engineers to run their applications, nor the time to build such software. Now, with Operators and the Operator Framework, there is a way for vendors or customers to provide that domain knowledge using a standardized reusable approach. The result is applications that run themselves or come with their own pre-programmed built-in SRE team. This is obviously a huge game-changer and differentiator for operating applications. In my opinion it is the only way to deal with the increased complexity we see today. This is the reason eventually, that every application will likely be deployed in containers on Kubernetes. It is simply no longer, with today’s complexity, possible for application domain knowledge to exist in a few people’s heads, that in turn need to operate application lifecycle manually. Think of an operator as the new Linux “Package Manager” and Kubernetes as the new “Linux”. Introducing the Operator Framework The Operator Framework is a toolkit to make it easy to build, scale and run operators. It includes the SDK, Operator Lifecycle Manager and Metering. Operator SDK – Provides tooling to build and package operators. It abstracts Kubernetes API. Operator Lifecycle Manager – Management control plane for operators. It governs who can access/deploy a given operator, namespaces where operators can run and lifecycle management, such as updates to an operator. Metering – Provides ability to record historical usage which in turn can be used for operator reporting. Understanding How It All Works Using the Operator SDK we create operators. An operator can be written in Go or Ansible. An operator provides one or more custom resource definitions (CRDs) to allow users to interact with the application using the standard kubernetes API. A CRD is simply an extention to the kubernetes API. It also provides a custom resource (CR) for how a user interacts with the CRD. In addition the operator provides methods for how to install, delete and update itself. The operator has the ability to watch CRDs. If a user creates, deletes or updates a CR, the operator will see that and call the necessary functions. Those functions can be implemented in Go or Ansible. Yep all you need to build an operator end-to-end is Ansible knowledge! Once you have an operator that can be deployed, it is time to look into Operator Lifecycle Management. OpenShift comes with OLM already built-in but it can also be added to any standard kubernetes cluster. OLM has the concept of a catalog. This is essentially an operator that provides one or more application bundles. OpenShift has several built-in catalogs. $ oc get catalogsource -n openshift-marketplace NAME DISPLAY TYPE PUBLISHER AGE certified-operators Certified Operators grpc Red Hat 18d community-operators Community Operators grpc Red Hat 18d redhat-operators Red Hat Operators grpc Red Hat 18d Red Hat, ISVs, Partners, anyone can bundle their application operators into a catalog. Operator Hub simply lists the applications exposed by a catalog via OLM. Each application bundle is composed of the operator, it&apos;s own k8s objects and a manifest. The manifest is exposed via the cluster server version (CSV) CRD and consumed in OLM. This allows for flexibility to version operators and their manifests separately. Summary In this article we discussed the Operator Framework for Kubernetes and OpenShift. A powerful framework for building, packaging and deploying kubernetes-native applications. The Operator Framework consists of the Operator SDK, Operator Lifecycle Management and Metering. Through Operators and the Operator Lifecycle Management the full power of kubernetes can be utilized, allowing application operators to have a built-in SRE approach which in turn increases application availability, programability and efficiency. Happy Operatoring! (c) 2020 Keith Tenzer &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;</summary></entry><entry><title type="html">OpenShift Operator Lifecycle Management Guide: Integrating Operators in OLM Part IV</title><link href="http://localhost:4000/openshift/openshift-operator-lifecycle-management-guide-integrating-operators-in-olm-part-iv/" rel="alternate" type="text/html" title="OpenShift Operator Lifecycle Management Guide: Integrating Operators in OLM Part IV" /><published>2020-04-23T00:00:00-07:00</published><updated>2020-04-23T00:00:00-07:00</updated><id>http://localhost:4000/openshift/openshift-operator-lifecycle-management-guide-integrating-operators-in-olm-part-iv</id><content type="html" xml:base="http://localhost:4000/openshift/openshift-operator-lifecycle-management-guide-integrating-operators-in-olm-part-iv/">&lt;h2&gt;&lt;img class=&quot;alignnone  wp-image-14443&quot; src=&quot;/assets/2020/04/38202270.png?w=173&amp;amp;h=173&quot; alt=&quot;38202270&quot; width=&quot;173&quot; height=&quot;173&quot; /&gt;&lt;/h2&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;In this article we will provide a hands-on guide to integrating your already built Operator with the Operator Lifecycle Manager (OLM). Using the Operator SDK and OPM tool we will create the application manifests and images so your application Operator can be managed through OLM.&lt;/p&gt;
&lt;p&gt;This article is part of a series that will walk you through understanding and building operators in Go or Ansible end-to-end.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://keithtenzer.com/2020/04/23/openshift-operator-getting-started-part-i/&quot;&gt;OpenShift Operator Getting Started Part I&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://keithtenzer.com/2020/01/27/openshift-operator-sdk-go-getting-started-guide-part-ii/&quot;&gt;OpenShift Operator SDK: Go Getting Started Guide Part II&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://keithtenzer.com/2020/04/23/openshift-operator-sdk-ansible-getting-started-guide-part-iii/&quot;&gt;OpenShift Operator SDK: Ansible Getting Started Guide Part III&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://keithtenzer.com/2020/04/23/openshift-operator-lifecycle-management-guide-integrating-operators-in-olm-part-iv/&quot;&gt;OpenShift Operator Lifecycle Management Guide: Integrating Operators in OLM Part IV&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;In order to add your Operator to OLM you will need two tools: the &lt;a href=&quot;https://github.com/operator-framework/operator-sdk/releases&quot;&gt;operator-sdk&lt;/a&gt; and &lt;a href=&quot;https://github.com/operator-framework/operator-registry/releases&quot;&gt;opm&lt;/a&gt;. Download the binaries for operator-sdk and opm. Install them under /usr/local/bin.&lt;/p&gt;
&lt;h2&gt;Create Cluster Service Version&lt;/h2&gt;
&lt;p&gt;The Cluster Service Version (CSV) is the manifest that allows your application operator to be bundled and exposed to the OLM API. The CSV is itself a CRD in kubernetes. Before creating a the CSV manifest you need the deployment scafolding. These are the objects and CRDs that your Operator require. These are created automatically when using the operator-sdk to create a application under the deploy directory.&lt;/p&gt;
&lt;pre&gt;$ operator-sdk new cars-operator --repo github.com/cars-operator&lt;/pre&gt;
&lt;p&gt;When generating an CSV you will provide the path to the deploy directory where those objects can be found.&lt;/p&gt;
&lt;pre&gt;$ operator-sdk generate csv --csv-version 1.0.0 --deploy-dir deploy&lt;/pre&gt;
&lt;p&gt;The generate csv command will create an olm_catalog/cars-operator/manifests directory. There you will find the &apos;clusterserviceversion&apos; yaml. You will however want to edit and add a lot. As such I have provided an &lt;a href=&quot;https://github.com/sa-mw-dach/podium/blob/master/podium-operator/deploy/olm-catalog/podium-operator/manifests/podium-operator.clusterserviceversion.yaml&quot;&gt;example&lt;/a&gt; csv for an Operator I built.&lt;/p&gt;
&lt;p&gt;Note: If you want to add an icon, so it shows up nicely in Operator Hub, the image needs to be a base64 bit stream inside the csv.&lt;/p&gt;
&lt;pre&gt;$ cat myimage.png | base64&lt;/pre&gt;
&lt;h2&gt;Create Application Operator Catalog Bundle&lt;/h2&gt;
&lt;p&gt;Now that we have generated and updated our CSV we can create an application bundle.&lt;/p&gt;
&lt;pre&gt;$ sudo operator-sdk bundle create quay.io/ktenzer/podium-operator-catalog:latest \
--channels beta --package podium-operator-catalog --directory \
deploy/olm-catalog/podium-operator/manifests&lt;/pre&gt;
&lt;p&gt;This will package the application manifest into a container image. Push the image to a public or private repository.&lt;/p&gt;
&lt;pre&gt;$ sudo docker push quay.io/ktenzer/podium-operator-catalog:latest&lt;/pre&gt;
&lt;h2&gt;Create Application Operator Catalog Index&lt;/h2&gt;
&lt;p&gt;Once we have bundled an application operator we can add it to a catalog index. The catalog index provides one or more application bundles to OLM. This is also what the Operator Hub directly interfaces with.&lt;/p&gt;
&lt;pre&gt;$ sudo opm index add -c docker --bundles \
quay.io/ktenzer/podium-operator-catalog:latest \
--tag quay.io/ktenzer/podium-operator-index:latest&lt;/pre&gt;
&lt;p&gt;Again an image is created that needs to be pushed to our private or public repository.&lt;/p&gt;
&lt;pre&gt;$ sudo docker push quay.io/ktenzer/podium-operator-index:latest&lt;/pre&gt;
&lt;h2&gt;Deploy Catalog Source&lt;/h2&gt;
&lt;p&gt;A catalog source is a CRD that defines an OLM catalog and points to a catalog index image, for example the one we just created.&lt;/p&gt;
&lt;p&gt;Create a catalog source yaml file that points to the catalog index image.&lt;/p&gt;
&lt;pre&gt;$ vi catalog_source.yaml
apiVersion: operators.coreos.com/v1alpha1
kind: CatalogSource
metadata:
  name: podium-operator-catalog
spec:
  sourceType: grpc
  &lt;strong&gt;image: quay.io/ktenzer/podium-operator-index:latest&lt;/strong&gt;
  displayName: Podium Operator Catalog
  publisher: Podium Community&lt;/pre&gt;
&lt;p&gt;Deploy the catalog source under the openshift-marketplace namespace or where you have deployed OLM.&lt;/p&gt;
&lt;pre&gt;$ oc create -f catalog_source.yaml -n openshift-marketplace&lt;/pre&gt;
&lt;p&gt;Below we can see a list of catalogs. Again each catalog can contain one or more applications. All are default in OpenShift, except the one in bold which I added.&lt;/p&gt;
&lt;pre&gt;$ oc get catalogsource -n openshift-marketplace
NAME                     DISPLAY                 TYPE PUBLISHER        AGE
certified-operators      Certified Operators     grpc Red Hat          18d
community-operators      Community Operators     grpc Red Hat          18d
&lt;strong&gt;podium-operator-catalog  Podium Operator Catalog grpc Podium Community 31m&lt;/strong&gt;
redhat-operators         Red Hat Operators       grpc Red Hat          18d&lt;/pre&gt;
&lt;p&gt;Each catalog has it&apos;s own operator / pod.&lt;/p&gt;
&lt;pre&gt;$ oc get pods -n openshift-marketplace
NAME                                    READY   STATUS    RESTARTS   AGE
certified-operators-76d9d8b886-bnsz7    1/1     Running   0          14h
community-operators-74d675f545-g7d2t    1/1     Running   0          18h
&lt;strong&gt;podium-operator-catalog-67gsk           1/1     Running   0          31m&lt;/strong&gt;
marketplace-operator-75f49679d7-n7v2r   1/1     Running   0          17d
redhat-operators-87d549bf4-mxf7w        1/1     Running   0          13h&lt;/pre&gt;
&lt;p&gt;Once a catalog operator exists, the applications it offers will show up in Operator Hub and can be installed.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;alignnone size-full wp-image-14528&quot; src=&quot;/assets/2020/04/podium_install.png&quot; alt=&quot;podium_install&quot; width=&quot;1094&quot; height=&quot;828&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Operators can be cluster-wide or namespace scoped. The Operator Lifecycle Management also provides a subscription, allowing updates to be pulled from various channels. This is similar to the relationship between an RPM and Repository.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;alignnone size-full wp-image-14529&quot; src=&quot;/assets/2020/04/podium_install2.png&quot; alt=&quot;podium_install2&quot; width=&quot;792&quot; height=&quot;743&quot; /&gt;&lt;/p&gt;
&lt;p&gt;After you deploy application via Operator Hub it will launch the operator. Cluster-wide operators will run in the openshift-operators namespace while namespaced operators will only run in a user defined namespace.&lt;/p&gt;
&lt;pre&gt;$ oc get pods -n openshift-operators
NAME                               READY   STATUS    RESTARTS   AGE
podium-operator-6855dc9478-q65bt   1/1     Running   0          38m&lt;/pre&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this article we stepped through the process of integrating an already existing operator with the Operator Lifecycle Manager. We saw how to generate an application manifest, build an application catalog bundle, add the bundle to an application index and finally deploy the application catalog, seamlessly integrating with Operator Hub. Using OLM provides not only a way to install applications but manage their lifecycle, releases and updates.&lt;/p&gt;
&lt;p&gt;Happy Operatoring!&lt;/p&gt;
&lt;p&gt;(c) 2020 Keith Tenzer&lt;/p&gt;</content><author><name>Keith Tenzer</name></author><category term="OpenShift" /><category term="Containers" /><category term="Kubernetes" /><category term="olm" /><category term="OpenShift" /><category term="Operator" /><category term="Operator Lifecycle Management" /><category term="operator-sdk" /><summary type="html">Overview In this article we will provide a hands-on guide to integrating your already built Operator with the Operator Lifecycle Manager (OLM). Using the Operator SDK and OPM tool we will create the application manifests and images so your application Operator can be managed through OLM. This article is part of a series that will walk you through understanding and building operators in Go or Ansible end-to-end. OpenShift Operator Getting Started Part I OpenShift Operator SDK: Go Getting Started Guide Part II OpenShift Operator SDK: Ansible Getting Started Guide Part III OpenShift Operator Lifecycle Management Guide: Integrating Operators in OLM Part IV In order to add your Operator to OLM you will need two tools: the operator-sdk and opm. Download the binaries for operator-sdk and opm. Install them under /usr/local/bin. Create Cluster Service Version The Cluster Service Version (CSV) is the manifest that allows your application operator to be bundled and exposed to the OLM API. The CSV is itself a CRD in kubernetes. Before creating a the CSV manifest you need the deployment scafolding. These are the objects and CRDs that your Operator require. These are created automatically when using the operator-sdk to create a application under the deploy directory. $ operator-sdk new cars-operator --repo github.com/cars-operator When generating an CSV you will provide the path to the deploy directory where those objects can be found. $ operator-sdk generate csv --csv-version 1.0.0 --deploy-dir deploy The generate csv command will create an olm_catalog/cars-operator/manifests directory. There you will find the &apos;clusterserviceversion&apos; yaml. You will however want to edit and add a lot. As such I have provided an example csv for an Operator I built. Note: If you want to add an icon, so it shows up nicely in Operator Hub, the image needs to be a base64 bit stream inside the csv. $ cat myimage.png | base64 Create Application Operator Catalog Bundle Now that we have generated and updated our CSV we can create an application bundle. $ sudo operator-sdk bundle create quay.io/ktenzer/podium-operator-catalog:latest \ --channels beta --package podium-operator-catalog --directory \ deploy/olm-catalog/podium-operator/manifests This will package the application manifest into a container image. Push the image to a public or private repository. $ sudo docker push quay.io/ktenzer/podium-operator-catalog:latest Create Application Operator Catalog Index Once we have bundled an application operator we can add it to a catalog index. The catalog index provides one or more application bundles to OLM. This is also what the Operator Hub directly interfaces with. $ sudo opm index add -c docker --bundles \ quay.io/ktenzer/podium-operator-catalog:latest \ --tag quay.io/ktenzer/podium-operator-index:latest Again an image is created that needs to be pushed to our private or public repository. $ sudo docker push quay.io/ktenzer/podium-operator-index:latest Deploy Catalog Source A catalog source is a CRD that defines an OLM catalog and points to a catalog index image, for example the one we just created. Create a catalog source yaml file that points to the catalog index image. $ vi catalog_source.yaml apiVersion: operators.coreos.com/v1alpha1 kind: CatalogSource metadata: name: podium-operator-catalog spec: sourceType: grpc image: quay.io/ktenzer/podium-operator-index:latest displayName: Podium Operator Catalog publisher: Podium Community Deploy the catalog source under the openshift-marketplace namespace or where you have deployed OLM. $ oc create -f catalog_source.yaml -n openshift-marketplace Below we can see a list of catalogs. Again each catalog can contain one or more applications. All are default in OpenShift, except the one in bold which I added. $ oc get catalogsource -n openshift-marketplace NAME DISPLAY TYPE PUBLISHER AGE certified-operators Certified Operators grpc Red Hat 18d community-operators Community Operators grpc Red Hat 18d podium-operator-catalog Podium Operator Catalog grpc Podium Community 31m redhat-operators Red Hat Operators grpc Red Hat 18d Each catalog has it&apos;s own operator / pod. $ oc get pods -n openshift-marketplace NAME READY STATUS RESTARTS AGE certified-operators-76d9d8b886-bnsz7 1/1 Running 0 14h community-operators-74d675f545-g7d2t 1/1 Running 0 18h podium-operator-catalog-67gsk 1/1 Running 0 31m marketplace-operator-75f49679d7-n7v2r 1/1 Running 0 17d redhat-operators-87d549bf4-mxf7w 1/1 Running 0 13h Once a catalog operator exists, the applications it offers will show up in Operator Hub and can be installed. Operators can be cluster-wide or namespace scoped. The Operator Lifecycle Management also provides a subscription, allowing updates to be pulled from various channels. This is similar to the relationship between an RPM and Repository. After you deploy application via Operator Hub it will launch the operator. Cluster-wide operators will run in the openshift-operators namespace while namespaced operators will only run in a user defined namespace. $ oc get pods -n openshift-operators NAME READY STATUS RESTARTS AGE podium-operator-6855dc9478-q65bt 1/1 Running 0 38m Summary In this article we stepped through the process of integrating an already existing operator with the Operator Lifecycle Manager. We saw how to generate an application manifest, build an application catalog bundle, add the bundle to an application index and finally deploy the application catalog, seamlessly integrating with Operator Hub. Using OLM provides not only a way to install applications but manage their lifecycle, releases and updates. Happy Operatoring! (c) 2020 Keith Tenzer</summary></entry></feed>