<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.22.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
<link rel="icon" href="/assets/main/me.png">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Red Hat OpenStack Platform 10 (Newton) Installation and Configuration Guide - Keith Tenzer’s Blog</title>
<meta name="description" content="Overview In this article we will setup an OpenStack environment based off Newton using the Red Hat OpenStack Platform. OpenStack is OpenStack but every distribution differs in what capabilities or technologies are supported and how OpenStack is installed, configured as well as upgraded. The Red Hat OpenStack Platform uses OpenStack director based on the TripleO (OpenStack on OpenStack) project to install, configure and update OpenStack. Director is a lifecycle management tool for OpenStack. Red Hat&#39;s approach is to make OpenStack easy to manage, without compromising on the &quot;Open&quot; part of OpenStack. If management of OpenStack can be simpler and the learning curve brought down then it has a real chance to be the next-gen virtualization platform. What company wouldn&#39;t want to be able to consume their internal IT resources like using AWS, GCE or Azure if they didn&#39;t give up anything to do so? We aren&#39;t there yet but Red Hat is making bold strides and as you will see in this article, is on a journey to make OpenStack consumable for everyone!  Red Hat OpenStack Platform The Red Hat OpenStack platform uses director to build, manage and upgrade Red Hat OpenStack. Director is in fact a minimal OpenStack deployment itself, with everything needed to deploy OpenStack. The main piece outside of the OpenStack core (Nova, Neutron, Glance, Swift and Heat) is Ironic. The Ironic project is focused on baremetal-as-a-service. Director allows you to add physical nodes to Ironic and assign them OpenStack roles: compute, control, storage, network, etc. Once roles are assigned an OpenStack environment can be deployed, upgraded and even scaled. As mentioned director is a complete life-cycle management tool that uses OpenStack to manage OpenStack. In this article we will deploy director (undercloud) on a single VM. We will add three baremetal nodes (VMs) and then deploy OpenStack (overcloud) in a minimal configuration (1 controller node and 1 compute node). I am able to run this on a laptop with just 12GB RAM. Lab Environment My idea for this configuration was build the most minimal OpenStack environment possible, something that would run on my laptop with just 12GB RAM using Red Hat OpenStack Director. In the end this experiment was successful and the configuration used is as follows:  KVM Hypervisor Physical Laptop: RHEL 7.3, CentOS or Fedora, Dual core, 12 GB RAM and 250GB disk Undercloud VM: RHEL 7.3, 2x vCPUs, 4GB RAM, 1 x NIC 8(provisioning), 1 x NIC (external) and 40GB disk Overcloud Controller VM: RHEL 7.3, 2 x vCPUs, 6GB RAM, 1 x NIC (provisioning), 2 x NICs (external) and 30GB disk Overcloud Compute VM: RHEL 7.3, 2 x vCPU, 4GB RAM, 1 x NIC (provisioning), 2 x NICs (external) and 20GB disk   Networking Setup In this configuration we are using virtual networks provided by the hypervisor host (my laptop). Create provisioning and external networks on KVM Hypervisor host. Ensure that NAT forwarding is enabled and DHCP is disabled on the external network. We run OpenStack overcloud on the external network. The provisioning network should be non-routable and DHCP disabled. The undercloud will handle DHCP services for the provisioning network and other IPs will be statically assigned. [Hypervisor] Create external network for the overcloud. [ktenzer@ktenzer ~]$ cat &gt; /tmp/external.xml &lt;&lt;EOF &lt;network&gt;    &lt;name&gt;external&lt;/name&gt;    &lt;forward mode=&#39;nat&#39;&gt;       &lt;nat&gt; &lt;port start=&#39;1024&#39; end=&#39;65535&#39;/&gt;       &lt;/nat&gt;    &lt;/forward&gt;    &lt;ip address=&#39;192.168.122.1&#39; netmask=&#39;255.255.255.0&#39;&gt;    &lt;/ip&gt; &lt;/network&gt;  Note: hypervisor is 192.168.122.1 and reachable via this IP from undercloud. [ktenzer@ktenzer ~]$ sudo virsh net-define /tmp/external.xml [ktenzer@ktenzer ~]$ sudo virsh net-autostart external [ktenzer@ktenzer ~]$ sudo virsh net-start external Create provisioning network for undercloud. Note: gateway is 192.168.126.254 as we will use 192.168.126.1 as IP for the VM running our undercloud. [ktenzer@ktenzer ~]$ cat &gt; /tmp/provisioning.xml &lt;&lt;EOF &lt;network&gt;    &lt;name&gt;provisioning&lt;/name&gt;    &lt;ip address=&#39;192.168.126.254&#39; netmask=&#39;255.255.255.0&#39;&gt;    &lt;/ip&gt; &lt;/network&gt; [ktenzer@ktenzer ~]$ sudo virsh net-define /tmp/provisioning.xml [ktenzer@ktenzer ~]$ sudo virsh net-autostart provisioning [ktenzer@ktenzer ~]$ sudo virsh net-start provisioning Deploy Undercloud First install Red Hat Enterprise Linux (RHEL) 7.3 on undercloud VM. Register with subscription manager and configure required RPM repositories for Red Hat OpenStack Platform. [Undercloud] [root@director ~]# subscription-manager register [root@director ~]#subscription-manager list --available \ subscription-manager attach --pool= [root@director ~]# subscription-manager repos --disable=* [root@director ~]# subscription-manager repos --enable=rhel-7-server-rpms \ --enable=rhel-7-server-extras-rpms \  --enable=rhel-7-server-rh-common-rpms \  --enable=rhel-ha-for-rhel-7-server-rpms \  --enable=rhel-7-server-openstack-10-rpms Update all pacakges and reboot. [root@director ~]# yum update -y [root@director ~]# systemctl reboot Install Director Packages. [root@director ~]# yum install -y python-tripleoclient Ensure host is defined in /etc/hosts. [root@director ~]# vi /etc/hosts  192.168.122.90 ospd.lab.com ospd Create Stack User. [root@director ~]# useradd stack [root@director ~]# passwd stack  # specify a password Configure user with sudo permissions. [root@director ~]# echo &quot;stack ALL=(root) NOPASSWD:ALL&quot; | tee -a /etc/sudoers.d/stack [root@director ~]# chmod 0440 /etc/sudoers.d/stack Switch to new stack user. [root@director ~]# su - stack [stack@director ~]$ Create directories for images and templates. Images are used to boot initial systems and provide baseline OS. Templates are used to customize deployment. [stack@director ~]$ mkdir ~/images [stack@director ~]$ mkdir ~/templates Configure Director using the sample. [stack@director ~]$ cp /usr/share/instack-undercloud/undercloud.conf.sample \ ~/undercloud.conf In my environment the 192.168.126.0/24 network is the undercloud network and used for provisioning as well as deploying the overcloud. [stack@undercloud ~]$ vi ~/undercloud.conf  [DEFAULT]  local_ip = 192.168.126.1/24  undercloud_public_vip = 192.168.126.2  undercloud_admin_vip = 192.168.126.3  local_interface = eth1  masquerade_network = 192.168.126.0/24  dhcp_start = 192.168.126.100  dhcp_end = 192.168.126.150  network_cidr = 192.168.126.0/24  network_gateway = 192.168.126.1  inspection_iprange = 192.168.126.130,192.168.126.99  generate_service_certificate = true  certificate_generation_ca = local Install the undercloud. [stack@odpd ~]$ openstack undercloud install #############################################################################  Undercloud install complete. The file containing this installation&#39;s passwords is at  /home/stack/undercloud-passwords.conf. There is also a stackrc file at /home/stack/stackrc. These files are needed to interact with the OpenStack services, and should be  secured. ############################################################################# Import overcloud images. [stack@odpd ~]$ source stackrc [stack@odpd ~]$ sudo yum install -y \ rhosp-director-images rhosp-director-images-ipa [stack@odpd ~]$ cd ~/images $ for i in \ /usr/share/rhosp-director-images/overcloud-full-latest-10.0.tar \ /usr/share/rhosp-director-images/ironic-python-agent-latest-10.0.tar; \ do tar -xvf $i; done [stack@odpd ~]$ openstack overcloud image upload --image-path \ /home/stack/images/ Configure DNS on undercloud network. [stack@odpd ~]$ neutron subnet-list +--------------------------------------+------+------------------+--------------------------------------------------------+ | id | name | cidr | allocation_pools | +--------------------------------------+------+------------------+--------------------------------------------------------+ | 294ff536-dc8b-49a3-8327-62d9792d30a6 | | 192.168.126.0/24 | {&quot;start&quot;: &quot;192.168.126.100&quot;, &quot;end&quot;: &quot;192.168.126.200&quot;} | +--------------------------------------+------+------------------+--------------------------------------------------------+ [stack@odpd ~]$ neutron subnet-update 294ff536-dc8b-49a3-8327-62d9792d30a6 \ --dns-nameserver 8.8.8.8 [Hypervisor] Registering Overcloud Nodes. Create VM hulls in KVM using virsh on hypervisor host. Note: You will need to change the disk path to suit your needs. ktenzer$ cd /home/ktenzer/VirtualMachines ktenzer$ sudo for i in {1..3}; do qemu-img create -f qcow2 -o preallocation=metadata overcloud-node$i.qcow2 60G; done ktenzer$ sudo for i in {1..3}; do virt-install --ram 4096 --vcpus 4 --os-variant rhel7 --disk path=/home/ktenzer/VirtualMachines/overcloud-node$i.qcow2,device=disk,bus=virtio,format=qcow2 --noautoconsole --vnc --network network:provisioning --network network:external --network network:external --name overcloud-node$i --cpu SandyBridge,+vmx --dry-run --print-xml &gt; /tmp/overcloud-node$i.xml; virsh define --file /tmp/overcloud-node$i.xml; done [Undercloud] Copy ssh key from undercloud system to KVM hypervisor host for stack user. [stack@odpd ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub stack@192.168.122.1 Save the MAC addresses of the NICs used for provisioning. Note: Ironic needs to know what MAC addresses a node has associated for provisioning network. [stack@odpd images]$ for i in {1..3}; do virsh \ -c qemu+ssh://stack@192.168.122.1/system domiflist overcloud-node$i \ | awk &#39;$3 == &quot;provisioning&quot; {print $5};&#39;; done &gt; /tmp/nodes.txt [stack@odpd images]$ cat /tmp/nodes.txt  52:54:00:7e:d8:01 52:54:00:f6:a6:73 52:54:00:c9:b2:84 [stack@undercloud ~]$ jq . &lt;&lt; EOF &gt; ~/instackenv.json {   &quot;ssh-user&quot;: &quot;stack&quot;,   &quot;ssh-key&quot;: &quot;$(cat ~/.ssh/id_rsa)&quot;,   &quot;power_manager&quot;: &quot;nova.virt.baremetal.virtual_power_driver.VirtualPowerManager&quot;,   &quot;host-ip&quot;: &quot;192.168.122.1&quot;,   &quot;arch&quot;: &quot;x86_64&quot;,   &quot;nodes&quot;: [     {       &quot;pm_addr&quot;: &quot;192.168.122.1&quot;,       &quot;pm_password&quot;: &quot;$(cat ~/.ssh/id_rsa)&quot;,       &quot;pm_type&quot;: &quot;pxe_ssh&quot;,       &quot;mac&quot;: [         &quot;$(sed -n 1p /tmp/nodes.txt)&quot;       ],       &quot;cpu&quot;: &quot;2&quot;,       &quot;memory&quot;: &quot;4096&quot;,       &quot;disk&quot;: &quot;60&quot;,       &quot;arch&quot;: &quot;x86_64&quot;,       &quot;pm_user&quot;: &quot;stack&quot;     },     {       &quot;pm_addr&quot;: &quot;192.168.122.1&quot;,       &quot;pm_password&quot;: &quot;$(cat ~/.ssh/id_rsa)&quot;,       &quot;pm_type&quot;: &quot;pxe_ssh&quot;,       &quot;mac&quot;: [         &quot;$(sed -n 2p /tmp/nodes.txt)&quot;       ],       &quot;cpu&quot;: &quot;4&quot;,       &quot;memory&quot;: &quot;2048&quot;,       &quot;disk&quot;: &quot;60&quot;,       &quot;arch&quot;: &quot;x86_64&quot;,       &quot;pm_user&quot;: &quot;stack&quot;     },     {       &quot;pm_addr&quot;: &quot;192.168.122.1&quot;,       &quot;pm_password&quot;: &quot;$(cat ~/.ssh/id_rsa)&quot;,       &quot;pm_type&quot;: &quot;pxe_ssh&quot;,       &quot;mac&quot;: [         &quot;$(sed -n 3p /tmp/nodes.txt)&quot;       ],       &quot;cpu&quot;: &quot;4&quot;,       &quot;memory&quot;: &quot;2048&quot;,       &quot;disk&quot;: &quot;60&quot;,       &quot;arch&quot;: &quot;x86_64&quot;,       &quot;pm_user&quot;: &quot;stack&quot;     }   ]  }  EOF Validate introspection configuration. [stack@odpd ~]$ curl -O  https://raw.githubusercontent.com/rthallisey/clapper/master/instackenv-validator.py Import nodes into Ironic and set them to bootable. [stack@odpd ~]$ openstack baremetal import --json ~/instackenv.json [stack@odpd ~]$ openstack baremetal configure boot [stack@odpd ~]$ openstack baremetal node list  +--------------------------------------+------+---------------+-------------+--------------------+-------------+  | UUID | Name | Instance UUID | Power State | Provisioning State | Maintenance |  +--------------------------------------+------+---------------+-------------+--------------------+-------------+  | ea61b158-9cbd-46d2-93e9-eadaccb1589b | None | None | power off | available | False |  | 11aaf849-361e-4bda-81f5-74c245f554af | None | None | power off | available | False |  | 275448c1-aa8d-4854-bb3b-bc73e1e1a794 | None | None | power off | available | False |  +--------------------------------------+------+---------------+-------------+--------------------+-------------+ Set nodes to managed. [stack@odpd ~]$ for node in $(openstack baremetal node list -c UUID \ -f value) ; do openstack baremetal node manage $node ; done List nodes. [stack@odpd ~]$ openstack baremetal node list  +--------------------------------------+------+---------------+-------------+--------------------+-------------+  | UUID | Name | Instance UUID | Power State | Provisioning State | Maintenance |  +--------------------------------------+------+---------------+-------------+--------------------+-------------+  | ea61b158-9cbd-46d2-93e9-eadaccb1589b | None | None | power off | manageable | False |  | 11aaf849-361e-4bda-81f5-74c245f554af | None | None | power off | manageable | False |  | 275448c1-aa8d-4854-bb3b-bc73e1e1a794 | None | None | power off | manageable | False |  +--------------------------------------+------+---------------+-------------+--------------------+-------------+ Run introspection against all managed nodes. Note: Nodes are booted using ramdisk and their hardware inspected. Introspection prepares nodes for deployment into overcloud. [stack@odpd ~]$ openstack overcloud node introspect --all-manageable \ --provide">


  <meta name="author" content="Keith Tenzer">
  
  <meta property="article:author" content="Keith Tenzer">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Keith Tenzer's Blog">
<meta property="og:title" content="Red Hat OpenStack Platform 10 (Newton) Installation and Configuration Guide">
<meta property="og:url" content="http://localhost:4000/openstack/red-hat-openstack-platform-10-newton-installation-and-configuration-guide/">


  <meta property="og:description" content="Overview In this article we will setup an OpenStack environment based off Newton using the Red Hat OpenStack Platform. OpenStack is OpenStack but every distribution differs in what capabilities or technologies are supported and how OpenStack is installed, configured as well as upgraded. The Red Hat OpenStack Platform uses OpenStack director based on the TripleO (OpenStack on OpenStack) project to install, configure and update OpenStack. Director is a lifecycle management tool for OpenStack. Red Hat&#39;s approach is to make OpenStack easy to manage, without compromising on the &quot;Open&quot; part of OpenStack. If management of OpenStack can be simpler and the learning curve brought down then it has a real chance to be the next-gen virtualization platform. What company wouldn&#39;t want to be able to consume their internal IT resources like using AWS, GCE or Azure if they didn&#39;t give up anything to do so? We aren&#39;t there yet but Red Hat is making bold strides and as you will see in this article, is on a journey to make OpenStack consumable for everyone!  Red Hat OpenStack Platform The Red Hat OpenStack platform uses director to build, manage and upgrade Red Hat OpenStack. Director is in fact a minimal OpenStack deployment itself, with everything needed to deploy OpenStack. The main piece outside of the OpenStack core (Nova, Neutron, Glance, Swift and Heat) is Ironic. The Ironic project is focused on baremetal-as-a-service. Director allows you to add physical nodes to Ironic and assign them OpenStack roles: compute, control, storage, network, etc. Once roles are assigned an OpenStack environment can be deployed, upgraded and even scaled. As mentioned director is a complete life-cycle management tool that uses OpenStack to manage OpenStack. In this article we will deploy director (undercloud) on a single VM. We will add three baremetal nodes (VMs) and then deploy OpenStack (overcloud) in a minimal configuration (1 controller node and 1 compute node). I am able to run this on a laptop with just 12GB RAM. Lab Environment My idea for this configuration was build the most minimal OpenStack environment possible, something that would run on my laptop with just 12GB RAM using Red Hat OpenStack Director. In the end this experiment was successful and the configuration used is as follows:  KVM Hypervisor Physical Laptop: RHEL 7.3, CentOS or Fedora, Dual core, 12 GB RAM and 250GB disk Undercloud VM: RHEL 7.3, 2x vCPUs, 4GB RAM, 1 x NIC 8(provisioning), 1 x NIC (external) and 40GB disk Overcloud Controller VM: RHEL 7.3, 2 x vCPUs, 6GB RAM, 1 x NIC (provisioning), 2 x NICs (external) and 30GB disk Overcloud Compute VM: RHEL 7.3, 2 x vCPU, 4GB RAM, 1 x NIC (provisioning), 2 x NICs (external) and 20GB disk   Networking Setup In this configuration we are using virtual networks provided by the hypervisor host (my laptop). Create provisioning and external networks on KVM Hypervisor host. Ensure that NAT forwarding is enabled and DHCP is disabled on the external network. We run OpenStack overcloud on the external network. The provisioning network should be non-routable and DHCP disabled. The undercloud will handle DHCP services for the provisioning network and other IPs will be statically assigned. [Hypervisor] Create external network for the overcloud. [ktenzer@ktenzer ~]$ cat &gt; /tmp/external.xml &lt;&lt;EOF &lt;network&gt;    &lt;name&gt;external&lt;/name&gt;    &lt;forward mode=&#39;nat&#39;&gt;       &lt;nat&gt; &lt;port start=&#39;1024&#39; end=&#39;65535&#39;/&gt;       &lt;/nat&gt;    &lt;/forward&gt;    &lt;ip address=&#39;192.168.122.1&#39; netmask=&#39;255.255.255.0&#39;&gt;    &lt;/ip&gt; &lt;/network&gt;  Note: hypervisor is 192.168.122.1 and reachable via this IP from undercloud. [ktenzer@ktenzer ~]$ sudo virsh net-define /tmp/external.xml [ktenzer@ktenzer ~]$ sudo virsh net-autostart external [ktenzer@ktenzer ~]$ sudo virsh net-start external Create provisioning network for undercloud. Note: gateway is 192.168.126.254 as we will use 192.168.126.1 as IP for the VM running our undercloud. [ktenzer@ktenzer ~]$ cat &gt; /tmp/provisioning.xml &lt;&lt;EOF &lt;network&gt;    &lt;name&gt;provisioning&lt;/name&gt;    &lt;ip address=&#39;192.168.126.254&#39; netmask=&#39;255.255.255.0&#39;&gt;    &lt;/ip&gt; &lt;/network&gt; [ktenzer@ktenzer ~]$ sudo virsh net-define /tmp/provisioning.xml [ktenzer@ktenzer ~]$ sudo virsh net-autostart provisioning [ktenzer@ktenzer ~]$ sudo virsh net-start provisioning Deploy Undercloud First install Red Hat Enterprise Linux (RHEL) 7.3 on undercloud VM. Register with subscription manager and configure required RPM repositories for Red Hat OpenStack Platform. [Undercloud] [root@director ~]# subscription-manager register [root@director ~]#subscription-manager list --available \ subscription-manager attach --pool= [root@director ~]# subscription-manager repos --disable=* [root@director ~]# subscription-manager repos --enable=rhel-7-server-rpms \ --enable=rhel-7-server-extras-rpms \  --enable=rhel-7-server-rh-common-rpms \  --enable=rhel-ha-for-rhel-7-server-rpms \  --enable=rhel-7-server-openstack-10-rpms Update all pacakges and reboot. [root@director ~]# yum update -y [root@director ~]# systemctl reboot Install Director Packages. [root@director ~]# yum install -y python-tripleoclient Ensure host is defined in /etc/hosts. [root@director ~]# vi /etc/hosts  192.168.122.90 ospd.lab.com ospd Create Stack User. [root@director ~]# useradd stack [root@director ~]# passwd stack  # specify a password Configure user with sudo permissions. [root@director ~]# echo &quot;stack ALL=(root) NOPASSWD:ALL&quot; | tee -a /etc/sudoers.d/stack [root@director ~]# chmod 0440 /etc/sudoers.d/stack Switch to new stack user. [root@director ~]# su - stack [stack@director ~]$ Create directories for images and templates. Images are used to boot initial systems and provide baseline OS. Templates are used to customize deployment. [stack@director ~]$ mkdir ~/images [stack@director ~]$ mkdir ~/templates Configure Director using the sample. [stack@director ~]$ cp /usr/share/instack-undercloud/undercloud.conf.sample \ ~/undercloud.conf In my environment the 192.168.126.0/24 network is the undercloud network and used for provisioning as well as deploying the overcloud. [stack@undercloud ~]$ vi ~/undercloud.conf  [DEFAULT]  local_ip = 192.168.126.1/24  undercloud_public_vip = 192.168.126.2  undercloud_admin_vip = 192.168.126.3  local_interface = eth1  masquerade_network = 192.168.126.0/24  dhcp_start = 192.168.126.100  dhcp_end = 192.168.126.150  network_cidr = 192.168.126.0/24  network_gateway = 192.168.126.1  inspection_iprange = 192.168.126.130,192.168.126.99  generate_service_certificate = true  certificate_generation_ca = local Install the undercloud. [stack@odpd ~]$ openstack undercloud install #############################################################################  Undercloud install complete. The file containing this installation&#39;s passwords is at  /home/stack/undercloud-passwords.conf. There is also a stackrc file at /home/stack/stackrc. These files are needed to interact with the OpenStack services, and should be  secured. ############################################################################# Import overcloud images. [stack@odpd ~]$ source stackrc [stack@odpd ~]$ sudo yum install -y \ rhosp-director-images rhosp-director-images-ipa [stack@odpd ~]$ cd ~/images $ for i in \ /usr/share/rhosp-director-images/overcloud-full-latest-10.0.tar \ /usr/share/rhosp-director-images/ironic-python-agent-latest-10.0.tar; \ do tar -xvf $i; done [stack@odpd ~]$ openstack overcloud image upload --image-path \ /home/stack/images/ Configure DNS on undercloud network. [stack@odpd ~]$ neutron subnet-list +--------------------------------------+------+------------------+--------------------------------------------------------+ | id | name | cidr | allocation_pools | +--------------------------------------+------+------------------+--------------------------------------------------------+ | 294ff536-dc8b-49a3-8327-62d9792d30a6 | | 192.168.126.0/24 | {&quot;start&quot;: &quot;192.168.126.100&quot;, &quot;end&quot;: &quot;192.168.126.200&quot;} | +--------------------------------------+------+------------------+--------------------------------------------------------+ [stack@odpd ~]$ neutron subnet-update 294ff536-dc8b-49a3-8327-62d9792d30a6 \ --dns-nameserver 8.8.8.8 [Hypervisor] Registering Overcloud Nodes. Create VM hulls in KVM using virsh on hypervisor host. Note: You will need to change the disk path to suit your needs. ktenzer$ cd /home/ktenzer/VirtualMachines ktenzer$ sudo for i in {1..3}; do qemu-img create -f qcow2 -o preallocation=metadata overcloud-node$i.qcow2 60G; done ktenzer$ sudo for i in {1..3}; do virt-install --ram 4096 --vcpus 4 --os-variant rhel7 --disk path=/home/ktenzer/VirtualMachines/overcloud-node$i.qcow2,device=disk,bus=virtio,format=qcow2 --noautoconsole --vnc --network network:provisioning --network network:external --network network:external --name overcloud-node$i --cpu SandyBridge,+vmx --dry-run --print-xml &gt; /tmp/overcloud-node$i.xml; virsh define --file /tmp/overcloud-node$i.xml; done [Undercloud] Copy ssh key from undercloud system to KVM hypervisor host for stack user. [stack@odpd ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub stack@192.168.122.1 Save the MAC addresses of the NICs used for provisioning. Note: Ironic needs to know what MAC addresses a node has associated for provisioning network. [stack@odpd images]$ for i in {1..3}; do virsh \ -c qemu+ssh://stack@192.168.122.1/system domiflist overcloud-node$i \ | awk &#39;$3 == &quot;provisioning&quot; {print $5};&#39;; done &gt; /tmp/nodes.txt [stack@odpd images]$ cat /tmp/nodes.txt  52:54:00:7e:d8:01 52:54:00:f6:a6:73 52:54:00:c9:b2:84 [stack@undercloud ~]$ jq . &lt;&lt; EOF &gt; ~/instackenv.json {   &quot;ssh-user&quot;: &quot;stack&quot;,   &quot;ssh-key&quot;: &quot;$(cat ~/.ssh/id_rsa)&quot;,   &quot;power_manager&quot;: &quot;nova.virt.baremetal.virtual_power_driver.VirtualPowerManager&quot;,   &quot;host-ip&quot;: &quot;192.168.122.1&quot;,   &quot;arch&quot;: &quot;x86_64&quot;,   &quot;nodes&quot;: [     {       &quot;pm_addr&quot;: &quot;192.168.122.1&quot;,       &quot;pm_password&quot;: &quot;$(cat ~/.ssh/id_rsa)&quot;,       &quot;pm_type&quot;: &quot;pxe_ssh&quot;,       &quot;mac&quot;: [         &quot;$(sed -n 1p /tmp/nodes.txt)&quot;       ],       &quot;cpu&quot;: &quot;2&quot;,       &quot;memory&quot;: &quot;4096&quot;,       &quot;disk&quot;: &quot;60&quot;,       &quot;arch&quot;: &quot;x86_64&quot;,       &quot;pm_user&quot;: &quot;stack&quot;     },     {       &quot;pm_addr&quot;: &quot;192.168.122.1&quot;,       &quot;pm_password&quot;: &quot;$(cat ~/.ssh/id_rsa)&quot;,       &quot;pm_type&quot;: &quot;pxe_ssh&quot;,       &quot;mac&quot;: [         &quot;$(sed -n 2p /tmp/nodes.txt)&quot;       ],       &quot;cpu&quot;: &quot;4&quot;,       &quot;memory&quot;: &quot;2048&quot;,       &quot;disk&quot;: &quot;60&quot;,       &quot;arch&quot;: &quot;x86_64&quot;,       &quot;pm_user&quot;: &quot;stack&quot;     },     {       &quot;pm_addr&quot;: &quot;192.168.122.1&quot;,       &quot;pm_password&quot;: &quot;$(cat ~/.ssh/id_rsa)&quot;,       &quot;pm_type&quot;: &quot;pxe_ssh&quot;,       &quot;mac&quot;: [         &quot;$(sed -n 3p /tmp/nodes.txt)&quot;       ],       &quot;cpu&quot;: &quot;4&quot;,       &quot;memory&quot;: &quot;2048&quot;,       &quot;disk&quot;: &quot;60&quot;,       &quot;arch&quot;: &quot;x86_64&quot;,       &quot;pm_user&quot;: &quot;stack&quot;     }   ]  }  EOF Validate introspection configuration. [stack@odpd ~]$ curl -O  https://raw.githubusercontent.com/rthallisey/clapper/master/instackenv-validator.py Import nodes into Ironic and set them to bootable. [stack@odpd ~]$ openstack baremetal import --json ~/instackenv.json [stack@odpd ~]$ openstack baremetal configure boot [stack@odpd ~]$ openstack baremetal node list  +--------------------------------------+------+---------------+-------------+--------------------+-------------+  | UUID | Name | Instance UUID | Power State | Provisioning State | Maintenance |  +--------------------------------------+------+---------------+-------------+--------------------+-------------+  | ea61b158-9cbd-46d2-93e9-eadaccb1589b | None | None | power off | available | False |  | 11aaf849-361e-4bda-81f5-74c245f554af | None | None | power off | available | False |  | 275448c1-aa8d-4854-bb3b-bc73e1e1a794 | None | None | power off | available | False |  +--------------------------------------+------+---------------+-------------+--------------------+-------------+ Set nodes to managed. [stack@odpd ~]$ for node in $(openstack baremetal node list -c UUID \ -f value) ; do openstack baremetal node manage $node ; done List nodes. [stack@odpd ~]$ openstack baremetal node list  +--------------------------------------+------+---------------+-------------+--------------------+-------------+  | UUID | Name | Instance UUID | Power State | Provisioning State | Maintenance |  +--------------------------------------+------+---------------+-------------+--------------------+-------------+  | ea61b158-9cbd-46d2-93e9-eadaccb1589b | None | None | power off | manageable | False |  | 11aaf849-361e-4bda-81f5-74c245f554af | None | None | power off | manageable | False |  | 275448c1-aa8d-4854-bb3b-bc73e1e1a794 | None | None | power off | manageable | False |  +--------------------------------------+------+---------------+-------------+--------------------+-------------+ Run introspection against all managed nodes. Note: Nodes are booted using ramdisk and their hardware inspected. Introspection prepares nodes for deployment into overcloud. [stack@odpd ~]$ openstack overcloud node introspect --all-manageable \ --provide">





  <meta name="twitter:site" content="@keithtenzer">
  <meta name="twitter:title" content="Red Hat OpenStack Platform 10 (Newton) Installation and Configuration Guide">
  <meta name="twitter:description" content="Overview In this article we will setup an OpenStack environment based off Newton using the Red Hat OpenStack Platform. OpenStack is OpenStack but every distribution differs in what capabilities or technologies are supported and how OpenStack is installed, configured as well as upgraded. The Red Hat OpenStack Platform uses OpenStack director based on the TripleO (OpenStack on OpenStack) project to install, configure and update OpenStack. Director is a lifecycle management tool for OpenStack. Red Hat&#39;s approach is to make OpenStack easy to manage, without compromising on the &quot;Open&quot; part of OpenStack. If management of OpenStack can be simpler and the learning curve brought down then it has a real chance to be the next-gen virtualization platform. What company wouldn&#39;t want to be able to consume their internal IT resources like using AWS, GCE or Azure if they didn&#39;t give up anything to do so? We aren&#39;t there yet but Red Hat is making bold strides and as you will see in this article, is on a journey to make OpenStack consumable for everyone!  Red Hat OpenStack Platform The Red Hat OpenStack platform uses director to build, manage and upgrade Red Hat OpenStack. Director is in fact a minimal OpenStack deployment itself, with everything needed to deploy OpenStack. The main piece outside of the OpenStack core (Nova, Neutron, Glance, Swift and Heat) is Ironic. The Ironic project is focused on baremetal-as-a-service. Director allows you to add physical nodes to Ironic and assign them OpenStack roles: compute, control, storage, network, etc. Once roles are assigned an OpenStack environment can be deployed, upgraded and even scaled. As mentioned director is a complete life-cycle management tool that uses OpenStack to manage OpenStack. In this article we will deploy director (undercloud) on a single VM. We will add three baremetal nodes (VMs) and then deploy OpenStack (overcloud) in a minimal configuration (1 controller node and 1 compute node). I am able to run this on a laptop with just 12GB RAM. Lab Environment My idea for this configuration was build the most minimal OpenStack environment possible, something that would run on my laptop with just 12GB RAM using Red Hat OpenStack Director. In the end this experiment was successful and the configuration used is as follows:  KVM Hypervisor Physical Laptop: RHEL 7.3, CentOS or Fedora, Dual core, 12 GB RAM and 250GB disk Undercloud VM: RHEL 7.3, 2x vCPUs, 4GB RAM, 1 x NIC 8(provisioning), 1 x NIC (external) and 40GB disk Overcloud Controller VM: RHEL 7.3, 2 x vCPUs, 6GB RAM, 1 x NIC (provisioning), 2 x NICs (external) and 30GB disk Overcloud Compute VM: RHEL 7.3, 2 x vCPU, 4GB RAM, 1 x NIC (provisioning), 2 x NICs (external) and 20GB disk   Networking Setup In this configuration we are using virtual networks provided by the hypervisor host (my laptop). Create provisioning and external networks on KVM Hypervisor host. Ensure that NAT forwarding is enabled and DHCP is disabled on the external network. We run OpenStack overcloud on the external network. The provisioning network should be non-routable and DHCP disabled. The undercloud will handle DHCP services for the provisioning network and other IPs will be statically assigned. [Hypervisor] Create external network for the overcloud. [ktenzer@ktenzer ~]$ cat &gt; /tmp/external.xml &lt;&lt;EOF &lt;network&gt;    &lt;name&gt;external&lt;/name&gt;    &lt;forward mode=&#39;nat&#39;&gt;       &lt;nat&gt; &lt;port start=&#39;1024&#39; end=&#39;65535&#39;/&gt;       &lt;/nat&gt;    &lt;/forward&gt;    &lt;ip address=&#39;192.168.122.1&#39; netmask=&#39;255.255.255.0&#39;&gt;    &lt;/ip&gt; &lt;/network&gt;  Note: hypervisor is 192.168.122.1 and reachable via this IP from undercloud. [ktenzer@ktenzer ~]$ sudo virsh net-define /tmp/external.xml [ktenzer@ktenzer ~]$ sudo virsh net-autostart external [ktenzer@ktenzer ~]$ sudo virsh net-start external Create provisioning network for undercloud. Note: gateway is 192.168.126.254 as we will use 192.168.126.1 as IP for the VM running our undercloud. [ktenzer@ktenzer ~]$ cat &gt; /tmp/provisioning.xml &lt;&lt;EOF &lt;network&gt;    &lt;name&gt;provisioning&lt;/name&gt;    &lt;ip address=&#39;192.168.126.254&#39; netmask=&#39;255.255.255.0&#39;&gt;    &lt;/ip&gt; &lt;/network&gt; [ktenzer@ktenzer ~]$ sudo virsh net-define /tmp/provisioning.xml [ktenzer@ktenzer ~]$ sudo virsh net-autostart provisioning [ktenzer@ktenzer ~]$ sudo virsh net-start provisioning Deploy Undercloud First install Red Hat Enterprise Linux (RHEL) 7.3 on undercloud VM. Register with subscription manager and configure required RPM repositories for Red Hat OpenStack Platform. [Undercloud] [root@director ~]# subscription-manager register [root@director ~]#subscription-manager list --available \ subscription-manager attach --pool= [root@director ~]# subscription-manager repos --disable=* [root@director ~]# subscription-manager repos --enable=rhel-7-server-rpms \ --enable=rhel-7-server-extras-rpms \  --enable=rhel-7-server-rh-common-rpms \  --enable=rhel-ha-for-rhel-7-server-rpms \  --enable=rhel-7-server-openstack-10-rpms Update all pacakges and reboot. [root@director ~]# yum update -y [root@director ~]# systemctl reboot Install Director Packages. [root@director ~]# yum install -y python-tripleoclient Ensure host is defined in /etc/hosts. [root@director ~]# vi /etc/hosts  192.168.122.90 ospd.lab.com ospd Create Stack User. [root@director ~]# useradd stack [root@director ~]# passwd stack  # specify a password Configure user with sudo permissions. [root@director ~]# echo &quot;stack ALL=(root) NOPASSWD:ALL&quot; | tee -a /etc/sudoers.d/stack [root@director ~]# chmod 0440 /etc/sudoers.d/stack Switch to new stack user. [root@director ~]# su - stack [stack@director ~]$ Create directories for images and templates. Images are used to boot initial systems and provide baseline OS. Templates are used to customize deployment. [stack@director ~]$ mkdir ~/images [stack@director ~]$ mkdir ~/templates Configure Director using the sample. [stack@director ~]$ cp /usr/share/instack-undercloud/undercloud.conf.sample \ ~/undercloud.conf In my environment the 192.168.126.0/24 network is the undercloud network and used for provisioning as well as deploying the overcloud. [stack@undercloud ~]$ vi ~/undercloud.conf  [DEFAULT]  local_ip = 192.168.126.1/24  undercloud_public_vip = 192.168.126.2  undercloud_admin_vip = 192.168.126.3  local_interface = eth1  masquerade_network = 192.168.126.0/24  dhcp_start = 192.168.126.100  dhcp_end = 192.168.126.150  network_cidr = 192.168.126.0/24  network_gateway = 192.168.126.1  inspection_iprange = 192.168.126.130,192.168.126.99  generate_service_certificate = true  certificate_generation_ca = local Install the undercloud. [stack@odpd ~]$ openstack undercloud install #############################################################################  Undercloud install complete. The file containing this installation&#39;s passwords is at  /home/stack/undercloud-passwords.conf. There is also a stackrc file at /home/stack/stackrc. These files are needed to interact with the OpenStack services, and should be  secured. ############################################################################# Import overcloud images. [stack@odpd ~]$ source stackrc [stack@odpd ~]$ sudo yum install -y \ rhosp-director-images rhosp-director-images-ipa [stack@odpd ~]$ cd ~/images $ for i in \ /usr/share/rhosp-director-images/overcloud-full-latest-10.0.tar \ /usr/share/rhosp-director-images/ironic-python-agent-latest-10.0.tar; \ do tar -xvf $i; done [stack@odpd ~]$ openstack overcloud image upload --image-path \ /home/stack/images/ Configure DNS on undercloud network. [stack@odpd ~]$ neutron subnet-list +--------------------------------------+------+------------------+--------------------------------------------------------+ | id | name | cidr | allocation_pools | +--------------------------------------+------+------------------+--------------------------------------------------------+ | 294ff536-dc8b-49a3-8327-62d9792d30a6 | | 192.168.126.0/24 | {&quot;start&quot;: &quot;192.168.126.100&quot;, &quot;end&quot;: &quot;192.168.126.200&quot;} | +--------------------------------------+------+------------------+--------------------------------------------------------+ [stack@odpd ~]$ neutron subnet-update 294ff536-dc8b-49a3-8327-62d9792d30a6 \ --dns-nameserver 8.8.8.8 [Hypervisor] Registering Overcloud Nodes. Create VM hulls in KVM using virsh on hypervisor host. Note: You will need to change the disk path to suit your needs. ktenzer$ cd /home/ktenzer/VirtualMachines ktenzer$ sudo for i in {1..3}; do qemu-img create -f qcow2 -o preallocation=metadata overcloud-node$i.qcow2 60G; done ktenzer$ sudo for i in {1..3}; do virt-install --ram 4096 --vcpus 4 --os-variant rhel7 --disk path=/home/ktenzer/VirtualMachines/overcloud-node$i.qcow2,device=disk,bus=virtio,format=qcow2 --noautoconsole --vnc --network network:provisioning --network network:external --network network:external --name overcloud-node$i --cpu SandyBridge,+vmx --dry-run --print-xml &gt; /tmp/overcloud-node$i.xml; virsh define --file /tmp/overcloud-node$i.xml; done [Undercloud] Copy ssh key from undercloud system to KVM hypervisor host for stack user. [stack@odpd ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub stack@192.168.122.1 Save the MAC addresses of the NICs used for provisioning. Note: Ironic needs to know what MAC addresses a node has associated for provisioning network. [stack@odpd images]$ for i in {1..3}; do virsh \ -c qemu+ssh://stack@192.168.122.1/system domiflist overcloud-node$i \ | awk &#39;$3 == &quot;provisioning&quot; {print $5};&#39;; done &gt; /tmp/nodes.txt [stack@odpd images]$ cat /tmp/nodes.txt  52:54:00:7e:d8:01 52:54:00:f6:a6:73 52:54:00:c9:b2:84 [stack@undercloud ~]$ jq . &lt;&lt; EOF &gt; ~/instackenv.json {   &quot;ssh-user&quot;: &quot;stack&quot;,   &quot;ssh-key&quot;: &quot;$(cat ~/.ssh/id_rsa)&quot;,   &quot;power_manager&quot;: &quot;nova.virt.baremetal.virtual_power_driver.VirtualPowerManager&quot;,   &quot;host-ip&quot;: &quot;192.168.122.1&quot;,   &quot;arch&quot;: &quot;x86_64&quot;,   &quot;nodes&quot;: [     {       &quot;pm_addr&quot;: &quot;192.168.122.1&quot;,       &quot;pm_password&quot;: &quot;$(cat ~/.ssh/id_rsa)&quot;,       &quot;pm_type&quot;: &quot;pxe_ssh&quot;,       &quot;mac&quot;: [         &quot;$(sed -n 1p /tmp/nodes.txt)&quot;       ],       &quot;cpu&quot;: &quot;2&quot;,       &quot;memory&quot;: &quot;4096&quot;,       &quot;disk&quot;: &quot;60&quot;,       &quot;arch&quot;: &quot;x86_64&quot;,       &quot;pm_user&quot;: &quot;stack&quot;     },     {       &quot;pm_addr&quot;: &quot;192.168.122.1&quot;,       &quot;pm_password&quot;: &quot;$(cat ~/.ssh/id_rsa)&quot;,       &quot;pm_type&quot;: &quot;pxe_ssh&quot;,       &quot;mac&quot;: [         &quot;$(sed -n 2p /tmp/nodes.txt)&quot;       ],       &quot;cpu&quot;: &quot;4&quot;,       &quot;memory&quot;: &quot;2048&quot;,       &quot;disk&quot;: &quot;60&quot;,       &quot;arch&quot;: &quot;x86_64&quot;,       &quot;pm_user&quot;: &quot;stack&quot;     },     {       &quot;pm_addr&quot;: &quot;192.168.122.1&quot;,       &quot;pm_password&quot;: &quot;$(cat ~/.ssh/id_rsa)&quot;,       &quot;pm_type&quot;: &quot;pxe_ssh&quot;,       &quot;mac&quot;: [         &quot;$(sed -n 3p /tmp/nodes.txt)&quot;       ],       &quot;cpu&quot;: &quot;4&quot;,       &quot;memory&quot;: &quot;2048&quot;,       &quot;disk&quot;: &quot;60&quot;,       &quot;arch&quot;: &quot;x86_64&quot;,       &quot;pm_user&quot;: &quot;stack&quot;     }   ]  }  EOF Validate introspection configuration. [stack@odpd ~]$ curl -O  https://raw.githubusercontent.com/rthallisey/clapper/master/instackenv-validator.py Import nodes into Ironic and set them to bootable. [stack@odpd ~]$ openstack baremetal import --json ~/instackenv.json [stack@odpd ~]$ openstack baremetal configure boot [stack@odpd ~]$ openstack baremetal node list  +--------------------------------------+------+---------------+-------------+--------------------+-------------+  | UUID | Name | Instance UUID | Power State | Provisioning State | Maintenance |  +--------------------------------------+------+---------------+-------------+--------------------+-------------+  | ea61b158-9cbd-46d2-93e9-eadaccb1589b | None | None | power off | available | False |  | 11aaf849-361e-4bda-81f5-74c245f554af | None | None | power off | available | False |  | 275448c1-aa8d-4854-bb3b-bc73e1e1a794 | None | None | power off | available | False |  +--------------------------------------+------+---------------+-------------+--------------------+-------------+ Set nodes to managed. [stack@odpd ~]$ for node in $(openstack baremetal node list -c UUID \ -f value) ; do openstack baremetal node manage $node ; done List nodes. [stack@odpd ~]$ openstack baremetal node list  +--------------------------------------+------+---------------+-------------+--------------------+-------------+  | UUID | Name | Instance UUID | Power State | Provisioning State | Maintenance |  +--------------------------------------+------+---------------+-------------+--------------------+-------------+  | ea61b158-9cbd-46d2-93e9-eadaccb1589b | None | None | power off | manageable | False |  | 11aaf849-361e-4bda-81f5-74c245f554af | None | None | power off | manageable | False |  | 275448c1-aa8d-4854-bb3b-bc73e1e1a794 | None | None | power off | manageable | False |  +--------------------------------------+------+---------------+-------------+--------------------+-------------+ Run introspection against all managed nodes. Note: Nodes are booted using ramdisk and their hardware inspected. Introspection prepares nodes for deployment into overcloud. [stack@odpd ~]$ openstack overcloud node introspect --all-manageable \ --provide">
  <meta name="twitter:url" content="http://localhost:4000/openstack/red-hat-openstack-platform-10-newton-installation-and-configuration-guide/">

  
    <meta name="twitter:card" content="summary">
    
  

  



  <meta property="article:published_time" content="2017-04-20T00:00:00-07:00">





  

  


<link rel="canonical" href="http://localhost:4000/openstack/red-hat-openstack-platform-10-newton-installation-and-configuration-guide/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Keith Tenzer",
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Keith Tenzer's Blog Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single wide">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Keith Tenzer's Blog
          <span class="site-subtitle">Cloud Computing and Code</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about/index.html">About</a>
            </li><li class="masthead__menu-item">
              <a href="/conferences-and-events/index.html">Conferences and Events</a>
            </li><li class="masthead__menu-item">
              <a href="/videos/index.html">Videos</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16">
            <path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path>
          </svg>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      


  
    



<nav class="breadcrumbs">
  <ol itemscope itemtype="https://schema.org/BreadcrumbList">
    
    
    
      
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="http://localhost:4000/" itemprop="item"><span itemprop="name">Home</span></a>
          <meta itemprop="position" content="1" />
        </li>
        <span class="sep">/</span>
      
      
        
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/categories/#openstack" itemprop="item"><span itemprop="name">Openstack</span></a>
          <meta itemprop="position" content="2" />
        </li>
        <span class="sep">/</span>
      
    
      
      
        <li class="current">Red Hat OpenStack Platform 10 (Newton) Installation and Configuration Guide</li>
      
    
  </ol>
</nav>

  


<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      
        <img src="/assets/main/me.png" alt="Keith Tenzer" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Keith Tenzer</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>Solutions Architect at Temporal</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Los Angeles, CA</span>
        </li>
      

      
        
          
        
          
        
          
            <li><a href="https://twitter.com/keithtenzer" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i><span class="label">Twitter</span></a></li>
          
        
          
        
          
            <li><a href="https://github.com/ktenzer" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Red Hat OpenStack Platform 10 (Newton) Installation and Configuration Guide">
    <meta itemprop="description" content="OverviewIn this article we will setup an OpenStack environment based off Newton using the Red Hat OpenStack Platform. OpenStack is OpenStack but every distribution differs in what capabilities or technologies are supported and how OpenStack is installed, configured as well as upgraded.The Red Hat OpenStack Platform uses OpenStack director based on the TripleO (OpenStack on OpenStack) project to install, configure and update OpenStack. Director is a lifecycle management tool for OpenStack. Red Hat&#39;s approach is to make OpenStack easy to manage, without compromising on the &quot;Open&quot; part of OpenStack. If management of OpenStack can be simpler and the learning curve brought down then it has a real chance to be the next-gen virtualization platform. What company wouldn&#39;t want to be able to consume their internal IT resources like using AWS, GCE or Azure if they didn&#39;t give up anything to do so? We aren&#39;t there yet but Red Hat is making bold strides and as you will see in this article, is on a journey to make OpenStack consumable for everyone!Red Hat OpenStack PlatformThe Red Hat OpenStack platform uses director to build, manage and upgrade Red Hat OpenStack. Director is in fact a minimal OpenStack deployment itself, with everything needed to deploy OpenStack. The main piece outside of the OpenStack core (Nova, Neutron, Glance, Swift and Heat) is Ironic. The Ironic project is focused on baremetal-as-a-service.Director allows you to add physical nodes to Ironic and assign them OpenStack roles: compute, control, storage, network, etc. Once roles are assigned an OpenStack environment can be deployed, upgraded and even scaled. As mentioned director is a complete life-cycle management tool that uses OpenStack to manage OpenStack.In this article we will deploy director (undercloud) on a single VM. We will add three baremetal nodes (VMs) and then deploy OpenStack (overcloud) in a minimal configuration (1 controller node and 1 compute node). I am able to run this on a laptop with just 12GB RAM.Lab EnvironmentMy idea for this configuration was build the most minimal OpenStack environment possible, something that would run on my laptop with just 12GB RAM using Red Hat OpenStack Director. In the end this experiment was successful and the configuration used is as follows:KVM Hypervisor Physical Laptop: RHEL 7.3, CentOS or Fedora, Dual core, 12 GB RAM and 250GB diskUndercloud VM: RHEL 7.3, 2x vCPUs, 4GB RAM, 1 x NIC 8(provisioning), 1 x NIC (external) and 40GB diskOvercloud Controller VM: RHEL 7.3, 2 x vCPUs, 6GB RAM, 1 x NIC (provisioning), 2 x NICs (external) and 30GB diskOvercloud Compute VM: RHEL 7.3, 2 x vCPU, 4GB RAM, 1 x NIC (provisioning), 2 x NICs (external) and 20GB diskNetworking SetupIn this configuration we are using virtual networks provided by the hypervisor host (my laptop). Create provisioning and external networks on KVM Hypervisor host. Ensure that NAT forwarding is enabled and DHCP is disabled on the external network. We run OpenStack overcloud on the external network. The provisioning network should be non-routable and DHCP disabled. The undercloud will handle DHCP services for the provisioning network and other IPs will be statically assigned.[Hypervisor]Create external network for the overcloud.[ktenzer@ktenzer ~]$ cat &gt; /tmp/external.xml &lt;&lt;EOF&lt;network&gt;   &lt;name&gt;external&lt;/name&gt;   &lt;forward mode=&#39;nat&#39;&gt;      &lt;nat&gt; &lt;port start=&#39;1024&#39; end=&#39;65535&#39;/&gt;      &lt;/nat&gt;   &lt;/forward&gt;   &lt;ip address=&#39;192.168.122.1&#39; netmask=&#39;255.255.255.0&#39;&gt;   &lt;/ip&gt;&lt;/network&gt;Note: hypervisor is 192.168.122.1 and reachable via this IP from undercloud.[ktenzer@ktenzer ~]$ sudo virsh net-define /tmp/external.xml[ktenzer@ktenzer ~]$ sudo virsh net-autostart external[ktenzer@ktenzer ~]$ sudo virsh net-start externalCreate provisioning network for undercloud.Note: gateway is 192.168.126.254 as we will use 192.168.126.1 as IP for the VM running our undercloud.[ktenzer@ktenzer ~]$ cat &gt; /tmp/provisioning.xml &lt;&lt;EOF&lt;network&gt;   &lt;name&gt;provisioning&lt;/name&gt;   &lt;ip address=&#39;192.168.126.254&#39; netmask=&#39;255.255.255.0&#39;&gt;   &lt;/ip&gt;&lt;/network&gt;[ktenzer@ktenzer ~]$ sudo virsh net-define /tmp/provisioning.xml[ktenzer@ktenzer ~]$ sudo virsh net-autostart provisioning[ktenzer@ktenzer ~]$ sudo virsh net-start provisioningDeploy UndercloudFirst install Red Hat Enterprise Linux (RHEL) 7.3 on undercloud VM. Register with subscription manager and configure required RPM repositories for Red Hat OpenStack Platform.[Undercloud][root@director ~]# subscription-manager register[root@director ~]#subscription-manager list --available \subscription-manager attach --pool=[root@director ~]# subscription-manager repos --disable=*[root@director ~]# subscription-manager repos --enable=rhel-7-server-rpms \--enable=rhel-7-server-extras-rpms \ --enable=rhel-7-server-rh-common-rpms \ --enable=rhel-ha-for-rhel-7-server-rpms \ --enable=rhel-7-server-openstack-10-rpmsUpdate all pacakges and reboot.[root@director ~]# yum update -y[root@director ~]# systemctl rebootInstall Director Packages.[root@director ~]# yum install -y python-tripleoclientEnsure host is defined in /etc/hosts.[root@director ~]# vi /etc/hosts 192.168.122.90 ospd.lab.com ospdCreate Stack User.[root@director ~]# useradd stack[root@director ~]# passwd stack  # specify a passwordConfigure user with sudo permissions.[root@director ~]# echo &quot;stack ALL=(root) NOPASSWD:ALL&quot; | tee -a /etc/sudoers.d/stack[root@director ~]# chmod 0440 /etc/sudoers.d/stackSwitch to new stack user.[root@director ~]# su - stack[stack@director ~]$Create directories for images and templates. Images are used to boot initial systems and provide baseline OS. Templates are used to customize deployment.[stack@director ~]$ mkdir ~/images[stack@director ~]$ mkdir ~/templatesConfigure Director using the sample.[stack@director ~]$ cp /usr/share/instack-undercloud/undercloud.conf.sample \~/undercloud.confIn my environment the 192.168.126.0/24 network is the undercloud network and used for provisioning as well as deploying the overcloud.[stack@undercloud ~]$ vi ~/undercloud.conf [DEFAULT] local_ip = 192.168.126.1/24 undercloud_public_vip = 192.168.126.2 undercloud_admin_vip = 192.168.126.3 local_interface = eth1 masquerade_network = 192.168.126.0/24 dhcp_start = 192.168.126.100 dhcp_end = 192.168.126.150 network_cidr = 192.168.126.0/24 network_gateway = 192.168.126.1 inspection_iprange = 192.168.126.130,192.168.126.99 generate_service_certificate = true certificate_generation_ca = localInstall the undercloud.[stack@odpd ~]$ openstack undercloud install############################################################################# Undercloud install complete.The file containing this installation&#39;s passwords is at /home/stack/undercloud-passwords.conf.There is also a stackrc file at /home/stack/stackrc.These files are needed to interact with the OpenStack services, and should be secured.#############################################################################Import overcloud images.[stack@odpd ~]$ source stackrc[stack@odpd ~]$ sudo yum install -y \rhosp-director-images rhosp-director-images-ipa[stack@odpd ~]$ cd ~/images $ for i in \/usr/share/rhosp-director-images/overcloud-full-latest-10.0.tar \/usr/share/rhosp-director-images/ironic-python-agent-latest-10.0.tar; \do tar -xvf $i; done[stack@odpd ~]$ openstack overcloud image upload --image-path \/home/stack/images/Configure DNS on undercloud network.[stack@odpd ~]$ neutron subnet-list+--------------------------------------+------+------------------+--------------------------------------------------------+| id | name | cidr | allocation_pools |+--------------------------------------+------+------------------+--------------------------------------------------------+| 294ff536-dc8b-49a3-8327-62d9792d30a6 | | 192.168.126.0/24 | {&quot;start&quot;: &quot;192.168.126.100&quot;, &quot;end&quot;: &quot;192.168.126.200&quot;} |+--------------------------------------+------+------------------+--------------------------------------------------------+[stack@odpd ~]$ neutron subnet-update 294ff536-dc8b-49a3-8327-62d9792d30a6 \--dns-nameserver 8.8.8.8[Hypervisor]Registering Overcloud Nodes. Create VM hulls in KVM using virsh on hypervisor host.Note: You will need to change the disk path to suit your needs.ktenzer$ cd /home/ktenzer/VirtualMachinesktenzer$ sudo for i in {1..3}; do qemu-img create -f qcow2 -o preallocation=metadata overcloud-node$i.qcow2 60G; donektenzer$ sudo for i in {1..3}; do virt-install --ram 4096 --vcpus 4 --os-variant rhel7 --disk path=/home/ktenzer/VirtualMachines/overcloud-node$i.qcow2,device=disk,bus=virtio,format=qcow2 --noautoconsole --vnc --network network:provisioning --network network:external --network network:external --name overcloud-node$i --cpu SandyBridge,+vmx --dry-run --print-xml &gt; /tmp/overcloud-node$i.xml; virsh define --file /tmp/overcloud-node$i.xml; done[Undercloud]Copy ssh key from undercloud system to KVM hypervisor host for stack user.[stack@odpd ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub stack@192.168.122.1Save the MAC addresses of the NICs used for provisioning.Note: Ironic needs to know what MAC addresses a node has associated for provisioning network.[stack@odpd images]$ for i in {1..3}; do virsh \-c qemu+ssh://stack@192.168.122.1/system domiflist overcloud-node$i \| awk &#39;$3 == &quot;provisioning&quot; {print $5};&#39;; done &gt; /tmp/nodes.txt[stack@odpd images]$ cat /tmp/nodes.txt 52:54:00:7e:d8:0152:54:00:f6:a6:7352:54:00:c9:b2:84[stack@undercloud ~]$ jq . &lt;&lt; EOF &gt; ~/instackenv.json{  &quot;ssh-user&quot;: &quot;stack&quot;,  &quot;ssh-key&quot;: &quot;$(cat ~/.ssh/id_rsa)&quot;,  &quot;power_manager&quot;: &quot;nova.virt.baremetal.virtual_power_driver.VirtualPowerManager&quot;,  &quot;host-ip&quot;: &quot;192.168.122.1&quot;,  &quot;arch&quot;: &quot;x86_64&quot;,  &quot;nodes&quot;: [    {      &quot;pm_addr&quot;: &quot;192.168.122.1&quot;,      &quot;pm_password&quot;: &quot;$(cat ~/.ssh/id_rsa)&quot;,      &quot;pm_type&quot;: &quot;pxe_ssh&quot;,      &quot;mac&quot;: [        &quot;$(sed -n 1p /tmp/nodes.txt)&quot;      ],      &quot;cpu&quot;: &quot;2&quot;,      &quot;memory&quot;: &quot;4096&quot;,      &quot;disk&quot;: &quot;60&quot;,      &quot;arch&quot;: &quot;x86_64&quot;,      &quot;pm_user&quot;: &quot;stack&quot;    },    {      &quot;pm_addr&quot;: &quot;192.168.122.1&quot;,      &quot;pm_password&quot;: &quot;$(cat ~/.ssh/id_rsa)&quot;,      &quot;pm_type&quot;: &quot;pxe_ssh&quot;,      &quot;mac&quot;: [        &quot;$(sed -n 2p /tmp/nodes.txt)&quot;      ],      &quot;cpu&quot;: &quot;4&quot;,      &quot;memory&quot;: &quot;2048&quot;,      &quot;disk&quot;: &quot;60&quot;,      &quot;arch&quot;: &quot;x86_64&quot;,      &quot;pm_user&quot;: &quot;stack&quot;    },    {      &quot;pm_addr&quot;: &quot;192.168.122.1&quot;,      &quot;pm_password&quot;: &quot;$(cat ~/.ssh/id_rsa)&quot;,      &quot;pm_type&quot;: &quot;pxe_ssh&quot;,      &quot;mac&quot;: [        &quot;$(sed -n 3p /tmp/nodes.txt)&quot;      ],      &quot;cpu&quot;: &quot;4&quot;,      &quot;memory&quot;: &quot;2048&quot;,      &quot;disk&quot;: &quot;60&quot;,      &quot;arch&quot;: &quot;x86_64&quot;,      &quot;pm_user&quot;: &quot;stack&quot;    }  ] } EOFValidate introspection configuration.[stack@odpd ~]$ curl -O https://raw.githubusercontent.com/rthallisey/clapper/master/instackenv-validator.pyImport nodes into Ironic and set them to bootable.[stack@odpd ~]$ openstack baremetal import --json ~/instackenv.json[stack@odpd ~]$ openstack baremetal configure boot[stack@odpd ~]$ openstack baremetal node list +--------------------------------------+------+---------------+-------------+--------------------+-------------+ | UUID | Name | Instance UUID | Power State | Provisioning State | Maintenance | +--------------------------------------+------+---------------+-------------+--------------------+-------------+ | ea61b158-9cbd-46d2-93e9-eadaccb1589b | None | None | power off | available | False | | 11aaf849-361e-4bda-81f5-74c245f554af | None | None | power off | available | False | | 275448c1-aa8d-4854-bb3b-bc73e1e1a794 | None | None | power off | available | False | +--------------------------------------+------+---------------+-------------+--------------------+-------------+Set nodes to managed.[stack@odpd ~]$ for node in $(openstack baremetal node list -c UUID \-f value) ; do openstack baremetal node manage $node ; doneList nodes.[stack@odpd ~]$ openstack baremetal node list +--------------------------------------+------+---------------+-------------+--------------------+-------------+ | UUID | Name | Instance UUID | Power State | Provisioning State | Maintenance | +--------------------------------------+------+---------------+-------------+--------------------+-------------+ | ea61b158-9cbd-46d2-93e9-eadaccb1589b | None | None | power off | manageable | False | | 11aaf849-361e-4bda-81f5-74c245f554af | None | None | power off | manageable | False | | 275448c1-aa8d-4854-bb3b-bc73e1e1a794 | None | None | power off | manageable | False | +--------------------------------------+------+---------------+-------------+--------------------+-------------+Run introspection against all managed nodes.Note: Nodes are booted using ramdisk and their hardware inspected. Introspection prepares nodes for deployment into overcloud.[stack@odpd ~]$ openstack overcloud node introspect --all-manageable \--provide">
    <meta itemprop="datePublished" content="2017-04-20T00:00:00-07:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Red Hat OpenStack Platform 10 (Newton) Installation and Configuration Guide
</h1>
          

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2017-04-20T00:00:00-07:00">April 20, 2017</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          22 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
        <h2><img class="alignnone size-full wp-image-9500" src="/assets/2017/04/tripleo_owl.png" alt="tripleo_owl" width="1024" height="350" /></h2>
<h2>Overview</h2>
<p>In this article we will setup an OpenStack environment based off Newton using the Red Hat OpenStack Platform. OpenStack is OpenStack but every distribution differs in what capabilities or technologies are supported and how OpenStack is installed, configured as well as upgraded.</p>
<p>The Red Hat OpenStack Platform uses OpenStack director based on the TripleO (<strong>O</strong>penStack <strong>o</strong>n <strong>O</strong>penStack) project to install, configure and update OpenStack. Director is a lifecycle management tool for OpenStack. Red Hat's approach is to make OpenStack easy to manage, without compromising on the "Open" part of OpenStack. If management of OpenStack can be simpler and the learning curve brought down then it has a real chance to be the next-gen virtualization platform. What company wouldn't want to be able to consume their internal IT resources like using AWS, GCE or Azure if they didn't give up anything to do so? We aren't there yet but Red Hat is making bold strides and as you will see in this article, is on a journey to make OpenStack consumable for everyone!</p>
<p><!--more--></p>
<h2>Red Hat OpenStack Platform</h2>
<p>The Red Hat OpenStack platform uses director to build, manage and upgrade Red Hat OpenStack. Director is in fact a minimal OpenStack deployment itself, with everything needed to deploy OpenStack. The main piece outside of the OpenStack core (Nova, Neutron, Glance, Swift and Heat) is Ironic. The Ironic project is focused on baremetal-as-a-service.</p>
<p>Director allows you to add physical nodes to Ironic and assign them OpenStack roles: compute, control, storage, network, etc. Once roles are assigned an OpenStack environment can be deployed, upgraded and even scaled. As mentioned director is a complete life-cycle management tool that uses OpenStack to manage OpenStack.</p>
<p>In this article we will deploy director (undercloud) on a single VM. We will add three baremetal nodes (VMs) and then deploy OpenStack (overcloud) in a minimal configuration (1 controller node and 1 compute node). I am able to run this on a laptop with just 12GB RAM.</p>
<h2>Lab Environment</h2>
<p>My idea for this configuration was build the most minimal OpenStack environment possible, something that would run on my laptop with just 12GB RAM using Red Hat OpenStack Director. In the end this experiment was successful and the configuration used is as follows:</p>
<ul>
<li><strong>KVM Hypervisor Physical Laptop:</strong> RHEL 7.3, CentOS or Fedora, Dual core, 12 GB RAM and 250GB disk</li>
<li><strong>Undercloud VM: </strong>RHEL 7.3, 2x vCPUs, 4GB RAM, 1 x NIC 8(provisioning), 1 x NIC (external) and 40GB disk</li>
<li><strong>Overcloud Controller VM:</strong> RHEL 7.3, 2 x vCPUs, 6GB RAM, 1 x NIC (provisioning), 2 x NICs (external) and 30GB disk</li>
<li><strong>Overcloud Compute VM:</strong> RHEL 7.3, 2 x vCPU, 4GB RAM, 1 x NIC (provisioning), 2 x NICs (external) and 20GB disk</li>
</ul>
<p><img class="alignnone size-full wp-image-9473" src="/assets/2017/04/osp_8_lab_network_setup2.png" alt="osp_8_lab_network_setup2" width="440" height="255" /></p>
<h2>Networking Setup</h2>
<p>In this configuration we are using virtual networks provided by the hypervisor host (my laptop). Create provisioning and external networks on KVM Hypervisor host. Ensure that NAT forwarding is enabled and DHCP is disabled on the external network. We run OpenStack overcloud on the external network. The provisioning network should be non-routable and DHCP disabled. The undercloud will handle DHCP services for the provisioning network and other IPs will be statically assigned.</p>
<p><strong>[Hypervisor]</strong></p>
<p>Create external network for the overcloud.</p>
<pre>[ktenzer@ktenzer ~]$ cat &gt; /tmp/external.xml &lt;&lt;EOF
&lt;network&gt;
   &lt;name&gt;external&lt;/name&gt;
   &lt;forward mode='nat'&gt;
      &lt;nat&gt; &lt;port start='1024' end='65535'/&gt;
      &lt;/nat&gt;
   &lt;/forward&gt;
   &lt;ip address='192.168.122.1' netmask='255.255.255.0'&gt;
   &lt;/ip&gt;
&lt;/network&gt;
</pre>
<p>Note: hypervisor is 192.168.122.1 and reachable via this IP from undercloud.</p>
<pre>[ktenzer@ktenzer ~]$ sudo virsh net-define /tmp/external.xml
[ktenzer@ktenzer ~]$ sudo virsh net-autostart external
[ktenzer@ktenzer ~]$ sudo virsh net-start external</pre>
<p>Create provisioning network for undercloud.</p>
<p>Note: gateway is 192.168.126.254 as we will use 192.168.126.1 as IP for the VM running our undercloud.</p>
<pre>[ktenzer@ktenzer ~]$ cat &gt; /tmp/provisioning.xml &lt;&lt;EOF
&lt;network&gt;
   &lt;name&gt;provisioning&lt;/name&gt;
   &lt;ip address='192.168.126.254' netmask='255.255.255.0'&gt;
   &lt;/ip&gt;
&lt;/network&gt;</pre>
<pre>[ktenzer@ktenzer ~]$ sudo virsh net-define /tmp/provisioning.xml
[ktenzer@ktenzer ~]$ sudo virsh net-autostart provisioning
[ktenzer@ktenzer ~]$ sudo virsh net-start provisioning</pre>
<h2>Deploy Undercloud</h2>
<p>First install Red Hat Enterprise Linux (RHEL) 7.3 on undercloud VM. Register with subscription manager and configure required RPM repositories for Red Hat OpenStack Platform.</p>
<p><strong>[Undercloud]</strong></p>
<pre class="screen">[root@director ~]# subscription-manager register</pre>
<pre class="screen">[root@director ~]#subscription-manager list --available \
subscription-manager attach --pool=</pre>
<pre class="screen">[root@director ~]# subscription-manager repos --disable=*</pre>
<pre class="screen">[root@director ~]# subscription-manager repos --enable=rhel-7-server-rpms \
--enable=rhel-7-server-extras-rpms \ 
--enable=rhel-7-server-rh-common-rpms \ 
--enable=rhel-ha-for-rhel-7-server-rpms \ 
--enable=rhel-7-server-openstack-10-rpms</pre>
<p>Update all pacakges and reboot.</p>
<pre class="screen">[root@director ~]# yum update -y</pre>
<pre class="screen">[root@director ~]# systemctl reboot</pre>
<p>Install Director Packages.</p>
<pre class="screen">[root@director ~]# yum install -y python-tripleoclient</pre>
<p>Ensure host is defined in /etc/hosts.</p>
<pre class="screen">[root@director ~]# vi /etc/hosts 
192.168.122.90 ospd.lab.com ospd</pre>
<p>Create Stack User.</p>
<pre class="screen">[root@director ~]# useradd stack
[root@director ~]# passwd stack  # specify a password</pre>
<p>Configure user with sudo permissions.</p>
<pre class="screen">[root@director ~]# echo "stack ALL=(root) NOPASSWD:ALL" | tee -a /etc/sudoers.d/stack
[root@director ~]# chmod 0440 /etc/sudoers.d/stack</pre>
<p>Switch to new stack user.</p>
<pre class="screen">[root@director ~]# su - stack
[stack@director ~]$</pre>
<p>Create directories for images and templates. Images are used to boot initial systems and provide baseline OS. Templates are used to customize deployment.</p>
<pre class="screen">[stack@director ~]$ mkdir ~/images</pre>
<pre class="screen">[stack@director ~]$ mkdir ~/templates</pre>
<p>Configure Director using the sample.</p>
<pre class="screen">[stack@director ~]$ cp /usr/share/instack-undercloud/undercloud.conf.sample \
~/undercloud.conf</pre>
<p>In my environment the 192.168.126.0/24 network is the undercloud network and used for provisioning as well as deploying the overcloud.</p>
<pre class="screen">[stack@undercloud ~]$ vi ~/undercloud.conf
 [DEFAULT]
 local_ip = 192.168.126.1/24
 undercloud_public_vip = 192.168.126.2
 undercloud_admin_vip = 192.168.126.3
 local_interface = eth1
 masquerade_network = 192.168.126.0/24
 dhcp_start = 192.168.126.100
 dhcp_end = 192.168.126.150
 network_cidr = 192.168.126.0/24
 network_gateway = 192.168.126.1
 inspection_iprange = 192.168.126.130,192.168.126.99
 generate_service_certificate = true
 certificate_generation_ca = local</pre>
<p>Install the undercloud.</p>
<pre>[stack@odpd ~]$ openstack undercloud install
#############################################################################
 Undercloud install complete.
The file containing this installation's passwords is at
 /home/stack/undercloud-passwords.conf.
There is also a stackrc file at /home/stack/stackrc.
These files are needed to interact with the OpenStack services, and should be
 secured.
#############################################################################</pre>
<p>Import overcloud images.</p>
<pre>[stack@odpd ~]$ source stackrc</pre>
<pre>[stack@odpd ~]$ sudo yum install -y \
rhosp-director-images rhosp-director-images-ipa</pre>
<pre>[stack@odpd ~]$ cd ~/images $ for i in \
/usr/share/rhosp-director-images/overcloud-full-latest-10.0.tar \
/usr/share/rhosp-director-images/ironic-python-agent-latest-10.0.tar; \
do tar -xvf $i; done</pre>
<pre>[stack@odpd ~]$ openstack overcloud image upload --image-path \
/home/stack/images/</pre>
<p>Configure DNS on undercloud network.</p>
<pre class="screen">[stack@odpd ~]$ neutron subnet-list
+--------------------------------------+------+------------------+--------------------------------------------------------+
| id | name | cidr | allocation_pools |
+--------------------------------------+------+------------------+--------------------------------------------------------+
| 294ff536-dc8b-49a3-8327-62d9792d30a6 | | 192.168.126.0/24 | {"start": "192.168.126.100", "end": "192.168.126.200"} |
+--------------------------------------+------+------------------+--------------------------------------------------------+</pre>
<pre>[stack@odpd ~]$ neutron subnet-update 294ff536-dc8b-49a3-8327-62d9792d30a6 \
--dns-nameserver 8.8.8.8</pre>
<p><strong>[Hypervisor]</strong></p>
<p>Registering Overcloud Nodes. Create VM hulls in KVM using virsh on hypervisor host.</p>
<p>Note: You will need to change the disk path to suit your needs.</p>
<pre>ktenzer$ cd /home/ktenzer/VirtualMachines
ktenzer$ sudo for i in {1..3}; do qemu-img create -f qcow2 -o preallocation=metadata overcloud-node$i.qcow2 60G; done
ktenzer$ sudo for i in {1..3}; do virt-install --ram 4096 --vcpus 4 --os-variant rhel7 --disk path=/home/ktenzer/VirtualMachines/overcloud-node$i.qcow2,device=disk,bus=virtio,format=qcow2 --noautoconsole --vnc --network network:provisioning --network network:external --network network:external --name overcloud-node$i --cpu SandyBridge,+vmx --dry-run --print-xml &gt; /tmp/overcloud-node$i.xml; virsh define --file /tmp/overcloud-node$i.xml; done</pre>
<p><strong>[Undercloud]</strong></p>
<p>Copy ssh key from undercloud system to KVM hypervisor host for stack user.</p>
<pre>[stack@odpd ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub stack@192.168.122.1</pre>
<p>Save the MAC addresses of the NICs used for provisioning.</p>
<p>Note: Ironic needs to know what MAC addresses a node has associated for provisioning network.</p>
<pre class="screen">[stack@odpd images]$ for i in {1..3}; do virsh \
-c qemu+ssh://stack@192.168.122.1/system domiflist overcloud-node$i \
| awk '$3 == "provisioning" {print $5};'; done &gt; /tmp/nodes.txt</pre>
<pre class="screen">[stack@odpd images]$ cat /tmp/nodes.txt 
52:54:00:7e:d8:01
52:54:00:f6:a6:73
52:54:00:c9:b2:84</pre>
<pre class="screen">[stack@undercloud ~]$ jq . &lt;&lt; EOF &gt; ~/instackenv.json
{
  "ssh-user": "stack",
  "ssh-key": "$(cat ~/.ssh/id_rsa)",
  "power_manager": "nova.virt.baremetal.virtual_power_driver.VirtualPowerManager",
  "host-ip": "192.168.122.1",
  "arch": "x86_64",
  "nodes": [
    {
      "pm_addr": "192.168.122.1",
      "pm_password": "$(cat ~/.ssh/id_rsa)",
      "pm_type": "pxe_ssh",
      "mac": [
        "$(sed -n 1p /tmp/nodes.txt)"
      ],
      "cpu": "2",
      "memory": "4096",
      "disk": "60",
      "arch": "x86_64",
      "pm_user": "stack"
    },
    {
      "pm_addr": "192.168.122.1",
      "pm_password": "$(cat ~/.ssh/id_rsa)",
      "pm_type": "pxe_ssh",
      "mac": [
        "$(sed -n 2p /tmp/nodes.txt)"
      ],
      "cpu": "4",
      "memory": "2048",
      "disk": "60",
      "arch": "x86_64",
      "pm_user": "stack"
    },
    {
      "pm_addr": "192.168.122.1",
      "pm_password": "$(cat ~/.ssh/id_rsa)",
      "pm_type": "pxe_ssh",
      "mac": [
        "$(sed -n 3p /tmp/nodes.txt)"
      ],
      "cpu": "4",
      "memory": "2048",
      "disk": "60",
      "arch": "x86_64",
      "pm_user": "stack"
    }
  ] 
} 
EOF</pre>
<p>Validate introspection configuration.</p>
<pre>[stack@odpd ~]$ curl -O 
https://raw.githubusercontent.com/rthallisey/clapper/master/instackenv-validator.py</pre>
<p>Import nodes into Ironic and set them to bootable.</p>
<pre>[stack@odpd ~]$ openstack baremetal import --json ~/instackenv.json</pre>
<pre>[stack@odpd ~]$ openstack baremetal configure boot</pre>
<pre>[stack@odpd ~]$ openstack baremetal node list
 +--------------------------------------+------+---------------+-------------+--------------------+-------------+
 | UUID | Name | Instance UUID | Power State | Provisioning State | Maintenance |
 +--------------------------------------+------+---------------+-------------+--------------------+-------------+
 | ea61b158-9cbd-46d2-93e9-eadaccb1589b | None | None | power off | available | False |
 | 11aaf849-361e-4bda-81f5-74c245f554af | None | None | power off | available | False |
 | 275448c1-aa8d-4854-bb3b-bc73e1e1a794 | None | None | power off | available | False |
 +--------------------------------------+------+---------------+-------------+--------------------+-------------+</pre>
<p>Set nodes to managed.</p>
<pre>[stack@odpd ~]$ for node in $(openstack baremetal node list -c UUID \
-f value) ; do openstack baremetal node manage $node ; done</pre>
<p>List nodes.</p>
<pre>[stack@odpd ~]$ openstack baremetal node list
 +--------------------------------------+------+---------------+-------------+--------------------+-------------+
 | UUID | Name | Instance UUID | Power State | Provisioning State | Maintenance |
 +--------------------------------------+------+---------------+-------------+--------------------+-------------+
 | ea61b158-9cbd-46d2-93e9-eadaccb1589b | None | None | power off | manageable | False |
 | 11aaf849-361e-4bda-81f5-74c245f554af | None | None | power off | manageable | False |
 | 275448c1-aa8d-4854-bb3b-bc73e1e1a794 | None | None | power off | manageable | False |
 +--------------------------------------+------+---------------+-------------+--------------------+-------------+</pre>
<p>Run introspection against all managed nodes.</p>
<p>Note: Nodes are booted using ramdisk and their hardware inspected. Introspection prepares nodes for deployment into overcloud.</p>
<pre>[stack@odpd ~]$ openstack overcloud node introspect --all-manageable \
--provide

</pre>
<p>Tag Control Nodes.</p>
<p>Note: tagging nodes allows us to associate a node with a specific role in the overcloud.</p>
<pre>[stack@odpd ~]$ openstack baremetal node set \
--property capabilities='profile:control,boot_option:local' \
0e30226f-f208-41d3-9780-15fa5fdabbde</pre>
<p>Tag Compute Nodes.</p>
<pre>[stack@odpd ~]$ openstack baremetal node set \
--property capabilities='profile:compute,boot_option:local' \
cd3e3422-e7db-45c7-9645-858503a2cdc8</pre>
<pre>[stack@odpd ~]$ openstack baremetal node set \
--property capabilities='profile:compute,boot_option:local' \
5078e1c1-fbe5-4d7f-a222-0c0fd32af423</pre>
<p>Check Overcloud Profiles.</p>
<pre>[stack@odpd ~]$ openstack overcloud profiles list 
+--------------------------------------+-----------+-----------------+-----------------+-------------------+ 
| Node UUID | Node Name | Provision State | Current Profile | Possible Profiles | 
+--------------------------------------+-----------+-----------------+-----------------+-------------------+ 
| 0e30226f-f208-41d3-9780-15fa5fdabbde | | available | control | | 
| cd3e3422-e7db-45c7-9645-858503a2cdc8 | | available | compute | | 
| 5078e1c1-fbe5-4d7f-a222-0c0fd32af423 | | available | compute | | 
+--------------------------------------+-----------+-----------------+-----------------+-------------------+</pre>
<h2>Deploy Overcloud</h2>
<p>There are two ways to deploy overcloud 1) default 2) customize. You will pretty much always want to customize your deployment but for starting out the default method can be a good way to simplify things and rule out potential problems. I recommend always doing default install just to get a baseline working environment and then throwing it away, redeploying with a customized install</p>
<p><strong>[Undercloud]</strong></p>
<p><strong>Option 1: Default Deployment</strong></p>
<p>The default deployment will put the overcloud on the provisioning network. That means you end up with one network hosting both undercloud and overcloud. The external network is not used.</p>
<pre>[stack@odpd ~]$ openstack overcloud deploy --templates --control-scale 1 \
--compute-scale 1 --neutron-tunnel-types vxlan --neutron-network-type vxlan</pre>
<p><strong>Option 2: Customized Deployment</strong></p>
<p>The really nice thing about director is you have a high degree of customization. In this example we are setting overcloud up on a single 192.168.122.0/24 network. However normally you would have separate networks for OpenStack management, API, public, storage, etc.</p>
<p>Clone my github repository.</p>
<pre>[stack@odpd ~]$ git clone https://github.com/ktenzer/openstack-heat-templates.git</pre>
<p>Copy my templates to your local ~/templates directory.</p>
<pre>[stack@odpd ~]$ cp ~/openstack-heat-templates/director/lab/osp10/templates/* ~/templates</pre>
<p>Deploy overcloud using templates.</p>
<pre>[stack@odpd ~]$ openstack overcloud deploy --templates \
-e /usr/share/openstack-tripleo-heat-templates/environments/network-isolation.yaml \
-e ~/templates/network-environment.yaml \ 
-e /usr/share/openstack-tripleo-heat-templates/environments/low-memory-usage.yaml \ 
-e ~/templates/firstboot-environment.yaml --control-scale 1 \ 
--compute-scale 1 --control-flavor control \ 
--compute-flavor compute --ntp-server pool.ntp.org \ 
--neutron-network-type vxlan --neutron-tunnel-types vxlan

2017-04-12 14:46:11Z [overcloud.AllNodesDeploySteps]: CREATE_COMPLETE Stack CREATE completed successfully
2017-04-12 14:46:12Z [overcloud.AllNodesDeploySteps]: CREATE_COMPLETE state changed
2017-04-12 14:46:12Z [overcloud]: CREATE_COMPLETE Stack CREATE completed successfully

Stack overcloud CREATE_COMPLETE

Started Mistral Workflow. Execution ID: 000ecec3-46aa-4e3f-96d9-8a240d34d6aa
/home/stack/.ssh/known_hosts updated.
Original contents retained as /home/stack/.ssh/known_hosts.old
Overcloud Endpoint: http://192.168.122.106:5000/v2.0
Overcloud Deployed</pre>
<p>List overcloud nodes.</p>
<pre>[stack@odpd ~]$ nova list
+--------------------------------------+------------------------+--------+------------+-------------+--------------------------+
| ID | Name | Status | Task State | Power State | Networks |
+--------------------------------------+------------------------+--------+------------+-------------+--------------------------+
| 1e286764-9334-4ecd-9baf-e37a49a4fbd5 | overcloud-compute-0 | ACTIVE | - | Running | ctlplane=192.168.126.106 |
| a21a14f5-94df-4a3a-8629-ba8d851525ff | overcloud-controller-0 | ACTIVE | - | Running | ctlplane=192.168.126.103 |
+--------------------------------------+------------------------+--------+------------+-------------+--------------------------+</pre>
<p>Connect to overcloud controller from undercloud.</p>
<pre>[stack@odpd ~]$ ssh heat-admin@192.168.126.103</pre>
<p><strong>[Overcloud Controller]</strong></p>
<p>Get overcloud admin password.</p>
<p>Overcloud parameters generated during deployment such as password are stored in hiera.</p>
<pre>[root@overcloud-controller-0 ~]$ sudo -i</pre>
<pre>[root@overcloud-controller-0 ~]# hiera keystone::admin_password
<strong>HngV6vc4ZP2bZ78ePfgWAvHAh</strong></pre>
<p><strong>[Undercloud]</strong></p>
<p>Create overcloud keystone source file.</p>
<pre>[stack@odpd ~]$ vi overcloudrc
export OS_NO_CACHE=True
export OS_CLOUDNAME=overcloud
export OS_AUTH_URL=http://192.168.122.106:5000/v2.0
export NOVA_VERSION=1.1
export COMPUTE_API_VERSION=1.1
export OS_USERNAME=admin
export OS_PASSWORD=<strong>HngV6vc4ZP2bZ78ePfgWAvHAh</strong>
export PYTHONWARNINGS="ignore:Certificate has no, ignore:A true SSLContext object is not available"
export OS_TENANT_NAME=admin
export PS1='[\u@\h \W]$ '</pre>
<p>Source overcloudrc.</p>
<pre>[stack@odpd ~]$ source overcloudrc</pre>
<p>List hypervisor hosts in overcloud.</p>
<pre>[stack@odpd ~]$ nova hypervisor-list
+----+---------------------------------+-------+---------+
| ID | Hypervisor hostname | State | Status |
+----+---------------------------------+-------+---------+
| 1 | overcloud-compute-0.localdomain | up | enabled |
+----+---------------------------------+-------+---------+</pre>
<h2>Troubleshooting Deployment</h2>
<p>Let's face it in OpenStack there is a lot that can go wrong. I like this quote from Dirk Wallerstorfer.</p>
<blockquote><p><em>"In short, OpenStack networking is a lot like Venice—there are masquerades and bridges all over the place!"</em></p>
<p>-Dirk Wallerstorfer</p>
<p>source: <a href="https://www.dynatrace.com/blog/openstack-network-mystery-2-bytes-cost-me-two-days-of-trouble/">https://www.dynatrace.com/blog/openstack-network-mystery-2-bytes-cost-me-two-days-of-trouble/</a></p></blockquote>
<p><strong>[Undercloud]</strong></p>
<p>Red Hat is making it much easier to troubleshoot deployment problems.While the deployment is running you can follow along in Heat by showing nested steps.</p>
<pre>[stack@odpd ~]$ heat stack-list --show-nested</pre>
<p>If for some reason the deployment fails, there is now a command to gather up all the information to make it really easy to find out what happened.</p>
<pre>[stack@odpd ~]$ openstack stack failures list --long overcloud</pre>
<h2>Summary</h2>
<p>OpenStack is the way of the future for virtualization platforms and I think in the future many traditional virtualization environments will be moving to OpenStack. The choice is simple either they will stay on-premise and become OpenStack or move to public cloud. Of course there will be those that stick with traditional virtualization, there are still lots and lots of mainframes around but clear trend will be to public cloud or OpenStack. The only thing holding OpenStack back is complexity and manageability. Red Hat is focused on making OpenStack simple without losing the "Open" in OpenStack. In other words without compromising on what makes OpenStack a great cloud computing platform. As you have seen in this article Red Hat OpenStack Platform is making great strides and the fact that you can setup an OpenStack environment using enterprise production grade tooling on a 12GB RAM laptop is a good sign.</p>
<p>Happy OpenStacking!</p>
<p>(c) 2017 Keith Tenzer</p>
<p>&nbsp;</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#ironic" class="page__taxonomy-item" rel="tag">Ironic</a><span class="sep">, </span>
    
      <a href="/tags/#newton" class="page__taxonomy-item" rel="tag">Newton</a><span class="sep">, </span>
    
      <a href="/tags/#openstack" class="page__taxonomy-item" rel="tag">OpenStack</a><span class="sep">, </span>
    
      <a href="/tags/#openstack-director" class="page__taxonomy-item" rel="tag">OpenStack Director</a><span class="sep">, </span>
    
      <a href="/tags/#rhosp" class="page__taxonomy-item" rel="tag">RHOSP</a><span class="sep">, </span>
    
      <a href="/tags/#tripleo" class="page__taxonomy-item" rel="tag">TripleO</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#openstack" class="page__taxonomy-item" rel="tag">OpenStack</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2017-04-20T00:00:00-07:00">April 20, 2017</time></p>


      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?via=keithtenzer&text=Red+Hat+OpenStack+Platform+10+%28Newton%29+Installation+and+Configuration+Guide%20http%3A%2F%2Flocalhost%3A4000%2Fopenstack%2Fred-hat-openstack-platform-10-newton-installation-and-configuration-guide%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fopenstack%2Fred-hat-openstack-platform-10-newton-installation-and-configuration-guide%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fopenstack%2Fred-hat-openstack-platform-10-newton-installation-and-configuration-guide%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/openshift/storage-for-containers-using-ceph-rbd-part-iv/" class="pagination--pager" title="Storage for Containers Using Ceph RBD - Part IV
">Previous</a>
    
    
      <a href="/rhev/rhev-4-1-lab-installation-and-configuration-guide/" class="pagination--pager" title="RHV 4.1 Lab Installation and Configuration Guide
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/temporal/my-first-day-at-temporal/" rel="permalink">My First Day at Temporal
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2022-08-15T00:00:00-07:00">August 15, 2022</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          9 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
Overview
Today is my first day at temporal and with that I wanted to share some thoughts around my decision, why Temporal and my experience thus far. As you...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/linux/blog-with-gitops-practices-and-github/" rel="permalink">Blog with Gitops Practices and GitHub
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2022-02-10T00:00:00-08:00">February 10, 2022</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          13 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Overview
Want to build your brand, while living the gitops revolution and not paying anything for it? That is exactly what this article will walk you through...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/linux/The-Fedora-Workstation-Experience/" rel="permalink">The Fedora Workstation Experience
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2022-01-10T00:00:00-08:00">January 10, 2022</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          10 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
Overview
A lot of people always ask me what is the best way to contribute to opensource? Of course contributing code, documentation, spreading the gospel or...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/openshift/building-ansible-operators-1-2-3/" rel="permalink">Building Ansible Operators 1-2-3
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2021-12-03T00:00:00-08:00">December 3, 2021</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          10 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Overview
In this article we will go step by step to build a Kubernetes Operator using Ansible and the Operator Framework. Operators provide the ability to no...</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="https://twitter.com/keithtenzer"" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
      
        
          <li><a href="https://github.com/ktenzer" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2022 Keith Tenzer. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>







  </body>
</html>
