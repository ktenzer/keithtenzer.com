<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.22.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
<link rel="icon" href="/assets/main/me.png">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Pacemaker - The Open Source, High Availability Cluster - Keith Tenzer’s Blog</title>
<meta name="description" content="Overview Pacemaker is a Open Source, High Availability cluster. Pacemaker and in general Linux clustering have been around for a very long time. Both matured greatly over the past 10 to 15 years.Today Pacemaker is a very simple streamlined bundle that includes the Pacemker clustering, fencing agents, resource agents and the heartbeat (corrosync). Pacemaker is available in Red Hat Enterprise Linux 7 as the High Availability option. In this article I will provide an overview of Pacemaker and a tutorial on how to setup a two-node pacemaker cluster for Apache using shared storage.   Pacemaker Basics As mentioned Pacemaker has a few components: clustering, fence agents, resource agents and corrosync. Clustering Pacemaker provides all the packages to configure and manage a high availability cluster using the CLI or GUI. Fence Agents Fencing is about shutting off a node that has become unstable or unresponsive so that it cannot damage the cluster or any cluster resources. The main reason we have fencing is to illiminate the possibility of a split-brain where multiple nodes access resources at same time. A split-brain can lead to data corruption and general cluster malfunction. There are two types of fencing agents in Pacemaker: power and storage. The most commonly used fencing agent is power. These agents connect to hardware such as UPS, blade chasis, iLO cards, etc and are responsible for fencing a node in the event that it becomes unresponsive. The other type of fencing is storage based. Typically storage-based fencing uses SCSI-3 PR (Persistent Reservation) that ensures only one node can ever write or access storage at time. This requires of course that the shared storage is used and that it supports SCSI-3 PR. The daemon or service responsible for fencing in Pacemaker is stonith (shoot the other node in the head). Open source clustering in regards to fencing design differs slightly from commercial clustering. Open source has always taken a very conservative approach and IMHO that is a good thing. The last thing you want it data corruption. If fencing does not work Pacemaker will make the entire cluster unavailable. This means manual intervention will be required to bring resources online but your data is safe. Commercial solutions have very elaborate and complex fencing proceadures and try to always ensure failover is automated even if a problem occurs. I am not saying commercial software isn&#39;t bullet-proof, just that there is a design difference in this regard. Resource Agents Pacemaker resource agents are packages that integrate applications. Resource agents understand a specific application and it&#39;s dependencies. As such using resource agent (if one exists) makes configuration of applications much simpler. It also ensures that best practice around clustering for given application are enforced. Corrosync Pacemaker requires a heartbeat for internal communications . The corrosync daemon provides inter-cluster communications between cluster nodes. It is also responsible for quorum if a quorum is used. Pacemaker Tutortial Now that we have a high-level understanding of Pacemaker it is time to get our hands a bit dirty and configure a cluster. For this tutorial we will use a simple example of a two-node cluster for Apache. We will configure the cluster, setup storage-based fencing and configure a resource group for Apache. Install Pacemaker Perform following steps on both cluster nodes  Install RHEL / CentOS 7.1 (minimal) Configure subscription and repos (RHEL 7)  #subscription-manager register #subscription-manager list --available #subscription-manager attach --pool=&lt;pool id&gt; #subscription-manager repos --enable=rhel-ha-for-rhel-7-server-rpms  Install Pacemaker packages  #yum update -y #yum install -y pcs fence-agents-all  Open firewall ports  #firewall-cmd --permanent --add-service=high-availability #firewall-cmd --reload  Set hacluster password  #echo CHANGEME | passwd --stdin hacluster  Enable services  #systemctl start pcsd.service #systemctl enable pcsd.service  Configure ISCSI client  #yum install -y iscsi-initiator-utils #vi /etc/iscsi/initiatorname.iscsi InitiatorName=iqn.2015-06.com.lab:&lt;hostname&gt; Setup Shared ISCSI Storage These steps are optional if you have ISCSI storage already configured. In these steps we will configure a third RHEL / CentOS system to provide shared storage using ISCSI.  Install RHEL / CentOS 7.1 (minimal) Install ISCSI packages  #yum install -y targetcli  Enable ISCSI service  #systemctl enable target  Create LVM disk (this will be the shared storage device)  #fdisk /dev/vdb (create new partition of type LVM) #pvcreate /dev/vdb1 #vgcreate cluster_vg /dev/vdb1 #lvcreate -L 1G cluster_vg -n cluster_disk1 #lvcreate -L 990M cluster_vg -n cluster_disk1 #mkfs -t ext4 /dev/cluster_vg/cluster_disk1  Configure ISCSI target  # targetcli /&gt; backstores/block create disk1 /dev/cluster_vg/cluster_disk1 /&gt; iscsi/ create iqn.2015-06.com.lab:rhel7 /&gt; /iscsi/iqn.2015-06.com.lab:rhel7/tpg1/portals/ create /&gt; iscsi/iqn.2015-06.com.lab:rhel7/tpg1/luns create /backstores/block/disk1 /&gt; iscsi/iqn.2015-06.com.lab:rhel7/tpg1/acls create iqn.2015-06.com.lab:pm-node1 /&gt; iscsi/iqn.2015-06.com.lab:rhel7/tpg1/acls create iqn.2015-06.com.lab:pm-node2 /&gt; exit  Open firewall ports  #firewall-cmd --permanent --add-port=3260/tcp #firewall-cmd --reload Create Cluster At this point both cluster nodes have all the cluster packages and have access to shared ISCSI storage. In this section we will configure the cluster on one of the nodes.  Authorize cluster nodes  #pcs cluster auth pm-node1.lab.com pm-node2.lab.com Username: hacluster Password: pm-node1.lab.com: Authorized pm-node2.lab.com: Authorized  Setup the cluster  #pcs cluster setup --start --name mycluster pm-node1.lab.com pm-node2.lab.com  Enable services  #pcs cluster enable --all  Check cluster status  # pcs cluster status Cluster Status: Last updated: Fri Jun 19 14:10:24 2015 Last change: Fri Jun 19 14:09:15 2015 Stack: corosync Current DC: pm-node1.lab.com (1) - partition with quorum Version: 1.1.12-a14efad 2 Nodes configured 0 Resources configured PCSD Status: pm-node1.lab.com: Online pm-node2.lab.com: Online Storage Fencing Before creating resource groups fencing needs to be configured. In this example we will use storage fencing and fence_scsi. Fencing is to be configured on one of the nodes.  Configure stonith  #pcs stonith create scsi fence_scsi pcmk_host_list=&quot;pm-node1.lab.com pm-node2.lab.com&quot; pcmk_monitor_action=&quot;metadata&quot; pcmk_reboot_action=&quot;off&quot;devices=&quot;/dev/mapper/cluster_vg-disk1&quot; meta provides=&quot;unfencing&quot;  Check status of fencing  #pcs stonith show  scsi (stonith:fence_scsi): Started Resource Group Now that the cluster is running and fencing has been configured we can setup the resource group. A resource group defines application dependencies and ensures application is started correctly in the event of a failover. A resource group is to be configured on one of the nodes.  Install application packages (Apache)  #yum install -y httpd wget  Open firewall ports  #firewall-cmd --permanent --add-service=http #fireall-cmd --reload  Configure Apache  #vi /etc/httpd/conf/httpd.conf &lt;Location /server-status&gt; SetHandler server-status Order deny,allow Deny from all Allow from 127.0.0.1 &lt;/Location&gt;  Mount shared storage  # mount /dev/cluster_vg /disk1 /var/www/ # mkdir /var/www/html # mkdir /var/www/cgi-bin # mkdir /var/www/error # restorecon -R /var/www # cat &lt;&lt;-END&gt;/var/www/html /index.html &lt;html&gt; &lt;body&gt;Hello&lt;/body&gt; &lt;/html&gt; END #umount /var/www  Configure LVM so it only starts volumes not owned by the cluster  Note: it is important to not allow LVM to start the cluster owned volume group, in this case cluster_vg. #vi /etc/lvm/lvm.conf volume_list = [ &quot;rhel&quot; ] use_lvmetad=0  Ensure boot image does not try and control cluster volume  #dracut -H -f /boot/initramfs-$(uname -r).img $(uname -r)  Create resource for LVM disk  #pcs resource create disk1 LVM volgrpname= cluster_vg exclusive= true --group apachegroup  Create resource for filesystem  #pcs resource create apache_fs Filesystem device=&quot;/dev/cluster_vg/disk1&quot; directory=&quot;/var/www&quot; fstype=&quot;ext4 &quot; --group apachegroup  Create resource for virtual IP address  #pcs resource create VirtualIP IPaddr2 ip=192.168.122.52 cidr_netmask=24 --group apachegroup  Create resource for website  #pcs resource create Website apache configfile=&quot;/etc/httpd/conf/httpd.conf&quot; statusurl=&quot;http://127.0.0.1/server-status&quot; --group apachegroup At this point the cluster should be configured and look somthing similar to our example. #pcs status Cluster name: mycluster Last updated: Mon Jun 22 14:47:49 2015 Last change: Mon Jun 22 12:25:14 2015 Stack: corosync Current DC: pm-node2.lab.com (2) - partition with quorum Version: 1.1.12-a14efad 2 Nodes configured 5 Resources configured Online: [ pm-node1.lab.com pm-node2.lab.com ] Full list of resources: Resource Group: apachegroup  disk1 (ocf::heartbeat:LVM): Started pm-node1.lab.com   VirtualIP (ocf::heartbeat:IPaddr2): Started pm-node1.lab.com   apache_fs (ocf::heartbeat:Filesystem): Started pm-node1.lab.com   Website (ocf::heartbeat:apache): Started pm-node1.lab.com   scsi (stonith:fence_scsi): Started pm-node2.lab.com PCSD Status:  pm-node1.lab.com: Online  pm-node2.lab.com: Online Daemon Status:  corosync: active/enabled  pacemaker: active/enabled  pcsd: active/enabled In the event that there are problems the &quot;pcs resource debug-start &lt;resource&gt;&quot; command can be used for troubleshooting. #pcs resource debug-start disk1 Pacemaker GUI I was pleasantly surprised with the new Pacemaker GUI. The upstream community has done a terrific job. The GUI can even be used to manage multiple clusters. Below are some screenshots to give you a better idea. To access GUI use the following URL and login as hacluster. https://pm-node1:2224 Below screenshot shows the interface for managing clusters.  Below screenshot shows the interface for managing nodes.  Below screenshot shows the interface for managing resources.  Summary Pacemaker is the result of open source innovation and long maturity. We have learned about Pacemaker basics and even configured a cluster for Apache using shared storage fencing. There is no doubt that Pacemaker is a viable alternative to commercial clustering from HP, IBM, Oracle and Microsoft. If you interested in traditional clustering I highly recommend giving Pacemaker a chance, you won&#39;t be disappointed. Happy Clustering! (c) 2015 Keith Tenzer">


  <meta name="author" content="Keith Tenzer">
  
  <meta property="article:author" content="Keith Tenzer">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Keith Tenzer's Blog">
<meta property="og:title" content="Pacemaker - The Open Source, High Availability Cluster">
<meta property="og:url" content="https://keithtenzer.com/clustering/pacemaker-the-open-source-high-availability-cluster/">


  <meta property="og:description" content="Overview Pacemaker is a Open Source, High Availability cluster. Pacemaker and in general Linux clustering have been around for a very long time. Both matured greatly over the past 10 to 15 years.Today Pacemaker is a very simple streamlined bundle that includes the Pacemker clustering, fencing agents, resource agents and the heartbeat (corrosync). Pacemaker is available in Red Hat Enterprise Linux 7 as the High Availability option. In this article I will provide an overview of Pacemaker and a tutorial on how to setup a two-node pacemaker cluster for Apache using shared storage.   Pacemaker Basics As mentioned Pacemaker has a few components: clustering, fence agents, resource agents and corrosync. Clustering Pacemaker provides all the packages to configure and manage a high availability cluster using the CLI or GUI. Fence Agents Fencing is about shutting off a node that has become unstable or unresponsive so that it cannot damage the cluster or any cluster resources. The main reason we have fencing is to illiminate the possibility of a split-brain where multiple nodes access resources at same time. A split-brain can lead to data corruption and general cluster malfunction. There are two types of fencing agents in Pacemaker: power and storage. The most commonly used fencing agent is power. These agents connect to hardware such as UPS, blade chasis, iLO cards, etc and are responsible for fencing a node in the event that it becomes unresponsive. The other type of fencing is storage based. Typically storage-based fencing uses SCSI-3 PR (Persistent Reservation) that ensures only one node can ever write or access storage at time. This requires of course that the shared storage is used and that it supports SCSI-3 PR. The daemon or service responsible for fencing in Pacemaker is stonith (shoot the other node in the head). Open source clustering in regards to fencing design differs slightly from commercial clustering. Open source has always taken a very conservative approach and IMHO that is a good thing. The last thing you want it data corruption. If fencing does not work Pacemaker will make the entire cluster unavailable. This means manual intervention will be required to bring resources online but your data is safe. Commercial solutions have very elaborate and complex fencing proceadures and try to always ensure failover is automated even if a problem occurs. I am not saying commercial software isn&#39;t bullet-proof, just that there is a design difference in this regard. Resource Agents Pacemaker resource agents are packages that integrate applications. Resource agents understand a specific application and it&#39;s dependencies. As such using resource agent (if one exists) makes configuration of applications much simpler. It also ensures that best practice around clustering for given application are enforced. Corrosync Pacemaker requires a heartbeat for internal communications . The corrosync daemon provides inter-cluster communications between cluster nodes. It is also responsible for quorum if a quorum is used. Pacemaker Tutortial Now that we have a high-level understanding of Pacemaker it is time to get our hands a bit dirty and configure a cluster. For this tutorial we will use a simple example of a two-node cluster for Apache. We will configure the cluster, setup storage-based fencing and configure a resource group for Apache. Install Pacemaker Perform following steps on both cluster nodes  Install RHEL / CentOS 7.1 (minimal) Configure subscription and repos (RHEL 7)  #subscription-manager register #subscription-manager list --available #subscription-manager attach --pool=&lt;pool id&gt; #subscription-manager repos --enable=rhel-ha-for-rhel-7-server-rpms  Install Pacemaker packages  #yum update -y #yum install -y pcs fence-agents-all  Open firewall ports  #firewall-cmd --permanent --add-service=high-availability #firewall-cmd --reload  Set hacluster password  #echo CHANGEME | passwd --stdin hacluster  Enable services  #systemctl start pcsd.service #systemctl enable pcsd.service  Configure ISCSI client  #yum install -y iscsi-initiator-utils #vi /etc/iscsi/initiatorname.iscsi InitiatorName=iqn.2015-06.com.lab:&lt;hostname&gt; Setup Shared ISCSI Storage These steps are optional if you have ISCSI storage already configured. In these steps we will configure a third RHEL / CentOS system to provide shared storage using ISCSI.  Install RHEL / CentOS 7.1 (minimal) Install ISCSI packages  #yum install -y targetcli  Enable ISCSI service  #systemctl enable target  Create LVM disk (this will be the shared storage device)  #fdisk /dev/vdb (create new partition of type LVM) #pvcreate /dev/vdb1 #vgcreate cluster_vg /dev/vdb1 #lvcreate -L 1G cluster_vg -n cluster_disk1 #lvcreate -L 990M cluster_vg -n cluster_disk1 #mkfs -t ext4 /dev/cluster_vg/cluster_disk1  Configure ISCSI target  # targetcli /&gt; backstores/block create disk1 /dev/cluster_vg/cluster_disk1 /&gt; iscsi/ create iqn.2015-06.com.lab:rhel7 /&gt; /iscsi/iqn.2015-06.com.lab:rhel7/tpg1/portals/ create /&gt; iscsi/iqn.2015-06.com.lab:rhel7/tpg1/luns create /backstores/block/disk1 /&gt; iscsi/iqn.2015-06.com.lab:rhel7/tpg1/acls create iqn.2015-06.com.lab:pm-node1 /&gt; iscsi/iqn.2015-06.com.lab:rhel7/tpg1/acls create iqn.2015-06.com.lab:pm-node2 /&gt; exit  Open firewall ports  #firewall-cmd --permanent --add-port=3260/tcp #firewall-cmd --reload Create Cluster At this point both cluster nodes have all the cluster packages and have access to shared ISCSI storage. In this section we will configure the cluster on one of the nodes.  Authorize cluster nodes  #pcs cluster auth pm-node1.lab.com pm-node2.lab.com Username: hacluster Password: pm-node1.lab.com: Authorized pm-node2.lab.com: Authorized  Setup the cluster  #pcs cluster setup --start --name mycluster pm-node1.lab.com pm-node2.lab.com  Enable services  #pcs cluster enable --all  Check cluster status  # pcs cluster status Cluster Status: Last updated: Fri Jun 19 14:10:24 2015 Last change: Fri Jun 19 14:09:15 2015 Stack: corosync Current DC: pm-node1.lab.com (1) - partition with quorum Version: 1.1.12-a14efad 2 Nodes configured 0 Resources configured PCSD Status: pm-node1.lab.com: Online pm-node2.lab.com: Online Storage Fencing Before creating resource groups fencing needs to be configured. In this example we will use storage fencing and fence_scsi. Fencing is to be configured on one of the nodes.  Configure stonith  #pcs stonith create scsi fence_scsi pcmk_host_list=&quot;pm-node1.lab.com pm-node2.lab.com&quot; pcmk_monitor_action=&quot;metadata&quot; pcmk_reboot_action=&quot;off&quot;devices=&quot;/dev/mapper/cluster_vg-disk1&quot; meta provides=&quot;unfencing&quot;  Check status of fencing  #pcs stonith show  scsi (stonith:fence_scsi): Started Resource Group Now that the cluster is running and fencing has been configured we can setup the resource group. A resource group defines application dependencies and ensures application is started correctly in the event of a failover. A resource group is to be configured on one of the nodes.  Install application packages (Apache)  #yum install -y httpd wget  Open firewall ports  #firewall-cmd --permanent --add-service=http #fireall-cmd --reload  Configure Apache  #vi /etc/httpd/conf/httpd.conf &lt;Location /server-status&gt; SetHandler server-status Order deny,allow Deny from all Allow from 127.0.0.1 &lt;/Location&gt;  Mount shared storage  # mount /dev/cluster_vg /disk1 /var/www/ # mkdir /var/www/html # mkdir /var/www/cgi-bin # mkdir /var/www/error # restorecon -R /var/www # cat &lt;&lt;-END&gt;/var/www/html /index.html &lt;html&gt; &lt;body&gt;Hello&lt;/body&gt; &lt;/html&gt; END #umount /var/www  Configure LVM so it only starts volumes not owned by the cluster  Note: it is important to not allow LVM to start the cluster owned volume group, in this case cluster_vg. #vi /etc/lvm/lvm.conf volume_list = [ &quot;rhel&quot; ] use_lvmetad=0  Ensure boot image does not try and control cluster volume  #dracut -H -f /boot/initramfs-$(uname -r).img $(uname -r)  Create resource for LVM disk  #pcs resource create disk1 LVM volgrpname= cluster_vg exclusive= true --group apachegroup  Create resource for filesystem  #pcs resource create apache_fs Filesystem device=&quot;/dev/cluster_vg/disk1&quot; directory=&quot;/var/www&quot; fstype=&quot;ext4 &quot; --group apachegroup  Create resource for virtual IP address  #pcs resource create VirtualIP IPaddr2 ip=192.168.122.52 cidr_netmask=24 --group apachegroup  Create resource for website  #pcs resource create Website apache configfile=&quot;/etc/httpd/conf/httpd.conf&quot; statusurl=&quot;http://127.0.0.1/server-status&quot; --group apachegroup At this point the cluster should be configured and look somthing similar to our example. #pcs status Cluster name: mycluster Last updated: Mon Jun 22 14:47:49 2015 Last change: Mon Jun 22 12:25:14 2015 Stack: corosync Current DC: pm-node2.lab.com (2) - partition with quorum Version: 1.1.12-a14efad 2 Nodes configured 5 Resources configured Online: [ pm-node1.lab.com pm-node2.lab.com ] Full list of resources: Resource Group: apachegroup  disk1 (ocf::heartbeat:LVM): Started pm-node1.lab.com   VirtualIP (ocf::heartbeat:IPaddr2): Started pm-node1.lab.com   apache_fs (ocf::heartbeat:Filesystem): Started pm-node1.lab.com   Website (ocf::heartbeat:apache): Started pm-node1.lab.com   scsi (stonith:fence_scsi): Started pm-node2.lab.com PCSD Status:  pm-node1.lab.com: Online  pm-node2.lab.com: Online Daemon Status:  corosync: active/enabled  pacemaker: active/enabled  pcsd: active/enabled In the event that there are problems the &quot;pcs resource debug-start &lt;resource&gt;&quot; command can be used for troubleshooting. #pcs resource debug-start disk1 Pacemaker GUI I was pleasantly surprised with the new Pacemaker GUI. The upstream community has done a terrific job. The GUI can even be used to manage multiple clusters. Below are some screenshots to give you a better idea. To access GUI use the following URL and login as hacluster. https://pm-node1:2224 Below screenshot shows the interface for managing clusters.  Below screenshot shows the interface for managing nodes.  Below screenshot shows the interface for managing resources.  Summary Pacemaker is the result of open source innovation and long maturity. We have learned about Pacemaker basics and even configured a cluster for Apache using shared storage fencing. There is no doubt that Pacemaker is a viable alternative to commercial clustering from HP, IBM, Oracle and Microsoft. If you interested in traditional clustering I highly recommend giving Pacemaker a chance, you won&#39;t be disappointed. Happy Clustering! (c) 2015 Keith Tenzer">





  <meta name="twitter:site" content="@keithtenzer">
  <meta name="twitter:title" content="Pacemaker - The Open Source, High Availability Cluster">
  <meta name="twitter:description" content="Overview Pacemaker is a Open Source, High Availability cluster. Pacemaker and in general Linux clustering have been around for a very long time. Both matured greatly over the past 10 to 15 years.Today Pacemaker is a very simple streamlined bundle that includes the Pacemker clustering, fencing agents, resource agents and the heartbeat (corrosync). Pacemaker is available in Red Hat Enterprise Linux 7 as the High Availability option. In this article I will provide an overview of Pacemaker and a tutorial on how to setup a two-node pacemaker cluster for Apache using shared storage.   Pacemaker Basics As mentioned Pacemaker has a few components: clustering, fence agents, resource agents and corrosync. Clustering Pacemaker provides all the packages to configure and manage a high availability cluster using the CLI or GUI. Fence Agents Fencing is about shutting off a node that has become unstable or unresponsive so that it cannot damage the cluster or any cluster resources. The main reason we have fencing is to illiminate the possibility of a split-brain where multiple nodes access resources at same time. A split-brain can lead to data corruption and general cluster malfunction. There are two types of fencing agents in Pacemaker: power and storage. The most commonly used fencing agent is power. These agents connect to hardware such as UPS, blade chasis, iLO cards, etc and are responsible for fencing a node in the event that it becomes unresponsive. The other type of fencing is storage based. Typically storage-based fencing uses SCSI-3 PR (Persistent Reservation) that ensures only one node can ever write or access storage at time. This requires of course that the shared storage is used and that it supports SCSI-3 PR. The daemon or service responsible for fencing in Pacemaker is stonith (shoot the other node in the head). Open source clustering in regards to fencing design differs slightly from commercial clustering. Open source has always taken a very conservative approach and IMHO that is a good thing. The last thing you want it data corruption. If fencing does not work Pacemaker will make the entire cluster unavailable. This means manual intervention will be required to bring resources online but your data is safe. Commercial solutions have very elaborate and complex fencing proceadures and try to always ensure failover is automated even if a problem occurs. I am not saying commercial software isn&#39;t bullet-proof, just that there is a design difference in this regard. Resource Agents Pacemaker resource agents are packages that integrate applications. Resource agents understand a specific application and it&#39;s dependencies. As such using resource agent (if one exists) makes configuration of applications much simpler. It also ensures that best practice around clustering for given application are enforced. Corrosync Pacemaker requires a heartbeat for internal communications . The corrosync daemon provides inter-cluster communications between cluster nodes. It is also responsible for quorum if a quorum is used. Pacemaker Tutortial Now that we have a high-level understanding of Pacemaker it is time to get our hands a bit dirty and configure a cluster. For this tutorial we will use a simple example of a two-node cluster for Apache. We will configure the cluster, setup storage-based fencing and configure a resource group for Apache. Install Pacemaker Perform following steps on both cluster nodes  Install RHEL / CentOS 7.1 (minimal) Configure subscription and repos (RHEL 7)  #subscription-manager register #subscription-manager list --available #subscription-manager attach --pool=&lt;pool id&gt; #subscription-manager repos --enable=rhel-ha-for-rhel-7-server-rpms  Install Pacemaker packages  #yum update -y #yum install -y pcs fence-agents-all  Open firewall ports  #firewall-cmd --permanent --add-service=high-availability #firewall-cmd --reload  Set hacluster password  #echo CHANGEME | passwd --stdin hacluster  Enable services  #systemctl start pcsd.service #systemctl enable pcsd.service  Configure ISCSI client  #yum install -y iscsi-initiator-utils #vi /etc/iscsi/initiatorname.iscsi InitiatorName=iqn.2015-06.com.lab:&lt;hostname&gt; Setup Shared ISCSI Storage These steps are optional if you have ISCSI storage already configured. In these steps we will configure a third RHEL / CentOS system to provide shared storage using ISCSI.  Install RHEL / CentOS 7.1 (minimal) Install ISCSI packages  #yum install -y targetcli  Enable ISCSI service  #systemctl enable target  Create LVM disk (this will be the shared storage device)  #fdisk /dev/vdb (create new partition of type LVM) #pvcreate /dev/vdb1 #vgcreate cluster_vg /dev/vdb1 #lvcreate -L 1G cluster_vg -n cluster_disk1 #lvcreate -L 990M cluster_vg -n cluster_disk1 #mkfs -t ext4 /dev/cluster_vg/cluster_disk1  Configure ISCSI target  # targetcli /&gt; backstores/block create disk1 /dev/cluster_vg/cluster_disk1 /&gt; iscsi/ create iqn.2015-06.com.lab:rhel7 /&gt; /iscsi/iqn.2015-06.com.lab:rhel7/tpg1/portals/ create /&gt; iscsi/iqn.2015-06.com.lab:rhel7/tpg1/luns create /backstores/block/disk1 /&gt; iscsi/iqn.2015-06.com.lab:rhel7/tpg1/acls create iqn.2015-06.com.lab:pm-node1 /&gt; iscsi/iqn.2015-06.com.lab:rhel7/tpg1/acls create iqn.2015-06.com.lab:pm-node2 /&gt; exit  Open firewall ports  #firewall-cmd --permanent --add-port=3260/tcp #firewall-cmd --reload Create Cluster At this point both cluster nodes have all the cluster packages and have access to shared ISCSI storage. In this section we will configure the cluster on one of the nodes.  Authorize cluster nodes  #pcs cluster auth pm-node1.lab.com pm-node2.lab.com Username: hacluster Password: pm-node1.lab.com: Authorized pm-node2.lab.com: Authorized  Setup the cluster  #pcs cluster setup --start --name mycluster pm-node1.lab.com pm-node2.lab.com  Enable services  #pcs cluster enable --all  Check cluster status  # pcs cluster status Cluster Status: Last updated: Fri Jun 19 14:10:24 2015 Last change: Fri Jun 19 14:09:15 2015 Stack: corosync Current DC: pm-node1.lab.com (1) - partition with quorum Version: 1.1.12-a14efad 2 Nodes configured 0 Resources configured PCSD Status: pm-node1.lab.com: Online pm-node2.lab.com: Online Storage Fencing Before creating resource groups fencing needs to be configured. In this example we will use storage fencing and fence_scsi. Fencing is to be configured on one of the nodes.  Configure stonith  #pcs stonith create scsi fence_scsi pcmk_host_list=&quot;pm-node1.lab.com pm-node2.lab.com&quot; pcmk_monitor_action=&quot;metadata&quot; pcmk_reboot_action=&quot;off&quot;devices=&quot;/dev/mapper/cluster_vg-disk1&quot; meta provides=&quot;unfencing&quot;  Check status of fencing  #pcs stonith show  scsi (stonith:fence_scsi): Started Resource Group Now that the cluster is running and fencing has been configured we can setup the resource group. A resource group defines application dependencies and ensures application is started correctly in the event of a failover. A resource group is to be configured on one of the nodes.  Install application packages (Apache)  #yum install -y httpd wget  Open firewall ports  #firewall-cmd --permanent --add-service=http #fireall-cmd --reload  Configure Apache  #vi /etc/httpd/conf/httpd.conf &lt;Location /server-status&gt; SetHandler server-status Order deny,allow Deny from all Allow from 127.0.0.1 &lt;/Location&gt;  Mount shared storage  # mount /dev/cluster_vg /disk1 /var/www/ # mkdir /var/www/html # mkdir /var/www/cgi-bin # mkdir /var/www/error # restorecon -R /var/www # cat &lt;&lt;-END&gt;/var/www/html /index.html &lt;html&gt; &lt;body&gt;Hello&lt;/body&gt; &lt;/html&gt; END #umount /var/www  Configure LVM so it only starts volumes not owned by the cluster  Note: it is important to not allow LVM to start the cluster owned volume group, in this case cluster_vg. #vi /etc/lvm/lvm.conf volume_list = [ &quot;rhel&quot; ] use_lvmetad=0  Ensure boot image does not try and control cluster volume  #dracut -H -f /boot/initramfs-$(uname -r).img $(uname -r)  Create resource for LVM disk  #pcs resource create disk1 LVM volgrpname= cluster_vg exclusive= true --group apachegroup  Create resource for filesystem  #pcs resource create apache_fs Filesystem device=&quot;/dev/cluster_vg/disk1&quot; directory=&quot;/var/www&quot; fstype=&quot;ext4 &quot; --group apachegroup  Create resource for virtual IP address  #pcs resource create VirtualIP IPaddr2 ip=192.168.122.52 cidr_netmask=24 --group apachegroup  Create resource for website  #pcs resource create Website apache configfile=&quot;/etc/httpd/conf/httpd.conf&quot; statusurl=&quot;http://127.0.0.1/server-status&quot; --group apachegroup At this point the cluster should be configured and look somthing similar to our example. #pcs status Cluster name: mycluster Last updated: Mon Jun 22 14:47:49 2015 Last change: Mon Jun 22 12:25:14 2015 Stack: corosync Current DC: pm-node2.lab.com (2) - partition with quorum Version: 1.1.12-a14efad 2 Nodes configured 5 Resources configured Online: [ pm-node1.lab.com pm-node2.lab.com ] Full list of resources: Resource Group: apachegroup  disk1 (ocf::heartbeat:LVM): Started pm-node1.lab.com   VirtualIP (ocf::heartbeat:IPaddr2): Started pm-node1.lab.com   apache_fs (ocf::heartbeat:Filesystem): Started pm-node1.lab.com   Website (ocf::heartbeat:apache): Started pm-node1.lab.com   scsi (stonith:fence_scsi): Started pm-node2.lab.com PCSD Status:  pm-node1.lab.com: Online  pm-node2.lab.com: Online Daemon Status:  corosync: active/enabled  pacemaker: active/enabled  pcsd: active/enabled In the event that there are problems the &quot;pcs resource debug-start &lt;resource&gt;&quot; command can be used for troubleshooting. #pcs resource debug-start disk1 Pacemaker GUI I was pleasantly surprised with the new Pacemaker GUI. The upstream community has done a terrific job. The GUI can even be used to manage multiple clusters. Below are some screenshots to give you a better idea. To access GUI use the following URL and login as hacluster. https://pm-node1:2224 Below screenshot shows the interface for managing clusters.  Below screenshot shows the interface for managing nodes.  Below screenshot shows the interface for managing resources.  Summary Pacemaker is the result of open source innovation and long maturity. We have learned about Pacemaker basics and even configured a cluster for Apache using shared storage fencing. There is no doubt that Pacemaker is a viable alternative to commercial clustering from HP, IBM, Oracle and Microsoft. If you interested in traditional clustering I highly recommend giving Pacemaker a chance, you won&#39;t be disappointed. Happy Clustering! (c) 2015 Keith Tenzer">
  <meta name="twitter:url" content="https://keithtenzer.com/clustering/pacemaker-the-open-source-high-availability-cluster/">

  
    <meta name="twitter:card" content="summary">
    
  

  



  <meta property="article:published_time" content="2015-06-22T00:00:00-07:00">





  

  


<link rel="canonical" href="https://keithtenzer.com/clustering/pacemaker-the-open-source-high-availability-cluster/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Keith Tenzer",
      "url": "https://keithtenzer.com/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Keith Tenzer's Blog Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Keith Tenzer's Blog
          <span class="site-subtitle">Cloud Computing and Code</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about/index.html">About</a>
            </li><li class="masthead__menu-item">
              <a href="/conferences-and-events/index.html">Conferences and Events</a>
            </li><li class="masthead__menu-item">
              <a href="/videos/index.html">Videos</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16">
            <path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path>
          </svg>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      


  
    



<nav class="breadcrumbs">
  <ol itemscope itemtype="https://schema.org/BreadcrumbList">
    
    
    
      
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="https://keithtenzer.com/" itemprop="item"><span itemprop="name">Home</span></a>
          <meta itemprop="position" content="1" />
        </li>
        <span class="sep">/</span>
      
      
        
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/categories/#clustering" itemprop="item"><span itemprop="name">Clustering</span></a>
          <meta itemprop="position" content="2" />
        </li>
        <span class="sep">/</span>
      
    
      
      
        <li class="current">Pacemaker - The Open Source, High Availability Cluster</li>
      
    
  </ol>
</nav>

  


<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      
        <img src="/assets/main/me.png" alt="Keith Tenzer" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Keith Tenzer</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>Principal Solutions Architect at Red Hat</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Los Angeles, CA</span>
        </li>
      

      
        
          
        
          
        
          
            <li><a href="https://twitter.com/keithtenzer" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i><span class="label">Twitter</span></a></li>
          
        
          
        
          
            <li><a href="https://github.com/ktenzer" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Pacemaker - The Open Source, High Availability Cluster">
    <meta itemprop="description" content="OverviewPacemaker is a Open Source, High Availability cluster. Pacemaker and in general Linux clustering have been around for a very long time. Both matured greatly over the past 10 to 15 years.Today Pacemaker is a very simple streamlined bundle that includes the Pacemker clustering, fencing agents, resource agents and the heartbeat (corrosync). Pacemaker is available in Red Hat Enterprise Linux 7 as the High Availability option. In this article I will provide an overview of Pacemaker and a tutorial on how to setup a two-node pacemaker cluster for Apache using shared storage.Pacemaker BasicsAs mentioned Pacemaker has a few components: clustering, fence agents, resource agents and corrosync.ClusteringPacemaker provides all the packages to configure and manage a high availability cluster using the CLI or GUI.Fence AgentsFencing is about shutting off a node that has become unstable or unresponsive so that it cannot damage the cluster or any cluster resources. The main reason we have fencing is to illiminate the possibility of a split-brain where multiple nodes access resources at same time. A split-brain can lead to data corruption and general cluster malfunction. There are two types of fencing agents in Pacemaker: power and storage. The most commonly used fencing agent is power. These agents connect to hardware such as UPS, blade chasis, iLO cards, etc and are responsible for fencing a node in the event that it becomes unresponsive. The other type of fencing is storage based. Typically storage-based fencing uses SCSI-3 PR (Persistent Reservation) that ensures only one node can ever write or access storage at time. This requires of course that the shared storage is used and that it supports SCSI-3 PR. The daemon or service responsible for fencing in Pacemaker is stonith (shoot the other node in the head).Open source clustering in regards to fencing design differs slightly from commercial clustering. Open source has always taken a very conservative approach and IMHO that is a good thing. The last thing you want it data corruption. If fencing does not work Pacemaker will make the entire cluster unavailable. This means manual intervention will be required to bring resources online but your data is safe. Commercial solutions have very elaborate and complex fencing proceadures and try to always ensure failover is automated even if a problem occurs. I am not saying commercial software isn&#39;t bullet-proof, just that there is a design difference in this regard.Resource AgentsPacemaker resource agents are packages that integrate applications. Resource agents understand a specific application and it&#39;s dependencies. As such using resource agent (if one exists) makes configuration of applications much simpler. It also ensures that best practice around clustering for given application are enforced.CorrosyncPacemaker requires a heartbeat for internal communications . The corrosync daemon provides inter-cluster communications between cluster nodes. It is also responsible for quorum if a quorum is used.Pacemaker TutortialNow that we have a high-level understanding of Pacemaker it is time to get our hands a bit dirty and configure a cluster. For this tutorial we will use a simple example of a two-node cluster for Apache. We will configure the cluster, setup storage-based fencing and configure a resource group for Apache.Install PacemakerPerform following steps on both cluster nodesInstall RHEL / CentOS 7.1 (minimal)Configure subscription and repos (RHEL 7)#subscription-manager register#subscription-manager list --available#subscription-manager attach --pool=&lt;pool id&gt;#subscription-manager repos --enable=rhel-ha-for-rhel-7-server-rpmsInstall Pacemaker packages#yum update -y#yum install -y pcs fence-agents-allOpen firewall ports#firewall-cmd --permanent --add-service=high-availability#firewall-cmd --reloadSet hacluster password#echo CHANGEME | passwd --stdin haclusterEnable services#systemctl start pcsd.service#systemctl enable pcsd.serviceConfigure ISCSI client#yum install -y iscsi-initiator-utils#vi /etc/iscsi/initiatorname.iscsiInitiatorName=iqn.2015-06.com.lab:&lt;hostname&gt;Setup Shared ISCSI StorageThese steps are optional if you have ISCSI storage already configured. In these steps we will configure a third RHEL / CentOS system to provide shared storage using ISCSI.Install RHEL / CentOS 7.1 (minimal)Install ISCSI packages#yum install -y targetcliEnable ISCSI service#systemctl enable targetCreate LVM disk (this will be the shared storage device)#fdisk /dev/vdb (create new partition of type LVM)#pvcreate /dev/vdb1#vgcreate cluster_vg /dev/vdb1#lvcreate -L 1G cluster_vg -n cluster_disk1#lvcreate -L 990M cluster_vg -n cluster_disk1#mkfs -t ext4 /dev/cluster_vg/cluster_disk1Configure ISCSI target# targetcli/&gt; backstores/block create disk1 /dev/cluster_vg/cluster_disk1/&gt; iscsi/ create iqn.2015-06.com.lab:rhel7/&gt; /iscsi/iqn.2015-06.com.lab:rhel7/tpg1/portals/ create/&gt; iscsi/iqn.2015-06.com.lab:rhel7/tpg1/luns create /backstores/block/disk1/&gt; iscsi/iqn.2015-06.com.lab:rhel7/tpg1/acls create iqn.2015-06.com.lab:pm-node1/&gt; iscsi/iqn.2015-06.com.lab:rhel7/tpg1/acls create iqn.2015-06.com.lab:pm-node2/&gt; exitOpen firewall ports#firewall-cmd --permanent --add-port=3260/tcp#firewall-cmd --reloadCreate ClusterAt this point both cluster nodes have all the cluster packages and have access to shared ISCSI storage. In this section we will configure the cluster on one of the nodes.Authorize cluster nodes#pcs cluster auth pm-node1.lab.com pm-node2.lab.comUsername: haclusterPassword:pm-node1.lab.com: Authorizedpm-node2.lab.com: AuthorizedSetup the cluster#pcs cluster setup --start --name mycluster pm-node1.lab.com pm-node2.lab.comEnable services#pcs cluster enable --allCheck cluster status# pcs cluster statusCluster Status:Last updated: Fri Jun 19 14:10:24 2015Last change: Fri Jun 19 14:09:15 2015Stack: corosyncCurrent DC: pm-node1.lab.com (1) - partition with quorumVersion: 1.1.12-a14efad2 Nodes configured0 Resources configuredPCSD Status:pm-node1.lab.com: Onlinepm-node2.lab.com: OnlineStorage FencingBefore creating resource groups fencing needs to be configured. In this example we will use storage fencing and fence_scsi. Fencing is to be configured on one of the nodes.Configure stonith#pcs stonith create scsi fence_scsi pcmk_host_list=&quot;pm-node1.lab.com pm-node2.lab.com&quot; pcmk_monitor_action=&quot;metadata&quot; pcmk_reboot_action=&quot;off&quot;devices=&quot;/dev/mapper/cluster_vg-disk1&quot; meta provides=&quot;unfencing&quot;Check status of fencing#pcs stonith show scsi (stonith:fence_scsi): StartedResource GroupNow that the cluster is running and fencing has been configured we can setup the resource group. A resource group defines application dependencies and ensures application is started correctly in the event of a failover. A resource group is to be configured on one of the nodes.Install application packages (Apache)#yum install -y httpd wgetOpen firewall ports#firewall-cmd --permanent --add-service=http#fireall-cmd --reloadConfigure Apache#vi /etc/httpd/conf/httpd.conf&lt;Location /server-status&gt;SetHandler server-statusOrder deny,allowDeny from allAllow from 127.0.0.1&lt;/Location&gt;Mount shared storage# mount /dev/cluster_vg /disk1 /var/www/# mkdir /var/www/html# mkdir /var/www/cgi-bin# mkdir /var/www/error# restorecon -R /var/www# cat &lt;&lt;-END&gt;/var/www/html /index.html&lt;html&gt;&lt;body&gt;Hello&lt;/body&gt;&lt;/html&gt;END#umount /var/wwwConfigure LVM so it only starts volumes not owned by the clusterNote: it is important to not allow LVM to start the cluster owned volume group, in this case cluster_vg.#vi /etc/lvm/lvm.confvolume_list = [ &quot;rhel&quot; ]use_lvmetad=0Ensure boot image does not try and control cluster volume#dracut -H -f /boot/initramfs-$(uname -r).img $(uname -r)Create resource for LVM disk#pcs resource create disk1 LVM volgrpname= cluster_vg exclusive= true --group apachegroupCreate resource for filesystem#pcs resource create apache_fs Filesystem device=&quot;/dev/cluster_vg/disk1&quot; directory=&quot;/var/www&quot; fstype=&quot;ext4 &quot; --group apachegroupCreate resource for virtual IP address#pcs resource create VirtualIP IPaddr2 ip=192.168.122.52 cidr_netmask=24 --group apachegroupCreate resource for website#pcs resource create Website apache configfile=&quot;/etc/httpd/conf/httpd.conf&quot; statusurl=&quot;http://127.0.0.1/server-status&quot; --group apachegroupAt this point the cluster should be configured and look somthing similar to our example.#pcs statusCluster name: myclusterLast updated: Mon Jun 22 14:47:49 2015Last change: Mon Jun 22 12:25:14 2015Stack: corosyncCurrent DC: pm-node2.lab.com (2) - partition with quorumVersion: 1.1.12-a14efad2 Nodes configured5 Resources configuredOnline: [ pm-node1.lab.com pm-node2.lab.com ]Full list of resources:Resource Group: apachegroup disk1 (ocf::heartbeat:LVM): Started pm-node1.lab.com  VirtualIP (ocf::heartbeat:IPaddr2): Started pm-node1.lab.com  apache_fs (ocf::heartbeat:Filesystem): Started pm-node1.lab.com  Website (ocf::heartbeat:apache): Started pm-node1.lab.com  scsi (stonith:fence_scsi): Started pm-node2.lab.comPCSD Status: pm-node1.lab.com: Online pm-node2.lab.com: OnlineDaemon Status: corosync: active/enabled pacemaker: active/enabled pcsd: active/enabledIn the event that there are problems the &quot;pcs resource debug-start &lt;resource&gt;&quot; command can be used for troubleshooting.#pcs resource debug-start disk1Pacemaker GUII was pleasantly surprised with the new Pacemaker GUI. The upstream community has done a terrific job. The GUI can even be used to manage multiple clusters. Below are some screenshots to give you a better idea.To access GUI use the following URL and login as hacluster.https://pm-node1:2224Below screenshot shows the interface for managing clusters.Below screenshot shows the interface for managing nodes.Below screenshot shows the interface for managing resources.SummaryPacemaker is the result of open source innovation and long maturity. We have learned about Pacemaker basics and even configured a cluster for Apache using shared storage fencing. There is no doubt that Pacemaker is a viable alternative to commercial clustering from HP, IBM, Oracle and Microsoft. If you interested in traditional clustering I highly recommend giving Pacemaker a chance, you won&#39;t be disappointed.Happy Clustering!(c) 2015 Keith Tenzer">
    <meta itemprop="datePublished" content="2015-06-22T00:00:00-07:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Pacemaker - The Open Source, High Availability Cluster
</h1>
          

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2015-06-22T00:00:00-07:00">June 22, 2015</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          13 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
        <h3>Overview</h3>
<p>Pacemaker is a <a href="http://clusterlabs.org/.">Open Source</a>, High Availability cluster. Pacemaker and in general Linux clustering have been around for a very long time. Both matured greatly over the past 10 to 15 years.Today Pacemaker is a very simple streamlined bundle that includes the Pacemker clustering, fencing agents, resource agents and the heartbeat (corrosync). Pacemaker is available in Red Hat Enterprise Linux 7 as the High Availability option. In this article I will provide an overview of Pacemaker and a tutorial on how to setup a two-node pacemaker cluster for Apache using shared storage.</p>
<p><a href="https://keithtenzer.files.wordpress.com/2015/06/fence-example-fc.png"><img class="alignnone  wp-image-1018" src="/assets/2015/06/fence-example-fc.png?w=300" alt="fence-example-fc" width="1190" height="956" /></a><br />
<!--more--></p>
<h3>Pacemaker Basics</h3>
<p>As mentioned Pacemaker has a few components: clustering, fence agents, resource agents and corrosync.</p>
<h4>Clustering</h4>
<p>Pacemaker provides all the packages to configure and manage a high availability cluster using the CLI or GUI.</p>
<h4>Fence Agents</h4>
<p>Fencing is about shutting off a node that has become unstable or unresponsive so that it cannot damage the cluster or any cluster resources. The main reason we have fencing is to illiminate the possibility of a split-brain where multiple nodes access resources at same time. A split-brain can lead to data corruption and general cluster malfunction. There are two types of fencing agents in Pacemaker: power and storage. The most commonly used fencing agent is power. These agents connect to hardware such as UPS, blade chasis, iLO cards, etc and are responsible for fencing a node in the event that it becomes unresponsive. The other type of fencing is storage based. Typically storage-based fencing uses SCSI-3 PR (Persistent Reservation) that ensures only one node can ever write or access storage at time. This requires of course that the shared storage is used and that it supports SCSI-3 PR. The daemon or service responsible for fencing in Pacemaker is stonith (shoot the other node in the head).</p>
<p>Open source clustering in regards to fencing design differs slightly from commercial clustering. Open source has always taken a very conservative approach and IMHO that is a good thing. The last thing you want it data corruption. If fencing does not work Pacemaker will make the entire cluster unavailable. This means manual intervention will be required to bring resources online but your data is safe. Commercial solutions have very elaborate and complex fencing proceadures and try to always ensure failover is automated even if a problem occurs. I am not saying commercial software isn't bullet-proof, just that there is a design difference in this regard.</p>
<h4>Resource Agents</h4>
<p>Pacemaker resource agents are packages that integrate applications. Resource agents understand a specific application and it's dependencies. As such using resource agent (if one exists) makes configuration of applications much simpler. It also ensures that best practice around clustering for given application are enforced.</p>
<h4>Corrosync</h4>
<p>Pacemaker requires a heartbeat for internal communications . The corrosync daemon provides inter-cluster communications between cluster nodes. It is also responsible for quorum if a quorum is used.</p>
<h3>Pacemaker Tutortial</h3>
<p>Now that we have a high-level understanding of Pacemaker it is time to get our hands a bit dirty and configure a cluster. For this tutorial we will use a simple example of a two-node cluster for Apache. We will configure the cluster, setup storage-based fencing and configure a resource group for Apache.</p>
<h4>Install Pacemaker</h4>
<p>Perform following steps on both cluster nodes</p>
<ul>
<li>Install RHEL / CentOS 7.1 (minimal)</li>
<li>Configure subscription and repos (RHEL 7)</li>
</ul>
<pre style="padding-left:30px;">#subscription-manager register</pre>
<pre style="padding-left:30px;">#subscription-manager list --available</pre>
<pre style="padding-left:30px;">#subscription-manager attach --pool=&lt;pool id&gt;</pre>
<pre style="padding-left:30px;">#subscription-manager repos --enable=rhel-ha-for-rhel-7-server-rpms</pre>
<ul>
<li>Install Pacemaker packages</li>
</ul>
<pre style="padding-left:30px;">#yum update -y</pre>
<pre style="padding-left:30px;">#yum install -y pcs fence-agents-all</pre>
<ul>
<li>Open firewall ports</li>
</ul>
<pre style="padding-left:30px;">#firewall-cmd --permanent --add-service=high-availability</pre>
<pre style="padding-left:30px;">#firewall-cmd --reload</pre>
<ul>
<li>Set hacluster password</li>
</ul>
<pre style="padding-left:30px;">#echo CHANGEME | passwd --stdin hacluster</pre>
<ul>
<li>Enable services</li>
</ul>
<pre style="padding-left:30px;">#systemctl start pcsd.service</pre>
<pre style="padding-left:30px;">#systemctl enable pcsd.service</pre>
<ul>
<li>Configure ISCSI client</li>
</ul>
<pre style="padding-left:30px;">#yum install -y iscsi-initiator-utils</pre>
<pre style="padding-left:30px;">#vi /etc/iscsi/initiatorname.iscsi</pre>
<pre style="padding-left:30px;">InitiatorName=iqn.2015-06.com.lab:&lt;hostname&gt;</pre>
<h4>Setup Shared ISCSI Storage</h4>
<p>These steps are optional if you have ISCSI storage already configured. In these steps we will configure a third RHEL / CentOS system to provide shared storage using ISCSI.</p>
<ul>
<li>Install RHEL / CentOS 7.1 (minimal)</li>
<li>Install ISCSI packages</li>
</ul>
<pre style="padding-left:30px;">#yum install -y targetcli</pre>
<ul>
<li>Enable ISCSI service</li>
</ul>
<pre style="padding-left:30px;">#systemctl enable target</pre>
<ul>
<li>Create LVM disk (this will be the shared storage device)</li>
</ul>
<pre style="padding-left:30px;">#fdisk /dev/vdb (create new partition of type LVM)</pre>
<pre style="padding-left:30px;">#pvcreate /dev/vdb1</pre>
<pre style="padding-left:30px;">#vgcreate cluster_vg /dev/vdb1</pre>
<pre style="padding-left:30px;">#lvcreate -L 1G cluster_vg -n cluster_disk1</pre>
<pre style="padding-left:30px;">#lvcreate -L 990M cluster_vg -n cluster_disk1</pre>
<pre style="padding-left:30px;">#mkfs -t ext4 /dev/cluster_vg/cluster_disk1</pre>
<ul>
<li>Configure ISCSI target</li>
</ul>
<pre style="padding-left:30px;"># targetcli
/&gt; backstores/block create disk1 /dev/cluster_vg/cluster_disk1
/&gt; iscsi/ create iqn.2015-06.com.lab:rhel7
/&gt; /iscsi/iqn.2015-06.com.lab:rhel7/tpg1/portals/ create
/&gt; iscsi/iqn.2015-06.com.lab:rhel7/tpg1/luns create /backstores/block/disk1
/&gt; iscsi/iqn.2015-06.com.lab:rhel7/tpg1/acls create iqn.2015-06.com.lab:pm-node1
/&gt; iscsi/iqn.2015-06.com.lab:rhel7/tpg1/acls create iqn.2015-06.com.lab:pm-node2
/&gt; exit</pre>
<ul>
<li>Open firewall ports</li>
</ul>
<pre style="padding-left:30px;">#firewall-cmd --permanent --add-port=3260/tcp</pre>
<pre style="padding-left:30px;">#firewall-cmd --reload</pre>
<p>Create Cluster</p>
<p>At this point both cluster nodes have all the cluster packages and have access to shared ISCSI storage. In this section we will configure the cluster on one of the nodes.</p>
<ul>
<li>Authorize cluster nodes</li>
</ul>
<pre style="padding-left:30px;">#pcs cluster auth pm-node1.lab.com pm-node2.lab.com
Username: hacluster
Password:
pm-node1.lab.com: Authorized
pm-node2.lab.com: Authorized</pre>
<ul>
<li>Setup the cluster</li>
</ul>
<pre style="padding-left:30px;">#pcs cluster setup --start --name mycluster pm-node1.lab.com pm-node2.lab.com</pre>
<ul>
<li>Enable services</li>
</ul>
<pre style="padding-left:30px;">#pcs cluster enable --all</pre>
<ul>
<li>Check cluster status</li>
</ul>
<pre style="padding-left:30px;"># pcs cluster status
Cluster Status:
Last updated: Fri Jun 19 14:10:24 2015
Last change: Fri Jun 19 14:09:15 2015
Stack: corosync
Current DC: pm-node1.lab.com (1) - partition with quorum
Version: 1.1.12-a14efad
2 Nodes configured
0 Resources configured
PCSD Status:
pm-node1.lab.com: Online
pm-node2.lab.com: Online</pre>
<h4>Storage Fencing</h4>
<p>Before creating resource groups fencing needs to be configured. In this example we will use storage fencing and fence_scsi. Fencing is to be configured on one of the nodes.</p>
<ul>
<li>Configure stonith</li>
</ul>
<pre style="padding-left:30px;">#pcs stonith create scsi fence_scsi pcmk_host_list="pm-node1.lab.com pm-node2.lab.com" pcmk_monitor_action="metadata" pcmk_reboot_action="off"devices="/dev/mapper/cluster_vg-disk1" meta provides="unfencing"</pre>
<ul>
<li>Check status of fencing</li>
</ul>
<pre style="padding-left:30px;">#pcs stonith show
 scsi (stonith:fence_scsi): Started</pre>
<h4>Resource Group</h4>
<p>Now that the cluster is running and fencing has been configured we can setup the resource group. A resource group defines application dependencies and ensures application is started correctly in the event of a failover. A resource group is to be configured on one of the nodes.</p>
<ul>
<li>Install application packages (Apache)</li>
</ul>
<pre style="padding-left:30px;">#yum install -y httpd wget</pre>
<ul>
<li>Open firewall ports</li>
</ul>
<pre style="padding-left:30px;">#firewall-cmd --permanent --add-service=http</pre>
<pre style="padding-left:30px;">#fireall-cmd --reload</pre>
<ul>
<li>Configure Apache</li>
</ul>
<pre style="padding-left:30px;">#vi /etc/httpd/conf/httpd.conf
&lt;Location /server-status&gt;
SetHandler server-status
Order deny,allow
Deny from all
Allow from 127.0.0.1
&lt;/Location&gt;</pre>
<ul>
<li>Mount shared storage</li>
</ul>
<pre style="padding-left:30px;"># mount /dev/cluster_vg /disk1 /var/www/
# mkdir /var/www/html
# mkdir /var/www/cgi-bin
# mkdir /var/www/error
# restorecon -R /var/www
# cat &lt;&lt;-END&gt;/var/www/html /index.html
&lt;html&gt;
&lt;body&gt;Hello&lt;/body&gt;
&lt;/html&gt;
END
#umount /var/www</pre>
<ul>
<li>Configure LVM so it only starts volumes not owned by the cluster</li>
</ul>
<p>Note: it is important to not allow LVM to start the cluster owned volume group, in this case cluster_vg.</p>
<pre style="padding-left:30px;">#vi /etc/lvm/lvm.conf</pre>
<pre style="padding-left:30px;">volume_list = [ "rhel" ]
use_lvmetad=0</pre>
<ul>
<li>Ensure boot image does not try and control cluster volume</li>
</ul>
<pre style="padding-left:30px;">#dracut -H -f /boot/initramfs-$(uname -r).img $(uname -r)</pre>
<ul>
<li>Create resource for LVM disk</li>
</ul>
<pre style="padding-left:30px;">#pcs resource create disk1 LVM volgrpname= cluster_vg exclusive= true --group apachegroup</pre>
<ul>
<li>Create resource for filesystem</li>
</ul>
<pre style="padding-left:30px;">#pcs resource create apache_fs Filesystem device="/dev/cluster_vg/disk1" directory="/var/www" fstype="ext4 " --group apachegroup</pre>
<ul>
<li>Create resource for virtual IP address</li>
</ul>
<pre style="padding-left:30px;">#pcs resource create VirtualIP IPaddr2 ip=192.168.122.52 cidr_netmask=24 --group apachegroup</pre>
<ul>
<li>Create resource for website</li>
</ul>
<pre style="padding-left:30px;">#pcs resource create Website apache configfile="/etc/httpd/conf/httpd.conf" statusurl="http://127.0.0.1/server-status" --group apachegroup</pre>
<p>At this point the cluster should be configured and look somthing similar to our example.</p>
<pre style="padding-left:30px;">#pcs status
Cluster name: mycluster
Last updated: Mon Jun 22 14:47:49 2015
Last change: Mon Jun 22 12:25:14 2015
Stack: corosync
Current DC: pm-node2.lab.com (2) - partition with quorum
Version: 1.1.12-a14efad
2 Nodes configured
5 Resources configured
Online: [ pm-node1.lab.com pm-node2.lab.com ]
Full list of resources:
Resource Group: apachegroup
 disk1 (ocf::heartbeat:LVM): Started pm-node1.lab.com 
 VirtualIP (ocf::heartbeat:IPaddr2): Started pm-node1.lab.com 
 apache_fs (ocf::heartbeat:Filesystem): Started pm-node1.lab.com 
 Website (ocf::heartbeat:apache): Started pm-node1.lab.com 
 scsi (stonith:fence_scsi): Started pm-node2.lab.com
PCSD Status:
 pm-node1.lab.com: Online
 pm-node2.lab.com: Online
Daemon Status:
 corosync: active/enabled
 pacemaker: active/enabled
 pcsd: active/enabled</pre>
<p>In the event that there are problems the "pcs resource debug-start &lt;resource&gt;" command can be used for troubleshooting.</p>
<pre style="padding-left:30px;">#pcs resource debug-start disk1</pre>
<h3>Pacemaker GUI</h3>
<p>I was pleasantly surprised with the new Pacemaker GUI. The upstream community has done a terrific job. The GUI can even be used to manage multiple clusters. Below are some screenshots to give you a better idea.</p>
<p>To access GUI use the following URL and login as hacluster.</p>
<pre style="padding-left:30px;">https://pm-node1:2224</pre>
<p>Below screenshot shows the interface for managing clusters.</p>
<p><a href="https://keithtenzer.files.wordpress.com/2015/06/pacemake_gui_0.png"><img class="alignnone  wp-image-1008" src="/assets/2015/06/pacemake_gui_0.png?w=300" alt="Pacemake_GUI_0" width="1160" height="437" /></a></p>
<p>Below screenshot shows the interface for managing nodes.</p>
<p><a href="https://keithtenzer.files.wordpress.com/2015/06/pacemake_gui_1.png"><img class="alignnone  wp-image-1009" src="/assets/2015/06/pacemake_gui_1.png?w=300" alt="Pacemake_GUI_1" width="1141" height="1012" /></a></p>
<p>Below screenshot shows the interface for managing resources.</p>
<p><a href="https://keithtenzer.files.wordpress.com/2015/06/pacemake_gui_2.png"><img class="alignnone  wp-image-1010" src="/assets/2015/06/pacemake_gui_2.png?w=300" alt="Pacemake_GUI_2" width="1122" height="965" /></a></p>
<h3>Summary</h3>
<p>Pacemaker is the result of open source innovation and long maturity. We have learned about Pacemaker basics and even configured a cluster for Apache using shared storage fencing. There is no doubt that Pacemaker is a viable alternative to commercial clustering from HP, IBM, Oracle and Microsoft. If you interested in traditional clustering I highly recommend giving Pacemaker a chance, you won't be disappointed.</p>
<p>Happy Clustering!</p>
<p>(c) 2015 Keith Tenzer</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#ha" class="page__taxonomy-item" rel="tag">HA</a><span class="sep">, </span>
    
      <a href="/tags/#linux" class="page__taxonomy-item" rel="tag">Linux</a><span class="sep">, </span>
    
      <a href="/tags/#pacemaker" class="page__taxonomy-item" rel="tag">Pacemaker</a><span class="sep">, </span>
    
      <a href="/tags/#rhel" class="page__taxonomy-item" rel="tag">RHEL</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#clustering" class="page__taxonomy-item" rel="tag">Clustering</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2015-06-22T00:00:00-07:00">June 22, 2015</time></p>


      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?via=keithtenzer&text=Pacemaker+-+The+Open+Source%2C+High+Availability+Cluster%20https%3A%2F%2Fkeithtenzer.com%2Fclustering%2Fpacemaker-the-open-source-high-availability-cluster%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fkeithtenzer.com%2Fclustering%2Fpacemaker-the-open-source-high-availability-cluster%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fkeithtenzer.com%2Fclustering%2Fpacemaker-the-open-source-high-availability-cluster%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/rhev/red-hat-enterprise-virtualization-rhev-hypervisor-host-options/" class="pagination--pager" title="Red Hat Enterprise Virtualization (RHEV) - Hypervisor Host Options
">Previous</a>
    
    
      <a href="/cloudforms/governing-the-cloud-with-cloudforms/" class="pagination--pager" title="Governing the Cloud with CloudForms
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/openshift/building-ansible-operators-1-2-3/" rel="permalink">Building Ansible Operators 1-2-3
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2021-12-03T00:00:00-08:00">December 3, 2021</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          9 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Overview
In this article we will go step by step in build a Kubernetes Operator using Ansible and the Operator Framework. Operators provide ability to not on...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/openshift/openshift-service-mesh-getting-started-guide/" rel="permalink">OpenShift Service Mesh Getting Started Guide
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2021-04-27T00:00:00-07:00">April 27, 2021</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          11 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">






Overview
In this article we will explore the OpenShift Service Mesh and deploy a demo application to better understand the various concepts. First you...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/openshift/openshift-4-aws-ipi-installation-getting-started-guide/" rel="permalink">OpenShift 4 AWS IPI Installation Getting Started Guide
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2021-01-18T00:00:00-08:00">January 18, 2021</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          8 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">


Happy new year as this will be the first post of 2021! 2020 was obviously a challenging year, my hope is I will have more time to devote to blogging in 20...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ansible/windows-automation-with-ansible-getting-started-guide/" rel="permalink">Windows Automation with Ansible: Getting Started Guide
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2020-05-19T00:00:00-07:00">May 19, 2020</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          14 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
Overview
In this article we will focus on how to get started with automation of windows using Ansible. Specifically we will look at installing 3rd party sof...</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="https://twitter.com/keithtenzer"" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
      
        
          <li><a href="https://github.com/ktenzer" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2021 Keith Tenzer. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>







  </body>
</html>
