<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.22.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
<link rel="icon" href="/assets/main/me.png">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>OpenStack: Integrating Ceph as Storage Backend - Keith Tenzer’s Blog</title>
<meta name="description" content="Overview In this article we will discuss why Ceph is Perfect fit for OpenStack. We will see how to integrate three prominent OpenStack use cases with Ceph: Cinder (block storage), Glance (images) and Nova (VM virtual disks). Integrating Ceph with OpenStack Series:  Integrating Ceph with OpenStack Cinder, Glance and Nova Integrating Ceph with Swift Integrating Ceph with Manila  Ceph provides unified scale-out storage, using commodity x86 hardware, that is self-healing and intelligently anticipates failures. It has become the defacto standard for software-defined storage. Ceph being an OpenSource project has enabled many vendors the ability to provide Ceph based software-defined storage systems. Ceph is not just limited to Companies like Red Hat, Suse, Mirantis, Ubuntu, etc. Integrated solutions from SanDisk, Fujitsu, HP, Dell, Samsung and many more exist today. There are even large-scale community built environments, Cern comes to mind, that provide storage services for 10,000s of VMs.  Ceph is by no means limited to OpenStack, however this is where Ceph started gaining traction. Looking at latest OpenStack user survey, Ceph is by a large margin the clear leader for OpenStack storage. Page 42 in OpenStack April 2016 User Survey reveals Ceph is 57% of OpenStack storage. The next is LVM (local storage) with 28% followed by NetApp with 9%. If we remove LVM, Ceph leads any other storage company by 48%, that is incredible. Why is that? There are several reasons but I will give you my top three:  Ceph is a scale-out unified storage platform. OpenStack needs two things from storage: ability to scale with OpenStack itself and do so regardless of block (Cinder), File (Manila) or Object (Swift). Traditional storage vendors need to provide two or three different storage systems to achieve this. They don&#39;t scale the same and in most cases only scale-up in never-ending migration cycles. Their management capabilities are never truly integrated across broad spectrum of storage use-cases. Ceph is cost-effective. Ceph leverages Linux as an operating system instead of something proprietary. You can choose not only whom you purchase Ceph from, but also where you get your hardware. It can be same vendor or different. You can purchase commodity hardware, or even buy integrated solution of Ceph + Hardware from single vendor. There are even hyper-converged options for Ceph that are emerging (running Ceph services on compute nodes). Ceph is OpenSource project just like OpenStack. This allows for a much tighter integration and cross-project development. Proprietary vendors are always playing catch-up since they have secrets to protect and their influence is usually limited in Opensource communities, especially in OpenStack context.  Below is an architecture Diagram that shows all the different OpenStack components that need storage. It shows how they integrate with Ceph and how Ceph provides a unified storage system that scales to fill all these use cases.  source: Red Hat Summit 2016 If you are interested in more topics relating to ceph and OpenStack I recommend following: http://ceph.com/category/ceph-and-openstack/ Ok enough talking about why Ceph and OpenStack are so great, lets get our hands dirty and see how to hook it up! If you don&#39;t have a Ceph environment you can follow this article on how to set one up quickly. Glance Integration Glance is the image service within OpenStack. By default images are stored locally on controllers and then copied to compute hosts when requested. The compute hosts cache the images but they need to be copied again, every time an image is updated. Ceph provides backend for Glance allowing images to be stored in Ceph, instead of locally on controller and compute nodes. This greatly reduces network traffic for pulling images and increases performance since Ceph can clone images instead of copying them. In addition it makes migrating between OpenStack deployments or concepts like multi-site OpenStack much simpler. Install ceph client used by Glance. [root@osp9 ~]# yum install -y python-rbd Create Ceph user and set home directory to /etc/ceph. [root@osp9 ~]# mkdir /etc/ceph [root@osp9 ~]# useradd ceph [root@osp9 ~]# passwd ceph Add ceph user to sudoers. cat &lt;&lt; EOF &gt;/etc/sudoers.d/ceph ceph ALL = (root) NOPASSWD:ALL Defaults:ceph !requiretty EOF On Ceph admin node. Create Ceph RBD Pool for Glance images. [ceph@ceph1 ~]$ sudo ceph osd pool create images 128 Create keyring that will allow Glance access to pool. [ceph@ceph1 ~]$ sudo ceph auth get-or-create client.images mon &#39;allow r&#39; osd &#39;allow class-read object_prefix rdb_children, allow rwx pool=images&#39; -o /etc/ceph/ceph.client.images.keyring Copy the keyring to /etc/ceph on OpenStack controller. [ceph@ceph1 ~]$ scp /etc/ceph/ceph.client.images.keyring root@osp9.lab:/etc/ceph Copy /etc/ceph/ceph.conf configuration to OpenStack controller. [ceph@ceph1 ~]$ scp /etc/ceph/ceph.conf root@osp9.lab:/etc/ceph Set permissions so Glance can access Ceph keyring. [root@osp9 ~(keystone_admin)]# chgrp glance /etc/ceph/ceph.client.images.keyring [root@osp9 ~(keystone_admin)]#chmod 0640 /etc/ceph/ceph.client.images.keyring Add keyring file to Ceph configuration. [root@osp9 ~(keystone_admin)]# vi /etc/ceph/ceph.conf [client.images] keyring = /etc/ceph/ceph.client.images.keyring Create backup of original Glance configuration. [root@osp9 ~(keystone_admin)]# cp /etc/glance/glance-api.conf /etc/glance/glance-api.conf.orig Update Glance configuration. [root@osp9 ~]# vi /etc/glance/glance-api.conf [glance_store] stores = glance.store.rbd.Store default_store = rbd rbd_store_pool = images rbd_store_user = images rbd_store_ceph_conf = /etc/ceph/ceph.conf Restart Glance. [root@osp9 ~(keystone_admin)]# systemctl restart openstack-glance-api Download Cirros images and add it into Glance. [root@osp9 ~(keystone_admin)]# wget http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img Convert QCOW2 to RAW. It is recommended for Ceph to always use RAW format. [root@osp9 ~(keystone_admin)]#qemu-img convert cirros-0.3.4-x86_64-disk.img cirros-0.3.4-x86_64-disk.raw Add image to Glance. [root@osp9 ~(keystone_admin)]#glance image-create --name &quot;Cirros 0.3.4&quot; --disk-format raw --container-format bare --visibility public --file cirros-0.3.4-x86_64-disk.raw +------------------+--------------------------------------+ | Property | Value | +------------------+--------------------------------------+ | checksum | ee1eca47dc88f4879d8a229cc70a07c6 | | container_format | bare | | created_at | 2016-09-07T12:29:23Z | | disk_format | qcow2 | | id | a55e9417-67af-43c5-a342-85d2c4c483f7 | | min_disk | 0 | | min_ram | 0 | | name | Cirros 0.3.4 | | owner | dd6a4ed994d04255a451da66b68a8057 | | protected | False | | size | 13287936 | | status | active | | tags | [] | | updated_at | 2016-09-07T12:29:27Z | | virtual_size | None | | visibility | public | +------------------+--------------------------------------+ Check that glance image exists in Ceph. [ceph@ceph1 ceph-config]$ sudo rbd ls images a55e9417-67af-43c5-a342-85d2c4c483f7 [ceph@ceph1 ceph-config]$ sudo rbd info images/a55e9417-67af-43c5-a342-85d2c4c483f7 rbd image &#39;a55e9417-67af-43c5-a342-85d2c4c483f7&#39;:  size 12976 kB in 2 objects  order 23 (8192 kB objects)  block_name_prefix: rbd_data.183e54fd29b46  format: 2  features: layering, striping  flags:  stripe unit: 8192 kB  stripe count: 1 Cinder Integration Cinder is the block storage service in OpenStack. Cinder provides an abstraction around block storage and allows vendors to integrate by providing a driver. In Ceph, each storage pool can be mapped to a different Cinder backend. This allows for creating storage services such as gold, silver or bronze. You can decide for example that gold should be fast SSD disks that are replicated three times, while silver only should be replicated two times and bronze should use slower disks with erasure coding. Create Ceph pool for cinder volumes. [ceph@ceph1 ~]$ sudo ceph osd pool create volumes 128 Create keyring to grant cinder access. [ceph@ceph1 ~]$ sudo ceph auth get-or-create client.volumes mon &#39;allow r&#39; osd &#39;allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rx pool=images&#39; -o /etc/ceph/ceph.client.volumes.keyring Copy keyring to OpenStack controllers. [ceph@ceph1 ~]$ scp /etc/ceph/ceph.client.volumes.keyring root@osp9.lab:/etc/ceph Create file that contains just the authentication key on OpenStack controllers. [ceph@ceph1 ~]$ sudo ceph auth get-key client.volumes |ssh osp9.lab tee client.volumes.key Set permissions on keyring file so it can be accessed by Cinder. [root@osp9 ~(keystone_admin)]# chgrp cinder /etc/ceph/ceph.client.volumes.keyring [root@osp9 ~(keystone_admin)]# chmod 0640 /etc/ceph/ceph.client.volumes.keyring Add keyring to Ceph configuration file on OpenStack controllers. [root@osp9 ~(keystone_admin)]#vi /etc/ceph/ceph.conf">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Keith Tenzer's Blog">
<meta property="og:title" content="OpenStack: Integrating Ceph as Storage Backend">
<meta property="og:url" content="http://localhost:4000/2016/09/12/openstack-integrating-ceph-as-storage-backend/">


  <meta property="og:description" content="Overview In this article we will discuss why Ceph is Perfect fit for OpenStack. We will see how to integrate three prominent OpenStack use cases with Ceph: Cinder (block storage), Glance (images) and Nova (VM virtual disks). Integrating Ceph with OpenStack Series:  Integrating Ceph with OpenStack Cinder, Glance and Nova Integrating Ceph with Swift Integrating Ceph with Manila  Ceph provides unified scale-out storage, using commodity x86 hardware, that is self-healing and intelligently anticipates failures. It has become the defacto standard for software-defined storage. Ceph being an OpenSource project has enabled many vendors the ability to provide Ceph based software-defined storage systems. Ceph is not just limited to Companies like Red Hat, Suse, Mirantis, Ubuntu, etc. Integrated solutions from SanDisk, Fujitsu, HP, Dell, Samsung and many more exist today. There are even large-scale community built environments, Cern comes to mind, that provide storage services for 10,000s of VMs.  Ceph is by no means limited to OpenStack, however this is where Ceph started gaining traction. Looking at latest OpenStack user survey, Ceph is by a large margin the clear leader for OpenStack storage. Page 42 in OpenStack April 2016 User Survey reveals Ceph is 57% of OpenStack storage. The next is LVM (local storage) with 28% followed by NetApp with 9%. If we remove LVM, Ceph leads any other storage company by 48%, that is incredible. Why is that? There are several reasons but I will give you my top three:  Ceph is a scale-out unified storage platform. OpenStack needs two things from storage: ability to scale with OpenStack itself and do so regardless of block (Cinder), File (Manila) or Object (Swift). Traditional storage vendors need to provide two or three different storage systems to achieve this. They don&#39;t scale the same and in most cases only scale-up in never-ending migration cycles. Their management capabilities are never truly integrated across broad spectrum of storage use-cases. Ceph is cost-effective. Ceph leverages Linux as an operating system instead of something proprietary. You can choose not only whom you purchase Ceph from, but also where you get your hardware. It can be same vendor or different. You can purchase commodity hardware, or even buy integrated solution of Ceph + Hardware from single vendor. There are even hyper-converged options for Ceph that are emerging (running Ceph services on compute nodes). Ceph is OpenSource project just like OpenStack. This allows for a much tighter integration and cross-project development. Proprietary vendors are always playing catch-up since they have secrets to protect and their influence is usually limited in Opensource communities, especially in OpenStack context.  Below is an architecture Diagram that shows all the different OpenStack components that need storage. It shows how they integrate with Ceph and how Ceph provides a unified storage system that scales to fill all these use cases.  source: Red Hat Summit 2016 If you are interested in more topics relating to ceph and OpenStack I recommend following: http://ceph.com/category/ceph-and-openstack/ Ok enough talking about why Ceph and OpenStack are so great, lets get our hands dirty and see how to hook it up! If you don&#39;t have a Ceph environment you can follow this article on how to set one up quickly. Glance Integration Glance is the image service within OpenStack. By default images are stored locally on controllers and then copied to compute hosts when requested. The compute hosts cache the images but they need to be copied again, every time an image is updated. Ceph provides backend for Glance allowing images to be stored in Ceph, instead of locally on controller and compute nodes. This greatly reduces network traffic for pulling images and increases performance since Ceph can clone images instead of copying them. In addition it makes migrating between OpenStack deployments or concepts like multi-site OpenStack much simpler. Install ceph client used by Glance. [root@osp9 ~]# yum install -y python-rbd Create Ceph user and set home directory to /etc/ceph. [root@osp9 ~]# mkdir /etc/ceph [root@osp9 ~]# useradd ceph [root@osp9 ~]# passwd ceph Add ceph user to sudoers. cat &lt;&lt; EOF &gt;/etc/sudoers.d/ceph ceph ALL = (root) NOPASSWD:ALL Defaults:ceph !requiretty EOF On Ceph admin node. Create Ceph RBD Pool for Glance images. [ceph@ceph1 ~]$ sudo ceph osd pool create images 128 Create keyring that will allow Glance access to pool. [ceph@ceph1 ~]$ sudo ceph auth get-or-create client.images mon &#39;allow r&#39; osd &#39;allow class-read object_prefix rdb_children, allow rwx pool=images&#39; -o /etc/ceph/ceph.client.images.keyring Copy the keyring to /etc/ceph on OpenStack controller. [ceph@ceph1 ~]$ scp /etc/ceph/ceph.client.images.keyring root@osp9.lab:/etc/ceph Copy /etc/ceph/ceph.conf configuration to OpenStack controller. [ceph@ceph1 ~]$ scp /etc/ceph/ceph.conf root@osp9.lab:/etc/ceph Set permissions so Glance can access Ceph keyring. [root@osp9 ~(keystone_admin)]# chgrp glance /etc/ceph/ceph.client.images.keyring [root@osp9 ~(keystone_admin)]#chmod 0640 /etc/ceph/ceph.client.images.keyring Add keyring file to Ceph configuration. [root@osp9 ~(keystone_admin)]# vi /etc/ceph/ceph.conf [client.images] keyring = /etc/ceph/ceph.client.images.keyring Create backup of original Glance configuration. [root@osp9 ~(keystone_admin)]# cp /etc/glance/glance-api.conf /etc/glance/glance-api.conf.orig Update Glance configuration. [root@osp9 ~]# vi /etc/glance/glance-api.conf [glance_store] stores = glance.store.rbd.Store default_store = rbd rbd_store_pool = images rbd_store_user = images rbd_store_ceph_conf = /etc/ceph/ceph.conf Restart Glance. [root@osp9 ~(keystone_admin)]# systemctl restart openstack-glance-api Download Cirros images and add it into Glance. [root@osp9 ~(keystone_admin)]# wget http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img Convert QCOW2 to RAW. It is recommended for Ceph to always use RAW format. [root@osp9 ~(keystone_admin)]#qemu-img convert cirros-0.3.4-x86_64-disk.img cirros-0.3.4-x86_64-disk.raw Add image to Glance. [root@osp9 ~(keystone_admin)]#glance image-create --name &quot;Cirros 0.3.4&quot; --disk-format raw --container-format bare --visibility public --file cirros-0.3.4-x86_64-disk.raw +------------------+--------------------------------------+ | Property | Value | +------------------+--------------------------------------+ | checksum | ee1eca47dc88f4879d8a229cc70a07c6 | | container_format | bare | | created_at | 2016-09-07T12:29:23Z | | disk_format | qcow2 | | id | a55e9417-67af-43c5-a342-85d2c4c483f7 | | min_disk | 0 | | min_ram | 0 | | name | Cirros 0.3.4 | | owner | dd6a4ed994d04255a451da66b68a8057 | | protected | False | | size | 13287936 | | status | active | | tags | [] | | updated_at | 2016-09-07T12:29:27Z | | virtual_size | None | | visibility | public | +------------------+--------------------------------------+ Check that glance image exists in Ceph. [ceph@ceph1 ceph-config]$ sudo rbd ls images a55e9417-67af-43c5-a342-85d2c4c483f7 [ceph@ceph1 ceph-config]$ sudo rbd info images/a55e9417-67af-43c5-a342-85d2c4c483f7 rbd image &#39;a55e9417-67af-43c5-a342-85d2c4c483f7&#39;:  size 12976 kB in 2 objects  order 23 (8192 kB objects)  block_name_prefix: rbd_data.183e54fd29b46  format: 2  features: layering, striping  flags:  stripe unit: 8192 kB  stripe count: 1 Cinder Integration Cinder is the block storage service in OpenStack. Cinder provides an abstraction around block storage and allows vendors to integrate by providing a driver. In Ceph, each storage pool can be mapped to a different Cinder backend. This allows for creating storage services such as gold, silver or bronze. You can decide for example that gold should be fast SSD disks that are replicated three times, while silver only should be replicated two times and bronze should use slower disks with erasure coding. Create Ceph pool for cinder volumes. [ceph@ceph1 ~]$ sudo ceph osd pool create volumes 128 Create keyring to grant cinder access. [ceph@ceph1 ~]$ sudo ceph auth get-or-create client.volumes mon &#39;allow r&#39; osd &#39;allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rx pool=images&#39; -o /etc/ceph/ceph.client.volumes.keyring Copy keyring to OpenStack controllers. [ceph@ceph1 ~]$ scp /etc/ceph/ceph.client.volumes.keyring root@osp9.lab:/etc/ceph Create file that contains just the authentication key on OpenStack controllers. [ceph@ceph1 ~]$ sudo ceph auth get-key client.volumes |ssh osp9.lab tee client.volumes.key Set permissions on keyring file so it can be accessed by Cinder. [root@osp9 ~(keystone_admin)]# chgrp cinder /etc/ceph/ceph.client.volumes.keyring [root@osp9 ~(keystone_admin)]# chmod 0640 /etc/ceph/ceph.client.volumes.keyring Add keyring to Ceph configuration file on OpenStack controllers. [root@osp9 ~(keystone_admin)]#vi /etc/ceph/ceph.conf">





  <meta name="twitter:site" content="@keithtenzer">
  <meta name="twitter:title" content="OpenStack: Integrating Ceph as Storage Backend">
  <meta name="twitter:description" content="Overview In this article we will discuss why Ceph is Perfect fit for OpenStack. We will see how to integrate three prominent OpenStack use cases with Ceph: Cinder (block storage), Glance (images) and Nova (VM virtual disks). Integrating Ceph with OpenStack Series:  Integrating Ceph with OpenStack Cinder, Glance and Nova Integrating Ceph with Swift Integrating Ceph with Manila  Ceph provides unified scale-out storage, using commodity x86 hardware, that is self-healing and intelligently anticipates failures. It has become the defacto standard for software-defined storage. Ceph being an OpenSource project has enabled many vendors the ability to provide Ceph based software-defined storage systems. Ceph is not just limited to Companies like Red Hat, Suse, Mirantis, Ubuntu, etc. Integrated solutions from SanDisk, Fujitsu, HP, Dell, Samsung and many more exist today. There are even large-scale community built environments, Cern comes to mind, that provide storage services for 10,000s of VMs.  Ceph is by no means limited to OpenStack, however this is where Ceph started gaining traction. Looking at latest OpenStack user survey, Ceph is by a large margin the clear leader for OpenStack storage. Page 42 in OpenStack April 2016 User Survey reveals Ceph is 57% of OpenStack storage. The next is LVM (local storage) with 28% followed by NetApp with 9%. If we remove LVM, Ceph leads any other storage company by 48%, that is incredible. Why is that? There are several reasons but I will give you my top three:  Ceph is a scale-out unified storage platform. OpenStack needs two things from storage: ability to scale with OpenStack itself and do so regardless of block (Cinder), File (Manila) or Object (Swift). Traditional storage vendors need to provide two or three different storage systems to achieve this. They don&#39;t scale the same and in most cases only scale-up in never-ending migration cycles. Their management capabilities are never truly integrated across broad spectrum of storage use-cases. Ceph is cost-effective. Ceph leverages Linux as an operating system instead of something proprietary. You can choose not only whom you purchase Ceph from, but also where you get your hardware. It can be same vendor or different. You can purchase commodity hardware, or even buy integrated solution of Ceph + Hardware from single vendor. There are even hyper-converged options for Ceph that are emerging (running Ceph services on compute nodes). Ceph is OpenSource project just like OpenStack. This allows for a much tighter integration and cross-project development. Proprietary vendors are always playing catch-up since they have secrets to protect and their influence is usually limited in Opensource communities, especially in OpenStack context.  Below is an architecture Diagram that shows all the different OpenStack components that need storage. It shows how they integrate with Ceph and how Ceph provides a unified storage system that scales to fill all these use cases.  source: Red Hat Summit 2016 If you are interested in more topics relating to ceph and OpenStack I recommend following: http://ceph.com/category/ceph-and-openstack/ Ok enough talking about why Ceph and OpenStack are so great, lets get our hands dirty and see how to hook it up! If you don&#39;t have a Ceph environment you can follow this article on how to set one up quickly. Glance Integration Glance is the image service within OpenStack. By default images are stored locally on controllers and then copied to compute hosts when requested. The compute hosts cache the images but they need to be copied again, every time an image is updated. Ceph provides backend for Glance allowing images to be stored in Ceph, instead of locally on controller and compute nodes. This greatly reduces network traffic for pulling images and increases performance since Ceph can clone images instead of copying them. In addition it makes migrating between OpenStack deployments or concepts like multi-site OpenStack much simpler. Install ceph client used by Glance. [root@osp9 ~]# yum install -y python-rbd Create Ceph user and set home directory to /etc/ceph. [root@osp9 ~]# mkdir /etc/ceph [root@osp9 ~]# useradd ceph [root@osp9 ~]# passwd ceph Add ceph user to sudoers. cat &lt;&lt; EOF &gt;/etc/sudoers.d/ceph ceph ALL = (root) NOPASSWD:ALL Defaults:ceph !requiretty EOF On Ceph admin node. Create Ceph RBD Pool for Glance images. [ceph@ceph1 ~]$ sudo ceph osd pool create images 128 Create keyring that will allow Glance access to pool. [ceph@ceph1 ~]$ sudo ceph auth get-or-create client.images mon &#39;allow r&#39; osd &#39;allow class-read object_prefix rdb_children, allow rwx pool=images&#39; -o /etc/ceph/ceph.client.images.keyring Copy the keyring to /etc/ceph on OpenStack controller. [ceph@ceph1 ~]$ scp /etc/ceph/ceph.client.images.keyring root@osp9.lab:/etc/ceph Copy /etc/ceph/ceph.conf configuration to OpenStack controller. [ceph@ceph1 ~]$ scp /etc/ceph/ceph.conf root@osp9.lab:/etc/ceph Set permissions so Glance can access Ceph keyring. [root@osp9 ~(keystone_admin)]# chgrp glance /etc/ceph/ceph.client.images.keyring [root@osp9 ~(keystone_admin)]#chmod 0640 /etc/ceph/ceph.client.images.keyring Add keyring file to Ceph configuration. [root@osp9 ~(keystone_admin)]# vi /etc/ceph/ceph.conf [client.images] keyring = /etc/ceph/ceph.client.images.keyring Create backup of original Glance configuration. [root@osp9 ~(keystone_admin)]# cp /etc/glance/glance-api.conf /etc/glance/glance-api.conf.orig Update Glance configuration. [root@osp9 ~]# vi /etc/glance/glance-api.conf [glance_store] stores = glance.store.rbd.Store default_store = rbd rbd_store_pool = images rbd_store_user = images rbd_store_ceph_conf = /etc/ceph/ceph.conf Restart Glance. [root@osp9 ~(keystone_admin)]# systemctl restart openstack-glance-api Download Cirros images and add it into Glance. [root@osp9 ~(keystone_admin)]# wget http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img Convert QCOW2 to RAW. It is recommended for Ceph to always use RAW format. [root@osp9 ~(keystone_admin)]#qemu-img convert cirros-0.3.4-x86_64-disk.img cirros-0.3.4-x86_64-disk.raw Add image to Glance. [root@osp9 ~(keystone_admin)]#glance image-create --name &quot;Cirros 0.3.4&quot; --disk-format raw --container-format bare --visibility public --file cirros-0.3.4-x86_64-disk.raw +------------------+--------------------------------------+ | Property | Value | +------------------+--------------------------------------+ | checksum | ee1eca47dc88f4879d8a229cc70a07c6 | | container_format | bare | | created_at | 2016-09-07T12:29:23Z | | disk_format | qcow2 | | id | a55e9417-67af-43c5-a342-85d2c4c483f7 | | min_disk | 0 | | min_ram | 0 | | name | Cirros 0.3.4 | | owner | dd6a4ed994d04255a451da66b68a8057 | | protected | False | | size | 13287936 | | status | active | | tags | [] | | updated_at | 2016-09-07T12:29:27Z | | virtual_size | None | | visibility | public | +------------------+--------------------------------------+ Check that glance image exists in Ceph. [ceph@ceph1 ceph-config]$ sudo rbd ls images a55e9417-67af-43c5-a342-85d2c4c483f7 [ceph@ceph1 ceph-config]$ sudo rbd info images/a55e9417-67af-43c5-a342-85d2c4c483f7 rbd image &#39;a55e9417-67af-43c5-a342-85d2c4c483f7&#39;:  size 12976 kB in 2 objects  order 23 (8192 kB objects)  block_name_prefix: rbd_data.183e54fd29b46  format: 2  features: layering, striping  flags:  stripe unit: 8192 kB  stripe count: 1 Cinder Integration Cinder is the block storage service in OpenStack. Cinder provides an abstraction around block storage and allows vendors to integrate by providing a driver. In Ceph, each storage pool can be mapped to a different Cinder backend. This allows for creating storage services such as gold, silver or bronze. You can decide for example that gold should be fast SSD disks that are replicated three times, while silver only should be replicated two times and bronze should use slower disks with erasure coding. Create Ceph pool for cinder volumes. [ceph@ceph1 ~]$ sudo ceph osd pool create volumes 128 Create keyring to grant cinder access. [ceph@ceph1 ~]$ sudo ceph auth get-or-create client.volumes mon &#39;allow r&#39; osd &#39;allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rx pool=images&#39; -o /etc/ceph/ceph.client.volumes.keyring Copy keyring to OpenStack controllers. [ceph@ceph1 ~]$ scp /etc/ceph/ceph.client.volumes.keyring root@osp9.lab:/etc/ceph Create file that contains just the authentication key on OpenStack controllers. [ceph@ceph1 ~]$ sudo ceph auth get-key client.volumes |ssh osp9.lab tee client.volumes.key Set permissions on keyring file so it can be accessed by Cinder. [root@osp9 ~(keystone_admin)]# chgrp cinder /etc/ceph/ceph.client.volumes.keyring [root@osp9 ~(keystone_admin)]# chmod 0640 /etc/ceph/ceph.client.volumes.keyring Add keyring to Ceph configuration file on OpenStack controllers. [root@osp9 ~(keystone_admin)]#vi /etc/ceph/ceph.conf">
  <meta name="twitter:url" content="http://localhost:4000/2016/09/12/openstack-integrating-ceph-as-storage-backend/">

  
    <meta name="twitter:card" content="summary">
    
  

  



  <meta property="article:published_time" content="2016-09-12T00:00:00-07:00">





  

  


<link rel="canonical" href="http://localhost:4000/2016/09/12/openstack-integrating-ceph-as-storage-backend/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Keith Tenzer",
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Keith Tenzer's Blog Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single wide">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Keith Tenzer's Blog
          <span class="site-subtitle">Cloud Computing and Code</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about/index.html">About</a>
            </li><li class="masthead__menu-item">
              <a href="/conferences-and-events/index.html">Conferences and Events</a>
            </li><li class="masthead__menu-item">
              <a href="/videos/index.html">Videos</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16">
            <path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path>
          </svg>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      


  
    



<nav class="breadcrumbs">
  <ol itemscope itemtype="https://schema.org/BreadcrumbList">
    
    
    
      
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="http://localhost:4000/" itemprop="item"><span itemprop="name">Home</span></a>
          <meta itemprop="position" content="1" />
        </li>
        <span class="sep">/</span>
      
      
        
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/categories/#2016" itemprop="item"><span itemprop="name">2016</span></a>
          <meta itemprop="position" content="2" />
        </li>
        <span class="sep">/</span>
      
    
      
      
        
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/categories/#09" itemprop="item"><span itemprop="name">09</span></a>
          <meta itemprop="position" content="3" />
        </li>
        <span class="sep">/</span>
      
    
      
      
        
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/categories/#12" itemprop="item"><span itemprop="name">12</span></a>
          <meta itemprop="position" content="4" />
        </li>
        <span class="sep">/</span>
      
    
      
      
        <li class="current">OpenStack: Integrating Ceph as Storage Backend</li>
      
    
  </ol>
</nav>

  


<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name"></h3>
    
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      

      

      
        <li>
          <a href="mailto:keith.tenzer@gmail.com">
            <meta itemprop="email" content="keith.tenzer@gmail.com" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span>
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="OpenStack: Integrating Ceph as Storage Backend">
    <meta itemprop="description" content="OverviewIn this article we will discuss why Ceph is Perfect fit for OpenStack. We will see how to integrate three prominent OpenStack use cases with Ceph: Cinder (block storage), Glance (images) and Nova (VM virtual disks).Integrating Ceph with OpenStack Series:Integrating Ceph with OpenStack Cinder, Glance and NovaIntegrating Ceph with SwiftIntegrating Ceph with ManilaCeph provides unified scale-out storage, using commodity x86 hardware, that is self-healing and intelligently anticipates failures. It has become the defacto standard for software-defined storage. Ceph being an OpenSource project has enabled many vendors the ability to provide Ceph based software-defined storage systems. Ceph is not just limited to Companies like Red Hat, Suse, Mirantis, Ubuntu, etc. Integrated solutions from SanDisk, Fujitsu, HP, Dell, Samsung and many more exist today. There are even large-scale community built environments, Cern comes to mind, that provide storage services for 10,000s of VMs.Ceph is by no means limited to OpenStack, however this is where Ceph started gaining traction. Looking at latest OpenStack user survey, Ceph is by a large margin the clear leader for OpenStack storage. Page 42 in OpenStack April 2016 User Survey reveals Ceph is 57% of OpenStack storage. The next is LVM (local storage) with 28% followed by NetApp with 9%. If we remove LVM, Ceph leads any other storage company by 48%, that is incredible. Why is that?There are several reasons but I will give you my top three:Ceph is a scale-out unified storage platform. OpenStack needs two things from storage: ability to scale with OpenStack itself and do so regardless of block (Cinder), File (Manila) or Object (Swift). Traditional storage vendors need to provide two or three different storage systems to achieve this. They don&#39;t scale the same and in most cases only scale-up in never-ending migration cycles. Their management capabilities are never truly integrated across broad spectrum of storage use-cases.Ceph is cost-effective. Ceph leverages Linux as an operating system instead of something proprietary. You can choose not only whom you purchase Ceph from, but also where you get your hardware. It can be same vendor or different. You can purchase commodity hardware, or even buy integrated solution of Ceph + Hardware from single vendor. There are even hyper-converged options for Ceph that are emerging (running Ceph services on compute nodes).Ceph is OpenSource project just like OpenStack. This allows for a much tighter integration and cross-project development. Proprietary vendors are always playing catch-up since they have secrets to protect and their influence is usually limited in Opensource communities, especially in OpenStack context.Below is an architecture Diagram that shows all the different OpenStack components that need storage. It shows how they integrate with Ceph and how Ceph provides a unified storage system that scales to fill all these use cases.source: Red Hat Summit 2016If you are interested in more topics relating to ceph and OpenStack I recommend following: http://ceph.com/category/ceph-and-openstack/Ok enough talking about why Ceph and OpenStack are so great, lets get our hands dirty and see how to hook it up!If you don&#39;t have a Ceph environment you can follow this article on how to set one up quickly.Glance IntegrationGlance is the image service within OpenStack. By default images are stored locally on controllers and then copied to compute hosts when requested. The compute hosts cache the images but they need to be copied again, every time an image is updated.Ceph provides backend for Glance allowing images to be stored in Ceph, instead of locally on controller and compute nodes. This greatly reduces network traffic for pulling images and increases performance since Ceph can clone images instead of copying them. In addition it makes migrating between OpenStack deployments or concepts like multi-site OpenStack much simpler.Install ceph client used by Glance.[root@osp9 ~]# yum install -y python-rbdCreate Ceph user and set home directory to /etc/ceph.[root@osp9 ~]# mkdir /etc/ceph[root@osp9 ~]# useradd ceph[root@osp9 ~]# passwd cephAdd ceph user to sudoers.cat &lt;&lt; EOF &gt;/etc/sudoers.d/cephceph ALL = (root) NOPASSWD:ALLDefaults:ceph !requirettyEOFOn Ceph admin node.Create Ceph RBD Pool for Glance images.[ceph@ceph1 ~]$ sudo ceph osd pool create images 128Create keyring that will allow Glance access to pool.[ceph@ceph1 ~]$ sudo ceph auth get-or-create client.images mon &#39;allow r&#39; osd &#39;allow class-read object_prefix rdb_children, allow rwx pool=images&#39; -o /etc/ceph/ceph.client.images.keyringCopy the keyring to /etc/ceph on OpenStack controller.[ceph@ceph1 ~]$ scp /etc/ceph/ceph.client.images.keyring root@osp9.lab:/etc/cephCopy /etc/ceph/ceph.conf configuration to OpenStack controller.[ceph@ceph1 ~]$ scp /etc/ceph/ceph.conf root@osp9.lab:/etc/cephSet permissions so Glance can access Ceph keyring.[root@osp9 ~(keystone_admin)]# chgrp glance /etc/ceph/ceph.client.images.keyring[root@osp9 ~(keystone_admin)]#chmod 0640 /etc/ceph/ceph.client.images.keyringAdd keyring file to Ceph configuration.[root@osp9 ~(keystone_admin)]# vi /etc/ceph/ceph.conf[client.images]keyring = /etc/ceph/ceph.client.images.keyringCreate backup of original Glance configuration.[root@osp9 ~(keystone_admin)]# cp /etc/glance/glance-api.conf /etc/glance/glance-api.conf.origUpdate Glance configuration.[root@osp9 ~]# vi /etc/glance/glance-api.conf[glance_store]stores = glance.store.rbd.Storedefault_store = rbdrbd_store_pool = imagesrbd_store_user = imagesrbd_store_ceph_conf = /etc/ceph/ceph.confRestart Glance.[root@osp9 ~(keystone_admin)]# systemctl restart openstack-glance-apiDownload Cirros images and add it into Glance.[root@osp9 ~(keystone_admin)]# wget http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.imgConvert QCOW2 to RAW. It is recommended for Ceph to always use RAW format.[root@osp9 ~(keystone_admin)]#qemu-img convert cirros-0.3.4-x86_64-disk.img cirros-0.3.4-x86_64-disk.rawAdd image to Glance.[root@osp9 ~(keystone_admin)]#glance image-create --name &quot;Cirros 0.3.4&quot; --disk-format raw --container-format bare --visibility public --file cirros-0.3.4-x86_64-disk.raw+------------------+--------------------------------------+| Property | Value |+------------------+--------------------------------------+| checksum | ee1eca47dc88f4879d8a229cc70a07c6 || container_format | bare || created_at | 2016-09-07T12:29:23Z || disk_format | qcow2 || id | a55e9417-67af-43c5-a342-85d2c4c483f7 || min_disk | 0 || min_ram | 0 || name | Cirros 0.3.4 || owner | dd6a4ed994d04255a451da66b68a8057 || protected | False || size | 13287936 || status | active || tags | [] || updated_at | 2016-09-07T12:29:27Z || virtual_size | None || visibility | public |+------------------+--------------------------------------+Check that glance image exists in Ceph.[ceph@ceph1 ceph-config]$ sudo rbd ls imagesa55e9417-67af-43c5-a342-85d2c4c483f7[ceph@ceph1 ceph-config]$ sudo rbd info images/a55e9417-67af-43c5-a342-85d2c4c483f7rbd image &#39;a55e9417-67af-43c5-a342-85d2c4c483f7&#39;: size 12976 kB in 2 objects order 23 (8192 kB objects) block_name_prefix: rbd_data.183e54fd29b46 format: 2 features: layering, striping flags: stripe unit: 8192 kB stripe count: 1Cinder IntegrationCinder is the block storage service in OpenStack. Cinder provides an abstraction around block storage and allows vendors to integrate by providing a driver. In Ceph, each storage pool can be mapped to a different Cinder backend. This allows for creating storage services such as gold, silver or bronze. You can decide for example that gold should be fast SSD disks that are replicated three times, while silver only should be replicated two times and bronze should use slower disks with erasure coding.Create Ceph pool for cinder volumes.[ceph@ceph1 ~]$ sudo ceph osd pool create volumes 128Create keyring to grant cinder access.[ceph@ceph1 ~]$ sudo ceph auth get-or-create client.volumes mon &#39;allow r&#39; osd &#39;allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rx pool=images&#39; -o /etc/ceph/ceph.client.volumes.keyringCopy keyring to OpenStack controllers.[ceph@ceph1 ~]$ scp /etc/ceph/ceph.client.volumes.keyring root@osp9.lab:/etc/cephCreate file that contains just the authentication key on OpenStack controllers.[ceph@ceph1 ~]$ sudo ceph auth get-key client.volumes |ssh osp9.lab tee client.volumes.keySet permissions on keyring file so it can be accessed by Cinder.[root@osp9 ~(keystone_admin)]# chgrp cinder /etc/ceph/ceph.client.volumes.keyring[root@osp9 ~(keystone_admin)]# chmod 0640 /etc/ceph/ceph.client.volumes.keyringAdd keyring to Ceph configuration file on OpenStack controllers.[root@osp9 ~(keystone_admin)]#vi /etc/ceph/ceph.conf">
    <meta itemprop="datePublished" content="2016-09-12T00:00:00-07:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">OpenStack: Integrating Ceph as Storage Backend
</h1>
          

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2016-09-12T00:00:00-07:00">September 12, 2016</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          22 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
        <h3><img class="alignnone size-full wp-image-4639" src="/assets/2016/09/cephopenstack.png" alt="cephopenstack" width="320" height="178" /></h3>
<h3>Overview</h3>
<p>In this article we will discuss why Ceph is Perfect fit for OpenStack. We will see how to integrate three prominent OpenStack use cases with Ceph: Cinder (block storage), Glance (images) and Nova (VM virtual disks).</p>
<p>Integrating Ceph with OpenStack Series:</p>
<ul>
<li><a href="https://keithtenzer.com/2016/09/12/openstack-integrating-ceph-as-storage-backend/">Integrating Ceph with OpenStack Cinder, Glance and Nova</a></li>
<li><a href="https://keithtenzer.com/2017/03/30/openstack-swift-integration-with-ceph/">Integrating Ceph with Swift</a></li>
<li><a href="https://keithtenzer.com/2017/03/28/openstack-manila-integration-with-ceph/">Integrating Ceph with Manila</a></li>
</ul>
<p>Ceph provides unified scale-out storage, using commodity x86 hardware, that is self-healing and intelligently anticipates failures. It has become the defacto standard for software-defined storage. Ceph being an OpenSource project has enabled many vendors the ability to provide Ceph based software-defined storage systems. Ceph is not just limited to Companies like Red Hat, Suse, Mirantis, Ubuntu, etc. Integrated solutions from SanDisk, Fujitsu, HP, Dell, Samsung and many more exist today. There are even large-scale community built environments, <a href="https://www.openstack.org/summit/vancouver-2015/summit-videos/presentation/ceph-at-cern-a-year-in-the-life-of-a-petabyte-scale-block-storage-service">Cern </a>comes to mind, that provide storage services for 10,000s of VMs.</p>
<p><!--more--></p>
<p>Ceph is by no means limited to OpenStack, however this is where Ceph started gaining traction. Looking at latest OpenStack user survey, Ceph is by a large margin the clear leader for OpenStack storage. Page 42 in <a href="https://www.openstack.org/assets/survey/April-2016-User-Survey-Report.pdf">OpenStack April 2016 User Survey</a> reveals Ceph is 57% of OpenStack storage. The next is LVM (local storage) with 28% followed by NetApp with 9%. If we remove LVM, Ceph leads any other storage company by 48%, that is incredible. Why is that?</p>
<p>There are several reasons but I will give you my top three:</p>
<ul>
<li><strong>Ceph is a scale-out unified storage platform</strong>. OpenStack needs two things from storage: ability to scale with OpenStack itself and do so regardless of block (Cinder), File (Manila) or Object (Swift). Traditional storage vendors need to provide two or three different storage systems to achieve this. They don't scale the same and in most cases only scale-up in never-ending migration cycles. Their management capabilities are never truly integrated across broad spectrum of storage use-cases.</li>
<li><strong>Ceph is cost-effective</strong>. Ceph leverages Linux as an operating system instead of something proprietary. You can choose not only whom you purchase Ceph from, but also where you get your hardware. It can be same vendor or different. You can purchase commodity hardware, or even buy integrated solution of Ceph + Hardware from single vendor. There are even hyper-converged options for Ceph that are emerging (running Ceph services on compute nodes).</li>
<li><strong>Ceph is OpenSource project just like OpenStack</strong>. This allows for a much tighter integration and cross-project development. Proprietary vendors are always playing catch-up since they have secrets to protect and their influence is usually limited in Opensource communities, especially in OpenStack context.</li>
</ul>
<p>Below is an architecture Diagram that shows all the different OpenStack components that need storage. It shows how they integrate with Ceph and how Ceph provides a unified storage system that scales to fill all these use cases.</p>
<p><img class="alignnone  wp-image-4666" src="/assets/2016/09/ceph-openstack.jpg" alt="ceph-openstack" width="622" height="388" /></p>
<p>source: <a href="http://image.slidesharecdn.com/peanutbutterandjellyredhatsummit2016-160712040211/95/peanut-butter-and-jelly-mapping-the-deep-integration-between-ceph-and-openstack-25-638.jpg?cb=1468296361">Red Hat Summit 2016</a></p>
<p>If you are interested in more topics relating to ceph and OpenStack I recommend following: <a href="http://ceph.com/category/ceph-and-openstack/">http://ceph.com/category/ceph-and-openstack/</a></p>
<p>Ok enough talking about why Ceph and OpenStack are so great, lets get our hands dirty and see how to hook it up!</p>
<p>If you don't have a Ceph environment you can follow this <a href="https://keithtenzer.com/2016/09/12/ceph-1-3-lab-installation-and-configuration-guide/">article</a> on how to set one up quickly.</p>
<h3>Glance Integration</h3>
<p>Glance is the image service within OpenStack. By default images are stored locally on controllers and then copied to compute hosts when requested. The compute hosts cache the images but they need to be copied again, every time an image is updated.</p>
<p>Ceph provides backend for Glance allowing images to be stored in Ceph, instead of locally on controller and compute nodes. This greatly reduces network traffic for pulling images and increases performance since Ceph can clone images instead of copying them. In addition it makes migrating between OpenStack deployments or concepts like multi-site OpenStack much simpler.</p>
<p>Install ceph client used by Glance.</p>
<pre>[root@osp9 ~]# yum install -y python-rbd</pre>
<p>Create Ceph user and set home directory to /etc/ceph.</p>
<pre>[root@osp9 ~]# mkdir /etc/ceph
[root@osp9 ~]# useradd ceph
[root@osp9 ~]# passwd ceph</pre>
<p>Add ceph user to sudoers.</p>
<pre class="screen">cat &lt;&lt; EOF &gt;/etc/sudoers.d/ceph
ceph ALL = (root) NOPASSWD:ALL
Defaults:ceph !requiretty
EOF</pre>
<p>On Ceph admin node.</p>
<p>Create Ceph RBD Pool for Glance images.</p>
<pre>[ceph@ceph1 ~]$ sudo ceph osd pool create images 128</pre>
<p>Create keyring that will allow Glance access to pool.</p>
<pre>[ceph@ceph1 ~]$ sudo ceph auth get-or-create client.images mon 'allow r' osd 'allow class-read object_prefix rdb_children, allow rwx pool=images' -o /etc/ceph/ceph.client.images.keyring</pre>
<p>Copy the keyring to /etc/ceph on OpenStack controller.</p>
<pre>[ceph@ceph1 ~]$ scp /etc/ceph/ceph.client.images.keyring root@osp9.lab:/etc/ceph</pre>
<p>Copy /etc/ceph/ceph.conf configuration to OpenStack controller.</p>
<pre>[ceph@ceph1 ~]$ scp /etc/ceph/ceph.conf root@osp9.lab:/etc/ceph</pre>
<p>Set permissions so Glance can access Ceph keyring.</p>
<pre>[root@osp9 ~(keystone_admin)]# chgrp glance /etc/ceph/ceph.client.images.keyring
[root@osp9 ~(keystone_admin)]#<span style="line-height:1.7;">chmod 0640 /etc/ceph/ceph.client.images.keyring</span></pre>
<p>Add keyring file to Ceph configuration.</p>
<pre>[root@osp9 ~(keystone_admin)]# vi /etc/ceph/ceph.conf
[client.images]
keyring = /etc/ceph/ceph.client.images.keyring</pre>
<p>Create backup of original Glance configuration.</p>
<pre>[root@osp9 ~(keystone_admin)]# cp /etc/glance/glance-api.conf /etc/glance/glance-api.conf.orig</pre>
<p>Update Glance configuration.</p>
<pre>[root@osp9 ~]# vi /etc/glance/glance-api.conf
[glance_store]
stores = glance.store.rbd.Store
default_store = rbd
rbd_store_pool = images
rbd_store_user = images
rbd_store_ceph_conf = /etc/ceph/ceph.conf</pre>
<p>Restart Glance.</p>
<pre>[root@osp9 ~(keystone_admin)]# systemctl restart openstack-glance-api</pre>
<p>Download Cirros images and add it into Glance.</p>
<pre>[root@osp9 ~(keystone_admin)]# wget http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img</pre>
<p>Convert QCOW2 to RAW. It is recommended for Ceph to always use RAW format.</p>
<pre>[root@osp9 ~(keystone_admin)]#qemu-img convert cirros-0.3.4-x86_64-disk.img cirros-0.3.4-x86_64-disk.raw</pre>
<p>Add image to Glance.</p>
<pre>[root@osp9 ~(keystone_admin)]#glance image-create --name "Cirros 0.3.4" --disk-format raw --container-format bare --visibility public --file cirros-0.3.4-x86_64-disk.raw</pre>
<pre>+------------------+--------------------------------------+
| Property | Value |
+------------------+--------------------------------------+
| checksum | ee1eca47dc88f4879d8a229cc70a07c6 |
| container_format | bare |
| created_at | 2016-09-07T12:29:23Z |
| disk_format | qcow2 |
| id | a55e9417-67af-43c5-a342-85d2c4c483f7 |
| min_disk | 0 |
| min_ram | 0 |
| name | Cirros 0.3.4 |
| owner | dd6a4ed994d04255a451da66b68a8057 |
| protected | False |
| size | 13287936 |
| status | active |
| tags | [] |
| updated_at | 2016-09-07T12:29:27Z |
| virtual_size | None |
| visibility | public |
+------------------+--------------------------------------+</pre>
<p>Check that glance image exists in Ceph.</p>
<pre>[ceph@ceph1 ceph-config]$ sudo rbd ls images
a55e9417-67af-43c5-a342-85d2c4c483f7</pre>
<pre>[ceph@ceph1 ceph-config]$ sudo rbd info images/a55e9417-67af-43c5-a342-85d2c4c483f7
rbd image 'a55e9417-67af-43c5-a342-85d2c4c483f7':
 size 12976 kB in 2 objects
 order 23 (8192 kB objects)
 block_name_prefix: rbd_data.183e54fd29b46
 format: 2
 features: layering, striping
 flags:
 stripe unit: 8192 kB
 stripe count: 1</pre>
<h3>Cinder Integration</h3>
<p>Cinder is the block storage service in OpenStack. Cinder provides an abstraction around block storage and allows vendors to integrate by providing a driver. In Ceph, each storage pool can be mapped to a different Cinder backend. This allows for creating storage services such as gold, silver or bronze. You can decide for example that gold should be fast SSD disks that are replicated three times, while silver only should be replicated two times and bronze should use slower disks with erasure coding.</p>
<p>Create Ceph pool for cinder volumes.</p>
<pre>[ceph@ceph1 ~]$ sudo ceph osd pool create volumes 128</pre>
<p>Create keyring to grant cinder access.</p>
<pre>[ceph@ceph1 ~]$ sudo ceph auth get-or-create client.volumes mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rx pool=images' -o /etc/ceph/ceph.client.volumes.keyring</pre>
<p>Copy keyring to OpenStack controllers.</p>
<pre>[ceph@ceph1 ~]$ scp /etc/ceph/ceph.client.volumes.keyring root@osp9.lab:/etc/ceph</pre>
<p>Create file that contains just the authentication key on OpenStack controllers.</p>
<pre>[ceph@ceph1 ~]$ sudo ceph auth get-key client.volumes |ssh osp9.lab tee client.volumes.key</pre>
<p>Set permissions on keyring file so it can be accessed by Cinder.</p>
<pre>[root@osp9 ~(keystone_admin)]# chgrp cinder /etc/ceph/ceph.client.volumes.keyring
[root@osp9 ~(keystone_admin)]# chmod 0640 /etc/ceph/ceph.client.volumes.keyring</pre>
<p>Add keyring to Ceph configuration file on OpenStack controllers.</p>
<pre>[root@osp9 ~(keystone_admin)]#vi /etc/ceph/ceph.conf

[client.volumes]
keyring = /etc/ceph/ceph.client.volumes.keyring</pre>
<p>Give KVM Hypervisor access to Ceph.</p>
<pre>[root@osp9 ~(keystone_admin)]# uuidgen |tee /etc/ceph/cinder.uuid.txt</pre>
<p>Create a secret in virsh so KVM can access Ceph pool for cinder volumes.</p>
<pre>[root@osp9 ~(keystone_admin)]#vi /etc/ceph/cinder.xml

&lt;secret ephemeral="no" private="no"&gt;
 &lt;uuid&gt;ce6d1549-4d63-476b-afb6-88f0b196414f&lt;/uuid&gt;
 &lt;usage type="ceph"&gt;
 &lt;name&gt;client.volumes secret&lt;/name&gt;
 &lt;/usage&gt;
&lt;/secret&gt;</pre>
<pre>[root@osp9 ceph]# virsh secret-define --file /etc/ceph/cinder.xml</pre>
<pre>[root@osp9 ~(keystone_admin)]# virsh secret-set-value --secret ce6d1549-4d63-476b-afb6-88f0b196414f --base64 $(cat /etc/ceph/client.volumes.key)</pre>
<p>Add Ceph backend for Cinder.</p>
<pre>[root@osp9 ~(keystone_admin)]#vi /etc/cinder/cinder.conf

[rbd]
volume_driver = cinder.volume.drivers.rbd.RBDDriver
rbd_pool = volumes
rbd_ceph_conf = /etc/ceph/ceph.conf
rbd_flatten_volume_from_snapshot = false
rbd_max_clone_depth = 5
rbd_store_chunk_size = 4
rados_connect_timeout = -1
glance_api_version = 2
rbd_user = volumes
rbd_secret_uuid = ce6d1549-4d63-476b-afb6-88f0b196414f</pre>
<p>Restart Cinder service on all controllers.</p>
<pre>[root@osp9 ~(keystone_admin)]# openstack-service restart cinder</pre>
<p>Create a cinder volume.</p>
<pre>[root@osp9 ~(keystone_admin)]# cinder create --display-name="test" 1
+--------------------------------+--------------------------------------+
| Property | Value |
+--------------------------------+--------------------------------------+
| attachments | [] |
| availability_zone | nova |
| bootable | false |
| consistencygroup_id | None |
| created_at | 2016-09-08T10:58:17.000000 |
| description | None |
| encrypted | False |
| id | d251bb74-5c5c-4c40-a15b-2a4a17bbed8b |
| metadata | {} |
| migration_status | None |
| multiattach | False |
| name | test |
| os-vol-host-attr:host | None |
| os-vol-mig-status-attr:migstat | None |
| os-vol-mig-status-attr:name_id | None |
| os-vol-tenant-attr:tenant_id | dd6a4ed994d04255a451da66b68a8057 |
| replication_status | disabled |
| size | 1 |
| snapshot_id | None |
| source_volid | None |
| status | creating |
| updated_at | None |
| user_id | 783d6e51e611400c80458de5d735897e |
| volume_type | None |
+--------------------------------+--------------------------------------+


List new cinder volume

[root@osp9 ~(keystone_admin)]# cinder list
+--------------------------------------+-----------+------+------+-------------+----------+-------------+
| ID | Status | Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+------+------+-------------+----------+-------------+
| d251bb74-5c5c-4c40-a15b-2a4a17bbed8b | available | test | 1 | - | false | |
+--------------------------------------+-----------+------+------+-------------+----------+-------------+</pre>
<p>List cinder volume in ceph.</p>
<pre>[ceph@ceph1 ~]$ sudo rbd ls volumes
volume-d251bb74-5c5c-4c40-a15b-2a4a17bbed8b</pre>
<pre>[ceph@ceph1 ~]$ sudo rbd info volumes/volume-d251bb74-5c5c-4c40-a15b-2a4a17bbed8b
rbd image 'volume-d251bb74-5c5c-4c40-a15b-2a4a17bbed8b':
 size 1024 MB in 256 objects
 order 22 (4096 kB objects)
 block_name_prefix: rbd_data.2033b50c26d41
 format: 2
 features: layering, striping
 flags:
 stripe unit: 4096 kB
 stripe count: 1</pre>
<h3>Integrating Ceph with Nova Compute</h3>
<p>Nova is the compute service within OpenStack. Nova stores virtual disks images associated with running VMs by default, locally on Hypervisor under /var/lib/nova/instances. There are a few drawbacks to using local storage on compute nodes for virtual disk images:</p>
<ul>
<li>Images are stored under root filesystem. Large images can cause filesystem to fill up, thus crashing compute nodes.</li>
<li>A disk crash on compute node could cause loss of virtual disk and as such a VM recovery would be impossible.</li>
</ul>
<p>Ceph is one of the storage backends that can integrate directly with Nova. In this section we will see how to configure that.</p>
<pre>[ceph@ceph1 ~]$ sudo ceph osd pool create vms 128</pre>
<p>Create authentication keyring for Nova.</p>
<pre>[ceph@ceph1 ~]$ sudo ceph auth get-or-create client.nova mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=vms, allow rx pool=images' -o /etc/ceph/ceph.client.nova.keyring</pre>
<p>Copy keyring to OpenStack controllers.</p>
<pre>[ceph@ceph1 ~]$ scp /etc/ceph/ceph.client.nova.keyring root@osp9.lab:/etc/ceph</pre>
<p>Create key file on OpenStack controllers.</p>
<pre>[ceph@ceph1 ~]$ sudo ceph auth get-key client.nova |ssh osp9.lab tee client.nova.key</pre>
<p>Set permissions on keyring file so it can be accessed by Nova service.</p>
<pre>[root@osp9 ~(keystone_admin)]# chgrp nova /etc/ceph/ceph.client.nova.keyring
[root@osp9 ~(keystone_admin)]# chmod 0640 /etc/ceph/ceph.client.nova.keyring</pre>
<p>Ensure the required rpm packages are installed.</p>
<pre>[root@osp9 ~(keystone_admin)]# yum list installed python-rbd ceph-common
Loaded plugins: product-id, search-disabled-repos, subscription-manager
Installed Packages
ceph-common.x86_64 1:0.94.5-15.el7cp @rhel-7-server-rhceph-1.3-mon-rpms
python-rbd.x86_64 1:0.94.5-15.el7cp @rhel-7-server-rhceph-1.3-mon-rpms</pre>
<p>Update Ceph configuration.</p>
<pre>[root@osp9 ~(keystone_admin)]#vi /etc/ceph/ceph.conf

[client.nova]
keyring = /etc/ceph/ceph.client.nova.keyring</pre>
<p>Give KVM access to Ceph.</p>
<pre>[root@osp9 ~(keystone_admin)]# uuidgen |tee /etc/ceph/nova.uuid.txt</pre>
<p>Create a secret in virsh so KVM can access Ceph pool for cinder volumes.</p>
<pre>[root@osp9 ~(keystone_admin)]#vi /etc/ceph/nova.xml

&lt;secret ephemeral="no" private="no"&gt;
&lt;uuid&gt;c89c0a90-9648-49eb-b443-c97adb538f23&lt;/uuid&gt;
&lt;usage type="ceph"&gt;
&lt;name&gt;client.volumes secret&lt;/name&gt;
&lt;/usage&gt;
&lt;/secret&gt;</pre>
<pre>[root@osp9 ~(keystone_admin)]# virsh secret-define --file /etc/ceph/nova.xml</pre>
<pre>[root@osp9 ~(keystone_admin)]# virsh secret-set-value --secret c89c0a90-9648-49eb-b443-c97adb538f23 --base64 $(cat /etc/ceph/client.nova.key)</pre>
<p>Make backup of Nova configuration.</p>
<pre>[root@osp9 ~(keystone_admin)]# cp /etc/nova/nova.conf /etc/nova/nova.conf.orig</pre>
<p>Update Nova configuration to use Ceph backend.</p>
<pre>[root@osp9 ~(keystone_admin)]#vi /etc/nova/nova.conf
force_raw_images = True
disk_cachemodes = writeback

[libvirt]
images_type = rbd
images_rbd_pool = vms
images_rbd_ceph_conf = /etc/ceph/ceph.conf
rbd_user = vms
rbd_secret_uuid = c89c0a90-9648-49eb-b443-c97adb538f23</pre>
<p>Restart Nova services.</p>
<pre>[root@osp9 ~(keystone_admin)]# systemctl restart openstack-nova-compute</pre>
<p>List Neutron networks.</p>
<pre>[root@osp9 ~(keystone_admin)]# neutron net-list
+--------------------------------------+---------+-----------------------------------------------------+
| id | name | subnets |
+--------------------------------------+---------+-----------------------------------------------------+
| 4683d03d-30fc-4dd1-9b5f-eccd87340e70 | private | ef909061-34ee-4b67-80a9-829ea0a862d0 10.10.1.0/24 |
| 8d35a938-5e4f-46a2-8948-b8c4d752e81e | public | bb2b65e7-ab41-4792-8591-91507784b8d8 192.168.0.0/24 |
+--------------------------------------+---------+-----------------------------------------------------+</pre>
<p>Start ephemeral VM instance using Cirros image that was added in the steps for Glance.</p>
<pre>[root@osp9 ~(keystone_admin)]# nova boot --flavor m1.small --nic net-id=4683d03d-30fc-4dd1-9b5f-eccd87340e70 --image='Cirros 0.3.4' cephvm
+--------------------------------------+-----------------------------------------------------+
| Property | Value |
+--------------------------------------+-----------------------------------------------------+
| OS-DCF:diskConfig | MANUAL |
| OS-EXT-AZ:availability_zone | |
| OS-EXT-SRV-ATTR:host | - |
| OS-EXT-SRV-ATTR:hypervisor_hostname | - |
| OS-EXT-SRV-ATTR:instance_name | instance-00000001 |
| OS-EXT-STS:power_state | 0 |
| OS-EXT-STS:task_state | scheduling |
| OS-EXT-STS:vm_state | building |
| OS-SRV-USG:launched_at | - |
| OS-SRV-USG:terminated_at | - |
| accessIPv4 | |
| accessIPv6 | |
| adminPass | wzKrvK3miVJ3 |
| config_drive | |
| created | 2016-09-08T11:41:29Z |
| flavor | m1.small (2) |
| hostId | |
| id | 85c66004-e8c6-497e-b5d3-b949a1666c90 |
| image | Cirros 0.3.4 (a55e9417-67af-43c5-a342-85d2c4c483f7) |
| key_name | - |
| metadata | {} |
| name | cephvm |
| os-extended-volumes:volumes_attached | [] |
| progress | 0 |
| security_groups | default |
| status | BUILD |
| tenant_id | dd6a4ed994d04255a451da66b68a8057 |
| updated | 2016-09-08T11:41:33Z |
| user_id | 783d6e51e611400c80458de5d735897e |
+--------------------------------------+-----------------------------------------------------+</pre>
<p>Wait until the VM is active.</p>
<pre>[root@osp9 ceph(keystone_admin)]# nova list
+--------------------------------------+--------+--------+------------+-------------+---------------------+
| ID | Name | Status | Task State | Power State | Networks |
+--------------------------------------+--------+--------+------------+-------------+---------------------+
| 8ca3e74e-cd52-42a6-acec-13a5b8bda53c | cephvm | ACTIVE | - | Running | private=10.10.1.106 |
+--------------------------------------+--------+--------+------------+-------------+---------------------+</pre>
<p>List images in Ceph vms pool. We should now see the image is stored in Ceph.</p>
<pre>[ceph@ceph1 ~]$ sudo rbd -p vms ls
8ca3e74e-cd52-42a6-acec-13a5b8bda53c_disk</pre>
<h3>Troubleshooting</h3>
<h4>Unable to delete Glance Images stored in Ceph RBD</h4>
<pre>[root@osp9 ceph(keystone_admin)]# nova image-list
+--------------------------------------+--------------+--------+--------+
| ID | Name | Status | Server |
+--------------------------------------+--------------+--------+--------+
| a55e9417-67af-43c5-a342-85d2c4c483f7 | Cirros 0.3.4 | ACTIVE | |
| 34510bb3-da95-4cb1-8a66-59f572ec0a5d | test123 | ACTIVE | |
| cf56345e-1454-4775-84f6-781912ce242b | test456 | ACTIVE | |
+--------------------------------------+--------------+--------+--------+</pre>
<pre>[root@osp9 ceph(keystone_admin)]# rbd -p images snap unprotect cf56345e-1454-4775-84f6-781912ce242b@snap
[root@osp9 ceph(keystone_admin)]# rbd -p images snap rm cf56345e-1454-4775-84f6-781912ce242b@snap
[root@osp9 ceph(keystone_admin)]# glance image-delete cf56345e-1454-4775-84f6-781912ce242b</pre>
<h3>Summary</h3>
<p>In this article we discussed how OpenStack and Ceph fit perfectly together. We discussed some of the use cases around glance, cinder and nova. Finally we went through steps to integrate Ceph with those use cases. I hope you enjoyed the article and found the information useful. Please share your feedback.</p>
<p>Happy Cephing!</p>
<p>(c) 2016 Keith Tenzer</p>
<p>&nbsp;</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#ceph" class="page__taxonomy-item" rel="tag">Ceph</a><span class="sep">, </span>
    
      <a href="/tags/#cinder" class="page__taxonomy-item" rel="tag">Cinder</a><span class="sep">, </span>
    
      <a href="/tags/#storage" class="page__taxonomy-item" rel="tag">Storage</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#openstack" class="page__taxonomy-item" rel="tag">OpenStack</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2016-09-12T00:00:00-07:00">September 12, 2016</time></p>


      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?via=keithtenzer&text=OpenStack%3A+Integrating+Ceph+as+Storage+Backend%20http%3A%2F%2Flocalhost%3A4000%2F2016%2F09%2F12%2Fopenstack-integrating-ceph-as-storage-backend%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2F2016%2F09%2F12%2Fopenstack-integrating-ceph-as-storage-backend%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2F2016%2F09%2F12%2Fopenstack-integrating-ceph-as-storage-backend%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/storage/ceph-1-3-lab-installation-and-configuration-guide/" class="pagination--pager" title="Ceph 1.3 Lab Installation and Configuration Guide
">Previous</a>
    
    
      <a href="/azure/openshift/enterprise-container-platform-in-the-cloud-openshift-on-azure-secured-by-azure-ad/" class="pagination--pager" title="Enterprise Container Platform in the Cloud: OpenShift on Azure secured by Azure AD
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/temporal/my-first-day-at-temporal/" rel="permalink">My First Day at Temporal
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2022-08-15T00:00:00-07:00">August 15, 2022</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          9 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
Overview
Today is my first day at temporal and with that I wanted to share some thoughts around my decision, why Temporal and my experience thus far. As you...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/linux/blog-with-gitops-practices-and-github/" rel="permalink">Blog with Gitops Practices and GitHub
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2022-02-10T00:00:00-08:00">February 10, 2022</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          13 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Overview
Want to build your brand, while living the gitops revolution and not paying anything for it? That is exactly what this article will walk you through...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/linux/The-Fedora-Workstation-Experience/" rel="permalink">The Fedora Workstation Experience
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2022-01-10T00:00:00-08:00">January 10, 2022</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          10 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
Overview
A lot of people always ask me what is the best way to contribute to opensource? Of course contributing code, documentation, spreading the gospel or...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/openshift/building-ansible-operators-1-2-3/" rel="permalink">Building Ansible Operators 1-2-3
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2021-12-03T00:00:00-08:00">December 3, 2021</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          10 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Overview
In this article we will go step by step to build a Kubernetes Operator using Ansible and the Operator Framework. Operators provide the ability to no...</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="https://twitter.com/keithtenzer"" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
      
        
          <li><a href="https://github.com/ktenzer" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2022 Keith Tenzer. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>







  </body>
</html>
