var store = [{
        "title": "AWS Security Lessons Learned",
        "excerpt":"My Story  Once upon a time not too long ago, I had a project to transport data in and out of S3 using Java. Things started off rather smoothly. Within about 30 minutes I had my Amazon Web Services (AWS) account active, had the Java SDK pulled into Eclipse through Maven and was testing my first AWS S3 API calls. Everything was fine except I embedded my AWS access key in the code! I spent a few hours prototyping, testing API calls and understanding how I wanted to structure the code for this project. Along the way I got side-tracked, switched to working on other things and eventually came back to my AWS S3 project. At this point I had a pretty good prototype for structuring data in S3 as well as getting data in and out. I wanted to save my work and had forgotten all about the fact that my AWS access key was still embedded in the code. I created a public Github repo and checked in the code, it had been a long day and was about to get much longer...  A few hours went by and I got a call from Amazon Web Services, there had been suspicious activity on my account. At this point I started to realise what might have happened and I rushed home. Upon logging in I found 20 XXL EC2 instances running in every AWS region and in just a few hours my bill was already over $700 USD. I couldn't believe it and thought why on Earth would anyone want to startup EC2 instances? I am just going to delete them, what was the purpose of this?  I quickly shutdown all the EC2 instances and expunged them, except one. I wanted to take a closer look and see what was actually running in the instance. Upon gaining access I looked at the process table and saw to my surprise Bitcoin processes running and chewing on the CPU. Bitcoin!!! But of course, it was now so clear, I provided some happy miners free CPU for a few hours and they took full advantage of my stupidity. This was the first time I had ever been hacked or had my security (knowingly) breeched. I felt vulnerable, foolish, and extremely frustrated.  Finally I phoned Amazon Web Services and explained what happened. They confirmed and told me that this happens all too often and this kind of theft is on the rise. Bitcoin miners scrub all code sites like Github and search for patterns like \"AWS Secret Key\". If they find them, they will try to deploy Bitcoin servers on unsuspecting accounts and why not it is easy money, right? In a quick Github search I found more keys than I knew what to do with, very disturbing. I tried to get Amazon Web Services to track down the culprits but unfortunately they used TOR which masked outbound traffic so there was no way to trace the source. Thankfully Amazon Web Services credited my account so I didn't have to pay the penalty for my carelessness.  What about DevOps?  More and more DevOps teams are using the cloud as it enables getting products to market faster in many cases. Many are startups and were born in the cloud. However a growing number are enterprise IT organisations not used to dealing with security in the cloud. Worse, many of these are shadow IT groups, meaning groups that have abandoned their internal IT and gone to the cloud without an enterprise strategy. This is very similar to my personal experience, they are just jumping into the cloud without a complete security strategy. There have already been numerous companies which have had to close their doors due to cloud security breaches and the threat is real. So what is different in the cloud? The main thing is that we are used to working and trusting our internal IT for protecting us. We as developers have become very security complacent and have not had to be responsible for securing our environments. With the cloud that all changes, everyone must be re-trained on security and become stewards for protecting company assets. The firewall is the people. Don't jump into the cloud until you have your security straight!  Lessons Learned   Lesson 1: Don't just jump into cloud and start doing stuff, ensure you have appropriate security setup for your environment first Lesson 2: Never ever save your AWS access key in code or any file in your code branch. As soon as you do this you have completely compromised your security Lesson 3: Always use multi-factor authentication Lesson 4: Always create an AWS IAM user and always give that IAM user only access to resources it needs through IAM policies before generating AWS keys Lesson 5: Store AWS key in user environment or credentials file outside of code branch Lesson 6: Update and change your keys on regular basis Lesson 7: Consider using only private repos in Github or ensure code is private and not public  Configure AWS Permissions   Create IAM user     Create group for each resource you want to grant for example, one group for S3, another for SNS, etc     Add permissions to the group    How to properly store AWS credentials  There are two options for storing the AWS access and secret key. You can either save them in a credentials file or the user environment. I would recommend the user environment and this is typically what I use.  Storing AWS key in credentials file   Create the following file: ~/.aws/credentials In the AWS console go to the user and create new access key by selecting the \"manage access keys\" option. Update the credentials file ~/.aws/credentials  [default] aws_access_key_id=&lt;access key&gt; aws_secret_access_key=&lt;secret key&gt;   Get credentials from credentials file. Below is an example in Java:  [code language=\"java\"]  public AWSCredentials getAWSCredentialsFromFile() throws Exception {       AWSCredentials credentials = null;       try {            credentials = new ProfileCredentialsProvider().getCredentials();       } catch (Exception e) {            throw new Exception (&quot;Cannot load the credentials from the credential profiles file. Please make sure that your credentials file is at the correct location (~/.aws/credentials), and is in valid format.&quot;, e);       }       return credentials;  }  [/code]  Storing AWS key in environment   In the AWS console go to the user and create new access key by selecting the \"manage access keys\" option. Add access key and secret key to the user's .bashrc file under ~/.bachrc  AWS_ACCESS_KEY_ID=&lt;access key&gt; AWS_SECRET_ACCESS_KEY=&lt;secret key&gt;   Get the credentials from user environment. Below is an example in Java:  [code language=\"java\"]  public AWSCredentials getAwsCredentialsFromEnvironment() throws IOException {       EnvironmentVariableCredentialsProvider credentials = new EnvironmentVariableCredentialsProvider();       return credentials.getCredentials();  }  [/code]  (c) 2014 Keith Tenzer  &nbsp;  ","categories": ["Cloud"],
        "tags": ["AWS","Cloud","DevOps","Java","Security"],
        "url": "/cloud/aws-security-lessons-learned/",
        "teaser": null
      },{
        "title": "The meaning of \"midnight code junkie\"",
        "excerpt":"There is something romantic about coding, the flickering of the monitor, your ideas coming to life, no rules and in the darkness of night when your are too tired to think of why something cannot be done, a magical moment may just happen. For the \"midnight code junkie\" it is about these moments of discovery, charting a path to the unknown and of course the journey itself. It isn't about user stories, scrums, or a software product...no it is just beautiful code that you create and most importantly share. In our wildest of dreams such code could one day come to life and be something useful to society. We code for those moments, when job and family time allow as we are \"midnight code junkies\".  If you're a \"midnight code junkie\" then speak up, tell your story, you are not alone!  (c) 2014 Keith Tenzer  ","categories": ["General"],
        "tags": [],
        "url": "/general/the-meaning-of-midnight-code-junkie/",
        "teaser": null
      },{
        "title": "Building Storage Services in OpenStack on NetApp - Part I of III",
        "excerpt":"Welcome to part one of the three-part series on creating storage services in OpenStack on NetApp. In this post we will give a brief overview of OpenStack, discuss distribution options and setup a OpenStack environment using Red Hat RDO. In part two of this series we will look at how to configure NetApp storage for OpenStack environments. In part three we will create storage services in OpenStack built on NetApp storage.   Building Storage Services in OpenStack on NetApp - Part II of III Building Storage Services in OpenStack on NetApp - Part III of III  Overview  OpenStack is an interrelated set of software components that enable you to build, manage and provision private or public cloud computing platforms. OpenStack pools compute, storage and networking resources together and allows virtual machines or other instances to be deployed using those resources on the fly enabling horizontal scaling.    OpenStack Projects  Each OpenStack software component is considered its own project or program. It has its own development team, resources and project schedule.  There are two important OpenStack project types:   Core projects - official projects with full access to OpenStack brand and assets Incubation projects - projects on an official track to become core projects with partial access to OpenStack branding and assets  The core projects for OpenStack are as follows:   Nova (compute) Cinder (block storage) Swift (object storage) Glance (images service) Keystone (identity and authentication) Horizon (dashboard and UI) Neutron (networking) Ceilometer (telemetry and stats) Heat (orchestration) Trove (database services) Sahara (data processing) Ironic (bare metal) Zaqar (queue services) Barbican (key management services) Designate (DNS services) Oslo (common libraries) TripleO (deployment)  Incubation projects such as Manilla which offers shared file services to OpenStack are very interesting to follow, however for deployments we should focus on the official core projects.  OpenStack Distributions  There is a lot of variation with OpenStack since it is a culmination of interrelated projects that form a cloud computing platform. You can certainly do your own custom deployment and just install the projects you need and define your own distribution however I would not recommend this, especially for production environments. The purpose of a distribution is to leverage the vendors expertise, wrap projects or dependencies together and give you a platform which can be supported. There are of course a plethora of distributions to choose from, even distribution methods. If you are interested in reviewing the distributions and support options visit the OpenStack marketplace. I would recommend choosing a distribution which supports the dependencies and someone who has a lot of experience as well as success in providing support for open source software.  In addition to the supported distributions there is a very interesting option from Red Hat called RDO. The concept is very similar to what Red Hat does in regards to Fedora actually. RDO allows you to run either Fedora, Red Hat Enterprise Linux (RHEL) or CentOS and combines those platforms with the most current release of OpenStack.  Deploying OpenStack using RDO  The first step to building a cloud computing platform is a proof of concept. Moving your infrastructure to a cloud computing platform changes everything, including your business processes. It is definitely not something that happens over night. RDO is the perfect thing to use to standup a OpenStack environment using production Linux with production OpenStack software releases. Red Hat does not offer commercial support for RDO so once a decision is made to deploy a production OpenStack environment you will most likely want to use a distribution from the OpenStack marketplace. One can deploy RDO with either a manual or automatic configuration, we will discuss both.  Steps for deploying RDO with manual configuration   Install Fedora 20 on bare metal  Note: I have tested Fedora 20 running on ESXi 5.5, just make sure you have virtual machine hardware version 10 (required for nested virtualization support)   Configure networking so that you have outbound access to the Internet Update current packages  sudo yum update -y  Setup RDO repositories  sudo yum install -y https://rdo.fedorapeople.org/rdo-release.rpm  Install packstack installer  sudo yum install -y openstack-packstack  Run packstack to install OpenStack  packstack --allinone   OpenStack takes a while to install, usually around 30 minutes, once the installation is done you can login into the horizon dashboard http://&lt;IP&gt;/dashboard. If you come across any issues check the workarounds.    Steps for deploying RDO with automatic configuration of NetApp Cinder Backend  Automatic configuration allows us to generate an answers file that we can then customize. The advantage is that we are able to do some of the configuration during installation and in case we ever need to repeat the installation we have a blueprint. Using the answers file is the recommended way to install RDO. In this case we will configure Cinder to use NetApp storage backend through RDO packstack installer.  packstack --gen-answer-file=~/packstack-answer.txt [code language=\"text\"]  CONFIG_CINDER_BACKEND=netapp  CONFIG_CINDER_NETAPP_STORAGE_FAMILY=ontap_cluster  CONFIG_CINDER_NETAPP_STORAGE_PROTOCOL=nfs  CONFIG_CINDER_NETAPP_VSERVER=&lt;storage virtual machine&gt;  CONFIG_CINDER_NETAPP_HOSTNAME=&lt;cluster mgmt vserver&gt;  CONFIG_CINDER_NETAPP_SERVER_PORT=80  CONFIG_CINDER_NETAPP_LOGIN=&lt;username&gt;  CONFIG_CINDER_NETAPP_PASSWORD=&lt;password&gt;  CONFIG_CINDER_NETAPP_NFS_SHARES_CONFIG=/etc/cinder/cdotNfs_exports.conf  CONFIG_CINDER_NETAPP_COPYOFFLOAD_TOOL_PATH=/usr/bin/na_copyoffload_64  [/code]  Note: before proceeding with the next step you need to make sure you have defined NFS exports in the /etc/cinder/cdotNfs_exports.conf and configured NetApp storage. These steps are explained in part II and part III of this series.  packstack --answer-file=~/packstack-answer.txt For more information check the detailed guide on using packstack to install Cinder with NetApp storage backends.  Authenticating to OpenStack environment  After installation completes two tenants are created: admin and demo. Keystone will create two files under /root which provide credentials for both tenants.    cat /root/keystonerc_admin   [code language=\"text\"]  export OS_USERNAME=admin  export OS_TENANT_NAME=admin  export OS_PASSWORD=d96c4ad2c6aa55af  export OS_AUTH_URL=http://&lt;IP&gt;:5000/v2.0/  export PS1='[\\u@\\h \\W(keystone_admin)]\\$ '  [/code]  You can use the admin user and password in the keystone source file to authenticate to Horizon or the OpenStack CLI.  Create Private Network  By default RDO will create a public network but for creating instances we need to use a private network. Since we would eventually like to connect instances to NetApp storage this network should be an internal private network that can reach our NetApp Storage Virtual Machines (SVMs). In my environment I used the same network for instances and SVM.   Under project-&gt;network-&gt;networks select the \"Create Network\" option to start the wizard Enter a name for network, I suggest private Add CIDR for network    &nbsp;   Add IP pool to be used for DHCP    Once our private network exists we are ready for instance creation  Launch Instance  At this point RDO has configured KVM as the hypervisor, installed OpenStack and configured OpenStack services. In order to verify things are working the best thing to do is to provision an instance. RDO configures one image called \"cirros\" and we can use this to test our installation.   In the Horizon dashboard under project-&gt;compute-&gt;instances select the \"Launch Instance\" button Give instance a name, select \"boot from image\" as boot source and select the \"cirros\" image     Select the \"networking\" tab and add the private network    &nbsp;   Launch the instance  The instance should start and be running shortly. This tests that the main core functionality between Nova (compute), Glance (images service) and Cinder (block storage). If things are working we are ready to configure NetApp storage for our OpenStack environment.  Troubleshooting  There are a few things I have run into and if you are having problems you should check them.   Disable \"SE Linux\", I have seen issues with \"SE Linux\" and OpenStack  setenforce 0 To make change permanent edit /etc/selinux/config   Check hostname resolution  make sure hostname can be resolved if dns is not configured check /etc/hosts and /etc/sysconfig/network   Check networking make sure you can ping out to the private networks and management networks If you are having network issues, temporarily shutdown firewall and iptables until you have things working and then turn them back on Check to make sure your hardware supports virtualization  Intel: grep --color vmx /proc/cpuinfo AMD: grep --color svm /proc/cpuinfo   Check the Nova, Glance and Cinder logs  /var/log/nova /var/log/glance /var/log/cinder    The next post in the series will take a look at how to configure the NetApp storage system for OpenStack environments.  (c) 2014 Keith Tenzer  ","categories": ["OpenStack"],
        "tags": ["Cloud","Clustered Data ONTAP","NetApp","OpenStack","Red Hat RDO"],
        "url": "/openstack/creating-storage-services-in-openstack-on-netapp-part-i-of-iii/",
        "teaser": null
      },{
        "title": "Building Storage Services in OpenStack on NetApp - Part II of III",
        "excerpt":"Welcome to part two of the three-part series on creating storage services in OpenStack on NetApp. In this post we will look at how to configure NetApp storage in a OpenStack environment. In part one of the series we looked at how to install and configure OpenStack. In part three we will look at how to configure storage services in OpenStack.   Building Storage Services in OpenStack on NetApp - Part I of III Building Storage Services in OpenStack on NetApp - Part III of III  Overview  NetApp has the most unified storage combined with the best feature set in the industry. NetApp provides the only storage operating system which allows you to manage your data on-premise, off-premise, in private clouds, public clouds or even hybrid clouds. NetApp is a perfect fit for OpenStack as a storage platform. In this post we will explore why, discuss options and show you how to properly configure your NetApp storage system for OpenStack.  Below is a diagram illustrating the broad-reaching data management capabilities of Data ONTAP:    Storage Considerations  There are currently two OpenStack services which support storage: Cinder (block) and Swift (object). Manilla (file services) is an incubation project and slated for the kilo release later in 2015. For the purpose of this post I am going to focus on Cinder.  E-Series vs FAS  NetApp offers two product lines for storage the E-Series and FAS lines. I would not recommend choosing between E-Series or FAS as they have different uses in terms of OpenStack. E-Series is best used for Swift as a backend to object storage. It is very fast and has a great price to performance value. Swift is an object store and abstracts storage management. You want a fast, resilient, highly available storage system and that is the E-Series. The E-Series storage also has a nice feature called \"Dynamic Disk Pools\" which implements CRUSH (erasure coding) and it reduces storage footprint from 3x to just 1.25x for Swift in OpenStack environments.  FAS is the clear choice when it comes to Cinder storage where you want data management features and capabilities exposed in OpenStack.  ONTAP 7mode vs Clustered Data ONTAP  In the FAS world there are also two choices. Personally I don't think it is much of a choice. Clustered Data ONTAP is the evolution of FAS storage. It is a scale-out architecture and scales in three dimensions: operations, performance and capacity. It is by far the most complete storage system ever built and certainly most capable storage system on the market today. Clustered Data ONTAP virtualizes storage into what are called Storage Virtual Machines (SVMs) which enable secure multi-tenancy. If we think about cloud computing platforms, multi-tenancy is the basis and a raw ingredient. Clustered Data ONTAP provides world-class data management, scalability in multiple dimensions, unified protocols (ISCSI, NFS, CIFS, and FC) and non-disruptive always-on operations in the same storage system. The only reason not to go with Clustered Data ONTAP is that you already have 7mode and don't have budget, otherwise it is no-brainer. NetApp of course supports 7mode with its OpenStack driver so this is an option for those that can't go with Clustered Data ONTAP.  Storage Protocol  There are two options for connecting storage to Cinder: ISCSI or NFS. At this time Fiber Chanel (FC) is not supported and if we consider cloud computing architectures, FC does not really have the horizontal scalability required. Let's face it we don't need a second segregated island for data traffic, IP networks are proven and scale, FC is simply out of this picture.  Cinder exposes block devices to virtual machines or instances but it can reside on both NFS or ISCSI storage. I would recommend NFS as NFS storage is much easier to manage than ISCSI. However if you are a SAN only shop or coming from a SAN environment, ISCSI might be a nice way to bridge into OpenStack and not have to make changes to how hosts connect to their storage. You can even use a combination of both. You could use NFS for creating Cinder volumes where instances reside and ISCSI for host mount points. Again I would recommend NFS for everything and that is what I am going to cover but I wanted to at least explain both options.  Configuring Clustered Data ONTAP  We have made the following storage decisions for our OpenStack Cinder configuration:   FAS (storage platform) Clustered Data ONTAP (storage operating system) NFS (protocol)  Storage Virtual Machines  Configure two storage virtual machines. One will be used to provide storage to Cinder volumes and the other for DR. At a minimum you will need two IPs for each SVM. One for the management LIF and the other the data LIF. Ensure that both SVMs data LIFs are reachable from the OpenStack host.  Create SVMs    vserver create -vserver &lt;svm&gt; -subtype default -rootvolume &lt;rootvol&gt; -aggregate &lt;aggr&gt; -rootvolume-security-style unix   Create management LIFs    net int create -vserver &lt;svm&gt; -lif data -role data -data-protocol nfs -home-node steve-02 -home-port e0a -address &lt;ip&gt; -netmask &lt;netmask&gt; -firewall-policy mgmt   Create Data LIFs    net int create -vserver &lt;svm&gt; -lif data -role data -data-protocol nfs -home-node steve-02 -home-port e0a -address &lt;ip&gt; -netmask &lt;netmask&gt; -firewall-policy data   Ensure NFS services are running on both SVMs    nfs status -vserver   If NFS is not running you can start NFS as follows:    nfs on -vserver   nfs start -vserver   Create Export policy  In order to export NetApp volumes to OpenStack via NFS we need an export policy. Create an export policy as follows:    export-policy create -vserver &lt;svm&gt; -policyname openstack     export-policy rule create -vserver &lt;svm&gt; -policyname openstack -clientmatch &lt;IP or CIDR&gt; -rorule any -rwrule any   Add export policy to SVM root volume otherwise OpenStack host wont have write permission    volume modify -vserver &lt;svm&gt; -volume &lt;rootvol&gt; -policy openstack   Create NetApp Volumes  NetApp volumes are containers for storing files or LUNs. In the case of OpenStack a NetApp volume is a container for Cinder volumes which is a container for block devices. We will create three volumes for Cinder and each volume will define a different storage service e.g. gold, silver and bronze. In addition we will create a fourth volume for storing glance images. The beauty here is that the storage admin can define storage services based on containers and expose them up through Cinder for consumption in OpenStack.  Clustered Data ONTAP allows us to mix SSDs and Spinning Disk in the same storage system to enable building services with different performance characteristics. One can create aggregates which contain NetApp volumes using SSDs, Spinning Disk or even a mix (hybrid aggregates). In addition Clustered Data ONTAP provides QoS at the IOP or bandwidth level which we can use to further define or even restrict our storage services.  Gold Volume  The gold volume will be our most expensive storage service but will have the best performance and availability. We will create a volume on the best performing aggregate and in addition mirror that volume to another SVM.    volume create -vserver &lt;svm&gt; -volume openstack_gold -aggregate &lt;aggr&gt; -size 500g -state online -type RW -policy openstack -snapshot-policy none -space-guarantee volume     volume mount -vserver &lt;svm&gt; -volume openstack_gold -junction-path /openstack_gold     snapmirror initialize -source-path &lt;svm&gt;:openstack_gold -destination-path &lt;svm dr&gt;:openstack_gold_dr   Silver Volume  The silver volume will have a good price to performance but not include DR. We will create volume from a best performing aggregate and enable compression for space savings.    volume create -vserver &lt;svm&gt; -volume openstack_silver -aggregate &lt;aggr&gt; -size 500g -state online -type RW -policy openstack -snapshot-policy none -space-guarantee volume     volume mount -vserver &lt;svm&gt; -volume openstack_silver -junction-path /openstack_silver     volume efficiency on -vserver &lt;svm&gt; -volume openstack_silver     volume efficiency modify -vserver &lt;svm&gt; -volume openstack_silver -compression true -inline-compression true   Bronze Volume  The bronze volume will have lower performing storage and be the most cost-effective service. We will create volume from lower performing aggregate and enable dedup for space savings. We will not enable DR.    volume create -vserver &lt;svm&gt; -volume openstack_bronze -aggregate &lt;aggr&gt; -size 500g -state online -type RW -policy openstack -snapshot-policy none -space-guarantee volume     volume mount -vserver &lt;svm&gt; -volume openstack_bronze -junction-path /openstack_bronze     volume efficiency on -vserver &lt;svm&gt; -volume openstack_bronze     volume efficiency policy create -vserver &lt;svm&gt; -policy bronze_dedupe -schedule  -duration - -enabled true   Glance Volume  Finally we will create a volume for storing images in Glance. This volume does not have high performance requirements but we will certainly enable dedup to greatly reduce the storage footprint.    volume create -vserver &lt;svm&gt; -volume openstack_glance -aggregate &lt;aggr&gt; -size 500g -state online -type RW -policy openstack -snapshot-policy none -space-guarantee volume     volume mount -vserver &lt;svm&gt; -volume openstack_glance -junction-path /openstack_glance     volume efficiency on -vserver &lt;svm&gt; -volume openstack_bronze     volume efficiency policy create -vserver &lt;svm&gt; -policy bronze_dedupe -schedule  -duration - -enabled true   The next post in the series will cover configuring the above NetApp volumes in OpenStack as storage services.  (c) 2014 Keith Tenzer  ","categories": ["OpenStack"],
        "tags": ["Cloud","Clustered Data ONTAP","NetApp","OpenStack"],
        "url": "/openstack/building-storage-services-in-openstack-on-netapp-part-ii-of-iii/",
        "teaser": null
      },{
        "title": "Building Storage Services in OpenStack on NetApp - Part III of III",
        "excerpt":"Welcome to part three of the three-part series on creating storage services in OpenStack on NetApp. In this post we will look at how to configure storage services within OpenStack utilising NetApp storage.  In part one of the series we looked at how to install and configure OpenStack. In part two we looked at how to configure underlying NetApp storage to support OpenStack storage services.   Building Storage Services in OpenStack on NetApp - Part I of III Building Storage Services in OpenStack on NetApp - Part II of III  Overview  OpenStack of course consists of many decoupled services that run independently but at the same time integrate with one another. NetApp has integrations in the following OpenStack services: Cinder (block storage), Glance (image services), Swift (object storage) and Manilla (shared file services).    Cinder Configuration  Cinder provides block storage services in OpenStack. One very important thing to keep in mind is that while Cinder presents block devices to compute resources, underlying storage such as NetApp can expose either ISCSI or NFS storage to Cinder. In my example below I am going to show you how to set up two NFS storage backends: one for primary storage and the other DR. A storage backend maps to a NetApp Clustered Data ONTAP storage virtual machine (SVM), so for each SVM you will create a storage backend in Cinder.  Edit the cinder.conf (/etc/cinder/cinder.conf)  Note: if we chose to do the RDO installation using answers file then we already configured the first storage backend cdotNfs and just need to add the backend for cdotNfsDr.  [code language=\"text\"]  enabled_backends=cdotNfs,cdotNfsDr  [cdotNfs]  volume_backend_name=cdotNfs  volume_driver=cinder.volume.drivers.netapp.common.NetAppDriver  netapp_server_hostname=&lt;cluster mgmt vserver&gt;  netapp_server_port=80  netapp_storage_protocol=nfs  netapp_storage_family=ontap_cluster  netapp_login=&lt;username&gt;  netapp_password=&lt;password&gt;  netapp_vserver=&lt;storage virtual machine&gt;  nfs_shares_config=/etc/cinder/cdotNfs_exports.conf  netapp_copyoffload_tool_path=/usr/bin/na_copyoffload_64  [cdotNfsDr]  volume_backend_name=cdotNfsDr  volume_driver=cinder.volume.drivers.netapp.common.NetAppDriver  netapp_server_hostname=&lt;cluster mgmt vserver&gt;  netapp_server_port=80  netapp_storage_protocol=nfs  netapp_storage_family=ontap_cluster  netapp_login=&lt;username&gt;  netapp_password=&lt;password&gt;  netapp_vserver=&lt;storage virtual machine&gt;  [/code]  Edit the cdotNfs_exports.conf (/etc/cinder/cdotNfs_exports.conf)  Make the three NetApp volumes on our storage virtual machine available to Cinder backend [cdotNfs]:  [code language=\"text\"]  192.168.160.100:/openstack_gold  192.168.160.100:/openstack_silver  192.168.160.100:/openstack_bronze  [/code]  Note: make sure you use a data LIF on the storage virtual machine  Verify Configuration  Once we have configured our storage backends we can verify them following the below steps:   . /root/keystonerc_admin service openstack-cinder-api restart service openstack-cinder-volume restart cinder service-list  The \"cinder service-list\" command will show us the two new storage backends:   &lt;hostname&gt;@cdotNfs &lt;hostname&gt;@cdotNfsDr  Glance  Glance is the images service in OpenStack. It stores and provides images as well as templates to the Nova compute service. NetApp storage provides two key benefits for Glance:   Save network bandwidth by efficiently cloning images via copy offload driver Reduce storage requirements by enabling dedup  Edit the glance-api.conf (/etc/glance/glance-api.conf)  Mount a NetApp volume (openstack_glance) on the OpenStack host or the host running the glance service. The volume must be mounted at boot so make sure you add it to the /etc/fstab.  [code language=\"text\"]  filesystem_store_datadir=/glance  filesystem_store_metadata_file=/etc/glance/metadata.conf  [/code]  Edit the metadat.conf (/etc/glance/metadata.conf)  The purpose of this file is to enable glance to store images and templates on NetApp storage.  [code language=\"text\"]  {  &quot;share_location&quot;: &quot;nfs://192.168.160.100:/openstack_glance&quot;,  &quot;mount_point&quot;: &quot;/glance&quot;,  &quot;type&quot;: &quot;nfs&quot;  }  [/code]  Enable copy offload driver  The copy offload driver allows Glance to use NetApp file cloning capabilities providing near instant image copies to the Nova compute service. Without copy offload images must be copied over the network between Glance and Nova.  Copy offload requirements:   The storage system must have Data ONTAP v8.2 or greater installed To configure the copy offload workflow, enable NFS v4.0 or greater and export it from the SVM The vStorage feature must be enabled on each storage virtual machine (SVM, also known as a Vserver) that is permitted to interact with the copy offload client. To set this feature, you can use the vserver nfs modify - -vstorage enabled –v4.0 enabled CLI command  To install the copy offload driver following the following steps:   Download copy offload tool Copy the binary to /usr/bin/na_copyoffload_64 Ensure the permissions are 755  Create Storage Services  One of the major features of cinder is the ability to configure storage services. Below is an example of how we can create a service: gold, silver and bronze from our NetApp storage volumes. The NetApp driver for OpenStack exposes capabilities which we can use to define services.  Below is a table of all the NetApp storage capabilities we can use to define services in OpenStack:     Extra spec DataType Description     netapp:raid_type String Limit the candidate volume list based on one of the following raid types: raid4, raid_dp.   netapp:disk_type String Limit the candidate volume list based on one of the following disk types: ATA, BSAS, EATA, FCAL,FSAS, LUN, MSATA, SAS, SATA, SCSI, XATA, XSAS, or SSD.   netapp:qos_policy_group String Specify the name of a QoS policy group, which defines measurable Service Level Objectives, that should be applied to the Cinder volume at the time of volume creation. Ensure that the QoS policy group object within Data ONTAP should be defined before a Cinder volume is created, and that the QoS policy group is not associated with the destination FlexVol volume.   netapp_mirrored Boolean Limit the candidate volume list to only the ones that are mirrored on the storage controller.   netapp_unmirrored Boolean Limit the candidate volume list to only the ones that are not mirrored on the storage controller.   netapp_dedup Boolean Limit the candidate volume list to only the ones that have deduplication enabled on the storage controller.   netapp_nodedup Boolean Limit the candidate volume list to only the ones that have deduplication disabled on the storage controller.   netapp_compression Boolean Limit the candidate volume list to only the ones that have compression enabled on the storage controller.   netapp_nocompression Boolean Limit the candidate volume list to only the ones that have compression disabled on the storage controller.   netapp_thin_provisioned Boolean Limit the candidate volume list to only the ones that support thin  provisioning on the storage controller.   netapp_thick_provisioned  Boolean Limit the candidate volume list to only the ones that support thick   provisioning on the storage controller.    Gold Service  Gold - Highest performance storage (SSD) and Disaster Recovery   cinder type-key gold set netapp_mirrored=true  Silver Service  Silver - Highest performance storage with compression   cinder type-key silver set netapp_compression=true  Bronze Service  Bronze Lower performance storage with dedup   cinder type-key bronze setnetapp_dedup=true  Note: you can add more than one capability to a storage service.   Create Instance using Storage Services  Finally we can use the storage services to create a cinder volume and spawn an instance based on the new cinder volume. Log into the OpenStack UI horizon. Under project-&gt;volumes select \"create volume\". You can now select a storage service (gold, silver or bronze), create a cinder volume and attach it to an OS image. Below is an example:    (c) 2014 Keith Tenzer  ","categories": ["OpenStack"],
        "tags": ["Cloud","NetApp","OpenStack","Storage"],
        "url": "/openstack/building-storage-services-in-openstack-on-netapp-part-iii-of-iii/",
        "teaser": null
      },{
        "title": "Rename network interfaces in RHEL 7",
        "excerpt":"Overview  RHEL 7 has a new feature called \"consistent network device naming\". The idea is to regardless of NIC hardware have consistent interface names and not names which can be change based on underlying hardware. Traditionally interface names were enumerated as eth[0,1,2...]. In many environments administrators would like to use names that refer to the purpose of the network interface especially when a host bridges into various networks e.g. mgmt[0,1,2...], storage[0,1,2...] or client[0,1,2...]. This post we show how to change interface names in RHEL 7 and CentOS 7.  Change Interface Names  The below procedure disables \"consistent network device naming\" and renames interfaces.    vi /etc/default/grub   [code language=\"text\"]  GRUB_TIMEOUT=5  GRUB_DISTRIBUTOR=”$(sed ‘s, release .*$,,g’ /etc/system-release)”  GRUB_DEFAULT=saved  GRUB_DISABLE_SUBMENU=true  GRUB_TERMINAL_OUTPUT=”console”  GRUB_CMDLINE_LINUX=”rd.lvm.lv=rootvg/usrlv rd.lvm.lv=rootvg/swaplv crashkernel=auto vconsole.keymap=us rd.lvm.lv=rootvg/rootlv vconsole.font=latarcyrheb-sun16 rhgb quiet”  GRUB_DISABLE_RECOVERY=”true”  [/code]  Change the GRUB_CMDLINE_LINUX and regenerate grub.cfg:    GRUB_CMDLINE_LINUX=”rd.lvm.lv=rootvg/usrlv rd.lvm.lv=rootvg/swaplv crashkernel=auto vconsole.keymap=us rd.lvm.lv=rootvg/rootlv vconsole.font=latarcyrheb-sun16 rhgb quiet net.ifnames=0 biosdevname=0“     grub2-mkconfig -o /boot/grub2/grub.cfg   Update the names of the network scripts    mv /etc/sysconfig/network-scripts/ifcfg-eno00012224 /etc/sysconfig/network-scripts/ifcfg-mgmt0   [code language=\"text\"]  TYPE=Ethernet  BOOTPROTO=none  DEFROUTE=yes  IPV4_FAILURE_FATAL=no  IPV6INIT=yes  IPV6_AUTOCONF=yes  IPV6_DEFROUTE=yes  IPV6_FAILURE_FATAL=no  NAME=mgmt0  UUID=ecfe9674-774e-4c5a-a3a6-a84336306b62  ONBOOT=yes  IPADDR0=10.10.10.90  PREFIX0=24  GATEWAY0=10.10.10.1  DNS1=10.10.10.5  DNS2=10.10.10.6  DOMAIN=mydomain.com  HWADDR=00:50:56:B4:6D:57  IPV6_PEERDNS=yes  IPV6_PEERROUTES=yes  DEVICE=mgmt0  [/code]  Reboot system with reconfiguration    shutdown -r now   (c) 2014 Keith Tenzer  ","categories": ["RHEL"],
        "tags": ["Linux","Red Hat"],
        "url": "/rhel/rename-network-interfaces-in-rhel-7/",
        "teaser": null
      },{
        "title": "KVM Installation and Configuration",
        "excerpt":"  Overview  KVM (Kernel Virtual Machine) is a Linux kernel module that allows a user space process to utilize hardware virtualization capabilities. When people refer to KVM they are usually talking about the hypervisor but there are in fact three components (or balls our penguin seems to be juggling): KVM (hardware acceleration), Qemu (hypervisor), and Libvirt (management library). KVM and Qemu started as separate projects but their code streams have been merged and hence the qemu-kvm package. In order to communicate with the KVM hypervisor libvirt is used which also has support for the management of other hypervisors such as ESX.  Install KVM  KVM consists of many packages, most notable are qemu-kvm, libvirt, virt-manager and virt-viewer. The virt-manager and virt-viewer packages are optional and only required for the GUI.    yum groupinstall \"Virtualisation Tools\" \"Virtualization Platform\"     yum install python-virtinst     yum install virt-manager virt-viewer     yum install qemu-kvm   Configure libvirtd  This step is mainly for development environments or management tools that require tcp access to libvirtd. In order to use the libvirt APIs we need to enable access through tcp and configure authentication. In this example we will use SASL authentication.    vi /etc/libvirt/libvirtd.conf   [code language=\"text\"]  listen_tls = 0  listen_tcp = 1  tcp_port=”16509”  listen_addr=”&lt;IP&gt;”  auth_tcp = “sasl”  [/code]  Note: you can disable authentication by setting auth_tcp=\"none\" but this is only recommended for development or test environments.  Configure Authentication Credentials  KVM has various authentication options and protocol support. Documentation regarding libvirt and remote connections as well as authentication can be found on the libvirt website. The most secure way to do things is with TLS and certificates however in the below example we are just using tcp and standard SASL user/password authentication.  Create a SASL user and password for the libvirt service.    saslpasswd2 -a libvirt kvmuser   Add authentication credentials so you are not prompted for credentials. This is critical for remote access tools requiring access over tcp.    vi /etc/libvirt/auth.conf   [code language=\"text\"]  [credentials-test]  authname=kvmuser  password=mypassword  [auth-libvirt-192.168.8.132]  credentials=test  [/code]  Upon adding credentials make sure you restart libvirtd.    service libvirtd restart   In order to test the credentials use the vrish cli command with the \"no_verify\" option. If things are working correctly you should not be prompted for credentials and get a list of virtual machines.    virsh -c qemu+tcp:///system?no_verify=1 list   (c) 2014 Keith Tenzer  ","categories": ["KVM"],
        "tags": ["Linux","OpenStack","Red Hat"],
        "url": "/kvm/kvm-installation-and-configuration/",
        "teaser": null
      },{
        "title": "KVM Development Guide",
        "excerpt":"Overview  This guide will go through how to configure your Java development environment, connect to KVM and provide some Java code examples for interfacing with KVM. Upon completing this guide you should be off and running with developing application interfaces for KVM. As mentioned in previous post KVM is made up of three components: KVM (hardware acceleration), Qemu (hypervisor) and Libvirt (management library). We will be purely focusing on Libvirt as it is the management library for KVM.  Development Environment Setup  This guide assumes you will be using Eclipse and Maven for jar package management. If you aren't no big deal, you can always download libraries and add them to CLASSPATH manually.  Configure KVM  Before we begin it is important to ensure KVM is properly configured to allow remote access over TCP. For instructions on doing this refer to KVM Installation and Configuration.  Configure Maven  Add JNA and Libvirt dependencies to pom.xml. Be careful which versions you use. Make sure the JNA version is not newer than the Libvirt version else you will have incompatibility issues.  [code language=\"xml\"]  &lt;dependency&gt;     &lt;groupId&gt;net.java.dev.jna&lt;/groupId&gt;     &lt;artifactId&gt;jna&lt;/artifactId&gt;     &lt;version&gt;3.4.0&lt;/version&gt;  &lt;/dependency&gt;  &lt;dependency&gt;     &lt;groupId&gt;org.libvirt&lt;/groupId&gt;     &lt;artifactId&gt;libvirt&lt;/artifactId&gt;     &lt;version&gt;0.4.9&lt;/version&gt;  &lt;/dependency&gt;      [/code]  Instruct Maven to download and make these jars available to Eclipse by running the following command from the root project folder:    mvn clean compile   Note: if you choose not to use Maven, you can simply download the JNA / Libvirt libraries and add them to your projects CLASSPATH in Eclipse.  Connecting to Libvirt  Now that we have configured Libvirt to allow remote TCP connections and have our Java libraries setup we can start coding!  Libvirt connection class  Below we will create a class which we can instantiate with a hostname. The class will provide a getConnection() method that returns the connection to a KVM host.  [code language=\"java\"]  import org.libvirt.Connect;  import org.libvirt.ConnectAuth;  import org.libvirt.ConnectAuthDefault;  import org.libvirt.LibvirtException;  public class KvmConnection {      private String hostname;      public KvmConnection(String host) {          this.hostname = host;      }      public Connect getConnection() throws Exception {          Connect conn = null;          try {              ConnectAuth defaultAuth = new ConnectAuthDefault();              conn = new Connect(&quot;qemu+tcp://&quot; + hostname + &quot;/system&quot;, defaultAuth, 0);           } catch (LibvirtException e) {              throw new Exception(e.getMessage(), e);          }          return conn;      }  }  [/code]  Note: the above example is using SASL authentication. Other authentication mechanisms such as TLS are possible but the URI does need to change if the authentication mechanism changes.   Working with Domains  In Libvirt Domains are Virtual Machines. Before we can perform APIs against a domain we must retrieve the Domain object which can be done by name, id or other methods. Libvirt also has APIs that return a list of active or inactive Domains. If we don't know the name or id of a Domain we would first get a list of all Domain Ids or names. In addition to the below code examples, the API reference for Libvirt Domains is also a good resource.  Listing Domains  The below method returns a list of active and inactive Domain objects:  [code language=\"java\"]  public List&lt;Domain&gt; getDomains(Connect conn) throws Exception {      List&lt;Domain&gt; domainList = new ArrayList&lt;Domain&gt;();      try {          int[] activeDomainIds = conn.listDomains();          String[] inactiveDomainNames = conn.listDefinedDomains();          if (activeDomainIds.length &gt; 0) {              for (int id : activeDomainIds) {                  Domain domain = conn.domainLookupByID(id);                  domainList.add(domain);              }          }          if (inactiveDomainNames.length &gt; 0) {              for (String name : inactiveDomainNames) {                  Domain domain = conn.domainLookupByName(name);                  domainList.add(domain);              }          }      } catch (Exception e) {          throw new Exception(e.getMessage(), e);      }      return domainList;   }  [/code]  Domain status  A domain can be either active, inactive or error. These are represented as integers in Libvirt. The below method will get the status of a Domain and save it as an Enum:  [code language=\"java\"]   public VmStatusEnum getVmStatus(int status) throws Exception {       VmStatusEnum vmState = null;       try {           if (status == 1) {               vmState = VmStatusEnum.ON;           } else if (status == 0) {               vmState = VmStatusEnum.OFF;           } else {               vmState = VmStatusEnum.ERROR;           }       } catch (Exception e) {           throw new Exception(e.getMessage(), e);       }       return vmState;   }  [/code]  Creating Domain Snapshot  The below code creates a Domain snapshot based on a given Connection object, Domain name and Snapshot name:  [code language=\"java\"]  public void createSnapshot(Connect conn, String vmName, String snapshotName) throws Exception {      if (snapshotName == null || snapshotName.isEmpty()) {          snapshotName = vmName + &quot;_snapshot&quot;;      }      String description = &quot;KVM snapshot created by XYZ&quot;;      try {          Domain domain = conn.domainLookupByName(vmName);          int status = domain.isActive();          VmStatusEnum vmStatusEnum = getVmStatus(status);          if (vmStatusEnum.equals(VmStatusEnum.ON)) {             String snapshotXML = &quot;&lt;domainsnapshot&gt;&lt;name&gt;&quot; + snapshotName + &quot;&lt;/name&gt;&lt;description&gt;&quot; + description + &quot;&lt;/description&gt;&lt;/domainsnapshot&gt;&quot;;             domain.snapshotCreateXML(snapshotXML);          } else if (vmStatusEnum.equals(VmStatusEnum.ERROR)) {             throw new Exception(String.format(VM_STATUS_ERROR, vmName));          }      } catch (LibvirtException e) {          LOGGER.error(String.format(SNAPSHOT_CREATE_FAILED, snapshotName, vmName));          throw new Exception(e.getMessage(), e);      }      LOGGER.info(String.format(SNAPSHOT_CREATE_SUCCESS, snapshotName, vmName));   }  [/code]  Working with Storage Pools  Libvirt uses Storage Pools to store images and virtual disks for Virtual Machines. Virtual disks are presented to Virtual Machines as block devices regardless of the underlying storage pool type. There are many different types of storage pools which can be configured. Aside from NFS, iSCSI and FC there is also integration to support Ceph RDB pools, Gluster pools and even ZFS pools. In addition to below code example the API reference for Libvirt Storage is a good resource.  Listing Storage Pools  Below is a method that returns a list of active and inactive Storage Pools:  [code language=\"java\"]  public List&lt;StoragePool&gt; getStoragePools(Connect conn) throws Exception {      List&lt;StoragePool&gt; storagePoolList = new ArrayList();      try {          String[] activeStoragePools = conn.listStoragePools();          String[] inactiveStoragePools = conn.listDefinedStoragePools();          if (activeStoragePools.length &gt; 0) {              for (String name : activeStoragePools) {                  StoragePool storagePool = conn.storagePoolLookupByName(name);                  storagePoolList.add(storagePool);              }          }          if (inactiveStoragePools.length &gt; 0) {              for (String name : inactiveStoragePools) {                  StoragePool storagePool = conn.storagePoolLookupByName(name);                  storagePoolList.add(storagePool);              }          }      } catch (Exception e) {          throw new Exception(e.getMessage(), e);      }      return storagePoolList;  }  [/code]  Conclusion  Through these examples you should have a good understanding of how to work with KVM using Java or even other languages. As far as working with multiple KVM hypervisors, check out oVirt which is open source management software built around KVM (similar in concept to vCenter for VMware environments). Red Hat who heavily contributes to upstream oVirt community uses it as the basis for Red Hat Enterprise Virtualization Management. I will be covering oVirt and Red Hat Enterprise Virtualization in later posts so stay tuned! As always if you have questions or need some help feel free to leave a post or get in contact with me directly. Happy Libvirting and safe travels!  (c) 2015 Keith Tenzer  ","categories": ["KVM"],
        "tags": ["Java","Linux"],
        "url": "/kvm/kvm-development-guide/",
        "teaser": null
      },{
        "title": "Red Hat Enterprise Virtualization (RHEV) Home Lab Configuration",
        "excerpt":"Overview  This post will cover installing Red Hat Enterprise Virtualization (RHEV) on an Intel NUC for the purpose of a lab environment. RHEV has two main components: RHEV-H and RHEV-M.  RHEV-H  Red Hat Enterprise Virtualization Hypervisor (RHEV-H) is a compact, full-featured virtualization platform built on Red Hat Enterprise Linux (RHEL). It has a very small footprint (less than 100MB) and is built from only a subset of RHEL components plus the Kernel-Based Virtual Machine (KVM).  RHEV-M  Red Hat Enterprise Virtualization Management (RHEV-M) is a virtual management console built on Red Hat Enterprise Linux (RHEL). It interacts with individual RHEV-H nodes using the Virtual Desktop Server Manager (VDSM). A VDSM agent is running on each of the RHEV-H nodes. RHEV-M allows administrators to manage multiple data centers and their network, compute and storage resources. In addition RHEV-M provides a central repository for storing virtual machines, disks, images and virtual machine snapshots.    Requirements   Intel NUC 34010WYKH Samsung SSD 840 EVO Crucial 8GB DDR3 1600 RAM (1.35V) RHEV Hypervisor RHEL 6.6  Install RHEV-H on Intel NUC  In order to install a RHEV-H node on our Intel NUC (bare-metal) we will need to download RHEV-H ISO and UNetbootin. The UNetbootin software allows us to create a bootable USB with the RHEV-H image.    Once you have bootable USB with the RHEV-H image you can simply start the NUC, insert USB and go through the RHEV-H installation.  Note: You may have to change the boot order in the BIOS of your Intel NUC or configure BIOS to boot from USB.  Install RHEL-M  In order to manage one or more RHEV-H nodes, a RHEL system running RHEV-M is required. You can use any 6.x version of RHEL for the RHEV-M system. As of the writing of this post RHEL 7 is not yet supported for RHEV-M. RHEL 7 will be supported with RHEV version 3.5. For the purpose of this lab I installed RHEL 6.6 as a Virtual Machine running under KVM on my PC.  Note: Make sure to allocate at least 4GB of RAM to the RHEV-M system.  Once RHEL 6.6 has been installed proceed with following steps:   Register subscription  subscription-manager register  Automatically attach subscriptions  subscription-manager attach --auto   OR   List available subscriptions  subscription-manager list --available  Attach a specific subscription (the pool id is listed in above command)    subscription-manager attach --pool=8a85f9814a7ea2ec014a813b19643ce5    Enable required channels    yum-config-manager --enable rhel-6-server-rpms     yum-config-manager --enable rhel-6-server-supplementary-rpms     yum-config-manager --enable rhel-6-server-rhevm-3.4-rpms     yum-config-manager --enable jb-eap-6-for-rhel-6-server-rpms  Install RHEV-M RPMs    yum update; yum install rhevm  Configure RHEV-M  engine-setup   After completing the installation you can go to following URL to connect to RHEV-M http://:80/ovirt-engine  Configure RHEV-M  Once the RHEV-H and RHEV-M systems are configured we can add the RHEV-H node in RHEV-M. The first step is to configure the oVirt engine from the RHEV-H node to point to the FQDN or IP of the RHEV-M host.  Configure oVirt engine on RHEV-H host    ssh -l admin &lt;RHEV-H FQDN or IP&gt;    Add RHEV-H host in RHEV-M  Connect to RHEV-M host through following URL: http://:80/ovirt-engine   Select the RHEV-H host and enter maintenance mode.    For the purpose of the lab we will use local storage. Since local storage is not shared across RHEV-H nodes a standalone data center will be configured that uses local storage.   Configure local storage domain      Attach ISO storage domain (configured when installing RHEV-M)    At this point we can start creating virtual machines, templates and various resource pools.If you have any questions regarding the Intel NUC, RHEV-H or RHEV-M setup please feel free to ask. In a future post I will cover API integration with RHEV-M (oVirt).  Troubleshooting   Can't add local storage domain - This problem happens when you have configured RHEV-H host more than once in RHEV-M. Remove anything under /data/images/rhev on RHEV-H host. Accessing root shell on RHEV-H host - you need to connect via ssh as admin user and then press \"F2\" in the RHEV-H menu. ISOs not available when creating new VMs - The ISO storage domain must be attached to the RHEV-H host. Make sure hostname of RHEV-M host resolves from RHEV-H node otherwise add entry to /etc/hosts file.  (c) 2015 Keith Tenzer  ","categories": ["RHEV"],
        "tags": ["KVM","Linux","Red Hat"],
        "url": "/rhev/red-hat-enterprise-virtualization-rhev-home-lab-configuration/",
        "teaser": null
      },{
        "title": "Red Hat Enterprise Virtualization Management (RHEV-M) - Overview, APIs and Code Examples",
        "excerpt":"Overview  Red Hat Enterprise Virtualization has two main components: RHEV-H and RHEV-M. In this post we will be focusing on RHEV-M which provides a REST API and various SDKs. By integrating with RHEV-M we can integrate a powerful set of virtualization APIs into any applications, processes or workflows. If you are new to RHEV or don't have a lab environment setup, check out this post on building a RHEV lab environment.  RHEV-H  Red Hat Enterprise Virtualization Hypervisor (RHEV-H) is a compact, full-featured virtualization platform built on Red Hat Enterprise Linux (RHEL). It has a very small footprint (less than 100MB) and is built from only a subset of RHEL components plus the Kernel-Based Virtual Machine (KVM).  RHEV-M  Red Hat Enterprise Virtualization Management (RHEV-M) is a virtual management console built on Red Hat Enterprise Linux (RHEL). It interacts with individual RHEV-H nodes using the Virtual Desktop Server Manager (VDSM). A VDSM agent is running on each of the RHEV-H nodes. RHEV-M allows administrators to manage multiple data centers and their network, compute and storage resources. In addition RHEV-M provides a central repository for storing virtual machines, disks, images and virtual machine snapshots. RHEV-M provides a common feature set such as HA, Load Balancing, Host Fencing, Networking, Storage and more across all hosts and virtual machines.    RHEV-M Architecture  RHEV-M is built on top of libvirt and allows for the management of many KVM (Kernel-Based Virtual Machine) hosts. Libvirt just allows for the management of a single KVM host. RHEV-M is downstream from the open-source project oVirt. Red Hat started the oVirt project and though the hypervisor backend is open (KVM, XEN, or VirtualBox) the project is currently focused on KVM. Originally oVirt was based on C# and .NET but has since ported to Java.  RHEV-M has several components: Engine, Interfaces, VDSM Agent and SPICE.  Engine  The RHEV-M engine runs on a standalone RHEL host. It inventories as well as exposes virtual resources and allows the administrator to organise those resources across many data centers. In addition it provides capabilities such as HA, authentication, load balancing, live migration, network management, storage management, virtual machine management, image management and monitoring across an entire virtual ecosystem based around the KVM hypervisor. The engine maintains a postgreSQL database for storing relational information about the environment.  Interfaces  RHEV-M offers a CLI and a web-based UI built around the Google Web-Framework (GWT). Since RHEV-M uses GWT it is also possible to create plugins and add UI extensions during runtime. RHEV-M offers a powerful REST API and SDKs for Java as well as Python. Later in this post we will give some Java code examples and take a closer look at the SDK.  VDSM  The Virtual Desktop Server Manager is an interface abstraction built around libvirt. Since libvirt was only designed to manage a single KVM hypervisor it was necessary to extend capabilities to allow management of 1000s of virtual machines across many KVM hypervisors. VDSM provides the necessary scalability around libvirt. VDSM has a few components. It has a daemon agent that runs on the KVM hypervisor (either RHEV-H or RHEL + KVM), a client component and a manager that is built into the RHEV-M engine.  SPICE  RHEV-M uses SPICE in order to display the console of a virtual machine through user interfaces. SPICE is responsible for handling console graphics and sending control commands.  Java Coding Examples  Below are several coding examples in Java using the SDK built on the RHEV-M REST API. These code examples will work with RHEV-M and oVirt. The Java SDK is available through maven or direct download from oVirt. To use via maven simply add following to your pom.xml:  [code language=\"xml\"]  &lt;dependency&gt;     &lt;groupId&gt;org.ovirt.engine.sdk&lt;/groupId&gt;     &lt;artifactId&gt;ovirt-engine-sdk-java&lt;/artifactId&gt;     &lt;version&gt;3.4.4.0&lt;/version&gt;  &lt;/dependency&gt;  [/code]  RHEV-M Connection  Below is a class that we can instantiate with various parameters. It returns a connection object that is used as the basis in other examples:  [code language=\"java\"]  import org.ovirt.engine.sdk.Api;  public class RhevConnection {      private String URL;      private String domain;      private String username;      private String password;      public RhevConnection(String hostname, Integer port, Boolean sslEnable, String username, String password, String domain) {          if (sslEnable) {              this.URL = &quot;https://&quot; + hostname + &quot;:&quot; + port + &quot;/api&quot;;          } else {              this.URL = &quot;http://&quot; + hostname + &quot;:&quot; + port + &quot;/api&quot;;          }          this.domain = domain;          this.username = username;          this.password = password;      }      public Api getConnection() throws Exception {          Api api = null;          try {              api = new Api(URL, username + &quot;@&quot; + domain, password, true);          } catch (Exception e) {              throw new Exception(e.getMessage(), e);          }          return api;      }  }  [/code]  Data Centers  The top-level organisational entity in RHEV-M is the data center. It is meant to be a logical or physical segregation of virtual resources. Data Centers contain storage domains, networks, clusters and of course hypervisor hosts. Below is a Java example that uses a defined model to return details about data centers:  [code language=\"java\"]      public List&lt;DataCenterModel&gt; inventoryDataCenters() throws Exception {          List&lt;DataCenterModel&gt; dcModelList = new ArrayList&lt;DataCenterModel&gt;();          try {              for (DataCenter dc : api.getDataCenters().list()) {                  DataCenterModel dcModel = new DataCenterModel();                  dcModel.setName(dc.getName());                  dcModel.setDescription(dc.getDescription());                  dcModel.setIsLocal(dc.getLocal());                  dcModel.setStatus(dc.getStatus().getState());                  dcModelList.add(dcModel);              }          } catch (Exception e) {              throw new Exception(e.getMessage(), e);          }          return dcModelList;      }  [/code]   Storage Domains  Storage domains provide storage management within RHEV-M. Virtual machine images and disks are stored in storage domains. A storage domain can be local to hypervisor host or shared across many hosts. A storage domain can be NFS, ISCSI, FC, Gluster, Ceph or ZFS. Below is a Java example that uses a defined model to return details of storage domains:  [code language=\"java\"]      public List&lt;StorageDomainModel&gt; inventoryStorageDomains() throws Exception {          List&lt;StorageDomainModel&gt; storageDomainModelList = new ArrayList&lt;StorageDomainModel&gt;();          try {              List&lt;StorageDomain&gt; storageDomainList = api.getStorageDomains().list();              for (StorageDomain storageDomain : storageDomainList) {                  StorageDomainModel storageDomainModel = new StorageDomainModel();                  storageDomainModel.setName(storageDomain.getName());                  storageDomainModel.setDescription(storageDomain.getDescription());                  storageDomainModel.setDomainType(storageDomain.getType());                  storageDomainModel.setStorageType(storageDomain.getStorage().getType());                  storageDomainModel.setPath(storageDomain.getStorage().getPath());                  storageDomainModel.setAvailable(storageDomain.getAvailable());                  storageDomainModel.setUsed(storageDomain.getUsed());                  storageDomainModelList.add(storageDomainModel);              }          } catch (Exception e) {              throw new Exception(e.getMessage(), e);          }          return storageDomainModelList;      }  [/code]  Hosts  In RHEV-M a host is a RHEL system running KVM and the VDSM agent. As mentioned you can either use RHEV-H which comes with everything you need out-of-the-box or RHEL and add KVM / VDSM packages manually. Below is a Java example that uses a defined model to return details of hosts:  [code language=\"java\"]      public List&lt;HostModel&gt; inventoryHosts() throws Exception {          List&lt;HostModel&gt; hostList = new ArrayList&lt;HostModel&gt;();          try {              List&lt;Host&gt; hosts = api.getHosts().list();              for (Host host : hosts) {                  HostModel hostModel = new HostModel();                  hostModel.setName(host.getName());                  hostModel.setIp(host.getAddress());                  hostModel.setCpuType(host.getCpu().getName());                  hostModel.setCpuCores(host.getCpu().getTopology().getCores());                  hostModel.setMemory(host.getMemory());                  hostModel.setOsType(host.getOs().getType());                  hostModel.setOsVersion(host.getOs().getVersion().getFullVersion());                  hostList.add(hostModel);              }          } catch (Exception e) {              throw new Exception(e.getMessage(), e);          }          return hostList;      }  [/code]  Virtual Machines  In RHEV-H virtual machines or as they are referred to in KVM domains provide the building blocks for the RHEV platform. Most actions and things of interest are typically performed on virtual machines. Below are several code examples of typical things administrators would perform on virtual machines.  Listing Virtual Machines  [code language=\"java\"]      public List&lt;VirtualMachineModel&gt; inventoryVirtualMachines() throws Exception {          List&lt;VirtualMachineModel&gt; vmModelList = new ArrayList&lt;VirtualMachineModel&gt;();          try {              List&lt;VM&gt; vmList = api.getVMs().list();              for (VM vm : vmList) {                  VirtualMachineModel vmModel = new VirtualMachineModel();                  vmModel.setName(vm.getName());                  vmModel.setCpuCores(vm.getCpu().getTopology().getCores());                  vmModel.setMemory(vm.getMemory());                  vmModel.setOsType(vm.getOs().getType());                  vmModel.setState(vm.getStatus().getState());                  String hostId = vm.getPlacementPolicy().getHost().getId();                  vmModel.setHostName(api.getHosts().getById(hostId).getName());                  vmModelList.add(vmModel);              }          } catch (Exception e) {              throw new Exception(e.getMessage(), e);          }          return vmModelList;      }  [/code]  Create Virtual Machine Snapshot  [code language=\"java\"]      public void createSnapshot(String vmName, String description) throws Exception {          try {              Snapshot snapshot = new Snapshot();              snapshot.setDescription(&quot;XYZ snapshot [&quot; + description + &quot;]&quot;);              api.getVMs().get(vmName).getSnapshots().add(snapshot);          } catch (Exception e) {              throw new Exception(e.getMessage(), e);          }      }  [/code]  Delete Virtual Machine Snapshot  [code language=\"java\"]      public void deleteSnapshot(String vmName, String snapshotId) throws Exception {          try {              api.getVMs().get(vmName).getSnapshots().getById(snapshotId).delete();          } catch (Exception e) {              throw new Exception(e.getMessage(), e);          }      }  [/code]  Restore Virtual Machine Snapshot  [code language=\"java\"]      public void restoreSnapshot(String vmName, String snapshotId) throws Exception {          try {              Action action = new Action();              action.setId(snapshotId);              api.getVMs().get(vmName).getSnapshots().getById(snapshotId).restore(action);          } catch (Exception e) {              throw new Exception(e.getMessage(), e);          }      }  [/code]  List Virtual Machine Snapshots  [code language=\"java\"]      public List&lt;SnapshotModel&gt; listSnapshots(String vmName) throws Exception {          List&lt;SnapshotModel&gt; snapshotList = new ArrayList&lt;SnapshotModel&gt;();          List&lt;VMSnapshot&gt; snapshots = api.getVMs().get(vmName).getSnapshots().list();          for (VMSnapshot snapshot : snapshots) {              SnapshotModel snapshotModel = new SnapshotModel();              snapshotModel.setId(snapshot.getId());              snapshotModel.setDescription(snapshot.getDescription());              snapshotModel.setDate(snapshot.getDate().toString());              snapshotList.add(snapshotModel);          }          return snapshotList;      }  [/code]  Start Virtual Machine  [code language=\"java\"]      public void startVirtualMachine(String vmName) throws Exception {          try {              Action action = new Action();              action.setName(vmName);              ;              api.getVMs().get(vmName).start(action);          } catch (Exception e) {              throw new Exception(e.getMessage(), e);          }      }  [/code]  Stop Virtual Machine  [code language=\"java\"]      public void stopVirtualMachine(String vmName) throws Exception {          try {              Action action = new Action();              action.setName(vmName);              ;              api.getVMs().get(vmName).shutdown(action);          } catch (Exception e) {              throw new Exception(e.getMessage(), e);          }      }  [/code]   Conclusion  RHEV is a powerful virtualization platform and certainly has not only the management but also feature capabilities to be on par with other virtualization platforms from VMware and Microsoft. The big advantage of RHEV over Microsoft or VMware is that everything is of course open-source and built in a thriving community. The industry has long wanted an enterprise open-source virtualization platform and now it has one! Hopefully this post was useful and informative. I was very impressed at the easy-of-use of RHEV-M interfaces and APIs. If you have any questions or comments please feel free to ask.  Happy RHEVing!  (c) 2015 Keith Tenzer  ","categories": ["RHEV"],
        "tags": ["Cloud Computing","Java","KVM","Linux","Red Hat","Virtualization"],
        "url": "/rhev/red-hat-enterprise-virtualization-management-rhev-m-overview-apis-and-code-examples/",
        "teaser": null
      },{
        "title": "RHEV - Importing OVA Templates",
        "excerpt":"Overview  Virtual machine disk images are specific to hypervisors and each hypervisor has its own format. Open Virtual Format (OVF) is a packaging standard designed to address portability and deployment of virtual machines across different hypervisors. The OVF structure consists of a number files: descriptor, manifest, certificate and disk images or resource files such as ISOs. In order to make things even more streamlined and simpler for deploying virtual appliances an Open Virtual Applicance (OVA) standard was created. OVA is a single file distribution of an OVF package, stored in TAR format. In this post we will explore how to import OVAs into RHEV.  Create Export Storage Domain  An Export Storage Domain is an NFS share made available to RHEV environment for purpose of importing templates such as the OVA.   Create an NFS share on an NFS server and export to all RHEV-H and RHEV-M hosts. A guide for how to do this on RHEL systems can be found here. Log on to the RHEV administrator portal and under system select the \"storage\" tab and \"New Domain\".     After adding Export Storage Domain it should become available momentarily.    Uploading OVA to Export Storage Domain  The OVA can be uploaded using the engine-image-uploader tool. If the tool is not present you can install it using the below command:    yum install rhevm-image-uploader   In order to upload OVA run the following command:    engine-image-uploader -e EXPORT_DOMAIN -N ATOMIC upload rhel-atomic-cloud-7.1-0.x86_64.rhevm.ova    Importing OVA as template  In order to create virtual machines from OVA we must first import that OVA as a template.   Log on to RHEV administrator portal and under system select the \"storage\" tab. Next select the EXPORT_DOMAIN and the \"Template Import\" tab. Select the template (in this case CFME) and import.    Creating Virtual Machine from Template  Now that we have the OVA template imported we can create a new virtual machine and simply select the template \"CFME\".    (c) 2015 Keith Tenzer  ","categories": ["RHEV"],
        "tags": ["OVA","Red Hat"],
        "url": "/rhev/rhev-importing-ova-templates/",
        "teaser": null
      },{
        "title": "To cloud or not to cloud...that is the question",
        "excerpt":"  Iceland  - has possibly the cheapest, most reliable energy source in the world and as such is not a bad place to build a cloud.  Attempting to understand cloud  There is still a ton of confusion around cloud computing. Not a day goes by that I don't talk to someone or read something about cloud that has nothing to do with cloud.  I am sure many of you are with me there. Everyone is talking cloud, everyone is using cloud but everyone thinks they are also doing cloud and few really are. There is no doubt cloud has a huge future but it is going to take time to get there for most as it means changing everything! It means re-writing the applications, business processes, consumption practices and governance policies. That is why when we talk about cloud, we are talking about a transformation. It is a choice and not all applications should move to the cloud or will move to the cloud.  Cloud is really about everything ephemeral. In cloud every workload has a life-cycle. It is born, lives and of course dies. Every workload is completely decoupled from the underlying infrastructure. If your workload requires any fixed resources it is not a cloud workload. In a cloud, application workloads can move at anytime to new infrastructure (compute, network and storage) and as such the underlying infrastructure is basically throw-away, meaning it is provisioned entirely from templates or services. If your infrastructure is not throw-away it is not cloud.  In a nutshell what is new about cloud vs existing enterprise infrastructure is the life-cycle and especially the part involving instance destruction. While destroying other people's sandbox may be fun, destroying your own is not and that is one of many reasons cloud is really hard to grasp. We don't like to throw things away, we have a tendency to be pack rats but if you are creating instances that don't expire it is not cloud.  For the last 25 years we have built silo-based infrastructure to support applications. We have built infrastructure without plans for retiring said infrastructure. We have customised application environments so uniquely that automation is not even possible. Cloud is changing our thinking but in order to adopt cloud we need to also change the behaviors of the last 25 years. If that wasn't difficult enough we still have to deal with existing enterprise infrastructure and applications that will probably never be cloud-ready.  Virtualization is not cloud  Many often confuse cloud for virtualization. Virtualization is a technology. There are many types of virtualization (compute, network and storage). Within those virtualization types are different implementations such as hypervisor vs container, software-defined-networking (SDN) vs network functional virtualization (NPV) and hyper-converged vs software-defined-storage (SDS). Cloud is not a technology but rather an architectural methodology of resource governance that utilises underlying virtualization technologies.  Three key ingredients  Cloud is definitely not easy, it is really hard. To put it down into just three things seems silly but I feel today it simply has too many meanings. My point here is to help identify what basic building blocks are required by cloud infrastructure.   Virtualize everything on ephemeral infrastructure around cloud platform such as OpenStack Automate everything One-click consumable tenant-based services  Cloud is about the applications  You have your cloud platform up and running, time to open the flood gates? Not so fast. If you don't have cloud applications you wont be in business long. So how do cloud applications differ from legacy applications?  Cloud applications are very challenging to build but incredibly simple to operationalize. Every component or service must be decoupled from one another and able to scale independently as well as horizontally. Each service must do one thing and one thing only otherwise we start incurring dependencies and our horizontal scaling model breaks.  Much more time is required to architect or design a cloud application vs a legacy application however there are clear advantages. One major advantage is developers no longer need to worry about how to scale their application. Cloud platforms such as Red Hat OpenShift can be used to deploy cloud applications in containers that provide automatic load balancing as well as scaling. If our application can be broken down into standalone components or services that interact using APIs then we may have a candidate for a cloud computing platform. Jeff Bezos Amazons CEO once mandated that all amazon services, libraries and components speak to one another using APIs and famously said:  \"Anyone who doesn’t do this will be fired.  Thank you; have a nice day!\"  This was perhaps the beginning of the age of cloud applications and reading Jeff Bezos mandate certainly gives us the right mindset for building cloud applications.  Summary  Cloud is not a technology but rather an architectural methodology of resource governance that utilises underlying virtualization technologies. Just because someone says private cloud, public cloud or hybrid cloud doesn't mean anything. Have a discussion about applications and workloads. Discuss which IT methodologies or architectures best suite those workloads. There is no one-size-fits-all approach. Cloud infrastructure is here and the future is bright but existing enterprise infrastructure isn't going anywhere either, we will have both! The biggest challenge and what will separate the best from the rest will be the ability to move workloads between these two different worlds. In order for this to happen compute, network and storage hardware must be entirely abstracted from cloud management stack. Choose your cloud platform and hardware vendors wisely. Lastly it all comes down to the applications and choosing the right platform to operate those applications, wether it be to cloud or not to cloud.  (c) 2015 Keith Tenzer  ","categories": ["General"],
        "tags": ["Amazon","Cloud","OpenShift","OpenStack","Red Hat"],
        "url": "/general/to-cloud-or-not-to-cloud-that-is-the-question/",
        "teaser": null
      },{
        "title": "Red Hat CloudForms Overview and Setup",
        "excerpt":"Overview  CloudForms is an upper-layer management abstraction that allows an organization to manage private, public and virtual infrastructure seamlessly from a single-pane-of-glass. CloudForms allows you to create powerful policies and apply them to all virtual infrastructure.  In CloudForms you can not only build workflows and automation but you can expose those workflows to end-users as services. You can build service catalogs and easy to consume UI forms. Finally you can collect information about all virtual infrastructure enabling chargeback, dashboard visualization, monitoring and alerting. CloudForms supports OpenStack, Amazon EC2, Red Hat Enterprise Virtualization Management (RHEV-M), VMware vCenter, Microsoft System Center Virtual Machine Manager (SCVMM) and all the virtual infrastructure those platforms provide.    The CloudForms product is downstream from the ManageIQ open-source project. Red Hat is a founding member and a large contributor to this community.  Terminology  Before sinking our teeth into Cloud Forms it is important to understand some terminology. CloudForms allows for the management of clouds and infrastructure.  Clouds  Providers - A cloud provider is a computing platform that manages instances and allows building of multi-tenant infrastructure services independent from underlying hypervisors. Cloud providers supported by CloudForms are Amazon EC2 and OpenStack.  Availability Zones - Logical or physical groupings of cloud resources.  Tenants - Cloud computing provides a common software platform. Tenants share the platform but are completely isolated from one another. They have their own data, network and security.  Flavors - Cloud computing offers infrastructure as a service. A flavor describes a specific type of instance and what kind of resources are made available to the instance. All instances are created from a type of flavor.  Security Groups - Tenant based security that defines Ingres and Egres access to instances.  Instances - Virtual Machines running under cloud infrastructure.  Infrastructure  Providers - A infrastructure provider is a management platform for managing virtual machines from a single type of hypervisor. Infrastructure providers supported by CloudForms are Red Hat Enterprise Virtualization Management (RHEV-M), VMware vCenter and Microsoft System Center Virtual Machine Manager (SCVMM).  Hosts - Hypervisors running on physical hardware providing virtual machines and infrastructure.  Clusters - Provide high availability and load balancing for a group of hosts. Clusters are groupings of compute hosts that share similar resources such as network and storage.  Virtual Machines - Operating systems running under a hypervisor.  Datastores - Storage locations that contain virtual images and disks.  Installation  Cloud Forms is distributed as an Open Virtual Applicance (OVA). There are OVAs for RHEV, OpenStack and VMware. The CloudForms OVAs are available for download here. If you are using RHEV you can follow the importing OVAs into RHEV guide.   Deploy new virtual machine based off OVA template Connect to console and login as user:admin password: smartvm Press \"enter\" and you will be in the appliance configuration     Configure static network settings Configure hostname Configure timezone, date and time Configure database  Choose internal database Enter a three-digit region id to create new region (can be any three-digit number)   Start EVM server process  At this point you have successfully configured the appliance and can now connect using your web browser to https://hostname.  Configuration  CloudForms is an appliance so after installation all management will occur through the web-based management portal. In order to start doing things with CloudForms we need to configure users, enable automation and add providers. Connect to CloudForms management portal as user: admin password: smartvm.  Creating a user  By default CloudForms will only create the administrator user admin. New users can be created under \"Configure\" -&gt; \"Configuration\" -&gt; \"Access Control\".    Here we can add users, groups and roles by simply selecting one of them and \"Configuration\" -&gt; \"Add new\".      Enable Automation  CloudForms has a lot of capabilities, one of them provisioning, requires automation to be enabled and by default it is not. Provisioning will allow CloudForms to create instances or virtual machines on the infrastructure that it is managing. In order to enable automation and as such provisioning navigate to \"Configure\" -&gt; \"Configuration\" -&gt; \"Settings. Under \"Zones\" -&gt; \"Zone\" select the CloufForms EVM server. Enable the \"Automation Engine\" and save.       Add Cloud Provider  In order to manage cloud infrastructure a provider must be added to CloudForms. Under \"Clouds\" -&gt; \"Providers\" select \"Add a new Cloud Provider\".    In this case we will add a OpenStack provider. Before adding provider make sure you validate credentials. Validation tests if CloudForms can communicate with the provider using the given credentials. CloudForms will let you add a provider regardless of if communication is working.   Add Infrastructure Provider  In order to manage virtual infrastructure an infrastructure provider must be added to CloudForms. Under \"Infrastructure\" -&gt; \"Providers\" select \"Add a new Infrastructure Provider\".    We will add a VMware vCenter provider.  Conclusion  CloudForms provides instance or virtual machine life-cycle management. The idea is that a workload has a life-span and during its life will run on different infrastructure. It could be on private cloud infrastructure such as that from Red Hat, VMware or Microsoft. It could be on public cloud infrastructure from Amazon EC2, Cloud Service Provider running OpenStack or your own OpenStack private cloud. CloudForms lets you seamlessly manage your entire virtual infrastructure with policies and powerful automation engines. As we move into the future it is clear the hybrid cloud will prevail and become increasingly important. The idea of workload life-cycle management and moving workloads effortlessly across different infrastructure management platforms will become the norm. The whole concept of a workload life-cycle is new to many organizations.  We are used to provisioning new environments but decommissioning them is something that occurs far less frequently. We need to change, infrastructure isn't built forever, it is built for now and when the time comes workloads must be decommissioned or moved to new infrastructure seamlessly. Beyond that, our various cloud or infrastructure platforms must be controlled or governed by central policies. Finally provisioning must be completely automated through one-click end-user service catalogs. This is what CloudForms is all about. CloudForms is taking huge strides to satisfy these needs and become the upper-layer multi-cloud management leader. Hopefully this post intrigues your interest to investigate this topic further.  (c) 2015 Keith Tenzer  ","categories": ["CloudForms"],
        "tags": ["Amazon","Cloud","Hyper-V","OpenStack","Red Hat","RHEV","VMware"],
        "url": "/cloudforms/red-hat-cloudforms-overview-and-setup/",
        "teaser": null
      },{
        "title": "OpenStack Multiple Node Configurations",
        "excerpt":"Node Types  OpenStack can be deployed in a single-node or multi-node configuration. For the purpose of this post I am going to assume you understand OpenStack basics and have at least done a basic installation on a single-node using RDO or another installer.  If not please refer to this post which covers the basics. OpenStack is of course a culmination of loosely coupled projects that define services. A node is nothing more than a grouping of OpenStack services that run on bare-metal, in a container or virtual machine. The purpose of a node is to provide horizontal scaling and HA. There are four possible node types in OpenStack: controller, compute, network and storage.  Controller Node  The controller node is the control plane for the OpenStack environment. The control pane handles identity (keystone), dashboard (Horizon), telemetry (ceilometer), orchestration (heat) and network server service (neutron).  Compute Node  The compute node runs a hypervisor (KVM, ESX, Hyper-V or XenServer). The compute node handles compute service (nova), telemetry (ceilometer) and network Open vSwitch agent service (neutron).  Network Node  The network node runs networking services (neutron). It runs the neutron services for L3, metadata, DHCP and Open vSwitch. The network node handles all networking between other nodes as well as tenant networking and routing. It provides services such as DHCP and floating IPs that allow instances to connect to public networks. Neutron sits on top of Open vSwitch using either the ml2 or openvswitch plugin. Using Open vSwitch Neutron builds three network bridges: br-int, br-tun and br-ex. The br-int bridge connects all instances. The br-tun bridge connects instances to the physical NIC of the hypervisor. The br-ex bridge connects instances to external (public) networks using floating IPs. Both the br-tun and br-int bridges are visible on compute and network nodes. The br-ex bridge is only visible on network nodes.  Storage Node  The storage node runs storage services. It handles image service (glance), block storage (cinder), object storage (swift) and in the future shared file storage (manila). Typically a storage node would run one type of storage service: object, block or file. Glance should run on nodes providing storage services for images (Cinder or Swift). Glance typically benefits from running on same node as its backing storage service. NetApp for example provides a storage backend that allows images to be cloned using the NetApp storage backend instead of the network.  Multi-node Configurations  While single-node configurations are acceptable for small environments, testing or POCs most production environments will require a multi-node configuration for various reasons. As mentioned multi-node configurations group similar OpenStack services and provide scalability as well as the possibility for high availability. One of the great things about OpenStack is the architecture. Every service is decoupled and all communication between services is done through RESTful API endpoints. This is the model architecture for cloud. The advantages are that we have tremendous flexibility in how to build a multi-node configuration. While a few standards have emerged there are many more possible variations and in the end we are not stuck to a rigid deployment model. The standards for deploying multi-node OpenStack are as a two-node, three-node or four-node configuration.  Two-node OpenStack Configuration  The two-node configuration has a controller and compute node. Here we can easily scale-out compute nodes. Most likely we would run just one controller node or we could setup an active / passive HA configuration for the controller node using Pacemaker. Below is an illustration of a two-node OpenStack configuration:    Three-node OpenStack Configuration  The three-node configuration has a controller, compute and network node. Here we can easily scale-out compute or network nodes. Most likely we would run just one controller node or we could setup an active / passive HA configuration for the controller node using Pacemaker. In addition we could also setup active / passive configuration for network node to achieve HA and horizontal scaling depending on resource requirements. Below is an illustration of a three-node OpenStack configuration:    Four-node OpenStack Configuration  The four-node configuration has a controller, compute, network and storage node. Here we can easily scale-out compute, network and storage nodes. Most likely we would run just one controller node or we could setup an active / passive HA configuration for the controller node using Pacemaker. In addition we could also setup active / passive configuration for network and storage nodes to achieve HA as well as horizontal scaling depending on resource requirements. Below is an illustration of a four-node OpenStack configuration:    Three-node OpenStack Installation  Before we start the installation we will need to provision three nodes running RHEL (Red Hat Enterprise Linux). The nodes can be bare-metal, containers or virtual machines. In my environment I created three virtual machines running RHEL 7 under RHEV 3.4 (Red Hat Enterprise Virtualization). If you are interested in how to setup RHEV you can get more information here. Below are the steps to deploy a three-node OpenStack installation using RHEL 7 and latest Red Hat OpenStack distribution:  Steps to preform on each node   Install RHEL 7 and enable one NIC interface (eth0) Register subscription  #subscription-manager register    List available subscriptions  #subscription-manager list --available  Attach a specific subscription (the pool id is listed in above command)    #subscription-manager attach --pool=8a85f9814a7ea2ec014a813b19433cc8  Clear existing repositories and enable correct ones to grab latest Red Hat OpenStack distro    #subscription-manager repos --disable=*     #subscription-manager repos --enable=rhel-7-server-rpms   #subscription-manager repos --enable=rhel-7-server-optional-rpms   #subscription-manager repos --enable=rhel-7-server-openstack-5.0-rpms   or    #subscription-manager repos --enable=rhel-7-server-openstack-6.0-rpms  Install required packages    #yum install -y yum-plugin-priorities yum-utils     #yum-config-manager --setopt=”rhel-7-server-openstack-5.0-rpms.priority=1” --enable rhel-7-server-openstack-5.0-rpms   or    #yum-config-manager --setopt=”rhel-7-server-openstack-6.0-rpms.priority=1” --enable rhel-7-server-openstack-6.0-rpms   #yum update -y     #yum install -y openstack-packstack    Setup hostname    #hostname ostack-ctr.openstack     #vi /etc/hostname   ostack-ctr.openstack     #vi /etc/sysconfig/network     HOSTNAME=ostack-ctr.openstack GATEWAY=192.168.2.1    Setup hosts file if DNS resolution is not configured    #vi /etc/hosts     127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.2.205 ostack-ctr ostack-ctr.openstack 192.168.2.206 ostack-cmp ostack-cmp.openstack 192.168.2.207 ostack-net ostack-net.openstack  Configure eth0 network interface    #vi /etc/sysconfig/network-scripts/ifcfg-eth0     TYPE=Ethernet BOOTPROTO=none DEFROUTE=yes IPV4_FAILURE_FATAL=no IPV6INIT=no IPV6_AUTOCONF=yes IPV6_DEFROUTE=yes IPV6_PEERDNS=yes IPV6_PEERROUTES=yes IPV6_FAILURE_FATAL=no NAME=eth0 UUID=6c53462f-f735-48c0-ae85-c0ec61a53688 ONBOOT=yes HWADDR=00:1A:4A:DE:DB:CC IPADDR0=192.168.2.205 PREFIX0=24 GATEWAY0=192.168.2.1 DNS1=192.168.2.1 NM_CONTROLLED=no   Note: ensure you set the MAC address (HWADDR) correctly. You can find the MAC address using the \"ip a\" command.   Disable Network Manager    #systemctl disable NetworkManager  reboot node  Steps to perform on controller node   Generate the default answers file  #packstack --gen-answer-file=/root/answer_file.txt  Update the following in answers file    CONFIG_CONTROLLER_HOSTS=192.168.2.205 CONFIG_COMPUTE_HOSTS=192.168.2.206 CONFIG_NETWORK_HOSTS=192.168.2.207 CONFIG_STORAGE_HOST=192.168.2.205 CONFIG_HORIZON_SSL=y CONFIG_PROVISION_DEMO=n  Install OpenStack using packstack answers file    #packstack --answer-file=/root/answer_file.txt   Network Node Configuration  In order to connect a existing physical network, eth0 must be added as a port to the Open vSwitch br-ex bridge. The below steps should be preformed on the network node:   Update network config for eth0  #vi /etc/sysconfig/network-scripts/ifcfg-eth0 DEVICE=eth0 HWADDR=00:1A:4A:DE:DB:9C TYPE=OVSPort DEVICETYPE=ovs OVS_BRIDGE=br-ex ONBOOT=yes   Note: ensure that the MAC address (HWADDR) is correct   Update network config for br-ex  #vi /etc/sysconfig/network-scripts/ifcfg-br-ex     DEVICE=br-ex DEVICETYPE=ovs TYPE=OVSBridge BOOTPROTO=static IPADDR=192.168.2.207 NETMASK=255.255.255.0 GATEWAY=192.168.2.1 DNS1=192.168.2.1 ONBOOT=yes    Add eth0 to br-ex bridge and restart networking    #ovs-vsctl add-port br-ex eth0 ; systemctl restart network.service    Verify Open vSwitch configuration on network node (eth0 should be connected to br-ex)  #ovs-vsctl show 348cc676-f177-4ee3-a522-8f02aeb4dcd6  Bridge br-int     fail_mode: secure     Port br-int        Interface br-int           type: internal  Bridge br-tun     Port patch-int        Interface patch-int           type: patch           options: {peer=patch-tun}     Port \"gre-1\"        Interface \"gre-1\"           type: gre           options: {in_key=flow, local_ip=\"192.168.2.207\", out_key=flow, remote_ip=\"192.168.2.206\"}     Port br-tun        Interface br-tun           type: internal  Bridge br-ex     Port \"eth0\"        Interface \"eth0\"     Port br-ex        Interface br-ex        type: internal  ovs_version: \"2.1.3\"   Networking Configuration  Public networks allow instances to connect to existing external networks. This is achieved by allocating floating IPs  from existing external networks that are shared across tenants. Private networks are tenant networks that provide complete isolation to all instances within a tenant.  Create Private Network    #neutron net-create private     #neutron subnet-create private 10.0.0.0/24 --name private_subnet   Create Public Network    #neutron net-create public --shared --router:external=True     #neutron subnet-create public 192.168.2.0/24 --name public_subnet --enable_dhcp=False --allocation-pool start=192.168.2.220,end=192.168.2.240 --gateway=192.168.2.1   Create Router    #neutron router-create router1     #neutron router-interface-add router1 private     #neutron router-gateway-set router1 public   Note: make sure you source the /root/keystonerc_admin file otherwise neutron commands will not work  Once we have created the private and public networks we should see the below network topology. In addition we can connect instances to just the private network or both by allocating a floating IP.    Summary  This guide covered the very basics of multi-node OpenStack deployments and networking. From here hopefully you should be able to deploy your own OpenStack multi-node configurations using Red Hat RDO. I really recommend setting up multi-node environments, it is the best way to understand and learn how the different OpenStack projects interact with one another. In addition if you would like to do everything from scratch without RDO or another distro you can follow the complete manual guide here. I hope this guide has been interesting and useful. I always appreciate commends and feedback so please leave some.  Happy Stacking!  (c) 2015 Keith Tenzer  ","categories": ["OpenStack"],
        "tags": ["KVM","Linux","OpenStack","Red Hat"],
        "url": "/openstack/openstack-multiple-node-configurations/",
        "teaser": null
      },{
        "title": "Building Custom Dashboards in OpenStack Horizon",
        "excerpt":"Overview  Horizon is an OpenStack project responsible for providing a dashboard. It brings together all OpenStack projects in a single-pane-of-glass. The below diagram illustrates the connectivity between the Horizon dashboard and other OpenStack services.      Administrators typically don't like to have many different management interfaces and Horizon provides a framework for extending its dashboard services. By building custom dashboards it is possible to seamlessly integrate external components or services with OpenStack.  I have been working on integrating a powerful automation framework called Integra in OpenStack Horizon to allow tighter coupling of OpenStack and enterprise infrastructure. Integra takes automation to a new level by providing a powerful workflow engine that consumes provider exposed actions and allows users to automate without creating any technical debt. If you are interested in Integra you can read more about it at http://integra.emitrom.com.  Horizon Components  Horizon is built on Django which is a web application framework in Python. Django's primary goal is to ease creation of complex database-driven websites. Django emphasizes reusability and pluggability . Before getting into more detail about the code it is important to understand some basic terminology within the Horizon framework.  Terminology  Dashboard - This is the top level UI component, dashboards contains panel groups and panels. They are configured using the dasboard.py file.  Panel Groups - In Horizon panel groups organize similar panels together and provide a top-level drop-down. Panel groups are configured in the dashboard.py file.  Panel - The main UI component, each panel has its own directory and standardized directory structure. Panels are configured in the dashboard/panel/panel.py file.  Tab Groups - A tab group contains one or more tabs. A tab group can be configured per panel using the tabs.py file.  Tabs - Tabs are units within a tab group. It represents one view of the data.  Workflows - A workflow is a series of steps that allow for collecting user inputs. Workflows are created under dashboard/panel/workflows/workflow.py.  Workflow Steps - A workflow consists of one or more steps. A step is a wrapper around an action that understands its context within a workflow. Using workflows and steps we can build multiple input forms that guide a user through a complex configuration process.  Actions - An action allows us to spawn a workflow step. Actions are typically called from within a data table. Two of the most common actions are the DeleteAction and LinkAction.  Tables - Horizon includes a componetized API for dynamically creating tables in the UI. Every table renders correctly and consistently. Data tables are used for displaying information to the user. Tables are configured per panel in the tables.py file.  URLs - In Horizon URLs are needed to track context. At minimum a URL is required to display the main view but any LinkAction or actions that leave the main view will also require a URL. URLs are configured in the urls.py file.  Views - A view displays a data table and encompasses the main panel frame. Views are configured per panel in the views.py file.  Horizon Dashboard Directory Structure  Horizon requires a standard directory structure and strict file naming conventions. Below is an example of the directory structure for a dashboard called Integra that has five panels (actions, jobs, providers, workflows and schedules). As you can see each panel has its own sub-directory.    Below is an example of the directory structure for a panel called providers that belongs to dashboard Integra.    Note: the static and templates directories are always the same you just need to change name of the directory e.g. \"providers\" and update path in the _scripts.html, base.html as well as index.html. The __init__.py and models.py are never changed, just leave them as is.  Setup Development Environment  Since coding in \"vim\" is not very fun with Python, it is important to setup an IDE. In addition leveraging devstack provides an OpenStack development environment for Horizon that makes testing Horizon code much simpler. In fact Horizon provides some tools that not only help with mundane tasks but also provide a lightweight Django server for testing. You can use either Fedora or Ubuntu for your development environment, I went with Fedora.   Install Java 7 #yum install -y openjdk-7-jre   Java is required to run PyCharm Python IDE   Download and install PyCharm    https://www.jetbrains.com/pycharm/   Note: PyCharm is a good IDE but feel free to use a different IDE if you desire.   Install git #yum install -y git  Clone the devStack git repository #git clone https://github.com/openstack-dev/devstack.git  Run stack.sh #cd devstack;./stack.sh   You will be prompted for several passwords. To make things easy just use the same password for each component. The installation will take 5 - 10 minutes and when it completes you should see the below message.    Horizon is now available at http://192.168.2.211/ Keystone is serving at http://192.168.2.211:5000/v2.0/ Examples on using novaclient command line is in exercise.sh The default users are: admin and demo The password: integra This is your host ip: 192.168.2.211 2015-02-14 18:06:31.434 | stack.sh completed in 387 seconds.   Note: each time you want to shutdown devstack you run unstack.sh and each time you want to start devstack stack.sh.  Horizon Development Tool  One of the great things about devstack is that it includes an important development tool for Horizon. The run_tests.sh script located under /opt/stack/horizon starts the development server and creates default directory structure for a dashboard/panel.   Running the development server #./run_tests.sh --runserver 0.0.0.0:8877    Creating default dashboard and panel directory structure #mkdir openstack_dashboard/dashboards/mydashboard #./run_tests.sh -m startdash mydashboard --target openstack_dashboard/dashboards/mydashboard #mkdir openstack_dashboard/dashboards/mydashboard/mypanel #./run_tests.sh -m startpanel mypanel --dashboard=openstack_dashboard.dashboards.mydashboard --target=openstack_dashboard/dashboards/mydashboard/mypanel   Horizon Start Scripts  Dashboards are loaded through start scripts located under the horizon/openstack_dashboard/enabled directory. In this case I created a _50_integra.py. The number has to do with the order in which dashboards are loaded and rendered. It is similar to the pre-systemd concept of init scripts.    #vi /opt/stack/horizon/openstack_dashboard/enabled/_50_integra.py     DASHBOARD = 'integra'   DISABLED = False   ADD_INSTALLED_APPS = [ 'openstack_dashboard.dashboards.integra', ]   Code Examples  At this point let us dissect the code behind the dashboard itself and one of the dashboard panels. The code examples are derived from a project (Integra OpenStack UI ) that I am currently working on and are available in Github. This code is changing and influx so you might want to fork the repository from a known state.  Dashboard  Below is an example of a dashboard called Integra that contains four panels (Providers, Workflows, Schedules and Jobs).  [code language=\"python\"]  from django.utils.translation import ugettext_lazy as _  import horizon  class Providers(horizon.PanelGroup):      slug = &quot;providers&quot;      name = _(&quot;Providers&quot;)      panels = ('providers',)  class Workflows(horizon.PanelGroup):      slug = &quot;workflows&quot;      name = _(&quot;Workflows&quot;)      panels = ('workflows',)  class Schedules(horizon.PanelGroup):      slug = &quot;schedules&quot;      name = _(&quot;Schedules&quot;)      panels = ('schedules',)  class Jobs(horizon.PanelGroup):      slug = &quot;jobs&quot;      name = _(&quot;Jobs&quot;)      panels = ('jobs',)  class Integra(horizon.Dashboard):      name = _(&quot;Integra&quot;)      slug = &quot;integra&quot;      panels = (Providers, Workflows, Schedules, Jobs)      default_panel = 'providers'  horizon.register(Integra)  [/code]  The code is pretty straight forward. You have panel groups, panels and the horizon dashboard. Panels are organized under panel groups and then attached to the dashboard.  The above code will create the following dashboard and panel structure in Horizon. The providers table is generated from code we will discuss next.    Panel  Now lets dive into the panel \"providers\" that we have displayed above. In this case both the panel group and panel itself have the same name, but they don't have to and you can also of course have many panels.  The providers panel renders a data table from a REST API endpoint. If we look closely at the image above we can not only see a list of providers from Integra but we can take actions such as deleting a provider or adding a new provider.  panel.py  The panel.py defines the panel and registers it with the dashboard, in this case Integra.  [code language=\"python\"]  from django.utils.translation import ugettext_lazy as _  import horizon  from openstack_dashboard.dashboards.integra import dashboard  class Providers(horizon.Panel):      name = _(&quot;Providers&quot;)      slug = &quot;providers&quot;  dashboard.Integra.register(Providers)  [/code]  views.py  The views.py is responsible for dynamically rendering the panel frame. It is also responsible for rendering any actions that spawn a new frame. In this case we have the default view ProvidersIndexView that loads a table ProviderTable. The table then is defined in the tables.py.  We also have an action in order to add a new provider that spawns a new frame. When the view AddProviderView is invoked, a workflow instead of a table class will be called. The workflow is defined under integra/providers/workflows/add_provider.py.  [code language=\"python\"]  from horizon import exceptions, tables, workflows, forms, tabs  from openstack_dashboard.dashboards.integra.providers.tables import ProviderTable  from openstack_dashboard.dashboards.integra.providers import utils  from openstack_dashboard.dashboards.integra.providers.workflows.add_provider import AddProvider  class ProvidersIndexView(tables.DataTableView):      table_class = ProviderTable      template_name = 'integra/providers/index.html'      def get_data(self):          return utils.getProviders(self)  class AddProviderView(workflows.WorkflowView):      workflow_class = AddProvider      def get_initial(self):          initial = super(AddProviderView, self).get_initial()          return initial  [/code]  urls.py  The urls.py defines context URLs. A URL is required for the main panel frame and any new frames that are launching workflows. In our case we have two, the INDEX AND ADD_PROVIDER URLs. Notice that each URL is correlated with a view that is defined in the views.py.  [code language=\"python\"]  from django.conf.urls import patterns, url  from openstack_dashboard.dashboards.integra.providers import views  INDEX_URL = r'^$'  ADD_PROVIDER_URL = r'^add'  urlpatterns = patterns('openstack_dashboard.dashboards.integra.providers.views',      url(INDEX_URL, views.ProvidersIndexView.as_view(), name='index'),      url(ADD_PROVIDR_URL, views.AddProviderView.as_view(), name='add'),  )  [/code]  tables.py  The tables.py is responsible for the data table and providing user outputs. Our table displays Integra providers and allows for a couple of actions. It lets us add and delete a provider. It also lets us filter the provider list that is displayed.  The AddTableData and DeleteTableData classes are both LinkActions. The AddTableData will launch a workflow that gathers user inputs. This is how we can add a new provider.  The DeleteTableData class removes a provider from the provider table. Here we use the DeleteAction and call a method in the utils class. This method in turn makes a REST call to Integra in order to delete the specified provider.  At the bottom the table structure is created and the actions are embedded into the providers table. The meta class is a special inner-class for Django data tables that allow us to configure various table options. Finally notice how the filter is added to the table, this is pretty standard and found in many places within Horizon.  [code language=\"python\"]  from django.utils.translation import ugettext_lazy as _  from horizon import tables  from openstack_dashboard.dashboards.integra.providers import utils  class AddTableData(tables.LinkAction):      name = &quot;add&quot;      verbose_name = _(&quot;Add Provider&quot;)      url = &quot;horizon:integra:providers:add&quot;      classes = (&quot;btn-launch&quot;, &quot;ajax-modal&quot;)  class DeleteTableData(tables.DeleteAction):      data_type_singular = _(&quot;Provider&quot;)      data_type_plural = _(&quot;Providers&quot;)      def delete(self, request, obj_id):          utils.deleteProvider(self, obj_id)  class FilterAction(tables.FilterAction):      def filter(self, table, providers, filter_string):          filterString = filter_string.lower()          return [provider for provider in providers                  if filterString in provider.title.lower()]  class UpdateRow(tables.Row):      ajax = True      def get_data(self, request, post_id):          pass  class ProviderTable(tables.DataTable):      id = tables.Column(&quot;id&quot;,                            verbose_name=_(&quot;Id&quot;))      name = tables.Column(&quot;name&quot;,                            verbose_name=_(&quot;Name&quot;))      description = tables.Column(&quot;description&quot;,                            verbose_name=_(&quot;Description&quot;))      hostname = tables.Column(&quot;hostname&quot;,                            verbose_name=_(&quot;Hostname&quot;))      port = tables.Column(&quot;port&quot;,                            verbose_name=_(&quot;Port&quot;))      timeout = tables.Column(&quot;timeout&quot;,                            verbose_name=_(&quot;Timeout&quot;))      secured = tables.Column(&quot;secured&quot;,                            verbose_name=_(&quot;Secured&quot;))      class Meta:          name = &quot;integra&quot;          verbose_name = _(&quot;Providers&quot;)          row_class = UpdateRow          table_actions = (AddTableData,                           FilterAction)          row_actions = (DeleteTableData,)  [/code]  utils.py  The utils.py is a utility class. You can call it whatever you want, it is not required but in this case it is nice to separate the Integra REST calls from the rest of our Horizon application.  Three methods are defined in order to delete a provider, add a provider and get a list of all providers. We have already talked about adding and deleting a provider. The getProviders method returns a list of providers from Integra through the REST API. We have created a Provider model class that understands the structure of a provider object. One really nice thing about Python is that it natively handles JSON marshaling and since Integra returns JSON things are in this case quite simple.  [code language=\"python\"]  import traceback  import time  from time import mktime  from datetime import datetime  from requests.auth import HTTPBasicAuth  from django.template.defaultfilters import register  from django.utils.translation import ugettext_lazy as _  import requests  from horizon import exceptions  requests.packages.urllib3.disable_warnings()  integra_url = &quot;https://localhost:8443/rest&quot;  json_headers = {'Accept': 'application/json'}  class Provider:      &quot;&quot;&quot;      Provider data      &quot;&quot;&quot;      def __init__(self, id, name, description, hostname, port, timeout, secured):          self.id = id          self.name = name          self.description = description          self.hostname = hostname          self.port = port          self.timeout = timeout          self.secured = secured  def getProviders(self):      try:          r = requests.get(integra_url + &quot;/providers&quot;, verify=False, auth=HTTPBasicAuth('admin', 'integra'), headers=json_headers)          providers = []          for provider in r.json()['providers']:              providers.append(Provider(provider[u'id'], provider[u'name'], provider[u'description'], provider[u'hostname'], provider[u'port'], provider[u'timeout'], provider[u'secured']))          return providers      except:          exceptions.handle(self.request,                            _('Unable to get providers'))          return []  # request - horizon environment settings  # context - user inputs from form  def addProvider(self, request, context):      try:          name = context.get('name')          description = context.get('description')          hostname = context.get('hostname')          port = context.get('port')          timeout = context.get('timeout')          secured = context.get('secured')          payload = {'name': name, 'description': description, 'hostname': hostname, 'port': port, 'timeout': timeout, 'secured': secured}          requests.post(integra_url + &quot;/providers&quot;, json=payload, verify=False, auth=HTTPBasicAuth('admin', 'integra'), headers=json_headers)      except:          print &quot;Exception inside utils.addProvider&quot;          print traceback.format_exc()          exceptions.handle(self.request,                            _('Unable to add provider'))          return []  # id is required for table  def deleteProvider(self, id):      try:          requests.delete(integra_url + &quot;/providers/&quot; + id, verify=False, auth=HTTPBasicAuth('admin', 'integra'), headers=json_headers)      except:          print &quot;Exception inside utils.deleteProvider&quot;          print traceback.format_exc()          exceptions.handle(self.request,                            _('Unable to delete provider'))          return False  [/code]  add_provider.py  The add_provider.py is a workflow that contains a single workflow step. This is the workflow that is called when we add a new provider. It is responsible for getting user input.  The AddProvider class executes when the workflow is called. It calls the SetAddProviderDetails class which then calls the SetAddProviderDetailsAction class and returns the user inputs within the context object.  [code language=\"python\"]  import traceback  from horizon import workflows, forms, exceptions  from django.utils.translation import ugettext_lazy as _  from openstack_dashboard.dashboards.integra.providers import utils  class SetAddProviderDetailsAction(workflows.Action):      name = forms.CharField(          label=_(&quot;Name&quot;),          required=True,          max_length=80,          help_text=_(&quot;Name&quot;))      description = forms.CharField(          label=_(&quot;Description&quot;),          required=True,          max_length=120,          help_text=_(&quot;Description&quot;))      hostname = forms.CharField(          label=_(&quot;Hostname&quot;),          required=True,          max_length=120,          help_text=_(&quot;Hostname&quot;))      port = forms.IntegerField(          label=_(&quot;Port&quot;),          required=True,          min_value=1,          max_value=65535,          help_text=_(&quot;Port&quot;))      timeout = forms.IntegerField(          label=_(&quot;Timeout&quot;),          required=True,          min_value=1,          max_value=100000,          help_text=_(&quot;Timeout&quot;))      secured = forms.BooleanField(          label=_(&quot;Secured&quot;),          required=False,          help_text=_(&quot;Secured&quot;))      class Meta:          name = _(&quot;Details&quot;)      def __init__(self, request, context, *args, **kwargs):          self.request = request          self.context = context          super(SetProviderDetailsAction, self).__init__(              request, context, *args, **kwargs)  class SetAddProviderDetails(workflows.Step):      action_class = SetAddProviderDetailsAction      contributes = (&quot;name&quot;, &quot;description&quot;, &quot;hostname&quot;, &quot;port&quot;, &quot;timeout&quot;, &quot;secured&quot;)      def contribute(self, data, context):          if data:              context['name'] = data.get(&quot;name&quot;, &quot;&quot;)              context['description'] = data.get(&quot;description&quot;, &quot;&quot;)              context['hostname'] = data.get(&quot;hostname&quot;, &quot;&quot;)              context['port'] = data.get(&quot;port&quot;, &quot;&quot;)              context['timeout'] = data.get(&quot;timeout&quot;, &quot;&quot;)              context['secured'] = data.get(&quot;secured&quot;, &quot;&quot;)          return context  class AddProvider(workflows.Workflow):      slug = &quot;add&quot;      name = _(&quot;Add&quot;)      finalize_button_name = _(&quot;Add&quot;)      success_message = _('Added provider &quot;%s&quot;.')      failure_message = _('Unable to add provider &quot;%s&quot;.')      success_url = &quot;horizon:integra:providers:index&quot;      failure_url = &quot;horizon:integra:providers:index&quot;      default_steps = (SetAddProviderDetails,)      def format_status_message(self, message):           return message % self.context.get('name', 'unknown provider')      def handle(self, request, context):          try:              utils.addProvider(self, request, context)              return True          except Exception:              print traceback.format_exc()              exceptions.handle(request, _(&quot;Unable to add provider&quot;))              return False  [/code]  Below we can see the above code in action.    Dynamic Inputs  So far we have seen how we can build static input forms using CharField, IntegerChield or BooleanField. Next lets look at how to create dynamic inputs within workflows using ChoiceField. In our utils.py we already have an method getProviders that returns a list of provider objects. In addition we will add a new method for returning a list of provider actions.  utils.py  [code language=\"python\"]  def getProviders(self):      try:          r = requests.get(integra_url + &quot;/providers&quot;, verify=False, auth=HTTPBasicAuth('admin', 'integra'), headers=json_headers)          providers = []          for provider in r.json()['providers']:              providers.append(ProviderAction(provider[u'id'], provider[u'name'], provider[u'description']))          return providers      except:          exceptions.handle(self.request,                            _('Unable to retrieve list of posts.'))          return []  def getProviderActions(self):      try:          r = requests.get(integra_url + &quot;/provider_actions/&quot; + id, verify=False, auth=HTTPBasicAuth('admin', 'integra'), headers=json_headers)          providerActions = []          for providerAction in r.json()['providerActions']:              providerActions.append(ProviderAction(providerAction[u'id'], providerAction[u'name'], providerAction[u'description']))          return providerActions      except:          exceptions.handle(self.request,                            _('Unable to retrieve list of posts.'))          return []  [/code]  In our workflow action we can get a list of provider action objects and display them to the user using a ChoiceField. Notice the ChoiceField requires the id and name. Only the name is displayed but the id is required for mapping purposes.  add_workflow_action.py  [code language=\"python\"]  class SetAddDetailsAction(workflows.Action):      providerActionsChoices = [(providerAction.id, providerAction.name) for providerAction in providerActions]      providerChoices = [(provider.id, provider.name) for provider in providers]      name = forms.CharField(          label=_(&quot;Name&quot;),          required=True,          max_length=80,          help_text=_(&quot;Name&quot;))      description = forms.CharField(          label=_(&quot;Description&quot;),          required=True,          max_length=120,          help_text=_(&quot;Description&quot;))      provider = forms.ChoiceField(          label=_(&quot;Providers&quot;),          choices=providerChoices,          required=True,          help_text=_(&quot;Providers&quot;))      action = forms.ChoiceField(          label=_(&quot;Provider Actions&quot;),          choices=providerActionsChoices,          required=True,          help_text=_(&quot;Provider Actions&quot;))  [/code]  Below we can see two dynamic fields being generated. Both populate the ChoiceField from dynamic data that is recieved from the Integra REST API.    Conclusion  OpenStack Horizon is a powerful web-framework built on Django that is easily extended and Integra is a powerful provider based automation framework. We have seen how easy it is to create our own Horizon dashboard and interface with services outside of OpenStack through RESTful APIs. The above examples can be followed to accomplish virtually anything. Horizon is a glimpse into the future of infrastructure single-pane-of-glass management. This has been something we have been promised for years from the proprietary vendors and only now with OpenStack Horizon do we have some real hope.  If you are working on Horizon dashboards or have feedback I would love to hear about it?  Happy Stacking!  (c) 2015 Keith Tenzer  ","categories": ["OpenStack"],
        "tags": ["Dashboard","Horizon","OpenStack","Python"],
        "url": "/openstack/building-custom-dashboards-in-openstack-horizon/",
        "teaser": null
      },{
        "title": "Enterprise OpenStack: RHEL OSP",
        "excerpt":"Overview  As of today there are over eleven OpenStack services and more are coming. Each service has complete isolation from other services and that allows OpenStack to scale far beyond the reach of current computing platforms. However due to all these independent services, OpenStack can be very complicated to operationalize in enterprise environments.    Each service has a database or means for storing stateful data, requires load balancing and can scale at a different pace from that of other services. In addition OpenStack has over 1000 different configuration parameters, making it impossible to deploy and manage without complete automation through configuration management.  Enterprise OpenStack is about high availability, scalability, automation, configuration management, life-cycles and of course support. Choosing the right distribution is the most important decision you will ever make in regards to OpenStack. Below are some points to consider:   Support - How much experience does company have with Open Source, Linux, Ceph and KVM? Vision - How complete is the OpenStack story?  Does it extend to applications? Enterprise Features - High availability, scalability and support for diversified hardware? Configuration Management - Can OpenStack be managed centrally and deployment be 100% automated? Enterprise Management - Charge-back, governance policies, single-pane-of-glass, hybrid cloud and support for traditional platforms (VMware, Microsoft Hyper-V, RHEV)? Linux Containers - How will OpenStack provide infrastructure for DevOps and containerized next-gen applications? Lock-in - Are you free to choose your underlying hardware vendor for compute, network and storage?  I know at least one company that can talk about all of these points and that is Red Hat. In this post we will focus on the Red Hat OpenStack Platform (RHEL OSP).    Node Types  RHEL OSP defines four types of nodes: admin, controller, compute and Ceph storage node.  Admin Node  The admin node is responsible for management of an OpenStack environment. It provides configuration management through puppet and automated installation through foreman. The admin node is constantly evolving and mixes best of breed open source technologies together. The admin node will allow an OpenStack administrator to deploy OpenStack with 100% automation on bare-metal or virtual infrastructure. The admin node maintains configuration for a deployment centrally therefore you can not only deploy initial environment but you can grow and scale-out the environment in an automated manner. The admin node provides the following additional services for an OpenStack deployment:   DHCP DNS PXE TFTP  These services enable automated provisioning and configuration management. The admin node can provision two types of OpenStack nodes today: controller and compute.  Controller Node  The controller node runs all OpenStack services except for Nova (compute). The Red Hat best practice is to run three controller nodes. The admin node and the RHEL OSP installer will configure a pacemaker cluster that uses corosync for the cluster network as well as fencing nodes. All OpenStack services will be individually clustered with an HA proxy for load balancing. The main reason for running three controller nodes aside from scalability and availability is fencing. Having three nodes provides clear ownership in a pacemaker cluster and as such two nodes is not a valid configuration.  The controller node runs the following OpenStack services:   Horizon (dashboard) Keystone (identity) Nova scheduler (compute) Neutron (network) Neutron metadata (metadata - cloud-init) Neutron L3 Agent (L3 networking) Cinder (block storage) Swift (object storage) Heat (orchestration) Ceilometer (telemetry)  Compute Node  The compute node runs Nova (compute) and the Neutron openvswitch agent. Compute nodes do not provide any high availability, if a compute node goes down then all instances hosted on that node are down. The point of OpenStack though is to provide horizontal scaling. If you need more compute resources you simply add more compute nodes. Applications must be resilient and able to handle instances going down, this is a requirement that is often misunderstood when discussing cloud computing.  Ceph Storage Node  The Ceph storage node runs Ceph services such as the Object Storage Daemon (OSD) that provides storage to a Ceph cluster. These nodes are not at this time provisioned by the admin node and must be configured separately. The admin node will let the administrator configure Ceph as a storage back-end for Glance or Cinder but the Ceph cluster must be available. Ceph requires metadata services however and it is ideal to run these on the controller nodes but other than the Ceph metadata services nothing else Ceph related should be running on OpenStack controller nodes.  Ceph is the defacto storage for OpenStack because it meets the requirements for OpenStack storage very well. Ceph can scale far beyond other storage systems and like OpenStack it abstracts hardware allowing you to be free in choice of your hardware vendors.  Installing Admin Node  The first step to deploying enterprise OpenStack is to install and configure the admin node. The admin node must at a minimum be connected to the provisioning network. If the admin node goes down you will be unable to provision. In addition if the admin node is providing DNS or DHCP to OpenStack environment those services will be offline. Infrastructure decisions regarding the admin node require thoughtful planning.  Prepare  Install RHEL 7.1 #subscription-manager register #subscription-manager list --available #subscription-manager attach --pool=&lt;pool id&gt; #systemctl disable NetworkManager.service #systemctl stop NetworkManager.service #yum remove dnsmasq #subscription-manager repos --disable=* #subscription-manager repos --enable=rhel-7-server-rpms #subscription-manager repos --enable=rhel-7-server-openstack-6.0-installer-rpms #subscription-manager repos --enable=rhel-server-rhscl-7-rpms #yum update -y #yum install -y rhel-osp-installer Setup Provisioning Network  Before starting the installer ensure your network interface is setup correctly. Below is an example.  DEVICE=eth0 BOOTPROTO=none HWADDR=52:54:00:a5:fe:26 ONBOOT=yes HOTPLUG=yes TYPE=Ethernet IPADDR=192.168.122.99 NETMASK=255.255.255.0 PEERDNS=yes DNS1=192.168.122.99 DNS2=192.168.122.1 NM_CONTROLLED=no Install and Configure  #rhel-osp-installer Please select NIC on which you want provisioning enabled:  1. eth0 2. eth1 ? 1  Networking setup: Network interface: 'eth0' IP address: '192.168.122.99' Network mask: '255.255.255.0' Network address: '192.168.122.0' Host Gateway: '192.168.122.1' DHCP range start: '192.168.122.100' DHCP range end: '192.168.122.254' DHCP Gateway: '192.168.122.99' DNS forwarder: '192.168.122.1' Domain: 'lab.local' NTP sync host: '0.rhel.pool.ntp.org' Timezone: 'Europe/Berlin' Configure networking on this machine: ✓ Configure firewall on this machine: ✓  Ensure your DNS forwarder is a system that can provide external DNS. The DHCP gateway should be admin node itself since it provides DHCP unless of course you want to handle this externally.  How would you like to proceed?:  1. Proceed with the above values 2. Change Network interface 3. Change IP address 4. Change Network mask 5. Change Network address 6. Change Host Gateway 7. Change DHCP range start 8. Change DHCP range end 9. Change DHCP Gateway 10. Change DNS forwarder 11. Change Domain 12. Change NTP sync host 13. Change Timezone 14. Do not configure networking 15. Do not configure firewall 16. Cancel Installation  9  new value for DHCP Gateway 192.168.122.1  Change the NTP server, it is critical you provide an internal NTP server or an external one that is available as OpenStack replies heavily on NTP.  How would you like to proceed?:  1. Proceed with the above values 2. Change Network interface 3. Change IP address 4. Change Network mask 5. Change Network address 6. Change Host Gateway 7. Change DHCP range start 8. Change DHCP range end 9. Change DHCP Gateway 10. Change DNS forwarder 11. Change Domain 12. Change NTP sync host 13. Change Timezone 14. Do not configure networking 15. Do not configure firewall 16. Cancel Installation  12  Enter a list of NTP hosts, separated by commas. First in the list will be the default. clock.redhat.com Configure client authentication SSH public key: '' Root password: '*******************************************'  Please set a default root password for newly provisioned machines. If you choose not to set a password, it will be generated randomly. The password must be a minimum of 8 characters. You can also set a public ssh key which will be deployed to newly provisioned machines.  How would you like to proceed?: 1. Proceed with the above values 2. Change SSH public key 3. Change Root password 4. Toggle Root password visibility 3  new value for root password ********  enter new root password again to confirm ******** Now you should configure installation media which will be used for provisioning. Note that if you don't configure it properly, host provisioning won't work until you configure installation media manually.  Enter RHEL repo path: 1. Set RHEL repo path (http or https URL): http:// 2. Proceed with configuration 3. Skip this step (provisioning won't work) 1 Path: http://192.168.122.99:8120/RHEL7 Enter RHEL repo path: 1. Set RHEL repo path (http or https URL): http://192.168.122.99:8120/RHEL7 2. Proceed with configuration 3. Skip this step (provisioning won't work) 2 Enter your subscription manager credentials:  1. Subscription manager username: myuser 2. Subscription manager password: ******** 3. Comma or Space separated repositories: rhel-7-server-openstack-6.0-rpms rhel-7-server-openstack-6.0-installer-rpms rhel-7-server-rh-common-rpms 4. Subscription manager pool (recommended): mypool 5. Subscription manager proxy hostname: 6. Subscription manager proxy port: 7. Subscription manager proxy username: 8. Subscription manager proxy password: 9. Proceed with configuration 10. Skip this step (provisioning won't subscribe your machines) 9 Starting to seed provisioning data  Use 'base_RedHat_7' hostgroup for provisioning  Success!  * Foreman is running at https://admin.lab.local  Initial credentials are admin / 7wHcE3YZYHSRffmh  * Foreman Proxy is running at https://admin.lab.local:8443  * Puppetmaster is running at port 8140  The full log is at /var/log/rhel-osp-installer/rhel-osp-installer.log Configure Provisioning Media  The admin node needs to install RHEL on nodes it provisions. To do this we need to expose the RHEL install media through HTTP. Below is the process:  mount RHEL 7.1 install media in cdrom #mkdir /RHEL7 #mount -o ro /dev/cdrom /RHEL7 #cp -dpR /RHEL7 /var/www/html/. #chmod -R 755 /var/www/html/RHEL7 #semanage port -a -t http_port_t -p tcp 8120 vi /etc/httpd/conf.d/medium.conf Listen 8120 NameVirtualHost *:8120 &lt;VirtualHost *:8120&gt; DocumentRoot /var/www/html/ ServerName 192.168.122.99 &lt;Directory \"/var/www/html/\"&gt; Options All Indexes FollowSymLinks Order allow,deny Allow from all &lt;/Directory&gt; &lt;/VirtualHost&gt; #iptables -I INPUT 1 -p tcp -m multiport --ports 8120 -m comment --comment \"8120 accept - medium\" -j ACCEPT #iptables-save &gt; /etc/sysconfig/iptables #systemctl restart network.service #systemctl restart httpd Host Discovery  Now that the admin node has been installed and configured we can start our OpenStack deployment. The first step is host discovery. As mentioned OpenStack nodes can run on bare-metal or Virtual Machines. In either case you must configure host for boot via PXE. At minimum the controller node should have three NICs connecting to networks for provisioning / management, external and tenant. The compute node should have minimum two NICs for provisioning / management and tenant. You also generally want an additional network for public API and of course storage. Configure host networking properly before booting.  Once hosts are booted foreman will discover them and they will show up in the RHEL OSP installer under Hosts-&gt;Discovered Hosts.  Create Networks in RHEL OSP installer under Infrastructure-&gt;Subnet. You should have at minimum three subnets (provisioning / management, external and tenant). It is also recommend to separate public and storage.  Below both hosts have been discovered based on MAC address (names can be changed later).    Create an external subnet (you need to do this for every network).    Deploying OpenStack  The first step to deploying OpenStack is to create a new deployment by going to OpenStack Installer-&gt;Deployments. Creating a new deployment is a four step process that can be observed below.          Glance requires a storage back-end, in this case NFS was chosen.    Cinder also requires a storage back-end, in this case NFS was chosen. Note NetApp as being an option. Red Hat partners such as NetApp have started integrating into the RHEL OSP installer.    Once the deployment has been created we can assign hosts. In this configuration we have one controller and compute node (the minimum setup). Below we will assign controller and compute node to deployment. Keep in mind the beauty of the installer allows you to grow the environment and scale-out more controllers or compute nodes as required. I would recommend starting small until at least the kinks are worked out (see troubleshooting section).      Once our hosts have been assigned to a deployment we can update the hosts. In this case I changed the hostname but you can also configure network, domain and realm information for every host.    Finally we are ready for deployment. Under Infrastructure-&gt;Deployments select the deployment and deploy. The progress can be followed from the UI. It typically takes around 2 hours to deploy an OpenStack environment and periodically you will want to check on the progress. Once RHEL is installed you can log into controller and compute hosts to follow progress at more granular level. The install log is located under /var/log/foreman-installer/foreman-installer.log.    Congrats you have deployed an Enterprise OpenStack environment!    Monitoring  One of the great features with the RHEL OSP installer is monitoring. You get dashboards, reports and detailed host information that allow administrators to proactively monitor their deployments. Below are a few dashboards to give you an idea of the capability.    &nbsp;    Neutron Networking  At this point we have a running OpenStack environment. The last step is to setup OpenStack networking. In this example we will use vxlan for tenant tunneling traffic and a flat network for external access via floating IPs.  Configure internal tenant network using vxlan.  neutron net-create internal --provider:network_type vxlan neutron subnet-create internal --name internal_subnet --allocation-pool start=10.10.1.100,end=10.10.1.200 10.10.1.0/24 Configure external flat provider network  neutron net-create external --provider:network_type flat --provider:physical_network physnet-external --router:external=True neutron subnet-create external --name external_subnet --allocation-pool start=192.168.123.100,end=192.168.123.200 --disable-dhcp --gateway 192.168.123.1 192.168.123.0/24 Configure a OpenStack router without HA  neutron router-create prod-router --ha False Set router gateway to our external provider network and add interface to tenant network  neutron router-gateway-set prod-router external neutron router-interface-add prod-router internal_subnet When complete the network topology should look something like diagram below.    Troubleshooting  OpenStack is not for the faint of heart. If you are expecting click-next and grab a coffee you are in for a rude awakening. OpenStack requires a certain level of Open Source and Linux knowledge. You should understand how puppet and foreman work, these skills are recommended for troubleshooting (especially puppet). You also need decent skills in OpenStack networking (openvswitch) in general. Just because the RHEL OSP installer automates everything doesn't mean it is autopilot.  Before we get into troubleshooting lets understand the basic workflow of a RHEL OSP deployment.   provision nodes and install base RHEL. register nodes with subscription manager download packages from appropriate channels configure base networking puppet run on all controller nodes (openstack, openvswitch and pacemaker cluster configured) puppet run on all compute nodes (openstack nova and openvswitch configured)  Here are some common problems that I have run into, hopefully they are helpful.      Error: Could not retrieve catalog from remote server: Error 400 on SERVER: Local ip for ovs agent must be set when tunneling is enabled at  /etc/puppet/environments/production/modules/neutron/manifests/agents/ovs.pp:32 on node controller1.lab.local  Solution: this is a configuration problem. It will occur on the controller node and means that the controller does not have access to a particular network. In this case it was due to only giving the compute node access to the tenant network. This error requires re-configuring network and deployment in the admin node and starting over.   Error: Could not retrieve catalog from remote server: Error 400 on SERVER: Failed when searching for node ostack-ctr1.lab.local: Failed to find ostack-ctr1.lab.local via exec: Execution of '/etc/puppet/node.rb ostack-ctr1.lab.local' returned 1:  Warning: Not using cache on failed catalog  Error: Could not retrieve catalog; skipping run  Solution: This error was caused mainly by a bug fixed in RHEL OSP 6.0.1. Ensure you are running RHEL 6.0.1, RHEL 7.1 and have performed a yum update. This error requires re-install of the the admin node itself to update to latest version.   Error: Deployment completes but br-ex is missing.  Solution: the br-ex is an openvswitch bridge that handles external access for instances via floating IPs. If br-ex is missing you wont have ability to assign floating IPs to instances for external access. This could be a configuration problem in deployment. Edit the controller host and ensure the external network has the correct subnet.    In order to create br-ex openvswitch bridge manually follow these steps   Ensure physical NIC in this case eth1 is configured correctly  #cat /etc/sysconfig/network-scripts/ifcfg-eth1  DEVICE=eth1 DEVICETYPE=ovs TYPE=OVSPort OVS_BRIDGE=br-ex ONBOOT=yes BOOTPROTO=none  Ensure br-ex interface for openvswitch is configured correctly  #cat /etc/sysconfig/network-scripts/ifcfg-br-ex  IPADDR=\"192.168.123.43\" NETMASK=\"255.255.255.0\" GATEWAY=\"192.168.123.1\" ONBOOT=yes PEERROUTES=no NM_CONTROLLED=no DEFROUTE=no PEERDNS=no DEVICE=br-ex DEVICETYPE=ovs OVSBOOTPROTO=\"none\" TYPE=OVSBridge  Create br-ex bridge  #ovs-vsctl add-br br-ex  Add physical NIC to bridge as internal openvswitch port  #ovs-vsctl add-port br-ex eth1  Create a patch port on br-int that patches over to br-ex  #ovs-vsctl add-port br-int br-int-ex -- set Interface br-int-ex type=patch options:peer=phys-br-ex  Create a openvswitch patch port on br-ex that patches over to br-int  #ovs-vsctl add-port br-ex phy-br-ex -- set Interface phy-br-ex type=patch options:peer=br-int-ex      Bridge br-ex         Port \"eth1\"             Interface \"eth1\"         Port phy-br-ex             Interface phy-br-ex                 type: patch                 options: {peer=br-int-ex}         Port br-ex             Interface br-ex                 type: internal  Error: registering host with foreman (https://admin.osp.lab.com) could not send facts to foreman: connection refused - connect(2)  Solution: this error occurs during discovery of hosts by foreman. It indicates a DNS or firewall problem. RHEL OSP 6.0.1 fixed a lot of problems with discovery but it is important to ensure DNS and DHCP are working correctly from admin node.   Error: Openvswitch interfaces show down when issuing \"ip a\" command  Solution: this is not generally a problem. Openvswitch creates interfaces for every bridge for Linux compatibility reasons. They are not otherwise used. The kernel does not recognize these devices correctly and hence sees them as down interfaces.   Error: Puppet throws error no certificate found and waitforcert is disabled  Solution: this problem is fairly uncommon however if it happens you need to regenerate certificates. Below is the process.  On puppet master (admin node)  #puppet cert sign --all #puppet cert clean --all On puppet agent (controller or compute)  #rm -rf /var/lib/puppet/ssl/* vi /etc/puppet/puppet.conf certificate_revocation = false #puppet agent --no-daemonize --server admin.local.lab --onetime --verbose On puppet master (admin node)  #puppet cert --list #puppet cert sign \"controller or compute hostname\" Restarting Puppet  In case puppet fails for any reason you need to run the following command on the controller or compute node where the error occurred in order to restart puppet  #puppet agent -td Summary  Enterprise OpenStack is not just about support it is so much more. As we have seen to operationalize OpenStack in enterprise environments we require automation, provisioning, central management / monitoring, declarative configuration management, Linux experience / expertise and of course world class support from a premier Open Source company like Red Hat. The biggest difference between OpenStack distributions and if you will feature disparity lies within the admin node. OpenStack is OpenStack but how it is deployed and maintained will determine your success. This is just the beginning, if you like the current capabilities you are going to love what is coming down the pipe with Triple-O (OpenStack on OpenStack) and over / under clouds. Things are just getting started!  As always if you have feedback, ideas or suggestions I would love to hear about them.  Happy Stacking!  (c) 2015 Keith Tenzer  ","categories": ["OpenStack"],
        "tags": ["Ceph","Cloud Computing","KVM","OpenStack","Red Hat","RHEL","RHEL OSP"],
        "url": "/openstack/enterprise-openstack-rhel-osp/",
        "teaser": null
      },{
        "title": "Containers at Scale with Kubernetes on OpenStack",
        "excerpt":"Overview  Containers are a burning hot topic right now. Several open source technologies have come together to allow containers to be operated at scale and in the enterprise. In this article I will talk about these technologies and explain how to build container infrastructure at scale with OpenStack. There are three main components that come together to create container-based infrastructure: Red Hat Enterprise Linux Atomic Host (RHEL Atomic), Docker and Google's Kubernetes. RHEL Atomic provides an optimized operating system for running containers. Docker delivers container portability and a packaging standard. Kubernetes adds orchestration and management of Docker containers across a massively scalable cluster of RHEL Atomic hosts.      RHEL Atomic  RHEL Atomic is an optimized container operating system based on Red Hat Enterprise Linux 7 (RHEL 7). The name atomic refers to how updates are managed. RHEL Atomic does not use yum but rather OSTree for managing updates. Software updates are handled atomically across the entire system. Not only this but you can rollback to the systems previous state if the new upgraded state is for some reason not desired. The intention is to reduce risk during upgrades and make the entire process seamless. When we consider the density of containers vs virtual machines to be around 10X, upgrades and maintenance become that much more critical.  RHEL Atomic provides both Docker and Kubernetes. Underneath the hood it leverages SELinux (security), Cgroups (process isolation) and Namespaces (network isolation). It is an Operating System that is optimized to run containers. In addition RHEL Atomic provides enterprise features such as security, isolation, performance and management to the containerized world.  Docker  Docker is often a misused term when referring to containers. Docker is not a container, instead it is a platform for running containers. Docker provides a packaging format, tool-set and all the plumbing needed for running containers within a single host. Docker also provides a hub for sharing Docker images.  Docker images consist of a Base-OS and various layers that allow one to build an application stack (application and its dependencies). Docker images are immutable, you don't update them. Instead you create a new image by adding or making changes to the various layers. This is the future of application deployment and is not only more efficient but magnitudes faster than the traditional approach with virtual machines.  Red Hat is providing a docker repository for certified, tested, secure and supported Docker images similar to how RPMs are currently provided.  All Docker images run in a container and all the containers share the same Linux kernel, RHEL Atomic.  Kubernetes  Kubernetes is an orchestration engine built around Docker. It allows administrators to manage Docker containers at scale across many physical or virtual hosts. Kubernetes has three main components: master, node or minion and pod.  Master  The Kubernetes master is the control plane and provides several services. The scheduler handles placement of pods. It also provides a replication controller that ensures pods are replicated according to policy. The master also maintains the state of the cluster and relies on ETCD which is a distributed key/value store for those capabilities. Finally Restful APIs for performing operations on nodes, pods, replication controllers and services are provided by the Kubernetes master.   Node  The Kubernetes node or minion as it is often referred to runs pods. Placement of pod on a Kubernetes node is as mentioned determined by the scheduler on the Kubernetes master. The Kubernetes node runs several important services: kubelet and kube-proxy. The kubelet is responsible for node level pod management. In addition Kubernetes allows for the creation of services that expose applications to the outside world. The kube-proxy is responsible for managing Kubernetes services within a node. Since pods are meant to be mortal, the idea behind services is providing an abstraction that lives independently of a pod.  Pod  The Kubernetes pod is one or more tightly coupled containers that are scheduled onto the same host. Containers within pods share some resources such as storage and networking. A pod provides a single unit of horizontal scaling and replication across the Kubernetes cluster.  Now that we have a good feel for the components involved it is time to sink our teeth into Kubernetes. First I would like to recognize two colleagues Sebastian Hetze and Scott Collier. I have used their initial work around Kubernetes configurations in this article as my basis.  Configure Kubernetes Nodes in OpenStack  Kubernetes nodes or minions can be deployed and configured automatically on OpenStack. If more compute power is required for our container infrastructure we simply need to deploy additional Kubernetes nodes. OpenStack is the perfect infrastructure for running containers at scale. Below are the steps required to deploy Kubernetes nodes on OpenStack.   Download the RHEL Atomic cloud image (QCOW2) Add RHEL Atomic Cloud Image to Glance in OpenStack Create atomic security group  #neutron security-group-create atomic --description \"RHEL Atomic security group\" #neutron security-group-rule-create atomic --protocol tcp --port-range-min 10250 --port-range-max 10250 --direction ingress --remote-ip-prefix 0.0.0.0/0 #neutron security-group-rule-create atomic --protocol tcp --port-range-min 4001 --port-range-max 4001 --direction egress --remote-ip-prefix 0.0.0.0/0 #neutron security-group-rule-create atomic --protocol tcp --port-range-min 5000 --port-range-max 5000 --direction egress --remote-ip-prefix 0.0.0.0/0 #neutron security-group-rule-create --protocol icmp --direction ingress default  Create user-data to automate deployment using cloud-init  #cloud-config hostname: atomic01.lab.com password: redhat ssh_pwauth: True chpasswd: { expire: False }  ssh_authorized_keys: - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCfxcho9SipUCokS29C+AJNNLcrfpT4xsu9aErax3XSNThWbiJehUDufe86ZO4lqib4dekDEL6d7vBa3WlalzJaq/p/sy1xjYdRNE0vHQCxuWgG+NaL8KcxXDhrUa0UHMW8k8hw9xzOGaRx35LRP9+B0fq/W572XPWwEPRJo8WtSKFiqJZEBkai1IcF0CErj30d0/va9c3EYqkCEWbxuIRL+qoysH+MgFbs1jjjrvfJCLiZZo95MWp4nDrmxYNlmwMIvYrsRZfygeyYPiqVzO51gmGxcVRTbqgG0fSRVRHjUE3E4VfW9wm1qn8+rEc0iQB6ER0f6U/wtEAUmvd/g4Ef ktenzer@ktenzer.muc.csb  write_files: - content: | 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6  192.168.2.15 atomic01.lab.com atomic01 192.168.2.16 atomic02.lab.com atomic02 192.168.2.17 atomic03.lab.com atomic03 192.168.2.14 kubernetes.lab.com kubernetes path: /etc/hosts permissions: '0644' owner: root:root - content: | ### # kubernetes system config # # The following values are used to configure various aspects of all # kubernetes services, including # # kube-apiserver.service # kube-controller-manager.service # kube-scheduler.service # kubelet.service # kube-proxy.service  # Comma seperated list of nodes in the etcd cluster KUBE_ETCD_SERVERS=\"--etcd_servers=http://kubernetes.lab.com:4001\"  # logging to stderr means we get it in the systemd journal KUBE_LOGTOSTDERR=\"--logtostderr=true\"  # journal message level, 0 is debug KUBE_LOG_LEVEL=\"--v=0\"  # Should this cluster be allowed to run privleged docker containers KUBE_ALLOW_PRIV=\"--allow_privileged=false\" path: /etc/kubernetes/config permissions: '0644' owner: root:root - content: | ### # kubernetes kubelet (minion) config  # The address for the info server to serve on (set to 0.0.0.0 or \"\" for all interfaces) KUBELET_ADDRESS=\"--address=0.0.0.0\"  # The port for the info server to serve on KUBELET_PORT=\"--port=10250\"  # You may leave this blank to use the actual hostname KUBELET_HOSTNAME=\"\"  # Add your own! KUBELET_ARGS=--cluster_domain=kubernetes.local --cluster_dns=10.254.0.10 path: /etc/kubernetes/kubelet permissions: '0644' owner: root:root - content: | # /etc/sysconfig/docker OPTIONS='--selinux-enabled' DOCKER_CERT_PATH=/etc/docker ADD_REGISTRY='--add-registry registry.access.redhat.com' ADD_REGISTRY='--add-registry kubernetes.lab.com:5000' # BLOCK_REGISTRY='--block-registry ' # INSECURE_REGISTRY='--insecure-registry' # DOCKER_TMPDIR=/var/tmp # LOGROTATE=false path: /etc/sysconfig/docker permissions: '0644' owner: root:root - content: | # Flanneld configuration options  # etcd url location. Point this to the server where etcd runs FLANNEL_ETCD=\"http://kubernetes.lab.com:4001\"  # etcd config key. This is the configuration key that flannel queries # For address range assignment FLANNEL_ETCD_KEY=\"/flannel/network\"  # Any additional options that you want to pass FLANNEL_OPTIONS=\"eth0\" path: /etc/sysconfig/flanneld permissions: '0644' owner: root:root - content: | {} path: /var/lib/kubelet/auth permissions: '0644' owner: root:root  bootcmd: - systemctl enable kube-proxy - systemctl enable kubelet - systemctl enable flanneld  runcmd: - hostname -f &gt;/etc/hostname - hostname -i &gt;&gt;/etc/issue - echo '' &gt;&gt;/etc/issue  final_message: \"Cloud-init completed and the system is up, after $UPTIME seconds\"  Boot RHEL Atomic instances using the 'nova boot' CLI command  #nova boot --flavor m1.small --poll --image Atomic_7_1 --key-name atomic-key --security-groups prod-base,atomic --user_data user-data-openstack --nic net-id=e3f370ab-b6ac-4788-a739-7f8de8631518 Atomic1  Associate floating-ip to the RHEL Atomic instance  #nova floating-ip-associate Atomic1 192.168.2.15 Of course you will want to update the cloud-init user-data as well as the CLI commands according to your environment. In this example I did not have DNS so I updated the /etc/hosts file directly but this step is not required. I also did not attach a Red Hat subscription something you would probably want to do using the 'runcmd' option in cloud-init.  Configure Kubernetes Master  Once Kubernetes nodes have been deployed we can configure the Kubernetes master. The Kubernetes master runs Kubernetes, Docker and ETCD services. In addition an overlay network is required. There are many options to create an overlay network, in this case we have chosen to use flannel to provide those capabilities. Finally for the base OS, a minimum install of a current RHEL-7 release is required.   Register host with subscription-manager  #subscription-manager register attach --pool=&lt;pool id&gt; #subscription-manager repos --disable=* #subscription-manager repos --enable=rhel-7-server-rpms #subscription-manager repos --enable=rhel-7-server-extras-rpms #subscription-manager repos --enable=rhel-7-server-optional-rpms #yum -y update  Install required packages  #yum -y install docker docker-registry kubernetes flannel  Disable firewall  #systemctl stop firewalld #systemctl disable firewalld  Enable required services  #for SERVICES in docker.service docker-registry etcd kube-apiserver kube-controller-manager kube-scheduler flanneld; do  systemctl enable $SERVICES done  Configure Docker  #vi /etc/sysconfig/docker INSECURE_REGISTRY='--insecure-registry kubernetes.lab.com:5000'   Configure Kubernetes  #vi /etc/kubernetes/apiserver and set KUBE_API_ADDRESS=\"--address=0.0.0.0\" KUBE_MASTER=\"--master=http://kubernetes.lab.com:8080\" #vi /etc/kubernetes/config and set KUBE_ETCD_SERVERS=\"--etcd_servers=http://kubernetes.lab.com:4001\" #vi /etc/kubernets/controller-manager and set KUBELET_ADDRESSES=\"--machines=atomic01.lab.com,atomic02.lab.com,atomic03.lab.com\"  Configure Flannel  #vi /etc/sysconfig/flanneld and set FLANNEL_ETCD=\"http://kubernetes.lab.com:4001\" FLANNEL_ETCD_KEY=\"/flannel/network\" FLANNEL_OPTIONS=\"eth0\"  Start ETCD  #systemctl start etcd  Configure Flannel overlay network  #vi /root/flannel-config.json {  \"Network\": \"10.100.0.0/16\",  \"SubnetLen\": 24,  \"SubnetMin\": \"10.100.50.0\",  \"SubnetMax\": \"10.100.199.0\",  \"Backend\": {  \"Type\": \"vxlan\",  \"VNI\": 1  } } curl -L http://kubernetes.lab.com:4001/v2/keys/flannel/network/config -XPUT --data-urlencode value@/root/flannel-config.json  Load Docker Images  #systemctl start docker #systemctl start docker-registry #for IMAGES in rhel6 rhel7 fedora/apache; do  docker pull $IMAGES  docker tag $IMAGES kubernetes.lab.com:5000/$IMAGES  docker push kubernetes.lab.com:5000/$IMAGES done  Reboot host  systemctl reboot Container Administration using Kubernetes  Kubernetes provides a CLI and a Restful API for management. Currently there is no GUI. In a future article I will go into detail about using the API in order to build your own UI or integrate Kubernetes in existing dashboards. For the purpose of this article we will focus on kubectl, the Kubernetes CLI.  Deploy an Application  In this example we will deploy an Apache web server pod. Before deploying a pod we must ensure that Kubernetes nodes (minions) are ready.  [root@kubernete ~]# kubectl get minions NAME             LABELS STATUS atomic01.lab.com &lt;none&gt; Ready atomic02.lab.com &lt;none&gt; Ready atomic03.lab.com &lt;none&gt; Ready Next we need to create a JSON file for deploying a pod. The kubectl command uses JSON as input to make configuration updates and changes.  [root@kube-master ~]# vi apache-pod.json [code language=\"java\"]  {   &quot;apiVersion&quot;: &quot;v1beta1&quot;,   &quot;desiredState&quot;: {      &quot;manifest&quot;: {         &quot;containers&quot;: [         {            &quot;image&quot;: &quot;fedora/apache&quot;,            &quot;name&quot;: &quot;my-fedora-apache&quot;,            &quot;ports&quot;: [            {               &quot;containerPort&quot;: 80,               &quot;hostPort&quot;:80,               &quot;protocol&quot;: &quot;TCP&quot;            }         ]      }      ],         &quot;id&quot;: &quot;apache&quot;,         &quot;restartPolicy&quot;: {         &quot;always&quot;: {}      },         &quot;version&quot;: &quot;v1beta1&quot;,         &quot;volumes&quot;: null      }   },   &quot;id&quot;: &quot;apache&quot;,   &quot;kind&quot;: &quot;Pod&quot;,   &quot;labels&quot;: {      &quot;name&quot;: &quot;apache&quot;   },   &quot;namespace&quot;: &quot;default&quot;  }  [/code]  &nbsp;  [root@kube-master ~]# kubectl create -f apache-pod.json We can now get the status of our newly created Apache pod.  [root@kubernetes ~]# kubectl get pods POD    IP           CONTAINER(S)     IMAGE(S)      HOST              LABELS      STATUS apache 10.100.119.6 my-fedora-apache fedora/apache atomic02.lab.com/ name=apache Running Notice that the pod is running on atomic02.lab.com. The Kubernetes scheduler takes care of scheduling the pod on a node.   Create Services  In Kubernetes services are used to provide external access to an application running in a pod. The idea is that since pods are mortal and transient in nature a service should provide abstraction so applications do not need to understand underlying pod or containers infrastructure. Services use the kube-proxy to access applications from any Kubernetes node configured as public IPs in the service itself. In the example below we are creating a service that will be available from all three of the Kubernetes nodes atomic01.lab.com, atomic02.lab.com and atomic03.lab.com. The pod is running on atomic02.lab.com. Similar to pods, services also requirec a JSON file as input to kubectl.  [root@kubernetes ~]# vi apache-service.json [code language=\"java\"]  {     &quot;apiVersion&quot;: &quot;v1beta1&quot;,     &quot;containerPort&quot;: 80,     &quot;id&quot;: &quot;frontend&quot;,     &quot;kind&quot;: &quot;Service&quot;,     &quot;labels&quot;: {        &quot;name&quot;: &quot;frontend&quot;     },     &quot;port&quot;: 80,     &quot;publicIPs&quot;: [     &quot;192.168.2.15&quot;,&quot;192.168.2.16&quot;,&quot;192.168.2.17&quot;     ],     &quot;selector&quot;: {        &quot;name&quot;: &quot;apache&quot;     }  }  [/code]  [root@kube-master ~]# kubectl create -f apache-service.json We can now get the status of our newly created apache-frontend service.  [root@kubernetes ~]# kubectl get services NAME            LABELS               SELECTOR    IP            PORT apache-frontend name=apache-frontend name=apache 10.254.94.252 80 As one would expect, we can access our Apache pod externally through any of our three Kubernetes nodes.  [root@kubernetes ~]# curl http://atomic01.bigred.com Apache Creating Replication Controllers  So far we have seen how to create a pod containing one or more containers and build a service to expose the application externally. If we want to scale our application horizontally however we need to create a replication controller. In Kubernetes replication controllers are pods that have a replication policy. Kubernetes will create multiple pods across the cluster and a pod is our base unit of scaling. In the example below we will create a replication controller for our Apache web server that will ensure three replicas. The same service we already created can be used but this time an Apache pod will be running on each Kubernetes node. In our previous example we only had one Apache web server on atomic02.lab.com and though we could access it through any node it was done through the kube-proxy.  [root@kubernetes ~]# vi apache-replication-controller.json [code language=\"java\"]  {     &quot;apiVersion&quot;: &quot;v1beta1&quot;,     &quot;desiredState&quot;: {        &quot;podTemplate&quot;: {           &quot;desiredState&quot;: {              &quot;manifest&quot;: {              &quot;containers&quot;: [              {                 &quot;image&quot;: &quot;fedora/apache&quot;,                 &quot;name&quot;: &quot;my-fedora-apache&quot;,                 &quot;ports&quot;: [                 {                    &quot;containerPort&quot;: 80,                    &quot;hostPort&quot;: 80,                    &quot;protocol&quot;: &quot;TCP&quot;                 }                 ]              }              ],              &quot;id&quot;: &quot;apache&quot;,              &quot;restartPolicy&quot;: {                 &quot;always&quot;: {}              },              &quot;version&quot;: &quot;v1beta1&quot;,              &quot;volumes&quot;: null              }           },           &quot;labels&quot;: {              &quot;name&quot;: &quot;apache&quot;           }        },        &quot;replicaSelector&quot;: {           &quot;name&quot;: &quot;apache&quot;     },     &quot;replicas&quot;: 3     },     &quot;id&quot;: &quot;apache-controller&quot;,     &quot;kind&quot;: &quot;ReplicationController&quot;,     &quot;labels&quot;: {     &quot;name&quot;: &quot;apache&quot;     }  }  [/code]  [root@kube-master ~]# kubectl create -f apache-replication-controller.json We can now get the status of our newly created Apache replication controller.  [root@kubernetes ~]# kubectl get replicationcontrollers CONTROLLER        CONTAINER(S)     IMAGE(S)      SELECTOR    REPLICAS apache-controller my-fedora-apache fedora/apache name=apache 3 We can also see that the replication controller created three pods as expected.  [root@kubernetes ~]# kubectl get pods POD                                  IP           CONTAINER(S)     IMAGE(S)      HOST                 LABELS      STATUS fb9936f3-e21d-11e4-ad6e-000c295b1de9 10.100.119.6 my-fedora-apache fedora/apache atomic03.bigred.com/ name=apache Running fb9acf1a-e21d-11e4-ad6e-000c295b1de9 10.100.65.6  my-fedora-apache fedora/apache atomic02.bigred.com/ name=apache Running fb97a111-e21d-11e4-ad6e-000c295b1de9 10.100.147.6 my-fedora-apache fedora/apache atomic01.bigred.com/ name=apache Running Summary  In this article we discussed the different components required to run application containers at scale: RHEL Atomic, Docker and Kubernetes. We also saw how to deploy Kubernetes RHEL Atomic nodes on OpenStack. Having scalable application containers means little if your infrastructure underneath cannot scale and that is why OpenStack should be key to any enterprise container strategy. Finally we went into a lot of detail on how to configure Kubernetes pods, services and replication controllers. Running application containers at scale in the enterprise is a lot more than just Docker. It has only been until very recently that these best-of-breed open source technologies have come together and allowed such wonderful possibilities. This is a very exciting time, containers will change everything about how we deploy, run and manage our applications. Hopefully you found this article interesting and useful. If you have any feedback I would really like to hear it, please share.  Happy Containerizing!  (c) 2015 Keith Tenzer  ","categories": ["Containers"],
        "tags": ["Docker","Kubernetes","Linux","LXC","OpenStack","RHEL Atomic"],
        "url": "/containers/containers-at-scale-with-kubernetes-on-openstack/",
        "teaser": null
      },{
        "title": "Kubernetes Container Orchestration through Java APIs",
        "excerpt":"Overview  In this article we will look at how to drive Kubernetes using Java APIs. More general information about Kubernetes and containers can be found here. Kubernetes itself offers a RESTful API but of course most prefer to use a client that is already handling low-level HTTP protocol operations.   It turns out Fabric8 is providing an integration and management platform above kubernetes and docker. In addition to integrating container orchestration through Kubernetes, Fabric8 also integrates with OpenShift v3 and Jube. OpenShift v3 exposes Kubernetes extensions such as app templates and builds. These Kubernetes extensions allow the container to be bundled with the application code and thus greatly increase application deployment cycles. Jube is a pure Java implementation of Kubernetes to provide containers to non-Linux world. In this article we will just focus on Kubernetes using the Fabric8 Java API.  Setup  The easiest way to get access to the Fabric8 Kubernetes Java API is through maven. Both Kubernetes and Fabric8 are changing fast so make sure you update and stay current with versions. The below dependency can be added to your pom.xml file.  [code language=\"xml\"]  &lt;dependency&gt;     &lt;groupId&gt;io.fabric8&lt;/groupId&gt;     &lt;artifactId&gt;kubernetes-api&lt;/artifactId&gt;     &lt;version&gt;2.0.37&lt;/version&gt;  &lt;/dependency&gt;  [/code]  After updating the pom.xml you can run maven to add the new dependency to the local maven repository.  mvn clean compile Connection  In order to communicate with Kubernetes through the Fabric8 Java API a connection must be established. The Kubernetes API support HTTPS through TLS certificates but in this example we are just using HTTP and no authentication. The ConnectionExample class requires only IP and Port in order to return a kube connection object. By default unless specified the Kubernetes API will be read-only. In oder to get access to the read-write API the following environment parameters must be exported:  export KUBERNETES_SERVICE_HOST=192.168.2.14 export KUBERNETES_SERVICE_PORT=8080 [code language=\"java\"]  package examples;  import io.fabric8.kubernetes.api.KubernetesClient;  import io.fabric8.kubernetes.api.KubernetesFactory;  public class ConnectionExample {     private String ip;     private String port;     public ConnectionExample(String ip, String port) {        this.ip= ip;        this.port = port;     }     public KubernetesClient getConnection() {        final String URI = &quot;http://&quot; + ip+ &quot;:&quot; + port;        final KubernetesClient kubernetes = new KubernetesClient(new KubernetesFactory(URI));        return kubernetes;     }  }  [/code]  Once we have a Kubernetes connection object we can manage pods, replication controllers and services. We can perform typical RESTFul operations such as get, post, update and delete.  Pods  A pod is the most granular unit of management in Kubernetes and contains one or more containers that share the same resources. The main operations that can be performed are listing, creating and deleting pods.  Listing Pods  [code language=\"java\"]   public List&lt;KubePodModel&gt; getKubePods() {      List&lt;KubePodModel&gt; kubePodList = new ArrayList&lt;KubePodModel&gt;();      for (Pod pod : kubernetes.getPods().getItems()) {         KubePodModel model = new KubePodModel();         model.setId(pod.getId());         model.setApiVersion(pod.getApiVersion());         model.setCreationTime(pod.getCreationTimestamp());         model.setNamespace(pod.getNamespace());         model.setPodHostName(pod.getCurrentState().getHost());         model.setPodIp(pod.getCurrentState().getPodIP());         model.setStatus(pod.getCurrentState().getStatus());         kubePodList.add(podModel);      }      return kubePodModelList;   }  [/code]  Creating Pods  [code language=\"java\"]  public void createKubePod(String id, String namespace, List&lt;String&gt; containerPorts, String image, Map&lt;String, String&gt; labels) {     Pod pod = new Pod();     pod.setId(id);     pod.setLabels(labels);     PodState desiredState = new PodState();     ContainerManifest manifest = new ContainerManifest();     Container manifestContainer = new Container();     manifestContainer.setName(podId);     manifestContainer.setImage(image);     List&lt;Port&gt; ports = new ArrayList&lt;Port&gt;();     for (String containerInputPort : containerPorts) {        CharSequence inputStr = containerInputPort;        String patternStr = &quot;(\\\\S+):(\\\\S+)&quot;;        Pattern pattern = Pattern.compile(patternStr);        Matcher matcher = pattern.matcher(inputStr);        boolean matchFound = matcher.find();        if (matchFound) {           int contanierPort = Integer.valueOf(matcher.group(1));           String containerProto = matcher.group(2);           Port port = new Port();           port.setContainerPort(contanierPort);           port.setProtocol(containerProto);           ports.add(port);       }     }     manifestForContainer.setPorts(ports);     List&lt;Container&gt; containers = new ArrayList&lt;&gt;();     containers.add(manifestForContainer);     manifest.setContainers(containers);     desiredState.setManifest(manifest);     pod.setDesiredState(desiredState);     kubernetes.createPod(pod, namespace);  }  [/code]  Deleting Pods  [code language=\"java\"]  public void deleteKubePod(String id, String namespace) {     kubernetes.deletePod(id, namespace);  }  [/code]  Replication Controllers  A replication controller is a pod that has N replicas. The replication controller ensures the number of pod replicas defined also exists. The replication controller will automatically manage pod deployments.The main operations that can be performed are listing, creating and deleting replication controllers.  Listing Replication Controllers  [code language=\"java\"]  public List&lt;ControllerModel&gt; getKubeReplicationControllers() {     List&lt;ControllerModel&gt; controllerList = new ArrayList&lt;ReplicationControllerModel&gt;();     for (ReplicationController controller : kubernetes.getReplicationControllers().getItems()) {        ControllerModel model = new ControllerModel();        model.setId(controller.getId());        model.setApiVersion(controller.getApiVersion());        model.setCreationTime(controller.getCreationTimestamp());        model.setReplicas(controller.getCurrentState().getReplicas());        controllerList.add(model);     }     return controllerList;  }  [/code]  Creating Replication Controllers  [code language=\"java\"]  public void createKubeReplicationController(String id, String namespace, int replicas, Map&lt;String, String&gt; controllerLabels,     Map&lt;String, String&gt; selectorLabels, String podId, String image, List&lt;String&gt; containerPorts, Map&lt;String, String&gt; podLabels, String manifestId) {     ReplicationController controller = new ReplicationController();     controller.setId(id);     controller.setNamespace(namespace);     controller.setLabels(controllerLabels);     PodState podState = new PodState();     ReplicationControllerState repState = new ReplicationControllerState();     repState.setReplicas(replicas);     repState.setReplicaSelector(selectorLabels);     PodTemplate podTmpl = new PodTemplate();     podTmpl.setLabels(podLabels);     ContainerManifest manifest = new ContainerManifest();     manifest.setId(manifestId);     Container manifestContainer = new Container();     manifestContainer.setName(podId);     manifestContainer.setImage(image);     List&lt;Port&gt; ports = new ArrayList&lt;Port&gt;();     for (String containerInputPort : containerPorts) {        CharSequence inputStr = containerInputPort;        String patternStr = &quot;(\\\\S+):(\\\\S+)&quot;;        Pattern pattern = Pattern.compile(patternStr);        Matcher matcher = pattern.matcher(inputStr);        boolean matchFound = matcher.find();        if (matchFound) {           int contanierPort = Integer.valueOf(matcher.group(1));           String containerProtocol = matcher.group(2);           Port port = new Port();           port.setContainerPort(contanierPort);           port.setProtocol(containerProtocol);           ports.add(port);        }     }     manifestContainer.setPorts(ports);     List&lt;Container&gt; containers = new ArrayList&lt;&gt;();     containers.add(manifestContainer);     manifest.setContainers(containers);     podState.setManifest(manifest);     podTmpl.setDesiredState(podState);     repState.setPodTemplate(podTmpl);     controller.setDesiredState(repState);     kubernetes.createReplicationController(controller, namespace);  }  [/code]  Deleting Replication Controllers  [code language=\"java\"]  public void deleteReplicationController(String id, String namespace) {     kubernetes.deleteReplicationController(id, namespace);  }  [/code]  Services  A service exposes an application running in a pod externally using the Kubernetes proxy. It essentially maps public IPs / Ports on Kubernetes Minions (nodes) to internal container IPs / Ports. The main operations that can be performed are listing, creating and deleting services.  Listing Services  [code language=\"java\"]  public List&lt;ServiceModel&gt; getKubeServices() {     List&lt;ServiceModel&gt; serviceList = new ArrayList&lt;ServiceModel&gt;();     for (Service service : kubernetes.getServices().getItems()) {         ServiceModel model = new ServiceModel();         model.setId(service.getId());         model.setApiVersion(service.getApiVersion());         model.setCreationTime(service.getCreationTimestamp());         model.setIp(service.getPortalIP());         model.setPort(service.getPort());         model.setProtocol(service.getProtocol());         serviceList.add(serviceModel);     }     return serviceList;  }  [/code]  Creating Services  [code language=\"java\"]  public void createKubeService(String id, String namespace, int containerPort, int port, List&lt;String&gt; publicIps,     Map&lt;String, String&gt; labels, Map&lt;String, String&gt; selectors) {     Service service = new Service();     IntOrString containerPortObj = new IntOrString();     containerPortPojo.setIntVal(containerPort);     service.setId(id);     service.setContainerPort(containerPortObj);     service.setPort(port);     service.setPublicIPs(publicIps);     service.setLabels(labels);     service.setSelector(selectors);     kubernetes.createService(service, namespace);  }  [/code]  Deleting Services  [code language=\"java\"]  public void deleteService(String id, String namespace) {     kubernetes.deleteService(id, namespace);  }  [/code]  Summary  Kubernetes and Linux containers are rapidly developing and emerging technologies. We have seen how to easily orchestrate containers in a Kubernetes environment using the Fabric8 Java APIs. Container orchestration is about performing operations on various Kubernetes components such as pods, replication controllers and services. Using the APIs you can easily build Kubernetes functionality into any Java applications. I hope you found this article useful. If you have examples of orchestrating Kubernetes or containers through APIs please share!  Happy Containerizing!  (c) 2015 Keith Tenzer  ","categories": ["Containers"],
        "tags": ["API","Fabric8","Java","Kubernetes","Linux","LXC"],
        "url": "/containers/kubernetes-container-orchestration-through-java-apis/",
        "teaser": null
      },{
        "title": "Application Containers: A Practical HowTo Guide",
        "excerpt":"Overview  We have by now all heard plenty about Linux containers and for good reason. Containers change the way applications are operated and allow us to deploy applications at unprecedented speeds. Containers pick up where Virtual Machines left off, at the application layer. In this article we will focus on the journey to a container driven world and explore the phases along the way.   Container Rules  Before beginning our journey it is important to understand basic container rules:   A container should run one and only one application process Containers are immutable, if something needs to be changed the container is thrown away and re-created Containers are insulated from one another but not isolated in same way as VIrtual Machines Containers share same Linux Kernel  Application Discovery  The first phase inolves indentifying applications. Not all applications are ideal to run in containers. Similar to cloud infrastructure such as OpenStack, containers require a certain application design. An application should exhibit the following behavior:   Application functionality should be broken into components  components should be standalone services and have no dependencies on any other components all services should communicate with one another using external RESTful APIs   Application state change should be handled by using message buses or a distributed key/value store Application must scale horizontally not vertically Heavy components like databases should be operated on bare-metal or in Virtual Machines that can scale vertically  For the purpose of this article I chose to containerize an application that displays these characteristics. Integra is an integration, automation and orchestration platform. It exposes application capabilities through providers that provide standalone micro-services with a RESTful frontend. The Integra reactor is the brain and allows automation architects to build workflows from capabilities exposed by providers. There are providers for applicaitons, databases, hypervisors, storage systems and much more. The idea behind Integra is automate everything with no compromises. Integra sees no difference between backup, provisioning or other common tasks. Everything is a workflow that can be automated using a standard toolset.    Since Integra is composed of many services it is important to place each service in its own container. This means the reactor and every provider, even the CLI get its own container. The container will need to provide all dependencies in order to run the application including exposing ports. Each service or component of an application should use a different unique port. For Docker alone it doesn't matter as much but once we get into Kubernetes and pods this becomes very important since containers within a pod share same IP address.  Running Application in Container  Running applications in containers is not that much different than outside of a container. A container can run one command so typically we would create a small start script to lauch the application. Below is the start script run-integra.sh I am using for the Integra reactor:  #!/bin/sh  pgrep -f \"rest-1.0.2-uber.jar\" | awk '{system(\"kill \" $1)}'  exec /usr/bin/java -jar /integra/rest-1.0.2-uber.jar Before building our application container it is important to test and ensure things are working. Below are the commands I used to test the Integra reactor.  # docker pull debian # docker run -i -t debian /bin/bash At this point we are inside the container running debian as the base OS.  root@b4a37b0d8040:/# apt-get update root@b4a37b0d8040:/# apt-get install -y openjdk-7-jre We have now installed the application dependencies and can test the Integra reactor inside the container. Next we need to copy the application JAR to the container using the container id (long format).  # docker ps CONTAINER     ID IMAGE         COMMAND       CREATED         STATUS b4a37b0d8040  debian:latest    \"/bin/bash\"   5 minutes ago   Up 5 minutes # docker inspect -f b4a37b0d8040 b4a37b0d8040a0d624b7edc264425693a7e0e50444f72d09a54209ff6461b377 cp rest-1.0.2-uber.jar /var/lib/docker/devicemapper/mnt/b4a37b0d8040a0d624b7edc264425693a7e0e50444f72d09a54209ff6461b377/rootfs Now that we have copied the JAR file from our host OS to the container we can run the Integra reactor and ensure it works.  root@b4a37b0d8040:/# java -jar rest-1.0.2-uber.jar Finally we are ready to build our Docker application container!  Building Docker Image  Docker provides a standard for packaging containers. While container technology has been around for a long time in both Unix and Linux the tooling and portability that Docker provides is certainly game-changing. Docker uses a Dockerfile to define the container image. A docker image is a grouping of layers. In our example we have essentially three layers: the base OS (Debian), required dependencies (Java) and our application (JAR). Besides providing software layering a Dockerfile also enables us to expose application ports, run-time environment parameters and a tooling for executing standard OS commands. Below is the Dockerfile used to build the Integra Reactor.  # vi Dockerfile # Integra Reactor # VERSION 0.0.1 FROM debian MAINTAINER Keith Tenzer &lt;maintainer@domain.com&gt;  LABEL Description=\"This image is used to start the Integra Rest Server\" Vendor=\"Emitrom\" Version=\"1.02\" RUN apt-get update &amp;&amp; apt-get install -y openjdk-7-jre  RUN mkdir /integra  COPY rest/* /integra/  COPY run-integra.sh /  RUN chmod -R 755 /integra  RUN chmod 755 /run-integra.sh  ENV JAVA_OPTS=\"-Xms512m -Xmx1152m -XX:MaxPermSize=256m -XX:MaxNewSize=256m\"  EXPOSE 8080 8443  CMD [\"/run-integra.sh\"] Once we are ready we can build our docker image. The docker build command will create the image and make it available in our local docker repository.  # docker build -t integra/reactor:v1.0.2 . # docker images REPOSITORY          TAG       IMAGE ID        CREATED         VIRTUAL SIZE integra/reactor     v1.0.2    233be1b0b05f    16 hours ago    523.4 MB Sharing Docker Images  Docker provides a public registry called Docker Hub and in addition allows us to run our own private registry in order to share trusted images internally. Once the docker image is created it can be shared with a registry using the the docker push command. In order to allow Docker to communicate with an insecure private registry the docker daemon must be started with the --insecure-registry option as follows:  docker -d --insecure-registry kubernetes.lab.com:5000 &amp; Tags are used to allow for versioning of Docker images. A special tag called latest is used as default whenever a tag is not specified. For example above we issued the command 'docker run -i -t debian /bin/bash'. Since a tag was not specified docker run used latest. The most current version of a Docker image should also be taged with 'latest'.  docker tag emitrom/server:v1.02 kubernetes.lab.com:5000/emitrom/integra-server:v1.02 docker tag emitrom/integra-server:v1.02 kubernetes.lab.com:5000/emitrom/integra-server:latest Pusing Docker Image to Private Registry  docker push kubernetes.lab.com:5000/integra/reactor Pusing Docker Image to Docker Hub  # docker push integra/reactor Please login prior to push: Username: integra Password: ******* Email: maintainer@domain.com To see all images for the user integra we can go directly to Docker Hub or run a command.    # docker search integra |grep \"^integra/\" Running Docker Images  We have built our Docker image for the Integra reactor and shared it in Docker Hub or a private registry. At this point anyone can run the application on any system running Docker using two simple commands.  # docker pull integra/reactor # docker run -i -t -d integra/reactor This is the power of containers and portability of Docker. Think of how you would normally deploy such an application without containers? Think of the portability, your app can run on any system running Docker, anywhere.  Running Docker Images in Kubernetes  Running vanilla Docker is great for development or test environments but assuming we want to run this application in a production environment there are a few things missing. First we don't have any mechanism to orchestrate or handle deploying our application on multiple hosts. Next the target application contains many services, each being it's own container and connecting them together is quite a bit of work. There is no abstraction around services, a container may be temporary but an application service is certainly not. Finally we have no management around reliability or horizontal scaling. These are the gaps that Google's Kubernetes fills. For more information on setting up Kubernetes read this article. Kubernetes creates an abstraction around containers called a pod. A pod contains one or more tightly coupled containers. In this case if we want a holistic deployment of Integra and all its providers, reactor and CLI we can encapsulate the entire application in a Kubernetes pod. Once we have a pod independent services and replication policies can be created, Kubernetes handles all this automatically. Below is an example of a multiple container pod configuration.  # vi integra-all.json [code language=\"java\"]  {     &quot;apiVersion&quot;: &quot;v1beta1&quot;,     &quot;desiredState&quot;: {        &quot;manifest&quot;: {        &quot;containers&quot;: [        {           &quot;image&quot;: &quot;integra/reactor&quot;,           &quot;name&quot;: &quot;integra-reactor&quot;,           &quot;ports&quot;: [           {              &quot;containerPort&quot;: 8080,              &quot;hostPort&quot;: 8080,              &quot;protocol&quot;: &quot;TCP&quot;           }           ]        },        {           &quot;image&quot;: &quot;integra/aws-provider&quot;,           &quot;name&quot;: &quot;integra-aws&quot;,           &quot;ports&quot;: [           {              &quot;containerPort&quot;: 9771,              &quot;hostPort&quot;: 9771,              &quot;protocol&quot;: &quot;TCP&quot;           }           ]        },        {           &quot;image&quot;: &quot;integra/azure-provider&quot;,           &quot;name&quot;: &quot;integra-azure&quot;,           &quot;ports&quot;: [           {              &quot;containerPort&quot;: 9772,              &quot;hostPort&quot;: 9772,              &quot;protocol&quot;: &quot;TCP&quot;           }           ]        }        ],        &quot;id&quot;: &quot;integra-all&quot;,        &quot;restartPolicy&quot;: {        &quot;always&quot;: {}     },     &quot;version&quot;: &quot;v1beta1&quot;,     &quot;volumes&quot;: null     }   },   &quot;id&quot;: &quot;integra-all&quot;,   &quot;kind&quot;: &quot;Pod&quot;,   &quot;labels&quot;: {   &quot;name&quot;: &quot;integra-all&quot;   },   &quot;namespace&quot;: &quot;default&quot;  }  [/code]  We can issue the following command in Kubernetes to deploy our pod:  kubectl create -f integra-reactor-svc.json # kubectl get pods POD            IP             CONTAINER(S)      IMAGE(S)                         HOST                 LABELS             STATUS integra-all    10.100.77.3    integra-reactor   integra/reactor:latest           atomic01.lab.com/    name=integra-all   Running                               integra-aws       integra/aws-provider:latest                               integra-azure     integra/azure-provider:latest Next we need to expose the Integra reactor running on port 8080 as a service. The nice thing about pods is that all containers within pod communicate using the same IP address. This makes it much simpler to configure communication between related application services.  # vi integra-all-svc.json [code language=\"java\"]  {     &quot;apiVersion&quot;: &quot;v1beta1&quot;,     &quot;containerPort&quot;: 8080,     &quot;id&quot;: &quot;integra-reactor-svc&quot;,     &quot;kind&quot;: &quot;Service&quot;,     &quot;labels&quot;: {        &quot;name&quot;: &quot;integra-reactor-svc&quot;     },     &quot;port&quot;: 8080,     &quot;publicIPs&quot;: [        &quot;10.10.1.114&quot;,&quot;10.10.1.115&quot;,&quot;10.10.1.116&quot;     ],     &quot;selector&quot;: {        &quot;name&quot;: &quot;integra-all&quot;     }   }  [/code]  We can create the service and we are done!   kubectl create -f integra-reactor-svc.json Finally in order to test we can make a simple HTTP request to the Integra reactor.  curl -u admin:integra http://10.10.1.114:8080/rest  [code language=\"java\"]  &lt;appInfo&gt;     &lt;name&gt;Integra&lt;/name&gt;     &lt;version&gt;1.0.2&lt;/version&gt;     &lt;buildTimestamp&gt;20150420-2208&lt;/buildTimestamp&gt;  &lt;/appInfo&gt;  [/code]   Summary  In this article we explored how to approach and prepare applications running inside containers using the Docker platform. We have seen how to build docker images and use a docker registry to share images. Finally we have observed how to run containers inside Docker and Kubernetes. It doesn't stop there though, the story gets even better. In a future article I will discuss the need for PaaS and how PaaS can leverage these underlying technologies to provider even more value. In today's world IT is all about speed and innovation, if you don't have speed you cannot innovate, if you cannot innovate you will perish.  Happy Containerizing!  (c) 2015 Keith Tenzer   ","categories": ["Containers"],
        "tags": ["Docker","Kubernetes","Linux","LXC"],
        "url": "/containers/application-containers-a-practical-howto-guide/",
        "teaser": null
      },{
        "title": "Application Containers: Kubernetes and Docker from Scratch",
        "excerpt":"Overview  In this article we will look at how to configure a Kubernetes cluster using the Docker container format on CentOS or RHEL 7.1. For a detailed overview on Kubernetes and Docker take a look at this article. A Kubernetes cluster is comprised of a master and N nodes. The master acts as a control plane for the cluster and in this case also exposes a private Docker registry. A Kubernetes node runs Docker container images.    Requirements  In this article we will setup a master and one node. At minimum two hosts will be required. Kubernetes also has the following networking requirements:   all containers can communicate with all other containers without NAT all nodes can communicate with all containers (and vice-versa) without NAT the IP that a container sees itself as is the same IP that others see it as  In order to meet these networking requirements an overlay network  must be configured. Two commonly used overlay networks for Kuberentes are Flannel and Open vSwitch. In this article we will use Flannel.  Setup Kubernetes Master  Creating a Kubernetes master means configuring Kubernetes, Etcd, Flannel, Docker and a private Docker registry. The private Docker registry is used by the nodes in order to pull images.  Install packages and enable services  #yum update -y #yum install -y docker docker-registry etcd kubernetes flannel #for SERVICES in docker.service docker-registry etcd kube-apiserver kube-controller-manager kube-scheduler flanneld     do systemctl enable $SERVICES  done  Configure Private Docker Registry  #vi /etc/sysconfig/docker INSECURE_REGISTRY='--insecure-registry kube-master.lab.com:5000'  Configure Kuberentes API Server  #vi /etc/kubernetes/apiserver KUBE_API_ADDRESS=\"--address=0.0.0.0\" KUBE_API_PORT=\"--port=8080\" KUBE_ETCD_SERVERS=\"--etcd_servers=http://kube-master.lab.com:4001\"  Configure Kubernetes Master  #vi /etc/kubernetes/config KUBE_MASTER=\"--master=http://kube-master.lab.com:8080\"  Configure Kubernetes Nodes (kubelets)  #vi /etc/kubernetes/controller-manager KUBELET_ADDRESSES=\"--machines=kube-node1.lab.com\" Configure ETCD  #vi /etc/etcd/etcd.conf ETCD_LISTEN_PEER_URLS=\"http://localhost:2380,http://localhost:7001\" ETCD_LISTEN_CLIENT_URLS=\"http://0.0.0.0:4001,http://0.0.0.0:2379\" #systemctl start etcd Configure Overlay Network using Flannel  #vi /etc/sysconfig/flanneld FLANNEL_ETCD=\"http://kube-master.lab.com:4001\" FLANNEL_ETCD_KEY=\"/flannel/network\" FLANNEL_OPTIONS=\"eth0\" #vi /root/flannel-config.json {    \"Network\": \"10.100.0.0/16\",    \"SubnetLen\": 24,    \"SubnetMin\": \"10.100.50.0\",    \"SubnetMax\": \"10.100.199.0\",    \"Backend\": {    \"Type\": \"vxlan\",    \"VNI\": 1    }  } curl -L http://kube-master.lab.com:4001/v2/keys/flannel/network/config -XPUT --data-urlencode value@flannel-config.json Download Docker Images to private registry  #systemctl start docker #systemctl start docker-registry #for IMAGE in rhel6 rhel7  kubernetes/kube2sky:1.1 kubernetes/pause:go    do docker pull $IMAGE    docker tag $IMAGE kube-master.lab.com:5000/$IMAGE    docker push kube-master.lab.com:5000/$IMAGES done systemctl reboot Setup Kubernetes Node  In this example we will setup a Kubernetes node from scratch. It is also possible to use a container OS like RHEL Atomic as a Kubernetes node. RHEL Atomic is an OS optimized for running containers. Choosing whether to use RHEL Atomic or a standard RHEL depends greatly on your specific requirements.  Install Packages and enable services  #yum update -y #yum install -y docker docker-registry etcd kubernetes flannel #for SERVICES in docker.service kubelet kube-proxy flanneld    do  systemctl enable $SERVICES done Configure Kubernets Master  #vi /etc/kubernetes/config KUBE_MASTER=\"--master=http://kube-master.lab.com:8080\" Configure Kubernetes Node (kubelet)  #vi /etc/kubernetes/kubelet KUBELET_ADDRESS=\"--address=0.0.0.0\" KUBELET_PORT=\"--port=10250\"  KUBELET_HOSTNAME=\"\"  KUBELET_API_SERVER=\"--api_servers=http://kube-master.lab.com:8080\" Configure Docker  #vi /etc/sysconfig/docker ADD_REGISTRY='--add-registry registry.access.redhat.com' ADD_REGISTRY='--add-registry kube-master.lab.com:5000' Configure Flannel  #vi /etc/sysconfig/flanneld FLANNEL_ETCD=\"http://kube-master.lab.com:4001\" FLANNEL_ETCD_KEY=\"/flannel/network\" FLANNEL_OPTIONS=\"eth0\" #systemctl reboot Summary  In this article we went through the steps of building a Kubernetes cluster from scratch on RHEL or CentOS 7.1. As you have seen standing up a Kubernetes cluster can be done very easily. Hopefully you have found this article helpful, feedback is always greatly appreciated.  Happy Containerizing!  (c) 2015 Keith Tenzer   ","categories": ["Containers"],
        "tags": ["Docker","Kubernetes","LXC"],
        "url": "/containers/application-containers-kubernetes-and-docker-from-scratch/",
        "teaser": null
      },{
        "title": "Red Hat Enterprise Virtualization (RHEV) - Hypervisor Host Options",
        "excerpt":"Overview  Red Hat Enterprise Virtualization (RHEV) has two options for running a hypervisor host: 1) use the RHEV-H host 2) use Red Hat Enterprise Linux 6 or 7. Option 1 is similar to VMware ESXi, RHEV-H is an optimized OS for running Virtual Machines.  Option 2 allows you to configure a standard RHEL 6 or 7 host and add it to RHEV as a hypervisor.  Which option is best?  The answer here depends on your requirements but just having feedom of choice is an evolution and step in the right direction. If you would like to install extra packages or your hardware requires certain kernel modules then RHEL 7 is the best choice as you need that flexibility. If you dont need anything but the hypervisor itself then RHEV-H is the way to go.  Some additional use cases for using RHEL 6 or 7?   Application clustering using pacemaker Utilizing local hypervisor storage within cluster Special monitoring requirements  Red Hat Enterprise Virtualization Management (RHEV-M) is required before proceeding. For more general information about RHEV and configuring RHEV-M please check out this past article.  Adding RHEV Hypervisor Host  As mentioned RHEV-H is an optimized OS for running virtual machines. In order to configure RHEV-H please follow the below steps:   Download RHEV-H ISO Boot ISO image and follow standard installation steps ssh -l admin &lt;RHEV-H FQDN or IP&gt;  By default root account is disabled for login. The account used to configure RHEV-H is admin and upon logging in as admin you are provided with the following configuration menu.    Besides configuring network settings it is important to configure access to Red Hat Enterprise Virtualization Management (RHEV-M). Under the oVirt Engine menu the IP and port of the RHEV-M server must be configured. This will add the RHEV-H host to the Default datacenter in RHEV-M and from there all additional configuration can be done through RHEV-M.  Adding RHEL 7 Host  Instead of going with RHEV-H as mentioned we can build the hypervisor ourselves using RHEL 6 or RHEL 7 as baseline. One word of caution it is only possible to cluster hypervisors that are either RHEL 6 or RHEL 7, not a mix.  Install RHEL 7 minimal OS  Download and install RHEL 7 ISO. Choose a minimal install and configure static networking.  Required Repositories  #systemctl stop NetworkManager #systemctl disable NetworkManager #subscription-manager repos --enable=rhel-7-server-rpms #subscription-manager repos --enable=rhel-7-server-optional-rpms #subscription-manager repos --enable=rhel-7-server-rhev-mgmt-agent-rpms #yum update -y Configure Firewall  The RHEV Hypervisor requires iptables not firewalld. On RHEL 7 we need to disable firewalld and enable iptables. In addition we also need to disable Network Manager.  #systemctl stop firewalld #systemctl disable firewalld #systemctl stop NetworkManager #systemctl disable NetworkManager #yum -y install iptables-services #systemctl enable iptables.service #systemctl start iptables.service #iptables --flush #systemctl restart iptables.service Required iptables rules  Below are the required iptables rules for a RHEV hypervisor host. Note this is not required, RHEV-M will automatically configure these rules upon install this is just in case you change rules later.  # iptables -I INPUT -p tcp -m tcp --dport 54321 -j ACCEPT # iptables -I INPUT -p tcp -m tcp --dport 22 -j ACCEPT # iptables -I INPUT -p udp -m udp --dport 161 -j ACCEPT # iptables -I INPUT -p tcp -m tcp --dport 16514 -j ACCEPT # iptables -I INPUT -p tcp -m multiport --dports 5634:6166 -j ACCEPT # iptables -I INPUT -p tcp -m multiport --dports 49152:49216 -j ACCEPT # iptables -I INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT # iptables -I FORWARD -m physdev ! --physdev-is-bridged -j REJECT --reject-with icmp-host-prohibited # service iptables save Add RHEL Host to RHEV as Hypervisor  Once the RHEL host has been prepared it can be added to RHEV-M environment. RHEV-M will install all the required packages for RHEV and add host to desired cluster. From RHEV-M UI logon as admin@internal and under host select add a new host.    RHEV-M will use SSH in order to communicate with host and install packages as well as start services. RHEV-M will even configure the iptables rules as mentioned. It is important to add any additional iptables rules after the configuration is complete as RHEV-M will wipe previous iptables rules. Below we can see a new hypervisor host is being installed in the environment.    Once the installation is complete you can proceed to install any other packages and tweak things to your hearts content.  Configure local storage in cluster  Optionally you may want to use local storage of hypervisor host for images or other purposes. The easiest way to do this is by exporting mountpoints as NFS shares. In RHEV all storage in a cluster must be available to all hosts, therefore local storage is not an option unless you have a cluster consisting of just one RHEV host.  Install NFS Services on RHEV Host  #yum install nfs-utils rpcbind #systemctl enable rpcbind #systemctl enable nfs-server #systemctl start rpcbind #systemctl start nfs-server Exporting NFS mountpoints  In order to export NFS share to RHEV we need to create the mountpoint and ensure permissions are set correctly. In addition we need to ensure the mountpoint is also exported with read-write.  #mkdir /usr/share/rhev #chown -R 36:36 /usr/share/rhev/ #chmod -R 0755 /usr/share/rhev/ #vi /etc/exports #/usr/share/rhev 192.168.2.0/24(rw) #exportfs -a Configure NFS iptables rules  Once the NFS share is exported iptables rules need to be implemented in order to allow access to the NFS services.  Portmapper (rpcbind)  # iptables -I INPUT -p tcp -m tcp --dport 111 -j ACCEPT  # iptables -I INPUT -p udp -m udp --dport 111 -j ACCEPT Mountd (defined in /etc/sysconfig/nfs)    # iptables -I INPUT -p tcp -m tcp --dport 892 -j ACCEPT  # iptables -I INPUT -p udp -m udp --dport 892 -j ACCEPT Rquotad (defined in /etc/sysconfig/nfs)  # iptables -I INPUT -p tcp -m tcp --dport 875 -j ACCEPT  # iptables -I INPUT -p udp -m udp --dport 875 -j ACCEPT NFS Statd (defined in /etc/sysconfig/nfs)    # iptables -I INPUT -p tcp -m tcp --dport 662 -j ACCEPT  # iptables -I INPUT -p udp -m udp --dport 662 -j ACCEPT NFSD    # iptables -I INPUT -p tcp -m tcp --dport 2049 -j ACCEPT NFS Lock Manager (defined in /etc/sysconfig/nfs)    # iptables -I INPUT -p tcp -m tcp --dport 32803 -j ACCEPT NFS Lockd (defined in /etc/sysconfig)    # iptables -I INPUT -p udp -m udp --dport 32769 -j ACCEPT  #service iptables save Summary  Red Hat provides two Hypervisor options for RHEV. As we have seen you can use the optimized OS RHEV-H that is similar to VMware ESXi or you can build your own using RHEL 6 or 7 as base. The freedom of choice enables you to make the best decision for your IT infrastructure.  Happy RHEVing!  (c) 2015 Keith Tenzer  ","categories": ["RHEV"],
        "tags": ["KVM","Linux","RHEL","Virtualization"],
        "url": "/rhev/red-hat-enterprise-virtualization-rhev-hypervisor-host-options/",
        "teaser": null
      },{
        "title": "Pacemaker - The Open Source, High Availability Cluster",
        "excerpt":"Overview  Pacemaker is a Open Source, High Availability cluster. Pacemaker and in general Linux clustering have been around for a very long time. Both matured greatly over the past 10 to 15 years.Today Pacemaker is a very simple streamlined bundle that includes the Pacemker clustering, fencing agents, resource agents and the heartbeat (corrosync). Pacemaker is available in Red Hat Enterprise Linux 7 as the High Availability option. In this article I will provide an overview of Pacemaker and a tutorial on how to setup a two-node pacemaker cluster for Apache using shared storage.      Pacemaker Basics  As mentioned Pacemaker has a few components: clustering, fence agents, resource agents and corrosync.  Clustering  Pacemaker provides all the packages to configure and manage a high availability cluster using the CLI or GUI.  Fence Agents  Fencing is about shutting off a node that has become unstable or unresponsive so that it cannot damage the cluster or any cluster resources. The main reason we have fencing is to illiminate the possibility of a split-brain where multiple nodes access resources at same time. A split-brain can lead to data corruption and general cluster malfunction. There are two types of fencing agents in Pacemaker: power and storage. The most commonly used fencing agent is power. These agents connect to hardware such as UPS, blade chasis, iLO cards, etc and are responsible for fencing a node in the event that it becomes unresponsive. The other type of fencing is storage based. Typically storage-based fencing uses SCSI-3 PR (Persistent Reservation) that ensures only one node can ever write or access storage at time. This requires of course that the shared storage is used and that it supports SCSI-3 PR. The daemon or service responsible for fencing in Pacemaker is stonith (shoot the other node in the head).  Open source clustering in regards to fencing design differs slightly from commercial clustering. Open source has always taken a very conservative approach and IMHO that is a good thing. The last thing you want it data corruption. If fencing does not work Pacemaker will make the entire cluster unavailable. This means manual intervention will be required to bring resources online but your data is safe. Commercial solutions have very elaborate and complex fencing proceadures and try to always ensure failover is automated even if a problem occurs. I am not saying commercial software isn't bullet-proof, just that there is a design difference in this regard.  Resource Agents  Pacemaker resource agents are packages that integrate applications. Resource agents understand a specific application and it's dependencies. As such using resource agent (if one exists) makes configuration of applications much simpler. It also ensures that best practice around clustering for given application are enforced.  Corrosync  Pacemaker requires a heartbeat for internal communications . The corrosync daemon provides inter-cluster communications between cluster nodes. It is also responsible for quorum if a quorum is used.  Pacemaker Tutortial  Now that we have a high-level understanding of Pacemaker it is time to get our hands a bit dirty and configure a cluster. For this tutorial we will use a simple example of a two-node cluster for Apache. We will configure the cluster, setup storage-based fencing and configure a resource group for Apache.  Install Pacemaker  Perform following steps on both cluster nodes   Install RHEL / CentOS 7.1 (minimal) Configure subscription and repos (RHEL 7)  #subscription-manager register #subscription-manager list --available #subscription-manager attach --pool=&lt;pool id&gt; #subscription-manager repos --enable=rhel-ha-for-rhel-7-server-rpms  Install Pacemaker packages  #yum update -y #yum install -y pcs fence-agents-all  Open firewall ports  #firewall-cmd --permanent --add-service=high-availability #firewall-cmd --reload  Set hacluster password  #echo CHANGEME | passwd --stdin hacluster  Enable services  #systemctl start pcsd.service #systemctl enable pcsd.service  Configure ISCSI client  #yum install -y iscsi-initiator-utils #vi /etc/iscsi/initiatorname.iscsi InitiatorName=iqn.2015-06.com.lab:&lt;hostname&gt; Setup Shared ISCSI Storage  These steps are optional if you have ISCSI storage already configured. In these steps we will configure a third RHEL / CentOS system to provide shared storage using ISCSI.   Install RHEL / CentOS 7.1 (minimal) Install ISCSI packages  #yum install -y targetcli  Enable ISCSI service  #systemctl enable target  Create LVM disk (this will be the shared storage device)  #fdisk /dev/vdb (create new partition of type LVM) #pvcreate /dev/vdb1 #vgcreate cluster_vg /dev/vdb1 #lvcreate -L 1G cluster_vg -n cluster_disk1 #lvcreate -L 990M cluster_vg -n cluster_disk1 #mkfs -t ext4 /dev/cluster_vg/cluster_disk1  Configure ISCSI target  # targetcli /&gt; backstores/block create disk1 /dev/cluster_vg/cluster_disk1 /&gt; iscsi/ create iqn.2015-06.com.lab:rhel7 /&gt; /iscsi/iqn.2015-06.com.lab:rhel7/tpg1/portals/ create /&gt; iscsi/iqn.2015-06.com.lab:rhel7/tpg1/luns create /backstores/block/disk1 /&gt; iscsi/iqn.2015-06.com.lab:rhel7/tpg1/acls create iqn.2015-06.com.lab:pm-node1 /&gt; iscsi/iqn.2015-06.com.lab:rhel7/tpg1/acls create iqn.2015-06.com.lab:pm-node2 /&gt; exit  Open firewall ports  #firewall-cmd --permanent --add-port=3260/tcp #firewall-cmd --reload Create Cluster  At this point both cluster nodes have all the cluster packages and have access to shared ISCSI storage. In this section we will configure the cluster on one of the nodes.   Authorize cluster nodes  #pcs cluster auth pm-node1.lab.com pm-node2.lab.com Username: hacluster Password: pm-node1.lab.com: Authorized pm-node2.lab.com: Authorized  Setup the cluster  #pcs cluster setup --start --name mycluster pm-node1.lab.com pm-node2.lab.com  Enable services  #pcs cluster enable --all  Check cluster status  # pcs cluster status Cluster Status: Last updated: Fri Jun 19 14:10:24 2015 Last change: Fri Jun 19 14:09:15 2015 Stack: corosync Current DC: pm-node1.lab.com (1) - partition with quorum Version: 1.1.12-a14efad 2 Nodes configured 0 Resources configured PCSD Status: pm-node1.lab.com: Online pm-node2.lab.com: Online Storage Fencing  Before creating resource groups fencing needs to be configured. In this example we will use storage fencing and fence_scsi. Fencing is to be configured on one of the nodes.   Configure stonith  #pcs stonith create scsi fence_scsi pcmk_host_list=\"pm-node1.lab.com pm-node2.lab.com\" pcmk_monitor_action=\"metadata\" pcmk_reboot_action=\"off\"devices=\"/dev/mapper/cluster_vg-disk1\" meta provides=\"unfencing\"  Check status of fencing  #pcs stonith show  scsi (stonith:fence_scsi): Started Resource Group  Now that the cluster is running and fencing has been configured we can setup the resource group. A resource group defines application dependencies and ensures application is started correctly in the event of a failover. A resource group is to be configured on one of the nodes.   Install application packages (Apache)  #yum install -y httpd wget  Open firewall ports  #firewall-cmd --permanent --add-service=http #fireall-cmd --reload  Configure Apache  #vi /etc/httpd/conf/httpd.conf &lt;Location /server-status&gt; SetHandler server-status Order deny,allow Deny from all Allow from 127.0.0.1 &lt;/Location&gt;  Mount shared storage  # mount /dev/cluster_vg /disk1 /var/www/ # mkdir /var/www/html # mkdir /var/www/cgi-bin # mkdir /var/www/error # restorecon -R /var/www # cat &lt;&lt;-END&gt;/var/www/html /index.html &lt;html&gt; &lt;body&gt;Hello&lt;/body&gt; &lt;/html&gt; END #umount /var/www  Configure LVM so it only starts volumes not owned by the cluster  Note: it is important to not allow LVM to start the cluster owned volume group, in this case cluster_vg.  #vi /etc/lvm/lvm.conf volume_list = [ \"rhel\" ] use_lvmetad=0  Ensure boot image does not try and control cluster volume  #dracut -H -f /boot/initramfs-$(uname -r).img $(uname -r)  Create resource for LVM disk  #pcs resource create disk1 LVM volgrpname= cluster_vg exclusive= true --group apachegroup  Create resource for filesystem  #pcs resource create apache_fs Filesystem device=\"/dev/cluster_vg/disk1\" directory=\"/var/www\" fstype=\"ext4 \" --group apachegroup  Create resource for virtual IP address  #pcs resource create VirtualIP IPaddr2 ip=192.168.122.52 cidr_netmask=24 --group apachegroup  Create resource for website  #pcs resource create Website apache configfile=\"/etc/httpd/conf/httpd.conf\" statusurl=\"http://127.0.0.1/server-status\" --group apachegroup At this point the cluster should be configured and look somthing similar to our example.  #pcs status Cluster name: mycluster Last updated: Mon Jun 22 14:47:49 2015 Last change: Mon Jun 22 12:25:14 2015 Stack: corosync Current DC: pm-node2.lab.com (2) - partition with quorum Version: 1.1.12-a14efad 2 Nodes configured 5 Resources configured Online: [ pm-node1.lab.com pm-node2.lab.com ] Full list of resources: Resource Group: apachegroup  disk1 (ocf::heartbeat:LVM): Started pm-node1.lab.com   VirtualIP (ocf::heartbeat:IPaddr2): Started pm-node1.lab.com   apache_fs (ocf::heartbeat:Filesystem): Started pm-node1.lab.com   Website (ocf::heartbeat:apache): Started pm-node1.lab.com   scsi (stonith:fence_scsi): Started pm-node2.lab.com PCSD Status:  pm-node1.lab.com: Online  pm-node2.lab.com: Online Daemon Status:  corosync: active/enabled  pacemaker: active/enabled  pcsd: active/enabled In the event that there are problems the \"pcs resource debug-start &lt;resource&gt;\" command can be used for troubleshooting.  #pcs resource debug-start disk1 Pacemaker GUI  I was pleasantly surprised with the new Pacemaker GUI. The upstream community has done a terrific job. The GUI can even be used to manage multiple clusters. Below are some screenshots to give you a better idea.  To access GUI use the following URL and login as hacluster.  https://pm-node1:2224 Below screenshot shows the interface for managing clusters.    Below screenshot shows the interface for managing nodes.    Below screenshot shows the interface for managing resources.    Summary  Pacemaker is the result of open source innovation and long maturity. We have learned about Pacemaker basics and even configured a cluster for Apache using shared storage fencing. There is no doubt that Pacemaker is a viable alternative to commercial clustering from HP, IBM, Oracle and Microsoft. If you interested in traditional clustering I highly recommend giving Pacemaker a chance, you won't be disappointed.  Happy Clustering!  (c) 2015 Keith Tenzer  ","categories": ["Clustering"],
        "tags": ["HA","Linux","Pacemaker","RHEL"],
        "url": "/clustering/pacemaker-the-open-source-high-availability-cluster/",
        "teaser": null
      },{
        "title": "Governing the Cloud with CloudForms",
        "excerpt":"Overview  In a previous article it was stated that cloud in not a technology but rather an architectural methodology of resource governance that utilizes underlying virtualization technologies. Since cloud is more about the methodology, the technology platforms such as VMware vCenter, Red Hat Virtualization Management (RHEVM), Microsoft System Center, Amazon EC2 and OpenStack can change as application requirements or life cycles change. An application may start on a virtualization platform like VMware but over time move to a cloud platform such as OpenStack. It could even have components serviced by both. This is the exact concept behind what Gartner talks about when they refer to Bi-Modal IT. Gartner also says 50% of companies will screw this up. A bridge is definitely needed for managing applications between these various platforms. Red Hat CloudForms is exactly that bridge. It allows us to create an abstraction above the virtualization platforms so that governance and business process can remain consistent across the cloud.  In this article we will take a look at how to setup governance of cloud resources using CloudForms. If you are new to CloudForms I would suggest reading this article to get a proper introduction.  Before we get into creating policies that define resource governance in the cloud, we need to enable a few advanced features in CloudForms. \"Capacity and Utilization\" is needed for more detailed reporting and \"SmartState Analysis\" is required to get visibility inside virtual machines.  Configure Capcity and Utilization  CloudForms connects to infrastructure or cloud providers in order to report capacity and utilization. Therefore it is important to ensure the provider itself is collecting and reporting information. In this case we will configure Red Hat Enterprise Virtualization Management (RHEVM) provider to display capacity and utilization in CloudForms. Some steps are of course provider specific, in this case RHEVM specific.  Configure RHEVM  In RHEVM we need to configure reporting database user and allow remote access to the database.  #psql -U postgres CREATE ROLE newuser LOGIN UNENCRYPTED PASSWORD 'newuserpassword' SUPERUSER VALID UNTIL 'infinity'; \\q #iptables -I INPUT -p tcp -m tcp --dport 5432 -j ACCEPT #service  iptables  save #vi /var/lib/pgsql/data/pg_hba.conf host    all      all    0.0.0.0/0     md5 #vi /var/lib/pgsql/data/postgresql.conf listen_addresses  =  '*'  #service  postgresql  reload  Configure CloudForms  Once our reporting user and provider database is configured it is time to enable capacity and utilization in CloudForms.  Ensure all server roles are enabled except \"Database Synchronization\" and \"RHN Mirror\".    &nbsp;  Under \"Configure-&gt;Configure\" set the capacity and utilization policy for the CloudForms region. In this case we will enable all clusters and datastores.    Configure Smart State Analysis  SmartState analysis allows us to look inside virtual machines. CloudForms does this by mounting datastores and peering inside the virtual disks. CloudForms is able to see user accounts, installed packages and configuration files for given virtual machine. This data enables us to create resource governance policies as you will see later.  Under \"Configuration-&gt;Configuration-&gt;Settings\" edit the zone and enter IP of the CloudForms appliance you want to act as smart proxy. This is the system that will mount and connect to virtual machines for SmartState analysis. As such the CloudForms appliance requires access to shared storage used for datastores.    Finally the last step is to make CloudForms aware of what virtual machines are hosting CloudForm appliances. Under \"Infrastructure-&gt;Virtual Machines\" select appropriate virtual machine and edit management engine relationship. You can then match the virtual machine with the appropriate CloudForms engine. Note: this is new in CloudForms 3.2. Previously you would configure this under \"Configure-&gt;Configure-&gt;Smart Proxies\".     At this point we have configured both capacity and utilization in addition to SmartState analysis. You can now gather SmartState information about hosts and virtual machine by selecting \"Perform SmartState Analysis\" under configuration of host or virtual machine. Ideally you would create a schedule that runs the SmartState analysis on a regular interval for all hosts and virtual machines. You could also however create a control policy that runs SmartState analysis whenever hosts or virtual machines are started.  Governance Policies  CloudForms has two types of governance policies: compliance and control. Compliance policies are scheduled or run manually and check the compliance state of hosts or virtual machines. If a host or virtual machine is found out of compliance an action can be taken. Control policies differ only slightly in that a control policy is run based on an event occurring, so these are automated policies based on events where compliance policies are manually operated or scheduled via the administrator.  This may be a bit confusing at first but let me give an example. Lets suppose you want to protect against a known security vulnerability in virtual machines. You have two options and the only difference really is when do you check for vulnerability? If you check at a given time decided by administrator you would create compliance policy. If you check based on event happening like the virtual machine starting you would create a control policy.  CloudForms Terminology  Below is terminology relating to policies in CloudForms:  Policy Profile - Grouping of policies that can be applied to hosts or virtual machines.  Policy - Defines conditions and actions. Can be triggered by user in case of compliance policy or event in case of control policy.  Condition - Tests for a desired or non-desired state. Can be true or false. Conditions are used to trigger actions.  Action - Triggered based on condition matching true or false.  Event - Only relating to control policy, this is what triggers the control policy to execute.  Create Compliance Policy  In this example we will create a compliance policy that checks if the firewall is disabled on hypervisor hosts. If the firewall is disabled the condition would be true and an action will set those hosts to non-compliant.  Under \"Control-&gt;Explorer\" select the policies accordion. Create a new policy called \"Check Firewall\".     Next select the policy and add a new condition. Conditions allow us to check for state, in this case the state being that the firewall is disabled. You can have multiple conditions in an expression but in this example we just need one. Select field and \"Host / Node.OS:Firewall Rules Active\". Select false which means if the firewall is not active, the condition is true. Click the check mark to add the condition to the expression box.     The condition should now appear in the expression box and can now be saved.     Next we need to create a policy profile. This is simply a grouping of policies that we can apply to a host or virtual machine. In \"Control-&gt;Explorer\" select the policy profile accordion and under configuration dropdown add a new policy profile. You can give it a name \"RHEV Hypervisor\" and add policies by moving them from left to right.     Once we have a policy profile defined we can add it to the hypervisor hosts. Under \"Infrastructure-&gt;Hosts\" select the hosts and under policy dropdown select manage policies.    Now we can enable policy profiles on the selected hosts.     We can test the compliance policy by selecting the \"check compliance of last known configuration\" option from the host policy dropdown. When clicking on the host we can see if it is compliant under the compliance section.     In this case our host is not compliant because the firewall is disabled.  # systemctl status iptables iptables.service - IPv4 firewall with iptables  Loaded: loaded (/usr/lib/systemd/system/iptables.service; disabled)  Active: inactive (dead) Create Control Policy  As mentioned control policies are not that much different than compliance policies, they are simply driven from events. In this example we will do same check to see if firewall is enabled, only this time with control policy. Everytime a host is started and connected we will check if firewall is disabled. If the firewall is disabled we will have an action write to the audit log. Again both control and compliance policies can do same actions only difference is the trigger, in this case an event.  Under \"Control-&gt;Explore\" policies accordion select control policy and create a new control policy for host.     Since we are using the same condition to check if firewall is disabled we can simply edit condition and select the condition we already created. All policies can leverage same conditions.     Next we need to tell CloudForms what event will drive the policy into action. Select the policy and under the configuration dropdown \"edit policy event assignments\". In this example our event is the starting of a host.     Under \"Host Connect\" event we can add an action. In this case to generate an audit log message. Under configuration dropdown select \"edit action for this policy event\". Select \"generate audit event\" if the conditions are true, meaning the firewall is in fact disabled.     Finally under \"Control-Explore\" policy profile accordion we can add our new control policy to the existing profile policy \"RHEV Hypervisor\".    In order to test the control policy I can simply restart a host. Once host is restarted the control policy will automatically run and if the firewall is disabled an event will be written in the audit log. The audit log can be viewed under \"Configure-&gt;Configure\" diagnostics accordion.  [2015-07-12T00:01:31.139400 #6530:af7ea4]  INFO -- Success: MIQ(PolicyProfiles.profile_edit) userid: [admin] - [11fe79f2-2815-11e5-9d99-001a4aa04b00] Record updated (policies:[{\"Host / Node Compliance: Check Firewall\"=&gt;1000000000003}] to [{\"Host / Node Compliance: Check Firewall\"=&gt;1000000000003, \"Host / Node Control: Generate Audit Event for Firewall\"=&gt;1000000000006}])  Conclusion  In this article we have seen how to use Red Hat CloudForms to create both compliance and control policies that can govern our cloud infrastructure. Regardless of if your cloud consists of VMware vCenter, RHEV, Microsoft Systems Center, Amazon EC2 or even OpenStack common policies that define cloud governance can rule them all. The true power of CloudForms is allowing cloud administrators to build an abstraction layer around virtualization platforms that enables cloud governance. Regardless of what virtualization platform one uses, the policies should remain constant across all infrastructure. This is truly the only way to govern a cloud and create a standard operating environment. Hopefully you have found the information and topic discussed in this article informative. This is only of course the tip of the iceberg, but if it has gained your interest, I certainly implore you to consider looking into cloud management with CloudForms.  Happy cloud governance and rest easy with CloudForms!  (c) 2015 Keith Tenzer  ","categories": ["CloudForms"],
        "tags": ["Cloud","cloud management","governance","ManageIQ","Security"],
        "url": "/cloudforms/governing-the-cloud-with-cloudforms/",
        "teaser": null
      },{
        "title": "OpenShift Enterprise v3 Lab Configuration: Innovate Faster, Deliver Sooner",
        "excerpt":"Overview    OpenShift Enterprise v3 by Red Hat is about building and running next-gen applications. If we look around, we have seen startups in virtually every market segment, turning the competitive landscape upside down. Startup companies like NetFlix, Spotify and Uber have literally pushed the incumbents to the brink of extinction and overtaken entire industries in a very short period of time. How have they been able to rival incumbents 100 times their size? The answer is simple, by bringing innovation to the market faster, much faster. Complacency and overcoming previous successes are very challenging for incumbents. It is much easier for a startup to innovate than an existing company with a degree of legacy. OpenShift v3 will level the playing field and provide organizations the appropriate tooling to rapidly reduce their time-to-market.  OpenShift v3 allows organizations to deliver innovation faster by:   Maximizing time developers actually spend developing Enabling efficient clean hand-offs between Dev &amp; Ops (DevOps) Automating development pipelines and continuous integration / delivery Increasing speed of innovation through more frequent experimentation Providing state-of-the-art enterprise grade container infrastructure  In this article we will look at how to setup an OpenShift lab environment and get started on the journey to faster innovation cycles.    Pre Configuration Steps  OpenShift requires a master and one or more nodes. In this lab we will configure one master and a node. Install RHEL or CentOS 7.1 on two systems and configure hostname as well as network accordingly. On both systems run the following steps:  # subscription-manager repos --disable=\"*\" # subscription-manager repos --enable=\"rhel-7-server-rpms\" # subscription-manager repos --enable=\"rhel-7-server-extras-rpms\" # subscription-manager repos --enable=\"rhel-7-server-optional-rpms\" # subscription-manager repos --enable=\"rhel-7-server-ose-3.0-rpms\" #yum install wget git net-tools bind-utils iptables-services bridge-utils #yum install python-virtualenv #yum install gcc #yum install httpd-tools #yum install docker #yum update  Once all the packages are installed it is important to configure Docker so that is allows for insecure registry communication on local network only.  #vi /etc/sysconfig/docker OPTIONS=--selinux-enabled --insecure-registry 192.168.122.0/24 Setup ssh access from master to node.  #ssh-keygen #ssh-copy-id -i .ssh/id_rsa.pub root@ose3-node.lab.com Install OpenShift Enterprise v3  At this point both the master and node are prepared. We can now begin the install of OpenShift Enterprise v3. From the master run the following command:  #sh &lt;(curl -s https://install.openshift.com/ose) Note: if internet access is not available you can download the installer and run it locally on the master host.  https://install.openshift.com/portable/oo-install-ose.tgz  Configure OpenShift Enterprise v3  Once the installer completes an OpenShift master and node will exist. Now we can begin with the main configuration. By default OpenShift will use HTTP authentication. This is of course only recommended for lab or test environments. For production environments you will want to connect to LDAP or an identity management system. On the master we can edit the /etc/openshift/master/master-config.yaml and configure authentication.  #vi /etc/openshift/master/master-config.yaml identityProviders: - name: my_htpasswd_provider challenge: true login: true provider: apiVersion: v1 kind: HTPasswdPasswordIdentityProvider file: /root/users.htpasswd routingConfig:  subdomain: lab.com Next we need to create a standard user. OpenShift enterprise creates the system:admin account for default administration.  #htpasswd -c /root/users.htpasswd admin Optionally we can give the newly created admin user, OpenShift cluster-admin permisions.  #oadm policy add-cluster-role-to-user cluster-admin admin Configure Docker Registry  OpenShift uses the Docker registry for storing Docker container images. Anytime you build or change an application configuration, a new docker container is created and pushed to the registry. Each node can access this registry. You can and should use persistent storage for the registry. In this example we will use a host mountpoint on the node. The Docker registry runs as a container in the default namespace that only OpenShift admins can access.  On the node create a directory for the registry  #mkdir /images On the master login in using the system:admin account, switch to the default project and create a Docker registry.  # oc login Username: system:admin #oc project default #echo '{\"kind\":\"ServiceAccount\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"registry\"}}' | oc create -f - #oc edit scc privileged users: - system:serviceaccount:openshift-infra:build-controller - system:serviceaccount:default:registry #oadm registry --service-account=registry --config=/etc/openshift/master/admin.kubeconfig  --credentials=/etc/openshift/master/openshift-registry.kubeconfig --images='registry.access.redhat.com/openshift3/ose-${component}:${version}' --mount-host=/images Create Router  OpenShift v3 uses OpenVswitch as the software defined network. In order for isolation, proxy and load balancing capabilities a router is needed. The router similar to the Docker registry also runs in a container. Using the below command we can create a router in the default namespace.  #echo '{\"kind\":\"ServiceAccount\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"router\"}}' | oc create -f - #oc edit scc privileged users:  - system:serviceaccount:openshift-infra:build-controller  - system:serviceaccount:default:registry  - system:serviceaccount:default:router #oadm router router-1 --replicas=1 --credentials='/etc/openshift/master/openshift-router.kubeconfig' --images='registry.access.redhat.com/openshift3/ose-${component}:${version}'  Configure DNS  OpenShift v3 requires a working DNS environment in order to handle URL resolution. The requirement is to create a DNS wildcard that points to the router. This should be the public IP of the node where the router container is running. In our example we have created a local DNS server that acts as a forwarder for the 192.168.122.0 network. In addition we have implemented a DNS wildcard that points to our nodes public or physical IP, where the router container is running.  #yum install bind-utils bind #systemctl start named #systemctl enable named vi /etc/named.conf options {listen-on port 53 { 192.168.122.1; }; forwarders { 10.38.5.26; ; }; zone \"lab.com\" IN {  type master;  file \"/var/named/dynamic/lab.com.zone\";  allow-update { none; }; }; vi /var/named/dynamic/lab.com.zone $ORIGIN lab.com. $TTL 86400 @ IN SOA dns1.lab.com. hostmaster.lab.com. (  2001062501 ; serial  21600 ; refresh after 6 hours  3600 ; retry after 1 hour  604800 ; expire after 1 week  86400 ) ; minimum TTL of 1 day ; ;  IN NS dns1.lab.com. dns1 IN A 192.168.122.1   IN AAAA aaaa:bbbb::1 ose3-master IN A 192.168.122.60 ose3-node1 IN A 192.168.122.61 * 300 IN A 192.168.122.61 ; ;  Install and Configure GitHub Lab  In most cases you will probably want to configure a local GitHub server. This is of course optional. In the example we are using the public GitHub service, however you could easily do this on an internal GitHub server. You can setup the GitHub server on the OpenShift v3 master. For demos it is recommended to use GitHub lab, since it is much easier to install and configure.  #yum install curl openssh-server #systemctl enable sshd #systemctl start sshd #firewall-cmd --permanent --add-service=http #systemctl reload firewalld #curl https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.rpm.sh | bash #yum install gitlab-ce #gitlab-ctl reconfigure Once the above steps are complete you can access GitHub by connecting through browser to the host.  Username: root Password: 5iveL!fe Using OpenShift v3  At this point we should have a functioning OpenShift v3 environment. We can now build and deloy applications. Here we will see how to deploy a mysql database using scaling and build a ruby hello world application from GitHub.  Deploying MySQL database  Though using the OpenShift CLI or API is certainly possible, let us at this point use the UI. To login to the UI open a browser and point it at the IP of the OpenShift v3 master, for example: https://ose3-master.lab.com:8443/console/. Create a new project for hosting containers. In OpenShift v3 each project maps to a namespace in Kubernetes.  Under the demo project deploy a MySQL database by selecting \"create\" or \"getting started\". Make sure you add a label, this is explained later.  Once an application is created we see the status in the Overview.  Each time an application is deployed we have a container deployer and the running container. Once the deployment is complete the deployer container is deleted and we just have the running container. The \"oc get pods\" command shows us all pods within the namespace. A pod is a Kubernetes construct and means one or more Docker containers that share deployment template. Pods run on nodes, grouping containers within pods is a way to ensure certain containers are localized.  # oc get pods NAME            READY  REASON     RESTARTS  AGE mysql-1-deploy  1/1    Running    0         8s mysql-1-rz165   0/1    Running    0         5s For every application deployed, OpenShift will also create a replication controller and service. These are also Kubernetes constructs. A replication controller is used for auto-scaling and determines how many instances of a given pod should exist.  # oc get rc CONTROLLER  CONTAINER(S)  IMAGE(S)                              SELECTOR                REPLICAS mysql-1     mysql         .../openshift3/mysql-55-rhel7:latest  deployment=mysql-1,...  1 The service creates a URL for the application and handles dynamically routing to individual pods. This is handled by the kube-proxy layer and the OpenShift routing layer.  # oc get services NAME   LABELS                                        SELECTOR    IP(S)          PORT(S) mysql  demo=mysql,template=mysql-ephemeral-template  name=mysql  172.30.76.121  3306/TCP When creating applications it is very important to always define labels. Labels are applied to pods, replication controllers and services. When deleting an application it is very easy to for example reference the label instead of deleting individual components manually.  #oc delete all --selector=\"demo=mysql\" OpenShift v3 also supports auto-scaling. This capability leverages Kubernetes replication controllers. First we need to identify the replication controller using the \"oc get rc\" command. We can automatically scale our application by changing the number of replicas. In this example we will scale from one to three MySQL databases.  #oc scale --replicas=3 rc mysql-1 Upon scaling MySQL, we can quickly see the results in the UI.    Building Ruby Hello World Application  So far we have seen how to provision application components such as databases or middleware in seconds. We have also obeserved how we can effortlessly scale these components. In the following example, we will build our own application code in OpenShift v3. OpenShift will provide the Ruby runtime environment and automatically build, as well as launch a container with our hello world code from GitHub. OpenShift utilizes a technology called \"Source to Image\" (S2I) that efficiently builds the container. Instead of rebuilding the entire container each time, S2I is able to re-use previous builds and only change the application layer within the container. Docker containers are immutable so any change always requires creating a new container. This is a wasteful, time consuming process without OpenShift and S2I.  To build our application select \"create\" from the project page in the OpenShift UI. Enter the URL to the GitHub repository https://github.com/ktenzer/ruby-hello-world and select \"next\".    OpenShift asks us for the application build runtime. In this case we will select Ruby 2.0 since this is in fact a Ruby application.    In the final step we can provide any custom details about the build configuration and of course add a label.    OpenShift will create a container with Ruby 2.0 and our code from GitHub. It will also complete any required build steps. The end result is a complete application build, of a running application, inside a Docker container. Our application can now be automatically tested using Jenkins or other such continuous delivery tools. If tests pass, it can be automatically rolled out to production. Think about how much faster you can make code available to your customers with OpenShift? By selecting the URL for the Ruby hello world application we can also access the application directly.    &nbsp;    Troubleshooting  In this section we will go through some basic troubleshooting steps for OpenShift v3. In order to get logs we first need the pod name. Using the \"oc get pods\" command, we can get a list of pods.  #oc logs ruby-hello-world-1-65lgf You might consider adding 'puma' into your Gemfile. [2015-08-03 08:19:03] INFO WEBrick 1.3.1 [2015-08-03 08:19:03] INFO ruby 2.0.0 (2013-11-22) [x86_64-linux] [2015-08-03 08:19:03] INFO WEBrick::HTTPServer#start: pid=1 port=8080 10.1.0.4 - - [03/Aug/2015 08:49:15] \"GET / HTTP/1.1\" 200 2496 0.0117 [2015-08-03 08:49:45] ERROR Errno::ECONNRESET: Connection reset by peer  /opt/rh/ruby200/root/usr/share/ruby/webrick/httpserver.rb:80:in `eof?'  /opt/rh/ruby200/root/usr/share/ruby/webrick/httpserver.rb:80:in `run'  /opt/rh/ruby200/root/usr/share/ruby/webrick/server.rb:295:in `block in start_thread' Beyond looking at a pods logs we can also access journald for docker, openshift-master and openshift-node. Using the below journalctl commands we can get a list of current log messages for the major OpenShift components.  #journalctl -f -l -u docker #journalctl -f -l -u openshift-master #journalctl -f -l -u openshift-node Issue 1: Pod shows as pending and scheduled but never gets deployed on a node.  This problem can occur if the node docker image cache gets out of sync. In order to resolve this issue perform the following steps on the node:  #systemctl stop docker #rm -rf /var/lib/docker/* #reboot Summary  In this article we have seen how to deploy an OpenShift Enterprise v3 lab environment. We have seen how to use OpenShift in order to deploy and build applications. This is just the tip of the iceberg of course. In a world where speed and agility becomes increasingly important, it is clear that container infrastructure will become the future platform for running applications. You simply can't argue with being able to start 60 containers in the time it takes to start a single VM. Google deploys over two billion containers a week and everything you do from Google mail to search, runs in a container. Containers are enterprise ready and it is time to start understanding how to take advantage of this technology. OpenShift Enterprise v3 provides a platform for building and running applications on container infrastructure. OpenShift Enterprise v3 enables organizations to innovate faster, bringing that innovation to the market sooner. Don't let your organization be overtaken by the next startup! If you found this article informative or helpful please share your thoughts.  Happy OpenShifting!  (c) 2015 Keith Tenzer  ","categories": ["OpenShift"],
        "tags": ["Containers","Continous Delivery","DevOps","Docker","Kubernetes","Linux","OpenShift","PaaS"],
        "url": "/openshift/openshift-enterprise-v3-lab-configuration-innovate-faster-deliver-sooner/",
        "teaser": null
      },{
        "title": "OpenShift v3: Unlocking the Power of Persistent Storage",
        "excerpt":"Overview  In this article we will discuss and implement persistent storage in OpenShift v3. If you are new to OpenShift v3 you should first read the OpenShift v3 Lab Configuration article to get going.      Docker images are immutable and it is not possible to simply store persistent data within containers. When applications write to the Docker union file system, that data is lost as soon as the container is stopped. Docker provides a solution for persisting data, that allows administrator to mount a mount point existing on the container host (OpenShift node) within the container itself. It is similar to concept of raw device maps in virtual machines except with file systems.OpenShift v3 interfaces with Kubernetes and Kubernetes interfaces with Docker. As such we will mostly be referring to Kubernetes in this article. Kubernetes has a concept of pods which is a grouping of Docker containers that are co-existed. All Docker containers within a pod share same resources, including storage.  Ephemeral Storage  OpenShift v3 supports using ephemeral storage for all database templates. As mentioned using ephemeral storage means application data is written to the Docker union file system. All data is lost, as soon as the Kubernetes pod and as such container is stopped. In addition, since using ephemeral storage uses the Docker union file system, writes will be slow. If performance is desired, it is recommend to use persistent storage. The use case for ephemeral storage is mainly around automated testing. You don't need performance or to save data in order to test application functionality.  Persistent Storage  OpenShift v3 supports using persistent storage through Kubernetes storage plugins. Red Hat has contributed plugins for NFS, ISCSI, Ceph RBD and GlusterFS to Kubernetes. OpenShift v3 supports NFS, ISCSI, Ceph RBD or GlusterFS for persistent storage. As mentioned, Kubernetes deploys Docker containers within a pod and as such, is responsible for storage configuration. Details about the implementation of persistent storage in Kubernetes can be found here. Kubernetes allows you to create a pool of persistent volumes. Each persistent volume is mapped to a external storage file system. When persistent storage is requested from a pod, Kubernetes will claim a persistent volume from the pool of available volumes. The Kubernetes scheduler decides where to deploy pod. External storage is mounted on that node and presented to all containers within pod. If persistent storage is no longer needed, it can be reclaimed and made available to other pods. OpenShift v3 makes this all seamless to the user and hides the underlying complexity, as we will see.  Below is a snippet from the Docker configuration of a container using persistent storage. If you didn't have OpenShift v3 and Kubernetes, you would have to deal with this for every single Docker container.  [code language=\"java\"]  &quot;Volumes&quot;: {     &quot;/dev/termination-log&quot;: &quot;/var/lib/openshift/openshift.local.volumes/pods/6e1d5a40-471b-11e5-9680-525400bca113/containers/mysql/960dd543dc5f790ff2be72858b79c9df20bfae00ec9bffa333cd6e34e7aa36f9&quot;,     &quot;/var/lib/mysql/data&quot;: &quot;/var/lib/openshift/openshift.local.volumes/pods/6e1d5a40-471b-11e5-9680-525400bca113/volumes/kubernetes.io~nfs/pv0016&quot;,     &quot;/var/run/secrets/kubernetes.io/serviceaccount&quot;: &quot;/var/lib/openshift/openshift.local.volumes/pods/6e1d5a40-471b-11e5-9680-525400bca113/volumes/kubernetes.io~secret/default-token-ck4x7&quot;  },  &quot;VolumesRW&quot;: {     &quot;/dev/termination-log&quot;: true,     &quot;/var/lib/mysql/data&quot;: true,     &quot;/var/run/secrets/kubernetes.io/serviceaccount&quot;: false  },  &quot;VolumesRelabel&quot;: {     &quot;/dev/termination-log&quot;: &quot;&quot;,     &quot;/var/lib/mysql/data&quot;: &quot;&quot;,     &quot;/var/run/secrets/kubernetes.io/serviceaccount&quot;: &quot;ro&quot;  }  [/code]  Configure Persistent Storage  In order to configure persistent storage, the storage must be available to all OpenShift v3 nodes using NFS, ISCSI, Ceph RDB or GlusterFS. In this example, we will configure an NFS server on the OpenShift v3 master. For a lab environment this is fine but for production environments you will want to use external storage for obvious reasons.  Configure NFS Server on OpenShift v3 Master  The first step is to install NFS server and start the services.  #yum groupinstall -y file-server #systemctl enable rpcbind #systemctl enable nfs-server #systemctl start rpcbind #systemctl start nfs-server Once the services are running, we need to allow access through iptables. OpenShift v3 uses iptables and not firewalld.  #iptables-save &gt; pre-nfs-firewall-rules-server #iptables -I INPUT -m state --state NEW -p tcp -m multiport --dport 111,892,2049,32803 -s 0.0.0.0/0 -j ACCEPT #iptables -I INPUT -m state --state NEW -p udp -m multiport --dport 111,892,2049,32769 -s 0.0.0.0/0 -j ACCEPT #service iptables save Allow SELinux policy for sVirt to write to nfs shares. By default the SELinux sVirt policy prevents containers from writing to NFS shares.  #setsebool -P virt_use_nfs 1 Configure NFS Client on OpenShift v3 Nodes  On all nodes, we will want to install the nfs-utils so that nodes can mount NFS shares.  #yum install -y nfs-utils Configure Persistent Volumes  In order to configure persistent volumes we need to create a JSON or YAML template file. In this example we will use JSON but Kubernetes supports both. We will also create a pool of 20 persistent volumes. From here all steps will be performed on the OpenShift v3 master.  Create a JSON file that will be used as template for adding persistent volumes. Note: you need to replace the IP address with the IP of your OpenShift v3 master.  vi /root/PV.json {  \"apiVersion\": \"v1\",  \"kind\": \"PersistentVolume\",  \"metadata\": {  \"name\": \"pv0001\" }, \"spec\": {    \"capacity\": {    \"storage\": \"10Gi\"    },    \"accessModes\": [ \"ReadWriteOnce\" ],    \"nfs\": {       \"path\": \"/mnt/RBD/pv0001\",       \"server\": \"192.168.122.60\"    },    \"persistentVolumeReclaimPolicy\": \"Recycle\"    } } In order to automate things we will create a for loop, that will create the NFS shares, set permissions and create persistent volumes in OpenShift v3.  for i in `seq -w 0001 0020`; do SHARE=/mnt/RBD/pv$i; mkdir -p $SHARE; chmod 777 $SHARE; chown nfsnobody:nfsnobody $SHARE; echo \"$SHARE 192.168.122.0/24(rw,all_squash)\" &gt;&gt;/etc/exports; sed s/pv0001/pv$i/g /root/PV.json | oc create -f -; done We can now list the persistent storage volumes in OpenShift v3. Notice we have no claims yet.  #oc get pv NAME   LABELS CAPACITY    ACCESSMODES STATUS    CLAIM         REASON pv0001 &lt;none&gt; 10737418240 RWO         Available  pv0002 &lt;none&gt; 10737418240 RWO         Available  pv0003 &lt;none&gt; 10737418240 RWO         Available  pv0004 &lt;none&gt; 10737418240 RWO         Available  pv0005 &lt;none&gt; 10737418240 RWO         Available  pv0006 &lt;none&gt; 10737418240 RWO         Available  pv0007 &lt;none&gt; 10737418240 RWO         Available  pv0008 &lt;none&gt; 10737418240 RWO         Available  pv0009 &lt;none&gt; 10737418240 RWO         Available  pv0010 &lt;none&gt; 10737418240 RWO         Available  pv0011 &lt;none&gt; 10737418240 RWO         Available  pv0012 &lt;none&gt; 10737418240 RWO         Available  pv0013 &lt;none&gt; 10737418240 RWO         Available  pv0014 &lt;none&gt; 10737418240 RWO         Available  pv0015 &lt;none&gt; 10737418240 RWO         Available  pv0016 &lt;none&gt; 10737418240 RWO         Available pv0017 &lt;none&gt; 10737418240 RWO         Available  pv0018 &lt;none&gt; 10737418240 RWO         Available  pv0019 &lt;none&gt; 10737418240 RWO         Available  pv0020 &lt;none&gt; 10737418240 RWO         Available Create OpenShift v3 Application Using Persistent Storage  Now that everything is configured, we can do a quick demo of how persistent storage in OpenShift v3 actually works. We will deploy a ruby hello world application from GitHub that uses a persistent MySQL database.  First create a new project in OpenShift v3. This creates a namespace in Kubernetes.  #oc new-project demo Next deploy our ruby hello world application from GitHub. This will deploy a pod that builds the code from GitHub and then using STI (Source To Image), will deploy a running pod with our built application. The reason for this is that we have different dependencies required for building and running applications. You are hopefully starting to see the power of OpenShift v3. If not stay tuned!  #oc new-app https://github.com/openshift/ruby-hello-world Since we will want to access our application our service also needs to be exposed. OpenShift v3 will configure an HA proxy using router in openVswitch and an Apache vHost. Traffic is routed to the appropriate host based on the service name. The Apache vHost exposes the application based on the service name. Using vHosts allows OpenShift services to use the same ports and thus doesn't require unique ports. Behind the scenes Kubernetes is handling the routing from the OpenShift v3 node to the Docker container.  #oc expose service ruby-hello-world We can check OpenShift v3 UI or console to see the status of our build and deployment.    Once complete we should have a builder pod that has exited with 0 and a running pod.  #oc get pods NAME                     READY     REASON     RESTARTS AGE ruby-hello-world-1-build 0/1       ExitCode:0 0        3m ruby-hello-world-3-3gtig 1/1       Running    0        1m If we open a web browser and point it at the service name we should see the application. In this case it won't show us much since we don't yet have a database.    Lets add a persistent MySQL database and connect it to our ruby hello world application.  #oc process -n openshift mysql-persistent -v DATABASE_SERVICE_NAME=database | oc create -f - #oc env dc database --list | oc env dc ruby-hello-world -e - A MySQL database will be deployed using persistent storage. Once the database is deployed the pod should be running and the we should also see a persistent storage volume claim.  #oc get pods NAME                     READY     REASON     RESTARTS AGE database-1-2gv6j         1/1       Running    0        1m ruby-hello-world-1-build 0/1       ExitCode:0 0        3m ruby-hello-world-3-3gtig 1/1       Running    0        1m #oc get pv NAME   LABELS CAPACITY    ACCESSMODES STATUS    CLAIM           REASON pv0001 &lt;none&gt; 10737418240 RWO         Available  pv0002 &lt;none&gt; 10737418240 RWO         Available  pv0003 &lt;none&gt; 10737418240 RWO         Available  pv0004 &lt;none&gt; 10737418240 RWO         Available  pv0005 &lt;none&gt; 10737418240 RWO         Available  pv0006 &lt;none&gt; 10737418240 RWO         Available  pv0007 &lt;none&gt; 10737418240 RWO         Available  pv0008 &lt;none&gt; 10737418240 RWO         Available  pv0009 &lt;none&gt; 10737418240 RWO         Available  pv0010 &lt;none&gt; 10737418240 RWO         Available  pv0011 &lt;none&gt; 10737418240 RWO         Available  pv0012 &lt;none&gt; 10737418240 RWO         Available  pv0013 &lt;none&gt; 10737418240 RWO         Available  pv0014 &lt;none&gt; 10737418240 RWO         Available  pv0015 &lt;none&gt; 10737418240 RWO         Available  pv0016 &lt;none&gt; 10737418240 RWO         Bound      demo/database pv0017 &lt;none&gt; 10737418240 RWO         Available  pv0018 &lt;none&gt; 10737418240 RWO         Available  pv0019 &lt;none&gt; 10737418240 RWO         Available  pv0020 &lt;none&gt; 10737418240 RWO         Available Once we have verified everything we should also be able to use our ruby hello world application. Lets do a put for a key/value pair.    In order to demonstrate persistent storage, let us now delete the MySQL database pod. Don't worry the replication controller will automatically deploy a new pod and of course our data will be saved. If we were using ephemeral storage the data would be lost at this step.  #oc delete pod database-1-2gv6j Finally lets go back to our ruby hello world application and and do a get for our key \"keith\". We should see the value is \"tenzer\" thus confirming persistent storage is working.    Summary  In this article we have seen the power of OpenShift v3 in delivering a complete platform for building, deploying and running container-based applications. We have discussed the use cases behind ephemeral and persistent storage within OpenShift v3 ecosystem. Finally we have implemented and shown a compelling use case for persistent storage. OpenShift v3 is a platform for building and running next-gen applications using immutable container infrastructure. The goal is to deliver innovation faster. Hopefully this article has given you a glimpse at what is available today and inspired you to try things yourself. If you have any feedback or use cases for OpenShift v3, lets hear it!  A special thanks goes to Wolfram Richter, a mentor and colleague who helped tremendously in creating the content for this article.  Happy OpenShifting!  (c) 2015 Keith Tenzer  ","categories": ["OpenShift"],
        "tags": ["Containers","Docker","Ephemeral Storage","Kubernetes","NFS","OpenShift","Persistent Storage","Storage"],
        "url": "/openshift/openshift-v3-unlocking-the-power-of-persistent-storage/",
        "teaser": null
      },{
        "title": "OpenStack Kilo Lab Installation and Configuration Guide",
        "excerpt":"  Overview  In this article we will focus on installing and configuring OpenStack Kilo using RDO and the packstack installer. RDO is a community platform around Red Hat's OpenStack Platform. It allows you to test the latest OpenStack capabilities on a stable platform such as Red Hat Enterprise Linux (RHEL) or CentOS. This guide will take you through installing the OpenStack Kilo release, configuring networking, security groups, flavors, images and are other OpenStack related services. The outcome is a working OpenStack environment based on the Kilo release that you can use as a baseline for testing your applications with OpenStack capabilities. A big thanks to Red Hatter, Goetz Rieger who contributed some of this content.    Install and Configure OpenStack Kilo   Install RHEL or CentOS 7.1. Ensure name resolution is working  #vi /etc/hosts 192.168.122.80 osp7.lab.com osp7  Ensure the hostname is set statically.  #hostnamectl set-hostname osp7.lab.com  Disable network manager.  #systemctl disable NetworkManager.service  Disable firewalld to make configuration easier.   #systemctl disable firewalld.service  For RHEL systems register with subscription manager.   #subscription-manager register  #subscription-manager subscribe --auto  #subscription-manager repos --disable=*  #subscription-manager repos --enable=rhel-7-server-rpms  #subscription-manager repos --enable=rhel-7-server-openstack-7.0-rpms  Install yum-utils and update the system.   #yum install -y yum-utils  #yum update -y  #reboot  Install packstack packages.   #yum install -y openstack-packstack  Create packstack answers file for customizing the installer.   #packstack --gen-answer-file /root/answers.txt  Update the packstack answers file.  #vi /root/answers.txt  CONFIG_KEYSTONE_ADMIN_PW=redhat  CONFIG_HORIZON_SSL=y  CONFIG_PROVISION_DEMO=n  CONFIG_HEAT_INSTALL=y  CONFIG_HEAT_CLOUDWATCH_INSTALL=y  CONFIG_HEAT_CFN_INSTALL=y  CONFIG_SAHARA_INSTALL=y  CONFIG_TROVE_INSTALL=y  CONFIG_CEILOMETER_INSTALL=y  CONFIG_LBAAS_INSTALL=y  Install OpenStack using packstack from RDO.  #packstack --answer-file /root/answers.txt . /root/keystonerc_admin  Check status of openstack services.  #openstack-status  Backup the ifcfg-etho script.  #cp /etc/sysconfig/network-scripts/ifcfg-eth0 /root/  Configure external bridge for floating ip networks.  #vi /etc/sysconfig/network-scripts/ifcfg-eth0  DEVICE=eth0  ONBOOT=yes  TYPE=OVSPort  DEVICETYPE=ovs  OVS_BRIDGE=br-ex #vi /etc/sysconfig/network-scripts/ifcfg-br-ex  DEVICE=br-ex  BOOTPROTO=static  ONBOOT=yes  TYPE=OVSBridge  DEVICETYPE=ovs  USERCTL=yes  PEERDNS=yes  IPV6INIT=no  IPADDR=&lt;www.xxx.yyy.zzz&gt;  NETMASK=255.255.255.0  GATEWAY=&lt;GW IP&gt;  DNS1=&lt;DNS IP&gt;  Add the eht0 physical interface to the br-ex bridge in openVswitch for floating IP networks.  #ovs-vsctl add-port br-ex eth0 ; systemctl restart network.service  Create private network.  #neutron net-create private #neutron subnet-create private 10.10.1.0/24 --name private_subnet --allocation-pool start=10.10.1.100,end=10.10.1.200  Create public network. Note: these steps assume the physical network connected to eth0 is 192.168.122.0/24.  #neutron net-create public --router:external #neutron subnet-create public 192.168.122.0/24 --name public_subnet --allocation-pool start=192.168.122.100,end=192.168.122.200 --disable-dhcp --gateway 192.168.122.1  Add a new router and configure router interfaces.  #neutron router-create router1 --ha False #neutron router-gateway-set router1 public #neutron router-interface-add router1 private_subnet  Upload a glance image. In this case we will use a Cirros image because it is small and thus good for testing OpenStack.  #glance image-create --name \"Cirros 0.3.4\" --disk-format qcow2 --container-format bare --is-public True --copy http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img  Create a new m1.nano flavor for running Cirros image.  #nova flavor-create m1.nano 42 64 0 1  Create security group and allow all TCP ports.  #nova secgroup-create all \"Allow all tcp ports\" #nova secgroup-add-rule all TCP 1 65535 0.0.0.0/0  Create security group for base access  #nova secgroup-create base \"Allow Base Access\" #nova secgroup-add-rule base TCP 22 22 0.0.0.0/0 #nova secgroup-add-rule base TCP 80 80 0.0.0.0/0 #nova secgroup-add-rule base ICMP -1 -1 0.0.0.0/0  Create a private ssh key for connecting to instances remotely.  #nova keypair-add admin  Create admin.pem file and add private key from output of keypair-add command.  #vi /root/admin.pem #chmod 400 /root/admin.pem  List the network IDs.  # neutron net-list  +--------------------------------------+---------+-------------------------------------------------------+  | id | name | subnets |  +--------------------------------------+---------+-------------------------------------------------------+  | d4f3ed19-8be4-4d56-9f95-cfbac9fdf670 | private | 92d82f53-6e0b-4eef-b8b9-cae32cf40457 10.10.1.0/24     |  | 37c024d6-8108-468c-bc25-1748db7f5e8f | public  | 22f2e901-186f-4041-ad93-f7b5ccc30a81 192.168.122.0/24 |  Start an instance and make sure to replace network id from above command.  #nova boot --flavor m1.nano --image \"Cirros 0.3.4\" --nic net-id=92d82f53-6e0b-4eef-b8b9-cae32cf40457 --key-name admin --security-groups all mycirros  Create a floating IP and assign it to the mycirros instance.  #nova floating-ip-create #nova floating-ip-associate mycirros &lt;FLOATING IP&gt;  Connect to mycirros instance using the private ssh key stored in the admin.pem file.  #ssh -i admin.pem cirros@192.168.122.233 Summary  This article was intended as a hands on guide for standing up an OpenStack Kilo lab using RDO. As mentioned RDO is a stable community platform built around Red Hat's OpenStack Platform. It provides the ability to test the latest OpenStack features against either an enterprise platform (RHEL) or community platform (CentOS). Hopefully you found the information in this article useful. If you have anything to add or feedback, feel free to leave your comments.  Happy OpenStacking!  (c) 2015 Keith Tenzer  ","categories": ["OpenStack"],
        "tags": ["CentOS","Kilo","KVM","Linux","OpenStack","Packstack","RDO","Red Hat","RHEL"],
        "url": "/openstack/openstack-kilo-lab-installation-and-configuration-guide/",
        "teaser": null
      },{
        "title": "Auto Scaling Instances with OpenStack",
        "excerpt":"  Overview  Intelligently and automatically scaling applications based on resource requirements is at the heart of the OpenStack value proposition. It is one of the key capabilities that differentiate cloud vs traditional infrastructure such as VMware. For those of us who have been in IT a while auto scaling was always a white unicorn, often discussed but never actually seen. In this article we will talk about how to do this in OpenStack using Heat and Ceilometer.    Heat  Orchestration and automation within OpenStack is handled by Heat. It is the brains of your cloud. Heat provides a declarative structure for defining IT processes using YAML. You could say the value of OpenStack is implemented by Heat as this is where your business processes exist. Heat will let you automatically provision infrastructure (compute, network and storage)  based on YAML templates. In addition Heat also lets you create policies around running infrastructure. One such policy is around auto scaling.  Ceilometer  Collecting and persisting utilization measurements within OpenStack is handled by Ceilometer. OpenStack attempts to handle IT infrastructure as a utility and as such metering is a critical aspect. Furthermore, enabling billing systems to provide pay-as-you-go consumption models, requires exact metering. Beyond billing, such metering is also key for auto scaling. Decisions made by Heat on when to scale applications, are based on data collected by Ceilometer.  Auto Scaling Heat Templates  A lot of the configuration information in this article comes from article written by Christian Berendt. In this auto scaling example, Heat and Ceilometer will be used to scale CPU bound virtual machines. Heat has the concept of a stack which is simply the environment itself. The Heat stack template describes the process or logic around how a Heat stack will be built and managed. This is where you can create an auto scaling group and configure Ceilometer thresholds. The environment template explains how to create the stack itself, what image or volume to use, network configuration, software to install and everything an instance or instances need to properly function. You can put everything into the Heat stack template, but separating the Heat stack template from the environment is much cleaner, at least in more complex configurations such as auto scaling.  Environment Template  Below we will create an environment template for a cirros image. The instance template will create an instance based on Cirros image, configure a cinder volume, add IP from private network, add floating IP from public network, add security group, private ssh-key and generate 100% CPU load through user-data. Note: you will need to make changes below depending on your OpenStack configuration. The OpenStack installation and configuration used for these examples can be found in this article.  #vi /etc/heat/templates/cirros.yaml [code language=\"java\"]  heat_template_version: 2014-10-16  description: A base Cirros 0.3.4 server  resources:    server:      type: OS::Nova::Server      properties:        block_device_mapping:          - device_name: vda            delete_on_termination: true            volume_id: { get_resource: volume }        flavor: m1.nano        key_name: admin        networks:          - port: { get_resource: port }    port:      type: OS::Neutron::Port      properties:        network: private        security_groups:          - all    floating_ip:      type: OS::Neutron::FloatingIP      properties:        floating_network: public    floating_ip_assoc:      type: OS::Neutron::FloatingIPAssociation      properties:        floatingip_id: { get_resource: floating_ip }        port_id: { get_resource: port }    volume:      type: OS::Cinder::Volume      properties:        image: 'Cirros 0.3.4'        size: 1  [/code]  Now that we have an environment template, we need to create a Heat resource type and link it to above file /etc/heat/templates/cirros.yaml.  #vi /root/environment.yaml [code language=\"java\"]  resource_registry:      &quot;OS::Nova::Server::Cirros&quot;: &quot;file:///etc/heat/templates/cirros.yaml&quot;  [/code]  Stack Template  Below we will define our Heat stack template. We will create the following resources: scaleup_group, scaleup_policy and cpu_alarm_high. The scaleup_group explains how the instance should be scaled and also defines the environment (OS::Nova::Server::Cirros) that points to the environment yaml file \"/etc/heat/templates/cirros.yaml\". The scaleup_policy defines how to handle a scale-up event. Finally we have a threshold, the cpu_alarm_high resource is used to trigger a scale-up event. Here we define the threshold and actions provided by Ceilometer.  #vi /root/example.yaml [code language=\"java\"]  heat_template_version: 2014-10-16  description: Example auto scale group, policy and alarm  resources:    scaleup_group:      type: OS::Heat::AutoScalingGroup      properties:        cooldown: 60        desired_capacity: 1        max_size: 3        min_size: 1        resource:          type: OS::Nova::Server::Cirros    scaleup_policy:      type: OS::Heat::ScalingPolicy      properties:        adjustment_type: change_in_capacity        auto_scaling_group_id: { get_resource: scaleup_group }        cooldown: 60        scaling_adjustment: 1    scaledown_policy:      type: OS::Heat::ScalingPolicy      properties:        adjustment_type: change_in_capacity        auto_scaling_group_id: { get_resource: scaleup_group }        cooldown: 60        scaling_adjustment: -1    cpu_alarm_high:      type: OS::Ceilometer::Alarm      properties:        meter_name: cpu_util        statistic: avg        period: 60        evaluation_periods: 1        threshold: 50        alarm_actions:          - {get_attr: [scaleup_policy, alarm_url]}        comparison_operator: gt    cpu_alarm_low:      type: OS::Ceilometer::Alarm      properties:        meter_name: cpu_util        statistic: avg        period: 60        evaluation_periods: 1        threshold: 10        alarm_actions:          - {get_attr: [scaledown_policy, alarm_url]}        comparison_operator: lt  [/code]  Update Ceilometer Collection Interval  By default Ceilometer will collect CPU data from instances every 10 minutes. For this example we want to change that to 60 seconds. Change the interval to 60 in the pipeline.yaml file and restart OpenStack services.  #vi /etc/ceilometer/pipeline.yaml [code language=\"java\"]  - name: cpu_source  interval: 60  meters:  - &quot;cpu&quot;  sinks:  - cpu_sink  [/code]  #openstack-service restart Running Heat Stack  At this point we are ready to run our auto scaling Heat stack. The expected results should be that a single Cirros instance is launched. It should have private and floating IPs.  #heat stack-create example -f /root/example.yaml -e /root/environment.yaml  +--------------------------------------+------------+--------------------+----------------------+  | id | stack_name | stack_status | creation_time |  +--------------------------------------+------------+--------------------+----------------------+  | 6fca513c-25a1-4849-b7ab-909e37f52eca | example | CREATE_IN_PROGRESS | 2015-08-31T16:18:02Z |  +--------------------------------------+------------+--------------------+----------------------+ Heat will create the stack and launch the one cirros instance.  #nova list  +--------------------------------------+-------------------------------------------------------+--------+------------+-------------+--------------------------------------+  | ID | Name | Status | Task State | Power State | Networks |  +--------------------------------------+-------------------------------------------------------+--------+------------+-------------+--------------------------------------+  | 3f627c84-06aa-4782-8c12-29409964cc73 | ex-qeki-3azno6me5gvm-pqmr5zd6kuhm-server-gieck7uoyrwc | ACTIVE | - | Running | private=10.10.1.156, 192.168.122.234 |  +--------------------------------------+-------------------------------------------------------+--------+------------+-------------+--------------------------------------+ Heat will also create two cpu alarms which are used to trigger scale-up or scale-down events.  ceilometer alarm-list +--------------------------------------+-------------------------------------+-------------------+----------+---------+------------+--------------------------------+------------------+ | Alarm ID | Name | State | Severity | Enabled | Continuous | Alarm condition | Time constraints | +--------------------------------------+-------------------------------------+-------------------+----------+---------+------------+--------------------------------+------------------+ | 04b4f845-f5b6-4c5a-8af0-59e03c22e6fa | example-cpu_alarm_high-rd5kysmlahvx | ok | low | True | True | cpu_util &gt; 50.0 during 1 x 60s | None | | ac81cd81-20b3-45f9-bea4-e51f00499602 | example-cpu_alarm_low-6t65kswutupz | ok | low | True | True | cpu_util &lt; 10.0 during 1 x 60s | None | +--------------------------------------+-------------------------------------+-------------------+----------+---------+------------+--------------------------------+------------------+ Automatically Scaling Up  Heat will scale instances up based on the cpu_alarm_high threshold. Once CPU utilization is above 50% instances will be scaled up. In order to generate CPU load, log into the instance and run the \"dd\" command.  $ssh -i admin.pem cirros@192.168.122.232 $sudo -i #dd if=/dev/zero of=/dev/null &amp; #dd if=/dev/zero of=/dev/null &amp; #dd if=/dev/zero of=/dev/null &amp; Upon running \"dd\" commands we should have close to 100% CPU utilization in our cirros instance. After 60 seconds we should see that Heat has scaled and we have two instances.  #ceilometer alarm-list +--------------------------------------+-------------------------------------+-------+----------+---------+------------+--------------------------------+------------------+ | Alarm ID | Name | State | Severity | Enabled | Continuous | Alarm condition | Time constraints | +--------------------------------------+-------------------------------------+-------+----------+---------+------------+--------------------------------+------------------+ | 04b4f845-f5b6-4c5a-8af0-59e03c22e6fa | example-cpu_alarm_high-rd5kysmlahvx | ok | low | True | True | cpu_util &gt; 50.0 during 1 x 60s | None | | ac81cd81-20b3-45f9-bea4-e51f00499602 | example-cpu_alarm_low-6t65kswutupz | alarm | low | True | True | cpu_util &lt; 10.0 during 1 x 60s | None | +--------------------------------------+-------------------------------------+-------+----------+---------+------------+--------------------------------+------------------+ #nova list  +--------------------------------------+-------------------------------------------------------+--------+------------+-------------+--------------------------------------+  | ID | Name | Status | Task State | Power State | Networks |  +--------------------------------------+-------------------------------------------------------+--------+------------+-------------+--------------------------------------+  | 3f627c84-06aa-4782-8c12-29409964cc73 | ex-qeki-3azno6me5gvm-pqmr5zd6kuhm-server-gieck7uoyrwc | ACTIVE | - | Running | private=10.10.1.156, 192.168.122.234 |  | 0f69dfbe-4654-474f-9308-1b64de3f5c18 | ex-qeki-qmvor5rkptj7-krq7i66h6n7b-server-b4pk3dzjvbpi | ACTIVE | - | Running | private=10.10.1.157, 192.168.122.235 |  +--------------------------------------+-------------------------------------------------------+--------+------------+-------------+--------------------------------------+ After additional 60 seconds we should see that Heat has scaled again and we have three instances. Since three is the maximum for this configuration, we will not scale any higher.  #nova list  +--------------------------------------+-------------------------------------------------------+--------+------------+-------------+--------------------------------------+  | ID | Name | Status | Task State | Power State | Networks |  +--------------------------------------+-------------------------------------------------------+--------+------------+-------------+--------------------------------------+  | 3f627c84-06aa-4782-8c12-29409964cc73 | ex-qeki-3azno6me5gvm-pqmr5zd6kuhm-server-gieck7uoyrwc | ACTIVE | - | Running | private=10.10.1.156, 192.168.122.234 |  | 0e805e75-aa6f-4375-b057-2c173b68f172 | ex-qeki-gajdwmu2cgm2-vckf4g2gpwis-server-r3smbhtqij76 | ACTIVE | - | Running | private=10.10.1.158, 192.168.122.236 |  | 0f69dfbe-4654-474f-9308-1b64de3f5c18 | ex-qeki-qmvor5rkptj7-krq7i66h6n7b-server-b4pk3dzjvbpi | ACTIVE | - | Running | private=10.10.1.157, 192.168.122.235 |  +--------------------------------------+-------------------------------------------------------+--------+------------+-------------+--------------------------------------+ Automatically Scaling Down  Heat will scale instances down based on the cpu_alarm_low threshold. Once CPU utilization is below 10% instances will be scaled down. We can simply kill the \"dd\" processes and watch Heat scale instances back down.  After stopping \"\"dd\" processes we should see that the cpu_alarm_low event is triggered. This will cause Heat to scale down and remove instance.  #ceilometer alarm-list +--------------------------------------+-------------------------------------+-------+----------+---------+------------+--------------------------------+------------------+ | Alarm ID | Name | State | Severity | Enabled | Continuous | Alarm condition | Time constraints | +--------------------------------------+-------------------------------------+-------+----------+---------+------------+--------------------------------+------------------+ | 04b4f845-f5b6-4c5a-8af0-59e03c22e6fa | example-cpu_alarm_high-rd5kysmlahvx | ok | low | True | True | cpu_util &gt; 50.0 during 1 x 60s | None | | ac81cd81-20b3-45f9-bea4-e51f00499602 | example-cpu_alarm_low-6t65kswutupz | alarm | low | True | True | cpu_util &lt; 10.0 during 1 x 60s | None | +--------------------------------------+-------------------------------------+-------+----------+---------+------------+--------------------------------+------------------+ After a few minutes we should be back to a single instance.  Summary  In this article we talked about how Heat and Ceilometer work together within OpenStack, providing the brains behind your cloud. We looked at a typical cloud use case around auto scaling and how to configure auto scaling through Heat. Hopefully you found this article informative. As always any feedback is greatly appreciated and as we say at Red Hat, sharing is caring.  Happy Auto Scaling!  (c) 2015 Keith Tenzer  ","categories": ["OpenStack"],
        "tags": ["Auto-Scaling","Ceilometer","cloud automation","Heat","Linux","OpenStack"],
        "url": "/openstack/auto-scaling-instances-with-openstack/",
        "teaser": null
      },{
        "title": "Cloud Systems Management: Satellite 6.1 Getting Started Guide",
        "excerpt":"  Overview  Satellite is responsible for system and configuration management. Satellite 6 offers a new architecture based off best-of-breed opensource projects: Foreman (provisioning), Katello (content management), Pulp (content management), Puppet (configuration management) and Candlepin (subscription management). Satellite 6 provides ITIL equivalent of media library for RPMs, ISOs, Puppet modules and container images. It allows you to build a standard operating environment (SOE) that defines how systems and applications are provisioned as well as maintained. Satellite 6 allows an organization the ability to manage all content through defined life-cycles. Control through RBAC can be defined so that different groups can control various aspects of a SOE. In this article we will focus on how to install and configure Satellite 6.1.    Installing Satellite 6.1  Before installing Satellite 6.1 it is recommend to install RHEL 7.1 minimum OS. In this guide we will configure a Satellite 6.1 server and capsule on the same system.  #subscription-manager register #subscription-manager list --available #subscription-manager attach --pool=8a85f9844b5685ed014b6e02a0c3144e #subscription-manager repos --disable=* #subscription-manager repos --enable rhel-7-server-rpms --enable rhel-server-rhscl-7-rpms --enable rhel-7-server-satellite-6.1-rpms Update all packages and install Katello.  #yum update -y #yum install -y katello Run the Katello installer and provide information about the environment. In this environment the Satellite 6.1 server is located on 192.168.122.0/24 network and has static hostname sat6.lab.com. In order to provision new servers Satellite requires DNS and DHCP. You can use external DNS and DHCP but in this example we are having Satellite configure both locally.  katello-installer -v -d --foreman-admin-password redhat \\ --capsule-dns true --capsule-dns-interface eth0 --capsule-dns-zone lab.com \\ --capsule-dns-forwarders 192.168.122.1 --capsule-dns-reverse 122.168.192.in-addr.arpa \\ --capsule-dhcp true --capsule-dhcp-interface eth0 --capsule-dhcp-range \"192.168.122.200 192.168.122.250\" \\ --capsule-dhcp-gateway 192.168.122.1 --capsule-dhcp-nameservers 192.168.122.70 \\ --capsule-tftp true --capsule-tftp-servername $(hostname) \\ --capsule-puppet true --capsule-puppetca true Configuring Satellite 6.1  The first step in configuration is to open the necessary firewall ports.  #firewall-cmd --permanent --direct --add-rule ipv4 filter OUTPUT 0 -o lo -p tcp -m tcp --dport 9200 -m owner --uid-owner foreman -j ACCEPT \\ &amp;&amp; firewall-cmd --permanent --direct --add-rule ipv6 filter OUTPUT 0 -o lo -p tcp -m tcp --dport 9200 -m owner --uid-owner foreman -j ACCEPT \\ &amp;&amp; firewall-cmd --permanent --direct --add-rule ipv4 filter OUTPUT 0 -o lo -p tcp -m tcp --dport 9200 -m owner --uid-owner foreman -j ACCEPT \\ &amp;&amp; firewall-cmd --permanent --direct --add-rule ipv6 filter OUTPUT 0 -o lo -p tcp -m tcp --dport 9200 -m owner --uid-owner foreman -j ACCEPT \\ &amp;&amp; firewall-cmd --permanent --direct --add-rule ipv4 filter OUTPUT 0 -o lo -p tcp -m tcp --dport 9200 -m owner --uid-owner root -j ACCEPT \\ &amp;&amp; firewall-cmd --permanent --direct --add-rule ipv6 filter OUTPUT 0 -o lo -p tcp -m tcp --dport 9200 -m owner --uid-owner root -j ACCEPT \\ &amp;&amp; firewall-cmd --permanent --direct --add-rule ipv4 filter OUTPUT 1 -o lo -p tcp -m tcp --dport 9200 -j DROP \\ &amp;&amp; firewall-cmd --permanent --direct --add-rule ipv6 filter OUTPUT 1 -o lo -p tcp -m tcp --dport 9200 -j DROP Allow HTTPS access from the web UI.  #firewall-cmd --permanent --add-service=https Reload the firewall rules so they are active.  # firewall-cmd --reload At this point you should be able to reach the web UI using HTTPS. In this environment the url is https://sat6.lab.com. Next we need to setup the hammer CLI. Configure hammer so that we automatically pass authentication credentials.  mkdir ~/.hammer cat &gt; .hammer/cli_config.yml &lt;&lt;EOF :foreman:     :host: 'https://sat-$GUID.rhpds.opentlc.com/'     :username: 'admin'     :password: 'redhat'  EOF Register Satellite Server in Red Hat Network (RHN).    Assign subscriptions to the Satellite server and download manifest from RHN.    Upload manifest file to Satellite server.  #hammer subscription upload --organization \"Default Organization\" --file /root/Satellite_Manifest.zip Note: In the next section we will be using the hammer CLI to configure Satellite. In this environment we are using the organization \"Default Organization\", you would probably change this to a more specific organization name. If so you need to first create a new organization.  Update the domain to lab.com. In this environment Satellite server and hosts are part of same domain.  #hammer domain update --name lab.com --organizations \"Default Organization\" --dns sat6.lab.com --locations 'Default Location Create a subnet in Satellite 6. In this environment the subnet is 192.168.122.0/24.  hammer subnet update --name VLAN_122 --dhcp-id 1 --dns-id 1 --tftp-id 1 --organizations \"Default Organization\" --domains lab.com --locations 'Default Location' Enable basic repositories.  #hammer repository-set enable --organization \"Default Organization\" --product 'Red Hat Enterprise Linux Server' --basearch='x86_64' --releasever='7Server' --name 'Red Hat Enterprise Linux 7 Server (RPMs)' #hammer repository-set enable --organization \"Default Organization\" --product 'Red Hat Enterprise Linux Server' --basearch='x86_64' --releasever='7Server' --name 'Red Hat Enterprise Linux 7 Server - Optional (RPMs)' #hammer repository-set enable --organization \"Default Organization\" --product 'Red Hat Enterprise Linux Server' --basearch='x86_64' --releasever='7.1' --name 'Red Hat Enterprise Linux 7 Server (Kickstart)' #hammer repository-set enable --organization \"Default Organization\" --product 'Red Hat Enterprise Linux Server' --basearch='x86_64' --name 'Red Hat Satellite Tools 6.1 (for RHEL 7 Server) (RPMs)' Enable EPEL repository for 3rd party packages.  #wget -q https://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-7  -O /root/RPM-GPG-KEY-EPEL-7 #hammer gpg create --key /root/RPM-GPG-KEY-EPEL-6  --name 'GPG-EPEL-6' --organization \"Default Organization\" #hammer gpg create --key /root/RPM-GPG-KEY-EPEL-7  --name 'GPG-EPEL-7' --organization \"Default Organization\" Create a new product for the EPEL repository. In Satellite 6 products are a groupings of external content outside of RHN. Products can contain RPM repositories, Puppet modules or container images.  #hammer product create --name='EPEL 3rd Party Packages' --organization \"Default Organization\" --description 'EPEL 3rd Party Packages' #hammer repository create --name='EPEL 7 - x86_64' --organization \"Default Organization\" --product='EPEL 3rd Party Packages' --content-type='yum' --publish-via-http=true --url=http://dl.fedoraproject.org/pub/epel/7/x86_64/ --checksum-type=sha256 --gpg-key=GPG-EPEL-7  Synchronize the repositories. This will take a while as all of the RPM packages will be downloaded. Note: you can also use the --async option to run tasks in parallel.  #hammer repository synchronize --organization \"Default Organization\" --product 'Red Hat Enterprise Linux Server'  --name 'Red Hat Enterprise Linux 7 Server Kickstart x86_64 7.1' #hammer repository synchronize --organization \"Default Organization\" --product 'Red Hat Enterprise Linux Server'  --name 'Red Hat Enterprise Linux 7 Server - Optional RPMs x86_64 7Server' #hammer repository synchronize --organization \"Default Organization\" --product 'Red Hat Enterprise Linux Server'  --name 'Red Hat Satellite Tools 6.1 for RHEL 7 Server RPMs x86_64' #hammer repository synchronize --organization \"Default Organization\" --product 'Red Hat Enterprise Linux Server'  --name 'Red Hat Enterprise Linux 7 Server RPMs x86_64 7Server' #hammer repository synchronize --organization \"$ORG\" --product 'EPEL 3rd Party Packages  --name  'EPEL 7 - x86_64' Create life cycles for development and production  #hammer lifecycle-environment create --organization \"Default Organization\" --description 'Development' --name 'DEV' --label development --prior Library #hammer lifecycle-environment create --organization \"Default Organization\" --description 'Production' --name 'PROD' --label production --prior 'DEV' Create content view for RHEL 7 base.  #hammer content-view create --organization \"Default Organization\" --name 'RHEL7_base' --label rhel7_base --description 'Core Build for RHEL 7' #hammer content-view add-repository --organization \"$ORG\" --name 'RHEL7_Base' --product 'Red Hat Enterprise Linux Server' --repository 'Red Hat Enterprise Linux 7 Server RPMs x86_64 7Server' #hammer content-view add-repository --organization \"$ORG\" --name 'RHEL7_Base' --product 'Red Hat Enterprise Linux Server' --repository 'Red Hat Enterprise Linux 7 Server - Optional RPMs x86_64 7Server' #hammer content-view add-repository --organization \"$ORG\" --name 'RHEL7_Base' --product 'Red Hat Enterprise Linux Server' --repository 'Red Hat Satellite Tools 6.1 for RHEL 7 Server RPMs x86_64' #hammer content-view add-repository --organization \"$ORG\" --name 'RHEL7_Base' --product 'EPEL 3rd Party Packages'  --repository  'EPEL 7 - x86_64' Publish and promote content view to the environments.  #hammer content-view publish --organization \"Default Organization\" --name RHEL7_Base --description 'Initial Publishing' #hammer content-view version promote --organization \"Default Organization\" --content-view RHEL7_Base --to-lifecycle-environment DEV #hammer content-view version promote --organization \"Default Organization\" --content-view RHEL7_Base --to-lifecycle-environment PROD Add activation keys.  #RHEL_SUB_ID=$(hammer --csv --csv-separator ':' subscription list --organization \"Default Organization\" | grep 'Red Hat Enterprise Linux Server Entry Level, Self-support' | cut -f 8 -d ':') #EPEL_SUB_ID=$(hammer --csv --csv-separator ':' subscription list --organization \"Default Organization\" | grep 'EPEL 3rd Party Packages' | cut -f 8 -d ':') #hammer activation-key create --organization \"Default Organization\" --description 'RHEL7 Key for DEV' --content-view 'RHEL7_Base' --unlimited-content-hosts yes --name ak-Reg_To_DEV --lifecycle-environment 'DEV' #hammer activation-key add-subscription --organization \"Default Organization\" --name ak-Reg_To_DEV--subscription-id $RHEL_SUB_ID #hammer activation-key add-subscription --organization \"Default Organization\" --name ak-Reg_To_DEV --subscription-id $EPEL_SUB_ID #hammer activation-key create --organization \"Default Organization\" --description 'RHEL7 Key for PROD' --content-view 'RHEL7_Base' --unlimited-content-hosts yes --name ak-Reg_To_PROD --lifecycle-environment 'PROD' #hammer activation-key add-subscription --organization \"Default Organization\" --name ak-Reg_To_PROD--subscription-id $RHEL_SUB_ID #hammer activation-key add-subscription --organization \"Default Organization\" --name ak-Reg_To_PROD --subscription-id $EPEL_SUB_ID Create a host group. A host group is a foreman construct and is used for automation of provisioning parameters. A host is provisioned based on its host group. The host group contains kickstart/provisioning templates, OS information, network information, activation keys, parameters, puppet environment and if virtual a compute profile. Note: you will need to change the hostname sat6.lab.com.  #hammer hostgroup create --architecture x86_64 --content-source-id 1 --content-view RHEL7_Base --domain lab.com --lifecycle-environment DEV --locations 'Default Location' --name RHEL7_DEV_Servers --organizations \"Default Organization\" --puppet-ca-proxy sat6.lab.com --puppet-proxy sat6.lab.com --subnet VLAN_122 --partition-table 'Kickstart default' --operatingsystem 'RedHat 6.6' --puppet-classes \"motd\" --medium \"$ORG\"/Library/Red_Hat_Server/Red_Hat_Enterprise_Linux_7_Server_Kickstart_x86_64_7_1 Add a compute host. Satellite 6 supports provisioning from bare metal or compute hosts. You can provision to VMware, RHEV / KVM, OpenStack or even Amazon EC2. In this environment we have configured KVM. KVM is running on system with IP 192.168.122.1.  On KVM host perform following steps.  #vi /etc/libvirt/libvirtd.conf listen_tls = 0 listen_tcp = 1 tcp_port = \"16509\" auth_tcp = \"none\" #vi /etc/sysconfig/libvirtd LIBVIRTD_ARGS=\"--listen\" Allow libvirtd communications on KVM host.  firewall-cmd --permanent --add-port=\"16509/tcp\" firewall-cmd --reload Add compute host on Satellite server.  #hammer compute-resource create --description 'LibVirt Compute Resource' --locations 'Default Location' --name Libvirt_CR --organizations \"Default Organization\" --url 'qemu+tcp://192.168.122.1/system/' --provider libvirt --set-console-password 0 Satellite Upgrades  In order to properly upgrade a Satellite environment the following steps should be taken.  #foreman-rake katello:reindex #yum update #katello-installer --upgrade Adding existing hosts  In order to add an existing host or new host without provisioning fun the following commands on a RHEL system.  Install Katello package from Satellite server.  #rpm -Uvh http://sat6.lab.com/pub/katello-ca-consumer-latest.noarch.rpm  Subscribe using activation key.  #subscription-manager register --org=\"Default_Organization\" --activationkey=\"DEV_CORE\"  #yum -y install --enablerepo rhel-7-server-satellite-tools-6.1-rpms katello-agent Configure Red Hat Access Insight  Red Hat insight will inform not only about security updates but also potential configuration issues according to Red Hat best practices. To configure follow below steps.  Update python-requests if not present install  yum update python-requests    Install Red Hat Access Insight  yum install redhat-access-insights Register System  redhat-access-insights --register HTTPS 404 Errors  Sometimes the yum cache gets out-of-sync. If this happens you will need to delete the cache and then try installing package again.  rm -fr /var/cache/yum/* Summary  Satellite 6.1 is a system and configuration management tool built on best-of-breed opensource technologies. There are huge advantages in regards to Standard Operating Environments (SOE) in bringing systems and configuration management capabilities together. In this article we focused on installing and configuring Satellite 6.1. We created a basic environment but this is just scratching the surface. Satellite is a tool for implementing a SOE and ITIL processes. For more information about building an SOE with Satellite 6.1, read the 10 Steps to build an SOE. I hope you found this article informative and useful. Please feel free to share your feedback.  Happy Satelliting!  (c) 2015 Keith Tenzer  ","categories": ["Satellite"],
        "tags": ["configuration management","Foreman","Katello","Puppet","Satellite 6","system management"],
        "url": "/satellite/cloud-systems-management-satellite-6-1-getting-started-guide/",
        "teaser": null
      },{
        "title": "OpenStack Tips and Tricks",
        "excerpt":"  Overview  In this article we will look at some common OpenStack Kilo configuration optimizations and other tricks. This is by no means a comprehensive guide, just things I have stumbled across that if documented would have saved me time. I continue to update this blog with things I learned. If you have some valuable tips or tricks then let me know so I can add those?    Nested Virtualization  Many run OpenStack on KVM for testing, learning, training or even demos. In order to get acceptable performance, the Hypervisor and guest must be configured to support Nested Virtualization.  Ensure KVM is enabled  #lsmod | grep -i kvm kvm_intel 148081 6  kvm 461126 1 kvm_intel #modinfo kvm_intel | grep -i nested parm: nested:bool   Unload KVM kernel module  #modprobe -r kvm_intel Enable nested virtualization in KVM hypervisor  #modprobe kvm_intel nested=Yes Create a guest for running OpenStack and edit the configuration to enable VMX.  #virsh edit osp7.lab.com  &lt;cpu mode='custom' match='exact'&gt;  &lt;model fallback='allow'&gt;SandyBridge&lt;/model&gt;  &lt;feature policy='require' name='vmx'/&gt;  &lt;/cpu&gt; Start guest and verify that nested virtualization support is enabled.  #ps -ef |grep qemu-kvm |grep vmx qemu 8557 1 29 09:04 ? 00:03:52 /usr/libexec/qemu-kvm -name osp7.lab.com -S -machine pc-i440fx-rhel7.0.0,accel=kvm,usb=off -cpu SandyBridge,+vmx Change Libvirt type to KVM in Nova.  #vi /etc/nova/nova.conf virt_type=kvm Fixing OpenStack Inconsistencies  Sometimes things just don't work as expected, that is life and software. In OpenStack sometimes objects in the database and the actual resource can be inconsistent. This means the resource exists in the database but not anywhere else. I have seen this happen with Cinder volumes when deleting Heat stacks for example. If this occurs, your last resort if the *force* delete commands fail is to go into database and remove resource. It goes with out saying that you need to use extreme caution, as you can cause data loss or even corruption within OpenStack.  Delete Cinder Volume  Sometimes it can take a really long time to delete cinder volumes. The default behavior is to zero blocks. You can change this by setting volume_clear=none in /etc/cinder/cinder.conf. If you want to determine why delete is taking so long you should see what processes are using the volume. For LVM backends you can follow these steps. First check lvdisplay to see if logical volume exists.  # lvdisplay Get the major and minor number for volume. # dmsetup info -c Name Maj Min Stat Open Targ Event UUID rhel-swap 253 0 L--w 2 1 0 LVM-I057BifDXT5pFxJ69IQuaLXouyIN6DDbltQCdDpuXeoSi3tFgBpYFQiETsCKO3CG rhel-root 253 1 L--w 1 1 0 LVM-I057BifDXT5pFxJ69IQuaLXouyIN6DDbSP3WbTBmapCefb1mQLbdSzqw8drUculQ cinder--volumes-volume--a052879b--9bbd--4285--8557--7c16337560c5 253 2 L--w 1 1 0 LVM-nnajQLl8dA7KqOsFJgYajjFVQrkM0wjfcB9fNeV0PL1R9TuuJIX3dNHqYbfmBvL7   Using lsof check processes running on volumes  # lsof | grep \"253,2\" dd 5526 root 1w BLK 253,2 0x35f600000 15042 /dev/dm-2 If processes are running (in this case dd), kill them and remove volume using lvremove. Once this is complete set volume status to available in cinder.   cinder reset-state --state available &lt;volume id&gt; Finally try deleting the volume. If all else fails you can go into database as last resort and delete things there.  #mysql cinder &gt; update volumes set deleted=1,status='deleted',deleted_at=now(),updated_at=now() where deleted=0 and id='$volume_uuid'; Detach a Volume from Cinder  #mysql nova &gt; delete from block_device_mapping where not deleted and volume_id='$volume_uuid' and project_id='$project_uuid'; Delete an Instance  #mysql nova_db &gt; update instances set deleted='1', vm_state='deleted', deleted_at='now()'' where uuid='$vm_uuid' and project_id='$project_uuid'; Change provision state of Ironic nodes  # mysql ironic &gt; UPDATE nodes SET provision_state=\"available\", target_provision_state=NULL, reservation=NULL WHERE uuid=&lt;uuid&gt;; Delete Ironic nodes  # mysql ironic &gt; delete from ports where uuid=\"0867df16-82c9-4358-9bc9-a36933c361e1\"; &gt; delete from nodes where uuid=\"92b6477c-d556-4958-9950-5c11ca57e459\"; Fixing Horizon Re-login issue  There is an issue in OpenStack Kilo with re-login because of bad cookie session. Here is how to fix the issue.  #vi /etc/openstack-dashboard/local_settings AUTH_USER_MODEL = 'openstack_auth.User' Heat Topology Images Broken  service openstack-cinder-volume restart There is an issue in OpenStack Kilo with the Heat topology images being broken. Here is how to fix it.  #vi /etc/httpd/conf.d/openstack-dashboard.conf Alias /static/dashboard /usr/share/openstack-dashboard/static/dashboard systemctl restart httpd Adding Cinder Volume for LVM backend  By default RDO will use a loopback device for the Cinder LVM backend. In order to change this you can follow the procedure below assuming disk is called /dev/vdb1.  #openstack-config --set /etc/cinder/cinder.conf DEFAULT lvm_type thin #systemctl restart openstack-cinder-volume #fdisk /dev/vdb1 #fdisk /dev/vdb1 #pvcreate /dev/vdb1 #vgcreate cinder_storage /dev/vdb1 #vgcreate cinder_storage /dev/vdb1 #vi /etc/cinder/cinder.conf [lvm] volume_group=my_new_cinder_storage volume_driver=cinder.volume.drivers.lvm.LVMVolumeDriver Force Deleting Keystone Endpoints  # mysql keystone MariaDB [keystone]&gt; delete from endpoint where id=\"07d77cefad0049b1ae5e1eb6692ebfa1\"; Adding NFS as Cinder Backend  Cinder can use many different backends and using an NFS backend provides a lot of flexibility in addition to removing compülexity with ISCSI/LVM.  If SELinux is enabled allow NFS access  setsebool -P virt_use_nfs on Create map file to make Cinder aware of NFS shares  #vi /etc/cinder/nfs_share 192.168.0.22:/usr/share/openstack chown root:cinder /etc/cinder/nfs_share chmod 0640 /etc/cinder/nfs_share Configure NFS backend in Cinder  openstack-config --set /etc/cinder/cinder.conf nfs nfs_shares_config /etc/cinder/nfs_share openstack-config --set /etc/cinder/cinder.conf nfs volume_driver cinder.volume.drivers.nfs.NfsDriver openstack-config --set /etc/cinder/cinder.conf nfs volume_backend_name nfsbackend Optionally you can add any required mount options  openstack-config --set /etc/cinder/cinder.conf nfs nfs_mount_options MOUNTOPTIONS # vi /etc/cinder/cinder.conf enabled_backends = lvm, nfs Restart Cinder volume service  openstack-service restart cinder-volume Configure NFS volume type so that is uses the correct backend in Cinder  cinder type-create nfstype  cinder type-key nfstype set volume_backend_name=nfsbackend Configuring RHEV for OpenStack  If you are using RHEV or any virtualization platform under OpenStack then you need to enable nested virtualization and ensure MAC Address Spoofing filters are disabled. Otherwise since OpenStack instance MAC address differs from that of the virtual machine packets will be dropped.  On RHEV-M  #engine-config -s \"UserDefinedVMProperties=macspoof=(true|false)\" #service ovirt-engine restart Edit VM and enable macspoof by setting parameter to 'true'    On Hypervisor Hosts  #yum install -y vdsm-hook-macspoof #wget http://mirrors.ibiblio.org/ovirt/pub/ovirt-3.5/rpm/el7/noarch/vdsm-hook-nestedvt-4.16.30-0.el7.centos.noarch.rpm #rpm -Uvh vdsm-hook-nestedvt-4.16.30-0.el7.centos.noarch.rpm #systemctl reboot On OpenStack Instance check to ensure nested virtualization active  #egrep 'svm|vmx' /proc/pcuinfo Remove Packstack (RDO)  If you want to upgrade or change OpenStack deployment and you are using RDO it may be necessary to remove the installation to start cleanly. The below process can be used to accomplish that.  Delete any VMs that may be running or configured  for x in $(virsh list --all | grep instance- | awk '{print $2}') ; do     virsh destroy $x ;     virsh undefine $x ; done ; Reconfigure network interfaces  Hopefully you saved your original network configuration. You need to replace /etc/sysconfig/network-scripts/ifcfg-* with your original configs or just set IP addresses on those interfaces.  #cp /root/ifcfg-eth0 /etc/sysconfig/network-scripts #rm /etc/sysconfig/network-scripts/ifcfg-br-ex Remove packages  yum remove -y nrpe openvswitch \"*nagios*\" puppet ntp ntp-perl ntpdate \"*openstack*\" \\ \"*nova*\" \"*keystone*\" \"*glance*\" \"*cinder*\" \"*swift*\" \\ mysql mysql-server httpd \"*memcache*\" scsi-target-utils \\ iscsi-initiator-utils perl-DBI perl-DBD-MySQL ; Ensure swift processes arent running  ps -ef | grep -i repli | grep swift | awk '{print $2}' | xargs kill ; Remove configuration data. Note if you are using NFS backend you need to unmount it.  rm -rf /etc/nagios /etc/yum.repos.d/packstack_* /root/.my.cnf \\ /var/lib/mysql/ /var/lib/glance /var/lib/nova /etc/nova /etc/swift \\ /srv/node/device*/* /var/lib/cinder/ /etc/rsync.d/frag* \\ /var/cache/swift /var/log/keystone ; Remove LVM volume  vgremove -f cinder-volumes ; Delete SSL certs  find /etc/pki/tls -name \"ssl_ps*\" | xargs rm -rf ; Unmount any leftover mounts  for x in $(df | grep \"/lib/\" | sed -e 's/.* //g') ; do     umount $x ; done #systemctl reboot Summary  This was a quick article focused on tips and tricks around OpenStack Kilo. I will continue to update this article with new tips and tricks. If you have anything you came across in OpenStack Kilo, please share.  Happy OpenStacking!  (c) 2015 Keith Tenzer  ","categories": ["OpenStack"],
        "tags": ["Kilo","KVM","OpenStack","RDO"],
        "url": "/openstack/openstack-tips-and-tricks/",
        "teaser": null
      },{
        "title": "Auto Scaling Applications with OpenStack Heat",
        "excerpt":"  Overview  In this article we will look at how to build a auto scaling application in OpenStack using Heat. This article builds on the following previous articles:  OpenStack Kilo Setup and Configuration  Auto Scaling Instances with OpenStack    As discussed in previous articles, Heat is the orchestration framework that not only automates provisioning but also provides policies around auto scaling. This article will build on previous article that showed how to automatically scale up or down instances. Here we will take things one step further and scale-up as well as scale-down a simple PHP web application. In addition to using Ceilometer to determine when a application should be scaled based on CPU load, Neutron will be used to provide not only networking but also Load Balancer As-a-Service (LBAAS). While you can of course use an external load balancer, out-of-the-box OpenStack uses ha-proxy.  You can get access to Heat templates below or directly from GitHub: https://github.com/ktenzer/openstack-heat-templates.  Update Ceilometer Collection Interval  By default Ceilometer will collect CPU data from instances every 10 minutes. For this example we want to change that to 60 seconds. Change the interval to 60 in the pipeline.yaml file and restart OpenStack services.  #vi /etc/ceilometer/pipeline.yaml [code language=\"java\"]  - name: cpu_source  interval: 60  meters:  - &amp;amp;amp;quot;cpu&amp;amp;amp;quot;  sinks:  - cpu_sink  [/code]  #openstack-service restart Heat Stack Environment  The Heat stack environment describes the unit we are dealing with and the unit of scale. The environment usually contains one or more instances and their dependencies. In this case the environment is a single instance that is a member of a load balancer. Metadata is used to create an association between all instances that are part of the stack template. This is important for metering and determining scaling events.  #vi /etc/heat/templates/lb-env.yaml heat_template_version: 2014-10-16 description: A load-balancer server parameters:   image:     type: string     description: Image used for servers   key_name:     type: string     description: SSH key to connect to the servers   flavor:     type: string     description: flavor used by the servers   pool_id:     type: string     description: Pool to contact   user_data:     type: string     description: Server user_data   metadata:     type: json   network:     type: string     description: Network used by the server  resources:   server:     type: OS::Nova::Server     properties:       flavor: {get_param: flavor}       image: {get_param: image}       key_name: {get_param: key_name}       metadata: {get_param: metadata}       user_data: {get_param: user_data}       networks:         - port: { get_resource: port }    member:     type: OS::Neutron::PoolMember     properties:       pool_id: {get_param: pool_id}       address: {get_attr: [server, first_address]}       protocol_port: 80    port:     type: OS::Neutron::Port     properties:       network: {get_param: network}       security_groups:         - base  outputs:   server_ip:     description: IP Address of the load-balanced server.     value: { get_attr: [server, first_address] }   lb_member:     description: LB member details.     value: { get_attr: [member, show] }  Heat Stack Template  The below heat stack template is used to create the Heat environment dependencies and determine auto scaling policies. In this example we are creating a load balancer and using already existing networks for tenant as well as Floating IP. The scale up and down policies are triggered by Ceilometer events based on CPU utilization. You will need to replace the information under parameters with your own information from your environment.  #vi /root/lb-webserver-fedora.yaml heat_template_version: 2014-10-16 description: AutoScaling Fedora 22 Web Application parameters:   image:     type: string     description: Image used for servers     default: Fedora 22   key_name:     type: string     description: SSH key to connect to the servers     default: admin   flavor:     type: string     description: flavor used by the web servers     default: m2.tiny   network:     type: string     description: Network used by the server     default: private   subnet_id:     type: string     description: subnet on which the load balancer will be located     default: 9daa6b7d-e647-482a-b387-dd5f855b88ef   external_network_id:     type: string     description: UUID of a Neutron external network     default: db17c885-77fa-45e8-8647-dbb132517960  resources:   webserver:     type: OS::Heat::AutoScalingGroup     properties:       min_size: 1       max_size: 3       cooldown: 60       desired_capacity: 1       resource:         type: file:///etc/heat/templates/lb-env.yaml         properties:           flavor: {get_param: flavor}           image: {get_param: image}           key_name: {get_param: key_name}           network: {get_param: network}           pool_id: {get_resource: pool}           metadata: {\"metering.stack\": {get_param: \"OS::stack_id\"}}           user_data:             str_replace:               template: |                 #!/bin/bash -v                 yum -y install httpd php                 systemctl enable httpd                 systemctl start httpd                 cat &lt; /var/www/html/hostname.php                                  EOF               params:                 hostip: 192.168.122.70                 fqdn: sat6.lab.com                 shortname: sat6    web_server_scaleup_policy:     type: OS::Heat::ScalingPolicy     properties:       adjustment_type: change_in_capacity       auto_scaling_group_id: {get_resource: webserver}       cooldown: 60       scaling_adjustment: 1    web_server_scaledown_policy:     type: OS::Heat::ScalingPolicy     properties:       adjustment_type: change_in_capacity       auto_scaling_group_id: {get_resource: webserver}       cooldown: 60       scaling_adjustment: -1    cpu_alarm_high:     type: OS::Ceilometer::Alarm     properties:       description: Scale-up if the average CPU &gt; 95% for 1 minute       meter_name: cpu_util       statistic: avg       period: 60       evaluation_periods: 1       threshold: 95       alarm_actions:         - {get_attr: [web_server_scaleup_policy, alarm_url]}       matching_metadata: {'metadata.user_metadata.stack': {get_param: \"OS::stack_id\"}}       comparison_operator: gt    cpu_alarm_low:     type: OS::Ceilometer::Alarm     properties:       description: Scale-down if the average CPU &lt; 15% for 60 minutes       meter_name: cpu_util       statistic: avg       period: 60       evaluation_periods: 1       threshold: 15       alarm_actions:         - {get_attr: [web_server_scaledown_policy, alarm_url]}       matching_metadata: {'metadata.user_metadata.stack': {get_param: \"OS::stack_id\"}}       comparison_operator: lt    monitor:     type: OS::Neutron::HealthMonitor     properties:       type: TCP       delay: 5       max_retries: 5       timeout: 5    pool:     type: OS::Neutron::Pool     properties:       protocol: HTTP       monitors: [{get_resource: monitor}]       subnet_id: {get_param: subnet_id}       lb_method: ROUND_ROBIN       vip:         protocol_port: 80    lb:     type: OS::Neutron::LoadBalancer     properties:       protocol_port: 80       pool_id: {get_resource: pool}    lb_floating:     type: OS::Neutron::FloatingIP     properties:       floating_network_id: {get_param: external_network_id}       port_id: {get_attr: [pool, vip, port_id]}  outputs:   scale_up_url:     description: &gt;       This URL is the webhook to scale up the autoscaling group.  You       can invoke the scale-up operation by doing an HTTP POST to this       URL; no body nor extra headers are needed.     value: {get_attr: [web_server_scaleup_policy, alarm_url]}   scale_dn_url:     description: &gt;       This URL is the webhook to scale down the autoscaling group.       You can invoke the scale-down operation by doing an HTTP POST to       this URL; no body nor extra headers are needed.     value: {get_attr: [web_server_scaledown_policy, alarm_url]}   pool_ip_address:     value: {get_attr: [pool, vip, address]}     description: The IP address of the load balancing pool   website_url:     value:       str_replace:         template: http://serviceip/hostname.php         params:           serviceip: { get_attr: [lb_floating, floating_ip_address] }     description: &gt;       This URL is the \"external\" URL that can be used to access the       website.   ceilometer_query:     value:       str_replace:         template: &gt;           ceilometer statistics -m cpu_util           -q metadata.user_metadata.stack=stackval -p 600 -a avg         params:           stackval: { get_param: \"OS::stack_id\" }     description: &gt;       This is a Ceilometer query for statistics on the cpu_util meter       Samples about OS::Nova::Server instances in this stack.  The -q       parameter selects Samples according to the subject's metadata.       When a VM's metadata includes an item of the form metering.X=Y,       the corresponding Ceilometer resource has a metadata item of the       form user_metadata.X=Y and samples about resources so tagged can       be queried with a Ceilometer query term of the form       metadata.user_metadata.X=Y.  In this case the nested stacks give       their VMs metadata that is passed as a nested stack parameter,       and this stack passes a metadata of the form metering.stack=Y,       where Y is this stack's ID.  Running Heat Stack  Once both the environment and stack yaml files exist we can run the Heat stack. Make sure you also download Fedora 22 cloud image to Glance. Using the CLI we can run the following commands:  #. /root/keystonerc_admin #heat stack-create webfarm -f /root/lb-webserver-stack.yaml You can monitor the Heat stack creation in Horizon under \"Orchestration-&gt;Stacks-&gt;Webfarm\". Horizon provides a very nice Heat stack topology view, where we can see how the environment dependencies fit together and if the deployment of the Heat stack was successful.    Once the Heat stack has been created we can view the outputs. These provide information on how to interact with the stack. In this case we are showing endpoints for triggering manual scale-up or scale-down events using REST endpoints, the Floating IP (this is IP we can use to access website) used by the Load Balancer and the Ceilometer command for getting the CPU utilization of the entire stack. This is useful for determining if the stack is scaling properly.    Looking in Horizon under \"Network-&gt;Load Balancers\" should reveal the Load Balancer.    Once the webserver is running the instance should be listed as an active member of the Load Balancer.    We should now be able to access the website URL listed in the Heat Stack Output: http://192.168.122.179/hostname.php.    Finally we can view the CPU performance data of the entire Heat stack using the Ceilometer command displayed in Heat stack output. Note the metadata associated to each instance that is part of the Heat stack template.  #ceilometer statistics -m cpu_util -q metadata.user_metadata.stack=8f86c3d5-15cf-4a64-b9e8-70215498c046 -p 600 -a avg Application Auto Scaling  There are two ways we can scale the application up. We can either generate sustained 95% CPU utilization or use the REST scale_up_url. In Mozilla you can install \"REST Easy\" plugin in order to do simple REST operations through the web browser. To use the Heat REST hooks we need to do a post using REST Easy.    Once the REST call returns a 200, the HTTP status code for successful, we should be able to see the event in Horizon under \"Orchestration-&gt;Stacks-&gt;Webfarm-&gt;Events\".    The scale up event will create an additional webserver and add it to the Load Balancer. This takes a few minutes as the instance needs to be stated and the HTTP server must be installed as well as configured. Once complete, the instance will go to active. You will see that the website URL gets routed through the Load Balancer, to all the webservers in the farm.    One thing to be aware of is that the application with scale down automatically if CPU utilization dips below 15% for 60 seconds. You may want to alter thresholds in the Heat stack template to suit your purposes. In addition you can generate load manually by allocating a Floating IP to one of the instances and following these steps:  #ssh -i admin.key fedora@192.168.122.152 $sudo -i #dd if=/dev/zero of=/dev/null &amp; #dd if=/dev/zero of=/dev/null &amp; #dd if=/dev/zero of=/dev/null &amp; Summary  In this article we have seen how to setup an auto scaling webserver farm using Heat. We have seen how Ceilometer and network services such as a Load Balancer play their important roles. Heat is the brains behind OpenStack, to know OpenStack is to understand it through Heat. The goal of this article is to really demonstrate and learn the strengths of OpenStack. Many organizations I speak with are trying to understand how OpenStack could add value and how it differentiates itself with traditional virtualization. Auto scaling applications is a major use case but it is really about services. Our auto scaling application leverages many services provided to us through OpenStack. Furthermore, OpenStack allows us to abstract processes behind services and that is the real difference maker. Hopefully this article has shed some light and provided insight in better understanding OpenStack use cases. If you have feedback or other interesting use cases, please share.  Happy OpenStacking!  (c) 2015 Keith Tenzer  ","categories": ["OpenStack"],
        "tags": ["Auto-Scaling","Ceilometer","HA-PROXY","Heat","Kilo","LBAAS","Load Balancer","OpenStack"],
        "url": "/openstack/auto-scaling-applications-with-openstack-heat/",
        "teaser": null
      },{
        "title": "HOWTO: OpenStack Deployment using TripleO and the Red Hat OpenStack Director",
        "excerpt":" \r Overview \r In this article we will look at how to deploy an OpenStack cloud using TripleO, the upstream project from the Red Hat OpenStack Director. Regardless of what OpenStack distribution you are using OpenStack is essentially OpenStack. Everyone has the same code-base to work with. The main differences between distributions are around what OpenStack projects are part of distribution, how it is supported and the deployment of the distribution. Every distribution has their own OpenStack deployment tool. Clearly deployments differ as they are based on support decisions each distribution makes. However many distributions have created their own proprietary installers. Shouldn't the OpenStack community unite around a common installer? What would be better than using OpenStack to deploy OpenStack? Why should OpenStack administrators have to learn separate proprietary tooling? Why should we be creating unnecessary vendor lock-in for OpenStack's deployment tooling? Installing OpenStack is one thing but what about upgrade and life-cycle management? \r  \r This is the promise of TripleO! The TripleO (OpenStack on OpenStack) project was started to solve these problems and bring unification around OpenStack deployment as well as eventually life-cycle management. This has taken quite some time and been a journey but finally the first distribution is using TripleO. Red Hat Enterprise OpenStack Platform 7 has shifted away from foreman/puppet and is now based largely on TripleO. Red Hat is bringing its expertise and learning over the past years around OpenStack deployments and contributing heavily to TripleO. \r TripleO Concepts \r Before getting into the weeds, we should understand some basic concepts. First TripleO uses OpenStack to deploy OpenStack. It mainly utilizes Ironic for provisioning and Heat for orchestration. Under the hood puppet is used for configuration management. TripleO first deploys an OpenStack cloud used to deploy other OpenStack clouds. This is referred to as the undercloud. The OpenStack cloud environment deployed from undercloud is known as overcloud. The networking requirement is that all systems share a non-routed provisioning network. TripleO also uses PXE to boot and install initial OS image (bootstrap). There are different types of nodes or roles a node can have. In addition to controller and compute you can have nodes for Cinder, CEPH or Swift storage. CEPH storage is also integrated and since most OpenStack deployments use CEPH this is an obvious advantage. \r Environment \r In this environment we have the KVM hypervisor host (Laptop), the undercloud (single VM) and overcloud (1 X compute, 1 Xcontroller). The undercloud and overcloud are all VMs running on the KVM hypervisor host (Laptop). The KVM hypervisor host is on the 192.168.122.0/24 network and has IP of 192.168.122.1. The undercloud runs on a single VM on the 192.168.122.0/24 management network and 192.168.126.0/24 (provisioning) netowrk. The undercloud has an IP address of 192.168.122.90 (eth0). The overcloud is on the 192.168.126.0/24 (provisioning) and 192.168.125.0/24 (external) network. This is a very simple network configuration. In a real production environment there will be many more networks used in overcloud. \r  \r Deploying Undercloud \r In this section we will configure the undercloud. Normally you would deploy OpenStack nodes on bare-metal but since this is designed to run on Laptop or in lab, we are using KVM virtualization. Before beginning install RHEL or CentOS 7.1 on your KVM hypervisor. \r Disable NetworkManager. \r undercloud# systemctl stop NetworkManager\r undercloud# systemctl disable NetworkManager\r Enable port forwarding. \r undercloud# vi /etc/sysctl.conf\r net.ipv4.ip_forward = 1\r undercloud# sysctl -p /etc/sysctl.conf\r Ensure hostname is static. \r undercloud# hostnamectl set-hostname undercloud.lab.com\r undercloud# systemctl restart network\r Register to subscription manager and enable appropriate repositories for RHEL. \r undercloud# subscription-manager register\r undercloud# subscription-manager list --available\r undercloud# subscription-manager attach --pool=8a85f9814f2c669b014f3b872de132b5\r undercloud# subscription-manager repos --disable=*\r undercloud# subscription-manager repos --enable=rhel-7-server-rpms --enable=rhel-7-server-optional-rpms --enable=rhel-7-server-extras-rpms --enable=rhel-7-server-openstack-7.0-rpms --enable=rhel-7-server-openstack-7.0-director-rpms\r Perform yum update and reboot system. \r undercloud# yum update -y &amp;&amp; reboot\r Install facter and ensure hostname is set properly in /etc/hosts. \r undercloud# yum install facter -y\r undercloud# ipaddr=$(facter ipaddress_eth0)\r undercloud# echo -e \"$ipaddr\\t\\tundercloud.lab.com\\tundercloud\" &gt;&gt; /etc/hosts\r Install TripleO packages. \r undercloud# yum install python-rdomanager-oscplugin -y \r Create a stack user. \r undercloud# useradd stack\r undercloud# echo \"redhat\" | passwd stack --stdin\r undercloud# echo \"stack ALL=(root) NOPASSWD:ALL\" | tee -a /etc/sudoers.d/stack undercloud# chmod 0440 /etc/sudoers.d/stack\r undercloud# su - stack\r Determine network settings for undercloud. At minimum you need two networks. One for provisioning and the other for the overcloud which should be external network. In this case we have two networks. The undercloud provisioning network 192.168.126.0/24 and the overcloud external network 192.168.125.0/24. \r [stack@undercloud ~]$ cp /usr/share/instack-undercloud/undercloud.conf.sample ~/undercloud.conf\r [stack@undercloud ~]$ vi ~/undercloud.conf\r  [DEFAULT]\r  local_ip = 192.168.126.1/24\r  undercloud_public_vip = 192.168.126.10\r  undercloud_admin_vip = 192.168.126.11\r  local_interface = eth1\r  masquerade_network = 192.168.126.0/24\r  dhcp_start = 192.168.126.100\r  dhcp_end = 192.168.126.120\r  network_cidr = 192.168.126.0/24\r  network_gateway = 192.168.126.1\r  discovery_iprange = 192.168.126.130,192.168.126.150\r  [auth]\r Install the undercloud. \r [stack@undercloud ~]$ openstack undercloud install\r #############################################################################\r instack-install-undercloud complete.\r The file containing this installation's passwords is at /home/stack/undercloud-passwords.conf.\r There is also a stackrc file at /home/stack/stackrc.\r These files are needed to interact with the OpenStack services, and should be secured.\r #############################################################################\r Verify undercloud. \r  [stack@undercloud ~]$ source ~/stackrc\r  [stack@undercloud ~]$ openstack catalog show nova\r  +-----------+------------------------------------------------------------------------------+\r  | Field | Value |\r  +-----------+------------------------------------------------------------------------------+\r  | endpoints | regionOne                                                                    |\r  |           | publicURL: http://192.168.126.1:8774/v2/e6649719251f40569200fec7fae6988a     |\r  |           | internalURL: http://192.168.126.1:8774/v2/e6649719251f40569200fec7fae6988a   |\r  |           | adminURL: http://192.168.126.1:8774/v2/e6649719251f40569200fec7fae6988a      |\r  |           |                                                                              |\r  | name      | nova                                                                         |\r  | type      | compute                                                                      |\r  +-----------+------------------------------------------------------------------------------+\r Deploying Overcloud \r The overcloud is as mentioned a separate cloud from the undercloud. They are not sharing any resources, other than the provisioning network. Over and under sometimes confuse people into thinking the overcloud is sitting on top of undercloud, from networking perspective. This is of course not the case. In reality the clouds are sitting side-by-side from one another. The term over and under really refers to a logical relationship between both clouds. We will do a minimal deployment for the overcloud, 1 X controller and 1 X compute. \r Create directory for storing undercloud images. These are the images used by Ironic to provision an OpenStack node. \r [stack@undercloud]$ mkdir ~/images\r Download images from https://access.redhat.com/downloads/content/191/ver=7/rhel---7/7/x86_64/product-downloads and copy to ~/images. \r [stack@undercloud images]$ ls -l\r total 2307076\r -rw-r-----. 1 stack stack 61419520 Oct 12 16:11 deploy-ramdisk-ironic-7.1.0-39.tar\r -rw-r-----. 1 stack stack 155238400 Oct 12 16:11 discovery-ramdisk-7.1.0-39.tar\r -rw-r-----. 1 stack stack 964567040 Oct 12 16:12 overcloud-full-7.1.0-39.tar\r Extract image tarballs. \r [stack@undercloud ~]$ cd ~/images\r [stack@undercloud ~]$ for tarfile in *.tar; do tar -xf $tarfile; done\r Upload images to Glance. \r [stack@undercloud ~]$ openstack overcloud image upload --image-path /home/stack/images\r [stack@undercloud ~]$ openstack image list\r  +--------------------------------------+------------------------+\r  | ID | Name |\r  +--------------------------------------+------------------------+\r  | 31c01b42-d164-4898-b615-4787c12d3a53 | bm-deploy-ramdisk |\r  | e38057f6-24f2-42d1-afae-bb54dead864d | bm-deploy-kernel |\r  | f1708a15-5b9b-41ac-8363-ffc9932534f3 | overcloud-full |\r  | 318768c2-5300-43cb-939d-44fb7abca7de | overcloud-full-initrd |\r  | 28422b76-c37f-4413-b885-cccb24a4611c | overcloud-full-vmlinuz |\r  +--------------------------------------+------------------------+\r Configure DNS for undercloud. The undercloud system is connected to a network 192.168.122.0/24 that provides DNS. \r [stack@undercloud]$ neutron subnet-list\r +--------------------------------------+------+------------------+--------------------------------------------------------+\r | id | name | cidr | allocation_pools |\r +--------------------------------------+------+------------------+--------------------------------------------------------+\r | 532f3344-57ed-4a2f-b438-67a5d60c71fc | | 192.168.126.0/24 | {\"start\": \"192.168.126.100\", \"end\": \"192.168.126.120\"} |\r +--------------------------------------+------+------------------+--------------------------------------------------------+\r [stack@undercloud ~]$ neutron subnet-update 532f3344-57ed-4a2f-b438-67a5d60c71fc --dns-nameserver 192.168.122.1\r Since we are in nested virtual environment it is necessary to tweak timeouts. \r undercloud# sudo su -\r undercloud# openstack-config --set /etc/nova/nova.conf DEFAULT rpc_response_timeout 600\r undercloud# openstack-config --set /etc/ironic/ironic.conf DEFAULT rpc_response_timeout 600\r undercloud# openstack-service restart nova \r undercloud# openstack-service restart ironic\r undercloud# exit\r \r Create provisioning and external networks on KVM Hypervisor host. Ensure that NAT forwarding and DHCP is enabled on the external network. The provisioning network should be non-routable and DHCP disabled. The undercloud will handle DHCP services for the provisioning network. \r [ktenzer@ktenzer ~]$ cat &gt; /tmp/external.xml &lt;&lt;EOF\r &lt;network&gt;\r    &lt;name&gt;external&lt;/name&gt;\r    &lt;forward mode='nat'&gt;\r       &lt;nat&gt; &lt;port start='1024' end='65535'/&gt;\r       &lt;/nat&gt;\r    &lt;/forward&gt;\r    &lt;ip address='192.168.125.1' netmask='255.255.255.0'&gt;\r       &lt;dhcp&gt; &lt;range start='192.168.125.2' end='192.168.125.254'/&gt;\r       &lt;/dhcp&gt;\r    &lt;/ip&gt;\r &lt;/network&gt;\r \r [ktenzer@ktenzer ~]$ virsh net-define /tmp/external.xml\r [ktenzer@ktenzer ~]$ virsh net-autostart external\r [ktenzer@ktenzer ~]$ virsh net-start external\r [ktenzer@ktenzer ~]$ cat &gt; /tmp/provisioning.xml &lt;&lt;EOF\r &lt;network&gt;\r    &lt;name&gt;provisioning&lt;/name&gt;\r    &lt;ip address='192.168.126.254' netmask='255.255.255.0'&gt;\r    &lt;/ip&gt;\r &lt;/network&gt;\r [ktenzer@ktenzer ~]$ virsh net-define /tmp/provisioning.xml\r [ktenzer@ktenzer ~]$ virsh net-autostart provisioning\r [ktenzer@ktenzer ~]$ virsh net-start provisioning\r Create VM hulls in KVM using virsh on hypervisor host. You will need to change the disk path to suit your needs. \r ktenzer# cd /home/ktenzer/VirtualMachines\r ktenzer# for i in {1..2}; do qemu-img create -f qcow2 -o preallocation=metadata overcloud-node$i.qcow2 60G; done\r ktenzer# for i in {1..2}; do virt-install --ram 4096 --vcpus 4 --os-variant rhel7 --disk path=/home/ktenzer/VirtualMachines/overcloud-node$i.qcow2,device=disk,bus=virtio,format=qcow2 --noautoconsole --vnc --network network:provisioning --network network:overcloud --name overcloud-node$i --cpu SandyBridge,+vmx --dry-run --print-xml &gt; /tmp/overcloud-node$i.xml; virsh define --file /tmp/overcloud-node$i.xml; done\r Enable access on KVM hypervisor host so that Ironic can control VMs. \r ktenzer# cat &lt;&lt; EOF &gt; /etc/polkit-1/localauthority/50-local.d/50-libvirt-user-stack.pkla\r [libvirt Management Access]\r Identity=unix-user:stack\r Action=org.libvirt.unix.manage\r ResultAny=yes\r ResultInactive=yes\r ResultActive=yes\r EOF\r Copy ssh key from undercloud system to KVM hypervisor host for stack user. \r undercloud$ ssh-copy-id -i ~/.ssh/id_rsa.pub stack@192.168.122.1\r Save the MAC addresses for the provisioning network on the VMs. Ironic needs to know what MAC addresses a node has associated for provisioning network. \r [stack@undercloud ~]$ for i in {1..2}; do virsh -c qemu+ssh://stack@192.168.122.1/system domiflist overcloud-node$i | awk '$3 == \"mgmt\" {print $5};'; done &gt; /tmp/nodes.txt\r [stack@undercloud ~]$ cat /tmp/nodes.txt\r 52:54:00:44:60:2b\r 52:54:00:ea:e7:2e\r Create JSON file for Ironic baremetal node configuration. In this case we are configuring two nodes which are of course the virtual machines we already created. The pm_addr IP is set to IP of the KVM hypervisor host. \r [stack@undercloud ~]$ jq . &lt;&lt; EOF &gt; ~/instackenv.json\r {\r   \"ssh-user\": \"stack\",\r   \"ssh-key\": \"$(cat ~/.ssh/id_rsa)\",\r   \"power_manager\": \"nova.virt.baremetal.virtual_power_driver.VirtualPowerManager\",\r   \"host-ip\": \"192.168.122.1\",\r   \"arch\": \"x86_64\",\r   \"nodes\": [\r     {\r       \"pm_addr\": \"192.168.122.1\",\r       \"pm_password\": \"$(cat ~/.ssh/id_rsa)\",\r       \"pm_type\": \"pxe_ssh\",\r       \"mac\": [\r         \"$(sed -n 1p /tmp/nodes.txt)\"\r       ],\r       \"cpu\": \"4\",\r       \"memory\": \"4096\",\r       \"disk\": \"60\",\r       \"arch\": \"x86_64\",\r       \"pm_user\": \"stack\"\r     },\r     {\r       \"pm_addr\": \"192.168.122.1\",\r       \"pm_password\": \"$(cat ~/.ssh/id_rsa)\",\r       \"pm_type\": \"pxe_ssh\",\r       \"mac\": [\r         \"$(sed -n 2p /tmp/nodes.txt)\"\r       ],\r       \"cpu\": \"4\",\r       \"memory\": \"4096\",\r       \"disk\": \"60\",\r       \"arch\": \"x86_64\",\r       \"pm_user\": \"stack\"\r     }\r   ]\r }\r EOF\r Validate JSON file. \r [stack@undercloud ~]$ curl -O https://raw.githubusercontent.com/rthallisey/clapper/master/instackenv-validator.py\r python instackenv-validator.py -f instackenv.json\r INFO:__main__:Checking node 192.168.122.1\r DEBUG:__main__:Identified virtual node\r INFO:__main__:Checking node 192.168.122.1\r DEBUG:__main__:Identified virtual node\r DEBUG:__main__:Baremetal IPs are all unique.\r DEBUG:__main__:MAC addresses are all unique.\r \r --------------------\r SUCCESS: instackenv validator found 0 errors\r \r \r Add nodes to Ironic \r [stack@undercloud ~]$ openstack baremetal import --json instackenv.json\r \r List newly added baremetal nodes. \r [stack@undercloud ~]$ openstack baremetal list\r +--------------------------------------+------+---------------+-------------+-----------------+-------------+\r | UUID | Name | Instance UUID | Power State | Provision State | Maintenance |\r +--------------------------------------+------+---------------+-------------+-----------------+-------------+\r | cd620ad0-4563-44a5-8078-531b7f906188 | None | None | power off | available | False |\r | 44df8163-7381-46a7-b016-a0dd18bfee53 | None | None | power off | available | False |\r +--------------------------------------+------+---------------+-------------+-----------------+-------------+\r Enable nodes for baremetal provisioning and inspect ram and kernel images. \r [stack@undercloud ~]$ openstack baremetal configure boot\r [stack@undercloud ~]$ ironic node-show cd620ad0-4563-44a5-8078-531b7f906188 | grep -A1 deploy\r \r | driver_info | {u'ssh_username': u'stack', u'deploy_kernel': u'50125b15-9de3-4f03-bfbb- |\r | | 76e740741b68', u'deploy_ramdisk': u'25b55027-ca57-4f15-babe- |\r | | 6e14ba7d0b0c', u'ssh_key_contents': u'-----BEGIN RSA PRIVATE KEY----- |\r [stack@undercloud ~]$ openstack image show 50125b15-9de3-4f03-bfbb-76e740741b68\r +------------------+--------------------------------------+\r | Field | Value |\r +------------------+--------------------------------------+\r | checksum | 061e63c269d9c5b9a48a23f118c865de |\r | container_format | aki |\r | created_at | 2015-10-12T10:22:38.000000 |\r | deleted | False |\r | disk_format | aki |\r | id | 50125b15-9de3-4f03-bfbb-76e740741b68 |\r | is_public | True |\r | min_disk | 0 |\r | min_ram | 0 |\r | name | bm-deploy-kernel |\r | owner | 2ad8c320cf7040ef9ec0440e94238f58 |\r | properties | {} |\r | protected | False |\r | size | 5027584 |\r | status | active |\r | updated_at | 2015-10-12T10:22:38.000000 |\r +------------------+--------------------------------------+\r [stack@undercloud ~]$ openstack image show 25b55027-ca57-4f15-babe-6e14ba7d0b0c\r +------------------+--------------------------------------+\r | Field | Value |\r +------------------+--------------------------------------+\r | checksum | eafcb9601b03261a7c608bebcfdff41c |\r | container_format | ari |\r | created_at | 2015-10-12T10:22:38.000000 |\r | deleted | False |\r | disk_format | ari |\r | id | 25b55027-ca57-4f15-babe-6e14ba7d0b0c |\r | is_public | True |\r | min_disk | 0 |\r | min_ram | 0 |\r | name | bm-deploy-ramdisk |\r | owner | 2ad8c320cf7040ef9ec0440e94238f58 |\r | properties | {} |\r | protected | False |\r | size | 56355601 |\r | status | active |\r | updated_at | 2015-10-12T10:22:40.000000 |\r +------------------+--------------------------------------+\r /pre&gt;\r Ironic at this point only supports IPMI booting and since we are using VMs we need to use ssh_pxe. This is a workaround to allow that to work.\r \r [stack@undercloud ~]$ sudo su -\r undercloud# cat &lt;&lt; EOF &gt; /usr/bin/bootif-fix\r #!/usr/bin/env bash\r \r while true;\r         do find /httpboot/ -type f ! -iname \"kernel\" ! -iname \"ramdisk\" ! -iname \"*.kernel\" ! -iname \"*.ramdisk\" -exec sed -i 's|{mac|{net0/mac|g' {} +;\r done\r EOF\r \r undercloud# chmod a+x /usr/bin/bootif-fix\r undercloud# cat &lt;&lt; EOF &gt; /usr/lib/systemd/system/bootif-fix.service\r [Unit]\r Description=Automated fix for incorrect iPXE BOOFIF\r \r [Service]\r Type=simple\r ExecStart=/usr/bin/bootif-fix\r \r [Install]\r WantedBy=multi-user.target\r EOF\r \r undercloud# systemctl daemon-reload\r undercloud# systemctl enable bootif-fix\r undercloud# systemctl start bootif-fix\r undercloud# exit\r Create new flavor for the baremetal nodes and set boot option to local. \r undercloud$ openstack flavor create --id auto --ram 4096 --disk 58 --vcpus 4 baremetal\r undercloud$ openstack flavor set --property \"cpu_arch\"=\"x86_64\" --property \"capabilities:boot_option\"=\"local\" baremetal\r Perform introspection on baremetal nodes. This will discover hardware and configure node roles. \r [stack@undercloud ~]$ openstack baremetal introspection bulk start\r Setting available nodes to manageable...\r Starting introspection of node: 79f2a51c-a0f0-436f-9e8a-c082ee61f938\r Starting introspection of node: 8ba244fd-5362-45fe-bb6c-5f15f2949912\r Waiting for discovery to finish...\r Discovery for UUID 79f2a51c-a0f0-436f-9e8a-c082ee61f938 finished successfully.\r Discovery for UUID 8ba244fd-5362-45fe-bb6c-5f15f2949912 finished successfully.\r Setting manageable nodes to available...\r Node 79f2a51c-a0f0-436f-9e8a-c082ee61f938 has been set to available.\r Node 8ba244fd-5362-45fe-bb6c-5f15f2949912 has been set to available.\r To check progress of introspection. \r [stack@undercloud ~]$ sudo journalctl -f -l -u openstack-ironic-discoverd -u openstack-ironic-discoverd-dnsmasq -f\r &nbsp; \r List the Ironic baremetal nodes. Nodes should be available if introspection worked. \r [stack@undercloud ~]$ ironic node-list \r +--------------------------------------+------+---------------+-------------+-----------------+-------------+\r | UUID | Name | Instance UUID | Power State | Provision State | Maintenance |\r +--------------------------------------+------+---------------+-------------+-----------------+-------------+\r | cd620ad0-4563-44a5-8078-531b7f906188 | None | None | power on | available | False |\r | 44df8163-7381-46a7-b016-a0dd18bfee53 | None | None | power on | available | False |\r +--------------------------------------+------+---------------+-------------+-----------------+-------------+\r Deploy overcloud. \r [stack@undercloud ~]$ openstack overcloud deploy --templates --control-scale 1 --compute-scale 1 --neutron-tunnel-types vxlan --neutron-network-type vxlan\r Overcloud Endpoint: http://192.168.126.119:5000/v2.0/\r Overcloud Deployed\r  \r Check status of Heat resources to monitor status of overcloud deployment. \r [stack@undercloud ~]$ heat resource-list -n 5 overcloud\r Once the OS install is complete on the baremetal nodes you can follow progress of the OpenStack overcloud configuration. \r [stack@undercloud ~]$ nova list\r +--------------------------------------+------------------------+--------+------------+-------------+-------------------------+\r | ID                                   | Name                   | Status | Task State | Power State | Networks                |\r +--------------------------------------+------------------------+--------+------------+-------------+-------------------------+\r | 507d1172-fc73-476b-960f-1d9bf7c1c270 | overcloud-compute-0    | ACTIVE | -          | Running     | ctlplane=192.168.126.103|\r | ff0e5e15-5bb8-4c77-81c3-651588802ebd | overcloud-controller-0 | ACTIVE | -          | Running     | ctlplane=192.168.126.102|\r +--------------------------------------+------------------------+--------+------------+-------------+-------------------------+\r [stack@undercloud ~]$ ssh heat-admin@192.168.126.102\r overcloud-controller-0$ sudo -i\r overcloud-controller-0# journalctl -f -u os-collect-config\r Deploying using the OpenStack Director UI \r The overcloud deployment can be done using the UI. You can even do the preliminary configuration using the CLI and run deployment from UI. \r  \r We can see exactly what OpenStack services will be configured in the overcloud. \r  \r Deployment status is shown and using the UI it is also to see when baremetal nodes have been completely provisioned. \r  \r Deployment details are available in the deployment log. \r  \r Once deployment is complete using the UI, the overcloud must be initialized. \r  \r Upon completion the overcloud is available and can be accessed. \r  \r Summary \r In this article we have discussed how OpenStack distributions have a proprietary mindset in regards to their deployment tools. We have discussed the need for a OpenStack community sponsored upstream project responsible for deployment and life-cycle management. That project is TripleO and Red Hat is the first distribution to ship its deployment tool based on TripleO. Using OpenStack to deploy OpenStack not only benefits entire community but also administrators and end-users. Finally we have seen how to deploy both the undercloud as well as overcloud using TripleO and the Red Hat OpenStack Director. Hopefully you found this article informative and useful. I would be very interested in hearing your feedback on this topic, so please share. \r Happy OpenStacking! \r (c) 2015 Keith Tenzer \r ","categories": ["OpenStack"],
        "tags": ["Director","Heat","Ironic","KVM","OpenStack","RHEL OSP 7"],
        "url": "/openstack/howto-openstack-deployment-using-tripleo-and-the-red-hat-openstack-director/",
        "teaser": null
      },{
        "title": "Advanced Deployment with TripleO and OpenStack Director",
        "excerpt":"  Overview  In this article we will look at some advanced deployment scenarios using TripleO and the OpenStack Director. This article builds on a previous article: HOWTO: OpenStack Deployment using TripleO and the Red Hat OpenStack Director. It assumes you have a working OpenStack environment built with Red Hat OpenStack Director and complete with undercloud as well as overcloud. We will tear down existing OpenStack environment, create a new deployment using custom OpenStack Ironic profiles, customize post-installation and show how to add additional compute resources.    Tear Down Existing Environment  One of the advantages of TripleO is that it uses OpenStack to deploy OpenStack. This makes it of course very easy to tear down and rebuilt environments. The basic idea is that immutable and reproducible infrastructure leads to much higher stability. Production environments can be tested properly because they are reproducible. Production is never changed, it is re-deployed and only the deployment configuration is ever changed. These are common practices for any software engineering group but only recently have been applied to Infrastructure itself.  Delete ironic baremetal nodes.  [stack@undercloud ~]$ for i in $(ironic node-list | grep -v UUID | awk ' { print $2 } '); do ironic node-delete $i; done Import the baremetal node configuration.  [stack@undercloud ~]$ openstack baremetal import --json instackenv.json Undeploy overcloud by deleting the Heat stack.  [stack@undercloud ~]$ heat stack-delete overcloud Ironic Profiles  In our original deployment we used a single Ironic profile. Normally however you would probably have different resource expectations for controller, compute and even ceph nodes. Using AHC-Tools (Automated Health Check) it is possible to build Ironic profiles that match specific hardware.  Install AHC-Tools.  [stack@undercloud ~]$ sudo yum install -y ahc-tools Configure AHC-Tools.  [stack@undercloud ~]$ sudo su - undercloud# sed 's/\\[discoverd/\\[ironic/' /etc/ironic-discoverd/discoverd.conf &gt; /etc/ahc-tools/ahc-tools.conf  [stack@undercloud ~]$ unset DIB_YUM_REPO_CONF [stack@undercloud ~]$ unset DIB_LOCAL_IMAGE [stack@undercloud ~]$ sudo systemctl stop bootif-fix  [stack@undercloud ~]$ source ~/stackrc  Enable discovery using AHC-Tools and re-deploy the undercloud.  [stack@undercloud ~]$ openstack-config --set ~/undercloud.conf DEFAULT discovery_runbench true [stack@undercloud ~]$ openstack undercloud install [stack@undercloud ~]$ sudo systemctl start bootif-fix [stack@undercloud ~]$ source ~/stackrc [stack@undercloud ~]$ openstack baremetal import --json instackenv.json [stack@undercloud ~]$ openstack baremetal configure boot  Ironic nodes should be set to available.  [stack@undercloud ~]$ ironic node-list +--------------------------------------+------+---------------+-------------+-----------------+-------------+ | UUID | Name | Instance UUID | Power State | Provision State | Maintenance | +--------------------------------------+------+---------------+-------------+-----------------+-------------+ | 88d5901b-8527-468b-8c7c-852d79096bcc | None | None | power off | available | False | | d1f47f58-3b29-4dc6-a16c-2999f3422b36 | None | None | power off | available | False | | 9e6f26df-22dd-4b2b-a862-78b29a284825 | None | None | power off | available | False | +--------------------------------------+------+---------------+-------------+-----------------+-------------+  Change Ironic node provision state to managed. Manual introspection requires nodes being set to managed.  [stack@undercloud ~]$ ironic node-set-provision-state 88d5901b-8527-468b-8c7c-852d79096bcc manage [stack@undercloud ~]$ ironic node-set-provision-state d1f47f58-3b29-4dc6-a16c-2999f3422b36 manage [stack@undercloud ~]$ ironic node-set-provision-state 9e6f26df-22dd-4b2b-a862-78b29a284825 manage Start introspection for each Ironic node.  [stack@undercloud ~]$ openstack baremetal introspection start 88d5901b-8527-468b-8c7c-852d79096bcc [stack@undercloud ~]$ openstack baremetal introspection start d1f47f58-3b29-4dc6-a16c-2999f3422b36 [stack@undercloud ~]$ openstack baremetal introspection start 9e6f26df-22dd-4b2b-a862-78b29a284825 Verify introspection completes. Check to ensure nodes power-on and in addition monitor introspection to ensure it completes.   [stack@undercloud ~]$ ironic node-list | grep \"power on\" | awk '{print $2\" (\"$8\" \"$9\")\";}'  To check introspection status view journalctl logs.  [stack@undercloud ~]$ sudo journalctl -u openstack-ironic-discoverd -f  Check introspection status using CLI.  [stack@undercloud ~]$ openstack baremetal introspection status 88d5901b-8527-468b-8c7c-852d79096bcc  +----------+-------+  | Field    | Value |  +----------+-------+  | error    | None  |   | finished | False |  +----------+-------+  Wait until introspection completes for all nodes.  [stack@undercloud ~]$ openstack baremetal introspection status 88d5901b-8527-468b-8c7c-852d79096bcc  +----------+-------+  | Field    | Value |  +----------+-------+  | error    | None  |  | finished | True  |  +----------+-------+ Once introspection completes we need to set the state of each node to 'available'. Again this is automatically done for us when doing automatic introspection.  [stack@undercloud ~]$ for i in $(ironic node-list | grep -v UUID | awk ' { print $2 } '); do ironic node-set-provision-state $i provide; done Generate report of benchmarking and introspection using ahc-tools.  undercloud$ ahc-report --full We can view indentical systems as well as systems that are not consistent with our configuration  undercloud$ ahc-report --categories | grep -A3 \"3 identical systems\" undercloud$ ahc-report --outliers | grep -i inconsistent | head -n5  Ironic hardware specs are stored in various configuration files. Once exists for compute, controller and ceph. In addition there is a configuration file that depicts the OpenStack overcloud topology.  Controller specs are stored in following file:  [stack@undercloud ~]$ cat /etc/ahc-tools/edeploy/control.specs [ ('disk', '$disk', 'size', 'gt(4)'), ('network', '$eth', 'ipv4', 'network(192.0.2.0/24)'), ('memory', 'total', 'size', 'ge(4294967296)'), ] Compute specs are stored in following file:  [stack@undercloud ~]$ cat /etc/ahc-tools/edeploy/compute.specs [ ('disk', '$disk', 'size', 'gt(4)'), ('network', '$eth', 'ipv4', 'network(192.0.2.0/24)'), ('memory', 'total', 'size', 'ge(4294967296)'), ] OpenStack overcloud topology is stored in following file:  [stack@undercloud ~]$ cat /etc/ahc-tools/edeploy/state [('control', '1'), ('compute', '*')] Change the compute and controller specs as follows:  undercloud$ sudo su -  undercloud# cat &lt;&lt; EOF &gt; /etc/ahc-tools/edeploy/control.specs [ ('cpu', 'logical', 'number', 'ge(4)'), ('disk', 'vda', 'size', 'gt(25)'), ] EOF undercloud# cat &lt;&lt; EOF &gt; /etc/ahc-tools/edeploy/compute.specs [ ('cpu', 'logical', 'number', 'ge(2)'), ('memory', 'total', 'size', 'ge(3000000)'), ] EOF Run AHC and have it match profiles with discovered hardware.  undercloud# ahc-match Node roles can now be viewed. In our case we have 1 controller and two compute nodes that were classified by AHC.  [stack@undercloud ~]$ for i in $(ironic node-list | grep -v UUID | awk '{print $2;}' \\ | sed -e /^$/d); do ironic node-show $i | grep -A1 properties; \\ echo \"=======\"; done;  | properties | {u'memory_mb': u'4096', u'cpu_arch': u'x86_64', u'local_gb': u'1', | | | u'cpus': u'4', u'capabilities': u'profile:control,boot_option:local'} | ======= | properties | {u'memory_mb': u'4096', u'cpu_arch': u'x86_64', u'local_gb': u'1', | | | u'cpus': u'4', u'capabilities': u'profile:compute,boot_option:local'} | =======Fgener | properties | {u'memory_mb': u'4096', u'cpu_arch': u'x86_64', u'local_gb': u'1', | | | u'cpus': u'4', u'capabilities': u'profile:compute,boot_option:local'} | Create new Ironic flavors for controller and compute nodes.  [stack@undercloud ~]$ openstack flavor create --id auto --ram 4096 --disk 58 --vcpus 4 control [stack@undercloud ~]$ openstack flavor create --id auto --ram 4096 --disk 58 --vcpus 2 compute Update flavors and map them to the Ironic profiles.  undercloud$ openstack flavor set --property \"cpu_arch\"=\"x86_64\" --property \"capabilities:boot_option\"=\"local\" --property \"capabilities:profile\"=\"control\" control  undercloud$ openstack flavor set --property \"cpu_arch\"=\"x86_64\" --property \"capabilities:boot_option\"=\"local\" --property \"capabilities:profile\"=\"compute\" compute  Post Installation Customization  Since the basis for TripleO is Heat, customization is thankfully quite simple. In this example we will install Nagios monitoring package to all OpenStack overcloud nodes as part of post-install phase.  Create post installation yaml files.  undercloud$ cat &lt;&lt; EOF &gt; ~/templates/firstboot-environment.yaml  resource_registry:    OS::TripleO::NodeUserData: /home/stack/my_templates/firstboot-config.yaml  EOF  undercloud$ cat &lt;&lt; EOF &gt; ~/templates/firstboot-config.yaml heat_template_version: 2014-10-16  resources:   userdata:     type: OS::Heat::MultipartMime     properties:       parts:       - config: {get_resource: repo_config}    repo_config:     type: OS::Heat::SoftwareConfig     properties:       config: |         #!/bin/bash         yum install -y nagios  outputs:   OS::stack_id:     value: {get_resource: userdata} EOF Deploy overcloud using custom Ironic profiles and post-installation customization.  [stack@undercloud ~]$ openstack overcloud deploy --templates --control-flavor control --compute-flavor compute --control-scale 1 --compute-scale 1 --neutron-tunnel-types vxlan --neutron-network-type vxlan -e ~/templates/firstboot-environment.yaml Deploying templates in the directory /usr/share/openstack-tripleo-heat-templates  /home/stack/.ssh/known_hosts updated.  Original contents retained as /home/stack/.ssh/known_hosts.old  PKI initialization in init-keystone is deprecated and will be removed.  Warning: Permanently added '192.168.126.104' (ECDSA) to the list of known hosts.  The following cert files already exist, use --rebuild to remove the existing files before regenerating:  /etc/keystone/ssl/certs/ca.pem already exists  /etc/keystone/ssl/private/signing_key.pem already exists  /etc/keystone/ssl/certs/signing_cert.pem already exists  Connection to 192.168.126.104 closed.  Overcloud Endpoint: http://192.168.126.104:5000/v2.0/  Overcloud Deployed Add Additional Compute Resources  As we have seen we can control hardware through Ironic profiles and easily customize the deployment of OpenStack using Heat. Next we will add an additional compute resource. As you have noticed, we have configured three baremetal nodes but have only deployed two (1 X Compute, 1 X Control). A second compute node can easily be added to the OpenStack environment on-the-fly. Again we see the power of immutable infrastructure and automation.  Re-run deployment.  [stack@undercloud ~]$ openstack overcloud deploy --templates --control-flavor control --compute-flavor compute --control-scale 1 --compute-scale 2 --neutron-tunnel-types vxlan --neutron-network-type vxlan -e ~/templates/firstboot-environment.yaml After deployment completes you should see an additional compute node.  [stack@undercloud ~]$ nova list +--------------------------------------+------------------------+--------+------------+-------------+---------------+ | ID | Name | Status | Task State | Power State | Networks                                                          | +--------------------------------------+------------------------+--------+------------+-------------+---------------+ | 53813f9b-7020-4386-9c10-5818633c21ee | overcloud-compute-0    | ACTIVE | - | Running | ctlplane=192.168.126.106   | | 76e2c3fd-7278-4ded-8893-e6c9d8b4a76f | overcloud-compute-1    | ACTIVE | - | Running | ctlplane=192.168.126.107   | | 204dd901-a56d-40b6-9b18-bd8ce3ed75ab | overcloud-controller-0 | ACTIVE | - | Running | ctlplane=192.168.126.108   | +--------------------------------------+------------------------+--------+------------+-------------+---------------+ Troubleshooting  Introspection Issues  Sometimes introspection may fail. Unfortunately there is no good way of ending the process. To cancel introspection follow the following steps:  [stack@undercloud ~]$ ironic node-set-power-state [NODE UUID] off [stack@undercloud ~]$ sudo rm /var/lib/ironic-discoverd/discoverd.sqlite [stack@undercloud ~]$ sudo systemctl restart openstack-ironic-discoverd Additionally you can change the discovery timeouty by editing the following file:  [stack@undercloud ~]$ sudo vi /etc/ironic-discoverd/discoverd.conf Note: ensure you use rtl8139 network driver and not virtio if using KVM. The performance of rtl8139 is much better and using virtio will cause PXE timeouts.  Summary  In this article we have seen how to configure Ironic profiles, customize overcloud deployment and add additional compute resources dynamically. The real value of an OpenStack deployment tool is in its ability to create true environment stability. Anyone can install OpenStack and make it look pretty but maintaining it is a completely different story. I would put my money on the latter. Hopefully you found this article of use. As always feedback is welcome so let's hear it!  Happy OpenStacking!  (c) 2015 Keith Tenzer  ","categories": ["OpenStack"],
        "tags": ["Heat","Ironic","Kilo","OpenStack Director","TripleO"],
        "url": "/openstack/advanced-deployment-with-tripleo-and-openstack-director/",
        "teaser": null
      },{
        "title": "Detecting Security Vulnerabilities in Docker Container Images",
        "excerpt":"  Overview  Containers, especially Docker container images have been on fire of late and it is simple to understand why? Docker container images give your development and operations organizations a major shot of adrenaline. The results are quite impressive. Applications are developed at never before seen speeds and as such organizations are able to deliver innovation to customers much faster. It's all so easy, just get on Docker Hub, download a container and run it. So why isn't everyone already doing this? Unfortunately it is not quite that simple. Enterprises have many other requirements such as security. Once IT operations gets involved they typically start asking a lot of questions. Who built this container? How is the container maintained? Who provides support for the software within the container? Does the software running within the container adhere to our security guidelines? How can we run security compliance checks within containers? How do we update software within containers?    Red Hat has stepped up and decided that container security is an area where Red Hat can bring significant value. CloudForms 4 is a cloud management platform. In addition to supporting virtual machines platforms (RHEV, Hyper-V, VMware, OpenStack, Amazon EC2 and Azure) now also supports containers through OpenShift 3 and Atomic Enterprise. CloudForms 4 integrates either directly with OpenShift 3 (PaaS with CI/CD) or Atomic Enterprise (Container IaaS). In fact, CloudForms 4 should work with any container platform that implements Kubernetes by Google as that is basis for both.  OpenShift 3 provides additional value on top of Atomic Enterprise around development workflows (CI / CD) and integration with development tooling to build automated pipelines. If you are building applications OpenShift 3 is what you want, period. If you are running containerized Applications then either OpenShift 3 or Atomic Enterprise. As part of integrating with containers CloudForms 4 has a feature called smartstate analysis. Previously this worked only on virtual machines and allowed you to scan software content inside the virtual machine. You could then build security policies and automatically handle compliance events, such as shutting down non-compliant virtual machines. The smartstate analysis feature is now enabled through CloudForms 4, for container workloads. Let's see how it works! In this article we will show how to connect OpenShift 3 to CloudForms 4 through it's provider and perform smartstate anylysis for a MySQL Docker container image.  Connecting CloudForms 4 to OpenShift 3  As mentioned CloudForms has providers and now with CloudForms 4 there is a OpenShift 3 provider. In order to add the provider we must create a service account in OpenShift 3 and provide CloudForms with a service token from OpenShift.  Create JSON for CFME service account  [root@ose3-master ~]# vi cfme.json {  \"apiVersion\": \"v1\",  \"kind\": \"ServiceAccount\",  \"metadata\": {  \"name\": \"cfme\"  } } Ensure you are in the default project of OpenShift 3. Note: Projects in OpenShift map to Kubernetes namespaces.  [root@ose3-master ~]# oc new-project management-infra Create the service account using JSON file.  [root@ose3-master ~]# oc create -f cfme.json Give the server account cluster-admin role.  [root@ose3-master ~]# oadm policy add-cluster-role-to-user cluster-admin system:serviceaccount:management-infra:cfme Get the token name from the service account.  [root@ose3-master ~]# oc get sa cfme -o yaml  apiVersion: v1  imagePullSecrets:  - name: cfme-dockercfg-1z8sc  kind: ServiceAccount  metadata:  creationTimestamp: 2015-11-26T12:53:46Z  name: cfme  namespace: default  resourceVersion: \"53311\"  selfLink: /api/v1/namespaces/default/serviceaccounts/cfme  uid: bb38592b-943c-11e5-924e-525400bca113  secrets:  - name: cfme-token-9cxzf  - name: cfme-dockercfg-1z8sc Get the token itself for authentification.  [root@ose3-master ~]# oc describe secret cfme-token-9cxzf Name: cfme-token-9cxzf  Labels: &lt;none&gt;  Annotations: kubernetes.io/service-account.name=cfme,kubernetes.io/service-account.uid=bb38592b-943c-11e5-924e-525400bca113 Type: kubernetes.io/service-account-token Data  ====  token: eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImNmbWUtdG9rZW4tOWN4emYiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiY2ZtZSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImJiMzg1OTJiLTk0M2MtMTFlNS05MjRlLTUyNTQwMGJjYTExMyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OmNmbWUifQ.XH00iOfbXI2jOQNgLVt1jEXp5412bVoCZCqJQ-UR-9iWLJA2ha4E22uc0omq1tZ4xsJRpNaDJzB4g96xH_izQNAFTXWm5R7ESdrGvPBNaK_q6K2mh4C_sVKBCgqQYaq23Yhkan-7dchNCrX2mTPA8tZ7GpW7SRW7jRakhZhD5ljEthFbo8kWsYlreOeVidV3voV__o25425Yn4p6VHYZ5-eGAzieMI6IQvnynDvwZPEkxECX78oyx-8uap_IYCrg1kJUfGezXx05SHeYomOhBLMI7VMs6pHMhVTRGymRBi8aypWjPltzanR3NELKsHOdUx6PAFJ7YPGWkn753nEgHQ Adding OpenShift 3 Provider  Once a service account for CloudForms has been created in OpenShift 3 we can add the provider.  In CloudForms 4 under Containers add a new provider.    Select OpenShift and copy/paste the token    After a few minutes you should see the container dashboard in CloudForms 4 populate with information. Note: we see Kubernetes constructs such as pods, replicators and services.    Add Security Context Constraints  Security context constraints allow OpenShift 3 administrators to control permissions on container pods. In order for CloudForms 4 to perform smartstate analysis on container images it needs super-privileged container access.  Create security context constraint YAML file. Ensure you add the cfme service account to list of users. Note: you need to allow privileged containers and host volume plugins as these are needed for smartstate analysis.  [root@ose3-master ~]# vi scc_cfme.yaml  kind: SecurityContextConstraints  apiVersion: v1  metadata:    name: scc-admin  allowHostDirVolumePlugin: true  allowPrivilegedContainer: true  runAsUser:    type: RunAsAny  seLinuxContext:    type: RunAsAny  fsGroup:    type: RunAsAny  supplementalGroups:    type: RunAsAny  users:  - system:serviceaccount:management-infra:cfme Create security context constraint.  [root@ose3-master ~]# oc create -f scc_cfme.yaml Run Smartstate Analysis  At this point everything is in place to run smartstate analysis and peer into individual Docker container images. In this case we will look into a MySQL container image.  In CloudForms 4 go to Container Images and find the desired container image.    Here we notice that the packages are 0. This means no smartstate analysis has run. To run smartstate analysis select the option under the configuration dropdown.    After smartstate analysis completes you will see the software packages installed within your container image.    By clicking on the packages you can see all the individual packages and most importantly their versions.    Container image software packages and versions are now known to CloudForms 4. In a future version of CloudForms 4 when active container management is introduced, you should be able to create security compliance policies and define actions in the event of a security violation. To learn more about creating compliance policies in CloudForms see this article that was previously published.  Summary  In this article we have seen how to enable smartstate analysis in CloudForms 4 in order to peer into container images. Container technology is without question revolutionizing the way we develop and run applications. The only thing standing in it's way has been security. This is a major step forward in the ability to ensure enterprise security compliance of container environments. Red Hat is focusing greatly on container security and enabling container technology in the enterprise. This is the start of much more to come so stay tuned, buckle up and enjoy the ride!  Happy Container Compliance with CloudForms 4 and OpenShift 3!  (c) 2015 Keith Tenzer  ","categories": ["CloudForms"],
        "tags": ["Containers","Docker","Linux","OpenShift","Security"],
        "url": "/cloudforms/detecting-security-vulnerabilities-in-docker-container-images/",
        "teaser": null
      },{
        "title": "2015 in review",
        "excerpt":"The WordPress.com stats helper monkeys prepared a 2015 annual report for this blog.    Here's an excerpt:  The concert hall at the Sydney Opera House holds 2,700 people. This blog was viewed about 56,000 times in 2015. If it were a concert at Sydney Opera House, it would take about 21 sold-out performances for that many people to see it.  Click here to see the complete report.  ","categories": ["General"],
        "tags": ["blog","Cloud","keith tenzer","Linux","opensource","Red Hat","technology"],
        "url": "/general/2015-in-review/",
        "teaser": null
      },{
        "title": "OpenStack Liberty Lab Installation and Configuration Guide",
        "excerpt":"  Overview  In this article we will focus on installing and configuring OpenStack Liberty using RDO and the packstack installer. RDO is a community platform around Red Hat's OpenStack Platform. It allows you to test the latest OpenStack capabilities on a stable platform such as Red Hat Enterprise Linux (RHEL) or CentOS. This guide will take you through installing the OpenStack Liberty release, configuring networking, security groups, flavors, images and are other OpenStack related services. The outcome is a working OpenStack environment based on the Liberty release that you can use as a baseline for testing your applications with OpenStack capabilities.    Install and Configure OpenStack Liberty   Install RHEL or CentOS 7.1. Ensure name resolution is working.  # vi /etc/hosts 192.168.122.80 osp8.lab osp8  Set hostname.  # hostnamectl set-hostname osp8.lab.com  Disable firewalld since this is for a lab environment.  # systemctl disable firewalld # systemctl stop firewalld  Disable NetworkManager, it is still not recommended for Liberty (at least RDO).  # systemctl stop NetworkManager # systemctl disable NetworkManager  For RHEL systems register with subscription manager.  # subscription-manager register # subscription-manager subscribe --auto # subscription-manager repos --disable=* # subscription-manager repos --enable=rhel-7-server-rpms # subscription-manager repos --enable=rhel-7-server-extras-rpms  # subscription-manager repos --enable=rhel-7-server-rh-common-rpms # subscription-manager repos --enable=rhel-7-server-openstack-8-rpms  Install yum-utils and update the system.  # yum install -y yum-utils # yum update -y  Reboot.  # systemctl reboot  Install packstack packages.  # yum install -y openstack-packstack You can install packstack by providing command-line options or using the answers file.  Option 1: Install using command-line options   # packstack --allinone --os-neutron-ovs-bridge-mappings=extnet:br-ex \\  --os-neutron-ovs-bridge-interfaces=br-ex:eth0 \\  --os-neutron-ml2-type-drivers=vxlan,flat \\  --os-heat-install=y --os-heat-cfn-install=y \\  --os-sahara-install=y --os-trove-install=y \\  --os-neutron-lbaas-install=y \\  --keystone-admin-passwd=redhat01 Option 2: Install using answers file   Create packstack answers file for customizing the installer.  # packstack --gen-answer-file /root/answers.txt  Update the packstack answers file and enable other OpenStack services. Note: as of the writing of this guide SSL is not working in combination with Horizon so don't enable SSL.  # vi /root/answers.txt  CONFIG_KEYSTONE_ADMIN_PW=redhat  CONFIG_PROVISION_DEMO=n  CONFIG_HEAT_INSTALL=y  CONFIG_HEAT_CFN_INSTALL=y  CONFIG_SAHARA_INSTALL=y  CONFIG_TROVE_INSTALL=y  CONFIG_CEILOMETER_INSTALL=y  CONFIG_LBAAS_INSTALL=y  Install OpenStack Liberty using packstack.  # packstack --answer-file /root/answers.txt  Source the keystone admin profile.  # . /root/keystonerc_admin  Check status of openstack services.  # openstack-status  Backup the ifcfg-etho script.  # cp /etc/sysconfig/network-scripts/ifcfg-eth0 /root/  Configure external bridge for floating ip networks.  # vi /etc/sysconfig/network-scripts/ifcfg-eth0  DEVICE=eth0  ONBOOT=yes  TYPE=OVSPort  DEVICETYPE=ovs  OVS_BRIDGE=br-ex # vi /etc/sysconfig/network-scripts/ifcfg-br-ex  DEVICE=br-ex  BOOTPROTO=static  ONBOOT=yes  TYPE=OVSBridge  DEVICETYPE=ovs  USERCTL=yes  PEERDNS=yes  IPV6INIT=no  IPADDR=&lt;www.xxx.yyy.zzz&gt;  NETMASK=255.255.255.0  GATEWAY=&lt;GW IP&gt;  DNS1=&lt;DNS IP&gt;  Add the eht0 physical interface to the br-ex bridge in openVswitch for floating IP networks.  # ovs-vsctl add-port br-ex eth0 ; systemctl restart network.service Configure OpenStack   Create private network.  # neutron net-create private # neutron subnet-create private 10.10.1.0/24 --name private_subnet --allocation-pool start=10.10.1.100,end=10.10.1.200  Create public network. Note: these steps assume the physical network connected to eth0 is 192.168.122.0/24.  # neutron net-create public --router:external # neutron subnet-create public 192.168.122.0/24 --name public_subnet --allocation-pool start=192.168.122.100,end=192.168.122.200 --disable-dhcp --gateway 192.168.122.1  Add a new router and configure router interfaces.  # neutron router-create router1 --ha False # neutron router-gateway-set router1 public # neutron router-interface-add router1 private_subnet  Upload a glance image. In this case we will use a Cirros image because it is small and thus good for testing OpenStack.  # yum install -y wget # wget http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img # glance image-create --name \"Cirros 0.3.4\" --disk-format qcow2 --container-format bare --visibility public --file /root/cirros-0.3.4-x86_64-disk.img  Create a new m1.nano flavor for running Cirros image.  # nova flavor-create m1.nano 42 64 0 1  Create security group and allow all TCP ports.  # nova secgroup-create all \"Allow all tcp ports\" # nova secgroup-add-rule all TCP 1 65535 0.0.0.0/0  Create security group for base access  # nova secgroup-create base \"Allow Base Access\" # nova secgroup-add-rule base TCP 22 22 0.0.0.0/0 # nova secgroup-add-rule base TCP 80 80 0.0.0.0/0 # nova secgroup-add-rule base ICMP -1 -1 0.0.0.0/0  Create a private ssh key for connecting to instances remotely.  # nova keypair-add admin  Create admin.pem file and add private key from output of keypair-add command.  # vi /root/admin.pem # chmod 400 /root/admin.pem  List the network IDs.  # neutron net-list  +--------------------------------------+---------+-------------------------------------------------------+  | id | name | subnets |  +--------------------------------------+---------+-------------------------------------------------------+  | d4f3ed19-8be4-4d56-9f95-cfbac9fdf670 | private | 92d82f53-6e0b-4eef-b8b9-cae32cf40457 10.10.1.0/24     |  | 37c024d6-8108-468c-bc25-1748db7f5e8f | public  | 22f2e901-186f-4041-ad93-f7b5ccc30a81 192.168.122.0/24 |  Start an instance and make sure to replace network id from above command.  # nova boot --flavor m1.nano --image \"Cirros 0.3.4\" --nic net-id=92d82f53-6e0b-4eef-b8b9-cae32cf40457 --key-name admin --security-groups all mycirros  Create a floating IP and assign it to the mycirros instance.  # nova floating-ip-create # nova floating-ip-associate mycirros &lt;FLOATING IP&gt;  Connect to mycirros instance using the private ssh key stored in the admin.pem file. Note: The first floating IP in the range 192.168.122.201.  # ssh -i admin.pem cirros@192.168.122.201 Nova Nested Virtualization  Most OpenStack Lab or test environments will install OpenStack on a hypervisor platform inside virtual machines. I would strongly recommend KVM. If you are running OpenStack on KVM (Nova nested virtualization) make sure to follow these tips and tricks to get the best performance.  Summary  This article was intended as a hands on guide for standing up an OpenStack Liberty lab environment using RDO. As mentioned RDO is a stable community platform built around Red Hat's OpenStack Platform. It provides the ability to test the latest OpenStack features against either an enterprise platform (RHEL) or community platform (CentOS). Hopefully you found the information in this article useful. If you have anything to add or feedback, feel free to leave your comments.  Happy OpenStacking!  (c) 2016 Keith Tenzer  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  ","categories": ["OpenStack"],
        "tags": ["Cloud","Liberty","OpenStack","RDO"],
        "url": "/openstack/openstack-liberty-lab-installation-and-configuration-guide/",
        "teaser": null
      },{
        "title": "Red Hat Enterprise Virtualization (RHEV) - Management Options",
        "excerpt":"  Overview  RHEV has two separate distinct layers, the hypervisor itself and management. The hypervisor layer, RHEV-H is of course built on Red Hat Enterprise Linux (RHEL) and utilizes KVM for the hypervisor technology. RHEV-H can be configured using pre-built RHEV-H image or using standard RHEL. The management layer, Red Hat Enterprise Virtualization Management (RHEV-M) provides management for a multi-hypervisor environment and uses concepts such as datacenters, clusters, networks and storage domains to describe virtualization resources. In this article we will focus on options for configuring RHEV-M. The upstream opensource project behind RHEV-M is oVirt. There are two options as of RHEV 3.5 for configuring RHEV-M, standalone or hosted engine.  Below are other articles you may find of interest relating to RHEV:   RHEV Overview and Home Lab Configuration RHEVM Overview and APIs RHEV Hypervisor Host Options    RHEV-M Standalone  RHEV-M standalone means configuring a RHEL physical or virtual system as a RHEV-M host.  PROS   Flexibility can install RHEV-M on any RHEL 6 host.  CONS   HA not built-in, you need to deal with HA on your own. Can't use RHEL 7. Requires additional host outside of virtualization environment. RHEV-M runs outside virtualization environment and as such cannot directly utilize virtualization resources such as storage.  To configure RHEV using this method follow the steps in this already posted article.  RHEV-M Hosted Engine  RHEV-M hosted engine means configuring RHEV-M as a virtual machine directly on a hypervisor host. RHEV-M lives inside the virtualization environment it is managing. Before configuring hypervisor host, RHEV-M must be configured. As such hosted engine is installed using yum directly on chosen hypervisor host. Simply install RHEL 7 and follow the steps listed further below in this article.  PROS   Takes advantage of RHEV, HA is built-in. Simplified and streamlined installation of RHEV. Does not require separate host system.  CONS   At least one hypervisor must be running in order to access RHEV-M. If there are multiple-separate virtualization environments complexity could be greater.  Follow the below guide to configure a RHEV environment using the hosted engine.   Enable required repositories.  [root@rhevh01 ~]# subscription-manager register [root@rhevh01 ~]# subscription-manager attach --pool=3848378728191899189 [root@rhevh01 ~]# subscription-manager repos --disable=* [root@rhevh01 ~]# subscription-manager repos --enable=rhel-7-server-rpms [root@rhevh01 ~]# subscription-manager repos --enable=rhel-7-server-supplementary-rpms [root@rhevh01 ~]# subscription-manager repos --enable=rhel-7-server-optional-rpms [root@rhevh01 ~]# subscription-manager repos --enable=rhel-7-server-rhev-mgmt-agent-rpms  [root@rhevh01 ~]# yum update -y The hosted engine requires an NFS or ISCSI share. In this case we will use NFSv3 and create a local share on our hypervisor host. This storage is only used for hosted engine and won't show up in RHEV-M.   Install iptables (optional). Note: as of writing of this article, firewalld does not work with RHEV 3.5, at least that has been my experience.  [root@rhevh01 ~]# yum -y install iptables-services  Disable firewalld.  [root@rhevh01 ~]# systemctl stop firewalld [root@rhevh01 ~]# systemctl disable firewalld  Enable iptables (optional).  [root@rhevh01 ~]# systemctl enable iptables [root@rhevh01 ~]# systemctl start iptables  Configure iptables rules for RHEV and NFS (optional).  [root@rhevh01 ~]# vi /etc/sysconfig/iptables *filter :INPUT ACCEPT [0:0] :FORWARD ACCEPT [0:0] :OUTPUT ACCEPT [64:6816] -A INPUT -p udp -m udp --dport 32769 -j ACCEPT -A INPUT -p tcp -m tcp --dport 32803 -j ACCEPT -A INPUT -p udp -m udp --dport 662 -j ACCEPT -A INPUT -p tcp -m tcp --dport 662 -j ACCEPT -A INPUT -p udp -m udp --dport 875 -j ACCEPT -A INPUT -p tcp -m tcp --dport 875 -j ACCEPT -A INPUT -p udp -m udp --dport 892 -j ACCEPT -A INPUT -p tcp -m tcp --dport 892 -j ACCEPT -A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT -A INPUT -p icmp -j ACCEPT -A INPUT -i lo -j ACCEPT -A INPUT -p tcp -m tcp --dport 54321 -j ACCEPT -A INPUT -p tcp -m tcp --dport 111 -j ACCEPT -A INPUT -p udp -m udp --dport 111 -j ACCEPT -A INPUT -p tcp -m tcp --dport 22 -j ACCEPT -A INPUT -p udp -m udp --dport 161 -j ACCEPT -A INPUT -p tcp -m tcp --dport 38465 -j ACCEPT -A INPUT -p tcp -m tcp --dport 38466 -j ACCEPT -A INPUT -p tcp -m tcp --dport 38467 -j ACCEPT -A INPUT -p tcp -m tcp --dport 2049 -j ACCEPT -A INPUT -p tcp -m tcp --dport 39543 -j ACCEPT -A INPUT -p tcp -m tcp --dport 55863 -j ACCEPT -A INPUT -p tcp -m tcp --dport 38468 -j ACCEPT -A INPUT -p udp -m udp --dport 963 -j ACCEPT -A INPUT -p tcp -m tcp --dport 965 -j ACCEPT -A INPUT -p tcp -m tcp --dport 16514 -j ACCEPT -A INPUT -p tcp -m multiport --dports 5900:6923 -j ACCEPT -A INPUT -p tcp -m multiport --dports 49152:49216 -j ACCEPT -A INPUT -j REJECT --reject-with icmp-host-prohibited -A FORWARD -m physdev ! --physdev-is-bridged -j REJECT --reject-with icmp-host-prohibited COMMIT  Restart iptables (optional).  [root@rhevh01 ~]# systemctl restart iptables  Configure NFS Services.  [root@rhevh01 ~]# yum install nfs-utils rpcbind [root@rhevh01 ~]# systemctl enable rpcbind [root@rhevh01 ~]# systemctl enable nfs-server [root@rhevh01 ~]# systemctl start rpcbind [root@rhevh01 ~]# systemctl start nfs-server  Configure NFS Share.  [root@rhevh01 ~]# mkdir /usr/share/rhev [root@rhevh01 ~]# chown -R 36:36 /usr/share/rhev [root@rhevh01 ~]# chmod -R 0755 /usr/share/rhev [root@rhevh01 ~]# vi /etc/exports  /usr/share/rhev 192.168.2.0/24(rw) [root@rhevh01 ~]# exportfs -a  Install Hosted Engine.  [root@rhevh01 ~]# yum install -y ovirt-hosted-engine-setup Note: if you are connecting via ssh you will need to forward X11. If you are using windows then you need to install Xming and Putty explained here.   Install X11 libraries (optional).  [root@rhevh01 ~]# yum groupinstall -y \"Server with GUI\"  Configure Hosted Engine. Note: in this example, screen is used and this is strongly recommended if installing through ssh connection. In fact, anytime you are working through ssh you should use screen, if something unexpected happens you can reconnect.  [root@rhevh01 ~]# yum install -y screen [root@rhevh01 ~]# screen hosted-engine --deploy --== VM CONFIGURATION ==-- Please specify the device to boot the VM from (cdrom, disk, pxe) [cdrom]: disk  Please specify path to OVF archive you would like to use [None]: /usr/share/rhev/rhevm-appliance-20150421.0-1.x86_64.rhevm.ova [ INFO ] Checking OVF archive content (could take a few minutes depending on archive size) [ INFO ] Checking OVF XML content (could take a few minutes depending on archive size) [WARNING] OVF does not contain a valid image description, using default.  Please specify an alias for the Hosted Engine image [hosted_engine]:  The following CPU types are supported by this host:  - model_SandyBridge: Intel SandyBridge Family  - model_Westmere: Intel Westmere Family  - model_Nehalem: Intel Nehalem Family  - model_Penryn: Intel Penryn Family  - model_Conroe: Intel Conroe Family  Please specify the CPU type to be used by the VM [model_SandyBridge]: [WARNING] Minimum requirements for CPUs not met  You may specify a unicast MAC address for the VM or accept a randomly generated default [00:16:3e:04:eb:78]:  Please specify the console type you would like to use to connect to the VM (vnc, spice) [vnc]: spice --== HOSTED ENGINE CONFIGURATION ==-- Enter the name which will be used to identify this host inside the Administrator Portal [hosted_engine_1]:  Enter 'admin@internal' user password that will be used for accessing the Administrator Portal:  Confirm 'admin@internal' user password:  Please provide the FQDN for the engine you would like to use.  This needs to match the FQDN that you will use for the engine installation within the VM.  Note: This will be the FQDN of the VM you are now going to create,  it should not point to the base host or to any other existing machine.  Engine FQDN: he01.lab [WARNING] Failed to resolve he01.lab using DNS, it can be resolved only locally  Please provide the name of the SMTP server through which we will send notifications [localhost]:  Please provide the TCP port number of the SMTP server [25]:  Please provide the email address from which notifications will be sent [root@localhost]:  Please provide a comma-separated list of email addresses which will get notifications [root@localhost]: [ INFO ] Stage: Setup validation [WARNING] Failed to resolve rhevh01.lab using DNS, it can be resolved only locally --== CONFIGURATION PREVIEW ==-- Bridge interface : eno1  Engine FQDN : he01.lab  Bridge name : rhevm  SSH daemon port : 22  Firewall manager : iptables  Gateway address : 192.168.0.1  Host name for web application : hosted_engine_1  Host ID : 1  Image alias : hosted_engine  Image size GB : 50  Storage connection : rhevh01.lab:/usr/share/rhev  Console type : qxl  Memory size MB : 4096  MAC address : 00:16:3e:04:eb:78  Boot type : disk  Number of CPUs : 1  OVF archive (for disk boot) : /usr/share/rhev/rhevm-appliance-20150421.0-1.x86_64.rhevm.ova  CPU Type : model_SandyBridge Please confirm installation settings (Yes, No)[Yes]: Once you confirm settings the hosted engine will be deployed and you will be prompted to connect to the console of the hosted engine.   Using remote-viewer connect to console of hosted engine.  [root@rhevh01 ~]# /bin/remote-viewer --spice-ca-file=/etc/pki/vdsm/libvirt-spice/ca-cert.pem spice://localhost?tls-port=5901 --spice-host-subject=\"C=EN, L=Test, O=Test, CN=Test\"    Configure authentication settings and set root password in the tools menu. Configure networking. Register with subscription manager.  [root@rhevm ~]# subscription-manager register [root@rhevm ~]# subscription-manager attach --pool=3948394898198989000922  Configure yum repositories.  [root@rhevm ~]# subscription-manager repos --disable=*  [root@rhevm ~]# subscription-manager repos --enable=rhel-6-server-rpms  [root@rhevm ~]# subscription-manager repos --enable=rhel-6-server-optional-rpms  [root@rhevm ~]# subscription-manager repos --enable=rhel-6-server-supplementary-rpms  [root@rhevm ~]# subscription-manager repos --enable=rhel-6-server-rhev-mgmt-agent-rpms  [root@rhevm ~]# subscription-manager repos --enable=rhel-6-server-rhevm-3.5-rpms  Installed RHEV-M.  # yum update -y # yum install -y rhevm # engine-setup  After completing engine-setup go back to hypervisor and press [1] to continue hosted engine setup.  [ INFO ] Engine replied: DB Up!Welcome to Health Status!  Enter the name of the cluster to which you want to add the host (Default) [Default]: [ INFO ] Waiting for the host to become operational in the engine. This may take several minutes... [ INFO ] Still waiting for VDSM host to become operational... [ INFO ] Still waiting for VDSM host to become operational... [ INFO ] The VDSM Host is now operational  Please shutdown the VM allowing the system to launch it as a monitored service.  The system will wait until the VM is down.  Shutdown the hosted engine VM.  [root@rhevm ~]# shutdown -h now Once the hosted VM is shutdown, the installation completes. Congrats, you have successfully deployed a RHEV environment with the hosted engine option.  Add Additional Hypervisor Host to RHEV-M Hosted Engine  Adding an additional hypervisor host using the hosted engine is a simple process.   Enable required repositories.  [root@rhevh01 ~]# subscription-manager register [root@rhevh01 ~]# subscription-manager attach --pool=3848378728191899189 [root@rhevh02 ~]# subscription-manager repos --disable=* [root@rhevh02 ~]# subscription-manager repos --enable=rhel-7-server-rpms [root@rhevh02 ~]# subscription-manager repos --enable=rhel-7-server-supplementary-rpms [root@rhevh02 ~]# subscription-manager repos --enable=rhel-7-server-optional-rpms [root@rhevh02 ~]# subscription-manager repos --enable=rhel-7-server-rhev-mgmt-agent-rpms  [root@rhevh02 ~]# yum update -y  Install iptables (optional). Note: as of writing of this article, firewalld does not work with RHEV 3.5.  [root@rhevh02 ~]# yum -y install iptables-services  Disable firewalld.  [root@rhevh02 ~]# systemctl stop firewalld [root@rhevh02 ~]# systemctl disable firewalld  Enable iptables (optional).  [root@rhevh02 ~]# systemctl enable iptables [root@rhevh02 ~]# systemctl start iptables  Configure iptables rules for RHEV and NFS (optional).  [root@rhevh02 ~]# vi /etc/sysconfig/iptables *filter :INPUT ACCEPT [0:0] :FORWARD ACCEPT [0:0] :OUTPUT ACCEPT [64:6816] -A INPUT -p udp -m udp --dport 32769 -j ACCEPT -A INPUT -p tcp -m tcp --dport 32803 -j ACCEPT -A INPUT -p udp -m udp --dport 662 -j ACCEPT -A INPUT -p tcp -m tcp --dport 662 -j ACCEPT -A INPUT -p udp -m udp --dport 875 -j ACCEPT -A INPUT -p tcp -m tcp --dport 875 -j ACCEPT -A INPUT -p udp -m udp --dport 892 -j ACCEPT -A INPUT -p tcp -m tcp --dport 892 -j ACCEPT -A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT -A INPUT -p icmp -j ACCEPT -A INPUT -i lo -j ACCEPT -A INPUT -p tcp -m tcp --dport 54321 -j ACCEPT -A INPUT -p tcp -m tcp --dport 111 -j ACCEPT -A INPUT -p udp -m udp --dport 111 -j ACCEPT -A INPUT -p tcp -m tcp --dport 22 -j ACCEPT -A INPUT -p udp -m udp --dport 161 -j ACCEPT -A INPUT -p tcp -m tcp --dport 38465 -j ACCEPT -A INPUT -p tcp -m tcp --dport 38466 -j ACCEPT -A INPUT -p tcp -m tcp --dport 38467 -j ACCEPT -A INPUT -p tcp -m tcp --dport 2049 -j ACCEPT -A INPUT -p tcp -m tcp --dport 39543 -j ACCEPT -A INPUT -p tcp -m tcp --dport 55863 -j ACCEPT -A INPUT -p tcp -m tcp --dport 38468 -j ACCEPT -A INPUT -p udp -m udp --dport 963 -j ACCEPT -A INPUT -p tcp -m tcp --dport 965 -j ACCEPT -A INPUT -p tcp -m tcp --dport 16514 -j ACCEPT -A INPUT -p tcp -m multiport --dports 5900:6923 -j ACCEPT -A INPUT -p tcp -m multiport --dports 49152:49216 -j ACCEPT -A INPUT -j REJECT --reject-with icmp-host-prohibited -A FORWARD -m physdev ! --physdev-is-bridged -j REJECT --reject-with icmp-host-prohibited COMMIT  Restart iptables (optional).  [root@rhevh02 ~]# systemctl restart iptables  Install Hosted Engine.  [root@rhevh02 ~]# yum install -y ovirt-hosted-engine-setup Note: if you are connecting via ssh you will need to forward X11. If you are using windows then you need to install Xming and Putty explained here.   Install X11 libraries (optional).  [root@rhevh02 ~]# yum groupinstall -y \"Server with GUI\"  Configure Hosted Engine. Note: in this example screen is used and this is strongly recommended if installing through ssh connection.  [root@rhevh02 ~]# yum install -y screen [root@rhevh02 ~]# screen hosted-engine --deploy When prompted for storage connection, provide the same NFS share used when originally configuring hosted engine [rhevh01.lab:/usr/share/rhev]. The install script will detect the storage connection and instead of deploying hosted engine, will add the hypervisor host to already running hosted engine.  In order to interact with the hosted engine, the hosted-engine CLI must be used. You can start, stop and get the status of the hosted engine using this CLI tool.  [root@rhevh01 ~]# hosted-engine --vm-status  --== Host 1 status ==--  Status up-to-date : True Hostname : rhevh01.lab Host ID : 1 Engine status : {\"reason\": \"bad vm status\", \"health\": \"bad\", \"vm\": \"up\", \"detail\": \"powering up\"} Score : 2400 Local maintenance : False Host timestamp : 386 Extra metadata (valid at timestamp):  metadata_parse_version=1  metadata_feature_version=1  timestamp=386 (Sat Jan 2 11:06:10 2016)  host-id=1  score=2400  maintenance=False  state=EngineStarting  --== Host 2 status ==--  Status up-to-date : False Hostname : rhevh02.lab Host ID : 2 Engine status : unknown stale-data Score : 2400 Local maintenance : False Host timestamp : 444 Extra metadata (valid at timestamp):  metadata_parse_version=1  metadata_feature_version=1  timestamp=444 (Sat Jan 2 11:06:17 2016)  host-id=2  score=2400  maintenance=False  state=EngineDown Summary  In this article we have examined the different options for deploying Red Hat Enterprise Virtualization Management (RHEV-M). We have discussed the various trade-offs when using RHEV-M standalone or hosted engine. Unless special requirements exist, the best option is definitely the hosted engine approach. Finally we have seen how to configure RHEV-M using both standalone (discussed in previous article) and hosted engine.  Virtualization technology has become a commodity but not its management. The once dominant proprietary virtualization solutions no longer provide the same competitive edge. Opensource virtualization such as RHEV allow independence and avoid vendor lock-in to costly proprietary management solutions.  The greatest benefit of KVM over other virtualization technologies is the APIs and management are literally wide open. OpenStack for example interfaces with KVM hypervisors directly, it doesn't need RHEV-M because KVM is open and follows open standards. OpenStack when interfacing with other virtualization technologies, such as VMware and Hyper-V, requires proprietary management which is not only costly but quite limiting. The question we all should be asking is, do we want to drag proprietary management solutions with us on our journey to the cloud? Now is the time to make the switch and move to KVM by implementing RHEV. If you find yourself on the journey to opensource virtualization, I strongly suggest reading the other articles posted above. Together we grow so please share.  Happy RHEVing!  (c) 2016 Keith Tenzer  ","categories": ["RHEV"],
        "tags": ["KVM","Linux","RHEV-H","RHEV-M"],
        "url": "/rhev/red-hat-enterprise-virtualization-rhev-management-options/",
        "teaser": null
      },{
        "title": "OpenStack Keystone: Integrating LDAP with IPA",
        "excerpt":"Overview    Keystone is the identity service in OpenStack responsible for authentication of users and services. Keystone leverages tokens which are transient in nature. In addition to authentication Keystone allows for policy management defining roles and responsibilities that govern users, services and tenants. Fine granular RBAC is also possible, Keystone allows for mapping capabilities directly to users. Finally, Keystone provides a catalog for all service endpoints within OpenStack. Most organizations will have either central AD or LDAP for managing users and services. In this article we will integrate Keystone with LDAP using central IPA server.    Setup IPA Server  A special thanks to Asaf Waizman and his team at Red Hat for providing a lot of insight on this subject. Before we configure Keystone we need to ensure a central identity management system is configured.   Install RHEL or CentOS 7.1 If using RHEL ensure subscription-manager is configured for appropriate repos  [root@ipa ~]# subscription-manager attach --pool=39482983298398232 [root@ipa ~]# subscription-manager repos --disable=*  [root@ipa ~]# subscription-manager repos --enable=rhel-7-server-rpms  [root@ipa ~]# subscription-manager repos --enable=rhel-7-server-aus-rpms  Install IPA Server  [root@ipa ~]# yum install ipa-server ipa-server-dns  Configure IPA Server  [root@ipa ~]# ipa-server-install  The log file for this installation can be found in /var/log/ipaserver-install.log ============================================================================== This program will set up the IPA Server.  This includes:  * Configure a stand-alone CA (dogtag) for certificate management  * Configure the Network Time Daemon (ntpd)  * Create and configure an instance of Directory Server  * Create and configure a Kerberos Key Distribution Center (KDC)  * Configure Apache (httpd)  To accept the default shown in brackets, press the Enter key.  Do you want to configure integrated DNS (BIND)? [no]: yes  Enter the fully qualified domain name of the computer on which you're setting up server software. Using the form &lt;hostname&gt;.&lt;domainname&gt; Example: master.example.com.   Server host name [ipa.lab.com]:   Warning: skipping DNS resolution of host ipa.lab.com The domain name has been determined based on the host name.  Please confirm the domain name [lab.com]:   The kerberos protocol requires a Realm name to be defined. This is typically the domain name converted to uppercase.  Please provide a realm name [LAB.COM]:  Certain directory server operations require an administrative user. This user is referred to as the Directory Manager and has full access to the Directory for system management tasks and will be added to the instance of directory server created for IPA. The password must be at least 8 characters long.  Directory Manager password:  Password (confirm):   The IPA server requires an administrative user, named 'admin'. This user is a regular system account used for IPA server administration.  IPA admin password:  Password (confirm):   Existing BIND configuration detected, overwrite? [no]: yes Do you want to configure DNS forwarders? [yes]:  Enter an IP address for a DNS forwarder, or press Enter to skip: 192.168.122.1 DNS forwarder 192.168.122.1 added. You may add another. Enter an IP address for a DNS forwarder, or press Enter to skip:  Checking DNS forwarders, please wait ... Do you want to configure the reverse zone? [yes]:  Please specify the reverse zone name [122.168.192.in-addr.arpa.]:  Using reverse zone(s) 122.168.192.in-addr.arpa.  The IPA Master Server will be configured with: Hostname: ipa.lab.com IP address(es): 192.168.122.100 Domain name: lab.com Realm name: LAB.COM  BIND DNS server will be configured to serve IPA domain with: Forwarders: 192.168.122.1 Reverse zone(s): 122.168.192.in-addr.arpa.  Continue to configure the system with these values? [no]: yes  The following operations may take some minutes to complete. Please wait until the prompt is returned. Configure Keystone Users in IPA  In this example we will configure three users in LDAP. An openstack user is needed for authentication of keystone itself within LDAP. An openstack admin user is needed to administer OpenStack and we will also create a tenant or normal OpenStack user.   Configure OpenStack user for Keystone authentication  [root@ipa ~]# kinit admin [root@ipa ~]# ipa user-add openstack --first=\"OpenStack\" --last \"Ldap\" [root@ipa ~]# ipa passwd openstack  New Password: changeme  Enter New Password again to verify: changeme  ----------------------------------------  Changed password for \"openstack@LAB.COM\"  ---------------------------------------- [root@ipa ~]# kinit openstack  Password for openstack@LAB.COM:  Password expired. You must change it now.  Enter new password:  Enter it again:  Configure OpenStack user, this will be a tenant user within OpenStack  [root@ipa ~]# kinit admin [root@ipa ~]# ipa user-add ospuser --first=\"ospuser\" --last \"ospuser\" [root@ipa ~]# ipa passwd ospuser  New Password:  changeme  Enter New Password again to verify:  changeme  --------------------------------------  Changed password for \"ospuser@LAB.COM\"  -------------------------------------- [root@ipa ~]# kinit ospuser  Password for ospuser@LAB.COM:  Password expired. You must change it now.  Enter new password:  Enter it again:  Configure OpenStack administrator, this will be admin within OpenStack  [root@ipa ~]# kinit admin [root@ipa ~]# ipa user-add ospadmin --first=\"ospadmin\" --last \"ospadmin\"  [root@ipa ~]# ipa passwd ospadmin  New Password: changeme  Enter New Password again to verify: changeme  ---------------------------------------  Changed password for \"ospadmin@LAB.COM\"  --------------------------------------- [root@ipa ~]# kinit ospadmin  Password for ospadmin@LAB.COM:  Password expired. You must change it now.  Enter new password:  Enter it again:  Create LDAP group for OpenStack users  [root@ipa ~]# kinit admin [root@ipa ~]# ipa group-add enabled_users --desc \"Users who can access OpenStack\" [root@ipa ~]# ipa group-add-member enabled_users --users=ospuser [root@ipa ~]# ipa group-add-member enabled_users --users=ospadmin  Enable LDAP ports in firewall  [root@ipa ~]# firewall-cmd --add-service ldap --permanent [root@ipa ~]# firewall-cmd --reload Configure Keystone Services in IPA  As mentioned Keystone authenticates both users and services. However in order to complete integration, we also need to authenticate OpenStack services in LDAP. A typical OpenStack deployment consists of the following services: glance, cinder, heat, neutron, nova and swift. If you are using additional OpenStack services such as manila, trove, sahara, etc you will also need to add additional accounts for those services.  [root@ipa ~]# kinit admin [root@ipa ~]# for p in glance cinder heat neutron nova swift; do echo \"configuring service $p\"; ipa user-add $p --cn=$p --first=\"$p\" --last \"$p\"; ipa group-add-member enabled_users --users=$p; ipa passwd $p; kinit $p; kinit admin; done Disabling service password expiration  Typically you don't want Keystone passwords for services to expire. In IPA the default is 90 days for password expiration. We will set the number to something that likely won't be reached in the lifetime of this OpenStack cloud. For production you will want to consider a better solution.  [root@ipa ~]# kinit admin [root@ipa ~]# ipa group-add --nonposix service-accounts [root@ipa ~]# ipa pwpolicy-add service-accounts --maxlife=20000 --minlife=0 --history=0 --maxfail=0 --priority=0 [root@ipa ~]# for user in cinder nova glance swift neutron openstack; do ipa group-add-member service-accounts --user=$user; done Configure Keystone  Now that we have an LDAP server with OpenStack users and services configured it is time to enable LDAP backend within Keystone.   Enable LDAP backend and configure authentication  [root@osp7.lab.com ~]# vi /etc/keystone/keystone.conf [identity]  driver = keystone.identity.backends.ldap.Identity  [assignment] driver = keystone.assignment.backends.sql.Assignment  [ldap] url = ldap://ipa.lab.com user = uid=openstack,cn=users,cn=accounts,dc=lab,dc=com password = redhat01 user_tree_dn=cn=users,cn=accounts,dc=lab,dc=com user_id_attribute=uid group_tree_dn=cn=groups,cn=accounts,dc=lab,dc=com user_enabled_emulation = True user_enabled_emulation_dn = cn=enabled_users,cn=groups,cn=accounts,dc=lab,dc=com user_allow_create = False user_allow_update = False user_allow_delete = False    Create roles for OpenStack administrator and user. This is how you would give external users roles within OpenStack.  [root@osp7.lab.com ~]# export SERVICE_TOKEN=$(openstack-config --get /etc/keystone/keystone.conf \"\" admin_token) [root@osp7.lab.com ~]# export SERVICE_ENDPOINT=http://osp7.lab.com:35357/v2.0/ [root@osp7.lab.com ~]# keystone user-role-add --user-id ospadmin --role admin --tenant admin [root@osp7.lab.com ~]# keystone user-role-add --user-id ospuser --role _member_ --tenant Project1  Update the OpenStack service configuration files with LDAP user/password  [root@osp7.lab.com ~]# vi /etc/nova/nova.conf admin_user = &lt;service&gt; admin_password = &lt;password&gt; admin_tenant_name = services Service should be the name of the OpenStack service (nova, cinder, heat, neutron, swift, glance, etc). The password should be password of the service user in LDAP.  Note: You need to do this for each OpenStack service, below are a list of the files that need to be updated for standard OpenStack services.  /etc/nova/nova.conf /etc/cinder/cinder.conf /etc/heat/heat.conf /etc/neutron/neutron.conf /etc/swift/swift.conf /etc/cinder/api-paste.ini /etc/glance/glance-api.conf /etc/glance/glance-cache.conf /etc/glance/glance-registry.conf  Restart OpenStack Services  [root@osp7.lab.com ~]# openstack-service restart [root@osp7.lab.com ~]# systemctl restart httpd  Test OpenStack and ensure things are working with LDAP backend  [root@osp7.lab.com ~]# vi ~/keystonerc_ospadmin  export OS_USERNAME=ospadmin  export OS_PROJECT_NAME=admin  export OS_PASSWORD='r3dh4t1!'  export OS_AUTH_URL=http://192.168.0.20:35357/v2.0/  export PS1='[\\u@\\h \\W(keystone_ospadmin)]\\$ ' [root@osp7.lab.com ~]# source ~/keystonerc_ospadmin [root@osp7.lab.com ~]# openstack user list +-----------+----------+ | ID        | Name     | +-----------+----------+ | openstack | Ldap     | | ospuser   | ospuser  | | ospadmin  | ospadmin | | glance    | glance   | | cinder    | cinder   | | heat      | heat     | | neutron   | neutron  | | nova      | nova     | | swift     | swift    | +-----------+----------+ Summary  In this article we have seen how to integrate LDAP (IPA) as a backend in Keystone. We have discussed that Keystone not only handles user but also service authentication within OpenStack providing a catalog for all service endpoints. Beyond these capabilities, Keystone is an integral part of any OpenStack architecture and one of the most crucial integration points with existing infrastructure. As always hopefully this article was helpful, all feedback is greatly appreciated!  Happy OpenStacking!  (c) 2016 Keith Tenzer  ","categories": ["OpenStack"],
        "tags": ["Keystone","LDAP"],
        "url": "/openstack/openstack-keystone-integrating-ldap-with-ipa/",
        "teaser": null
      },{
        "title": "OpenShift Enterprise 3.1 Lab Setup",
        "excerpt":"  Overview  OpenShift Enterprise is a PaaS platform that enables digital transformation. It lets you build and run traditional (mode 1) as well as cloud-native (mode 2) applications. OpenShift is built on two key technology components: Docker and Kubernetes. Docker provides a standard, consistent application packaging format. It enables OpenShift to easily move applications across the hybrid cloud. Kubernetes provides container orchestration and allows multiple container nodes running Docker to be clustered. Kubernetes provides scheduling for application containers.  OpenShift of course provides a lot on top of Docker and Kubernetes. This includes image registry, routing, SDN, developer experience, data persistence, enterprise-grade container runtime, build / deployment blueprints and much more.    OpenShift has two server roles: master and node. The master is the control plane and is responsible for Kubernetes, ETCD, Interfaces (API|GUI|CLI), build/deployments configs and the image registry. The node is where containers are instantiated and run. The node is running Kubernetes services, ETCD and of course Docker. In OpenShift you can use either Red Hat Enterprise Linux (RHEL) or RHEL Atomic as the operating system.  Prerequisites  One can of course install OpenShift enterprise on a single VM but even for a lab setup it is a good idea to separate the role of master and node. For this lab configuration we need two VMs both with Red Hat Enterprise Linux 7.2. One node will be the OpenShift master and the other OpenShift node. Ensure both have an additional 10GB disk, this will be used for Docker storage. On both nodes follow the below steps:  Ensure hostnames are configured in /etc/hosts.  #vi /etc/hosts 192.168.122.60 ose3-master.lab.com ose3-master 192.168.122.61 ose3-node1.lab.com ose3-node1 Register systems with subscription management and ensure appropriate repositories are configured.  #subscription-manager register #subscription-manager list --available #subscription-manager attach --pool=8a85f9814f2c669b01343948398938932 #subscription-manager repos --disable=\"*\" #subscription-manager repos --enable=\"rhel-7-server-rpms\" --enable=\"rhel-7-server-extras-rpms\" --enable=\"rhel-7-server-ose-3.1-rpms\" Disable Network Manager.  #systemctl disable NetworkManager #systemctl stop NetworkManager Install necessary software.  #yum install -y wget git net-tools bind-utils iptables-services bridge-utils bash-completion #yum update -y #yum install -y atomic-openshift-utils Reboot.  #systemctl reboot Install and configure Docker.  #yum install docker-1.8.2 #vi /etc/sysconfig/docker OPTIONS='--insecure-registry=172.30.0.0/16 --selinux-enabled' # cat &lt;&lt;EOF &gt; /etc/sysconfig/docker-storage-setup DEVS=/dev/vdb VG=docker-vg EOF #docker-storage-setup  #systemctl enable docker  #systemctl start docker Setup ssh keys, this is required by Ansible.  #ssh-keygen #ssh-copy-id -i ~/.ssh/id_rsa.pub ose3-node1.lab.com DNS is also a requirement. A colleague Ivan Mckinely was nice enough to create an ansible playbook for deploying a DNS server that supports OpenShift. To deploy DNS run following steps on a separate system running RHEL.  #yum install -y git #wget http://dl.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-5.noarch.rpm #rpm -ivh epel-release-7-5.noarch.rpm #yum install -y ansible #git clone https://github.com/ivanthelad/ansible-aos-scripts/tree/master/playbooks/roles #cd ansible-aos-scripts Edit inventory file and set dns to IP of the system that should be providing DNS. Also ensure nodes and masters have correct IPs for your OpenShift servers. In our case 192.168.122.100 is dns, 192.168.122.60 master and 192.168.122.61 node.  #vi inventory [dns] 192.168.122.100 [nodes] 192.168.122.61 [masters] 192.168.122.60 Configure ssh on DNS host  #ssh-keygen #ssh-copy-id -i ~/.ssh/id_rsa.pub localhost Run Ansible.  #ansible-playbook -i inventory playbooks/install_dnsmas.yml Install OpenShift Enterprise 3.1  There are two different installations, the quick and advanced installation. The quick installation provides a interactive menu that guides you through a basic install. The advanced installation allows you to directly configure the install yaml files used by Ansible and perform an unattended install. Things like enabling the OVS Multi-tenant SDN would be done using advanced deployment. A template for deploying an advanced installation of OpenShift is available here.  For the purpose of our lab setup, the basic install is enough. In addition since OpenShift Enterprise uses Ansible you can easily change things later and re-run Ansible. Ansible rocks!  Start basic installer.  #atomic-openshift-installer install Welcome to the OpenShift Enterprise 3 installation. Please confirm that following prerequisites have been met: * All systems where OpenShift will be installed are running Red Hat Enterprise  Linux 7. * All systems are properly subscribed to the required OpenShift Enterprise 3  repositories. * All systems have run docker-storage-setup (part of the Red Hat docker RPM). * All systems have working DNS that resolves not only from the perspective of  the installer but also from within the cluster. When the process completes you will have a default configuration for Masters and Nodes. For ongoing environment maintenance it's recommended that the official Ansible playbooks be used. For more information on installation prerequisites please see: https://docs.openshift.com/enterprise/latest/admin_guide/install/prerequisites.html Are you ready to continue? [y/N]: Choose OpenShift version to isntall.  This installation process will involve connecting to remote hosts via ssh. Any account may be used however if a non-root account is used it must have passwordless sudo access. User for ssh access [root]: &nbsp;  Which variant would you like to install? (1) OpenShift Enterprise 3.1 (2) OpenShift Enterprise 3.0 (3) Atomic Enterprise Platform 3.1 Choose a variant from above: [1]: Configure OpenShift master.  *** Host Configuration *** You must now specify the hosts that will compose your OpenShift cluster. Please enter an IP or hostname to connect to for each system in the cluster. You will then be prompted to identify what role you would like this system to serve in the cluster. OpenShift Masters serve the API and web console and coordinate the jobs to run across the environment. If desired you can specify multiple Master systems for an HA deployment, in which case you will be prompted to identify a *separate* system to act as the load balancer for your cluster after all Masters and Nodes are defined. If only one Master is specified, an etcd instance embedded within the OpenShift Master service will be used as the datastore. This can be later replaced with a separate etcd instance if desired. If multiple Masters are specified, a separate etcd cluster will be configured with each Master serving as a member. Any Masters configured as part of this installation process will also be configured as Nodes. This is so that the Master will be able to proxy to Pods from the API. By default this Node will be unschedulable but this can be changed after installation with 'oadm manage-node'. OpenShift Nodes provide the runtime environments for containers. They will host the required services to be managed by the Master. http://docs.openshift.com/enterprise/latest/architecture/infrastructure_components/kubernetes_infrastructure.html#master http://docs.openshift.com/enterprise/latest/architecture/infrastructure_components/kubernetes_infrastructure.html#node   Enter hostname or IP address: ose3-master.lab.com Will this host be an OpenShift Master? [y/N]: y Will this host be RPM or Container based (rpm/container)? [rpm]: Configure OpenShift node.  *** Installation Summary *** Hosts: - ose3-master.lab.com  - OpenShift Master  - OpenShift Node  - Etcd (Embedded) Total OpenShift Masters: 1 Total OpenShift Nodes: 1 NOTE: Add a total of 3 or more Masters to perform an HA installation. Do you want to add additional hosts? [y/N]: y Enter hostname or IP address: ose3-node1.lab.com Will this host be an OpenShift Master? [y/N]: n Will this host be RPM or Container based (rpm/container)? [rpm]: Review installation summary.  *** Installation Summary *** Hosts: - ose3-master.lab.com  - OpenShift Master  - OpenShift Node (Unscheduled)  - Etcd (Embedded) - ose3-node1.lab.com  - OpenShift Node (Dedicated) Total OpenShift Masters: 1 Total OpenShift Nodes: 2 NOTE: Add a total of 3 or more Masters to perform an HA installation. Do you want to add additional hosts? [y/N]: y You might want to override the default subdomain uses for exposed routes. If you don't know what this is, use the default value. New default subdomain (ENTER for none) []: apps.lab.com Check the installation details and proceed if everything looks right.  A list of the facts gathered from the provided hosts follows. Because it is often the case that the hostname for a system inside the cluster is different from the hostname that is resolveable from command line or web clients these settings cannot be validated automatically. For some cloud providers the installer is able to gather metadata exposed in the instance so reasonable defaults will be provided. Plese confirm that they are correct before moving forward. ose3-master.lab.com,192.168.122.60,192.168.122.60,ose3-master.lab.com,ose3-master.lab.com ose3-node1.lab.com,192.168.122.61,192.168.122.61,ose3-node1.lab.com,ose3-node1.lab.com Format: connect_to,IP,public IP,hostname,public hostname Notes:  * The installation host is the hostname from the installer's perspective.  * The IP of the host should be the internal IP of the instance.  * The public IP should be the externally accessible IP associated with the instance  * The hostname should resolve to the internal IP from the instances  themselves.  * The public hostname should resolve to the external ip from hosts outside of  the cloud. Do the above facts look correct? [y/N]: y Configure OpenShift  One of the advantages of using the advanced install is you can configure everything at install time. Since we went with basic install there are a few things that need to be done after installation is complete.  Authentication  By default authentication is set to deny all. There are various identity providers but for a lab setup it is easiest to use HTpassword.  #yum install -y httpd-tools #vi /etc/origin/master/master-config.yaml identityProviders:  - name: my_htpasswd_provider  challenge: true  login: true  mappingMethod: claim  provider:  apiVersion: v1  kind: HTPasswdPasswordIdentityProvider  file: /root/users.htpasswd #htpasswd -c /root/users.htpasswd admin Give the new admin user role cluster-admin in OpenShift  #oadm policy add-cluster-role-to-user cluster-admin admin Deploy Registry  The registry stores docker images that are used for OpenShift builds and deployments. For lab environment it is appropriate to use local disk but for production make sure you are using production grade storage and the --mount-host option.  #oadm registry --service-account=registry \\     --config=/etc/origin/master/admin.kubeconfig \\     --credentials=/etc/origin/master/openshift-registry.kubeconfig \\     --images='registry.access.redhat.com/openshift3/ose-${component}:${version}' Deploy Router  The router in OpenShift is responsible for connecting external services to the correct pod running containers for given service. External service has a DNS record and the router maps this to the pod/container IP which of course is very dynamic.  oadm router router --replicas=1 \\  --credentials='/etc/origin/master/openshift-router.kubeconfig' \\  --service-account=router Once both the registry and router are running you are ready to rock with Op  enShift Enterprise!    Aggregate Logging  OpenShift Enterprise supports Kibana and the ELK Stack for log aggregation. Any pod and container that log to STDOUT will have all their log messages aggregated. This provides centralized logging for all application components. Logging is completely integrated within OpenShift and the ELK Stack runs of course containerized within OpenShift.  #oc new-project logging #oc secrets new logging-deployer nothing=/dev/null #oc create -f - &lt;&lt;API apiVersion: v1 kind: ServiceAccount metadata:   name: logging-deployer secrets:    -name: logging -deployer API #oc policy add-role-to-user edit --serviceaccount logging-deployer  #oadm policy add-scc-to-user  \\     privileged system:serviceaccount:logging:aggregated-logging-fluentd  #oadm policy add-cluster-role-to-user cluster-reader \\     system:serviceaccount:logging:aggregated-logging-fluentd  #oc process logging-deployer-template -n openshift -v KIBANA_OPS_HOSTNAME=kibana-ops.lab.com,KIBANA_HOSTNAME=kibana.lab.com,ES_CLUSTER_SIZE=1,PUBLIC_MASTER_URL=https://ose3-master.lab.com:8443 | oc create -f - # oc get pods NAME READY STATUS RESTARTS AGE logging-deployer-9lqkt 0/1 Completed 0 15m When deployer is complete than create deployment templates  #oc process logging-support-template | oc create -f -  # oc get pods NAME READY STATUS RESTARTS AGE logging-deployer-9lqkt 0/1 Completed 0 15m logging-es-pm7uamdy-2-rdflo 1/1 Running 0 8m logging-kibana-1-e13r3 2/2 Running 0 13m Once ELK Stack is running update deployment so that persistent storage is used  #vi pvc.json  {      \"apiVersion\": \"v1\",     \"kind\": \"PersistentVolumeClaim\",     \"metadata\": {          \"name\": \"logging-es-1\"     },     \"spec\": {         \"accessModes\": [ \"ReadWriteOnce\" ],             \"resources\": {                 \"requests\": {                     \"storage\": \"10Gi\"                 }             }      } } #oc create -f pvc.json # oc get dc NAME TRIGGERS LATEST logging-es-pm7uamdy ConfigChange, ImageChange 2 #oc volume dc/logging-es-pm7uamdy --add --overwrite --name=elasticsearch-storage --type=persistentVolumeClaim --claim-name=logging-es-1 Enable Multi-tenant network  OpenShift Enterprise supports an OVS multi-tenant network configuration. By default OpenShift will configure ovs-subnet plugin. In subnet mode all pods and as such containers can access all other pods and containers within the Kubernetes cluster. In order to support isolation between projects the ovs-multitenant plugin is required. This steps are for switching from ovs-subnet to ovs-multitenant plugins.  Delete Registry and Router (if exist)  #oc project default #oc delete dc/docker-registry svc/docker-registry #oc delete dc/router svc/router On Master  Change network plugin to ovs-multitenant.  #vi /etc/origin/master/master-config.yaml networkConfig:  clusterNetworkCIDR: 10.1.0.0/16  hostSubnetLength: 8 # networkPluginName: redhat/openshift-ovs-subnet  networkPluginName: redhat/openshift-ovs-multitenant On Node  Change network plugin to ovs-multitenant. Note: Only change second iteration of networkPluginName.  #vi /etc/origin/node/node-config.yaml networkConfig:  mtu: 1450 # networkPluginName: redhat/openshift-ovs-subnet  networkPluginName: redhat/openshift-ovs-multitenant On All Nodes  #systemctl reboot  After reboot you will need to recreate the registry and router in the default project using the above steps. The default project has VNID 0 so all pods and containers can reach registry and router.  You can also enable network access between projects if desired.  Testing  In order to test multi-tenant network create two projects, you should not be able to access pods across projects.  #oc new-project project1  #oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-hello-world.git  #oc new-project project2  #oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-hello-world.git Get the pod id.  # oc get pods NAME READY STATUS RESTARTS AGE ruby-hello-world-1-build 0/1 Completed 0 33m ruby-hello-world-2-xde1a 1/1 Running 0 27m Get ip of pod.  # oc describe pod ruby-hello-world-2-xde1a Name: ruby-hello-world-2-xde1a Namespace: sandbox Image(s): 172.30.186.116:5000/sandbox/ruby-hello-world@sha256:c8339364954812a20061d3a97403af30837deacaa134cc611c5cc12279df6685 Node: ose3-node1.lab.com/192.168.122.61 Start Time: Wed, 13 Apr 2016 20:42:44 +0200 Labels: app=ruby-hello-world,deployment=ruby-hello-world-2,deploymentconfig=ruby-hello-world Status: Running Reason:  Message:  IP: 10.1.0.11 Switch to other project  #oc project project1 Try and use curl to access the ruby application in project 1 from project 2  #oc exec ruby-hello-world-2-xde1a -i -t bash  bash-4.2$ curl http://10.1.0.11:8080 Conclusion  In this article we deployed on an OpenShift Enterprise 3.1 environment for lab purpose. This is a perfect way to get your feet wet and start experimenting with your applications running on OpenShift. Unlike other PaaS solutions OpenShift supports running not only cloud-native (mode 2) applications but also traditional (mode 1) applications. Let's face it we aren't going to re-write 20+ years of application architecture overnight.  Another added bonus, anything that runs as a docker container can run within OpenShift. You don't need to inject code or change your application in order to make it work on OpenShift and that is why it can support already existing mode 1 traditional applications so well. It is time to see how container technology can be leveraged to improve the way we build and run all applications. OpenShift is the way!  Happy Openshifting!  (c) 2016 Keith Tenzer  ","categories": ["OpenShift"],
        "tags": ["Containers","Kubernetes","Linux"],
        "url": "/openshift/openshift-enterprise-3-1-lab-setup/",
        "teaser": null
      },{
        "title": "OpenStack Neutron: Configuring L3 HA",
        "excerpt":"  Overview  In this article we will look at how to setup L3 HA in OpenStack Neutron. OpenStack networking can be rather complex, certainly when coming from a traditional networking world. The basic principles behind networking have not changed but OpenStack introduces lots of abstractions that make end-to-end visibility of network traffic flows very difficult to follow. As such before we get into the material it would be good to provide an overview of L3 as it pertains to OpenStack Neutron.    In Neutron L3 allows you to provide external connectivity to instances running in tenant networks that by nature are non-routable. Tenant networks are L2 networks and as such can only communicate with instances on same tenant network. L3 is implemented in Neutron using floating ip addresses. A floating ip is a address of an external network that uses NAT (SNAT/DNAT) to forward traffic to instance ip on tenant network. The below diagram illustrates this connectivity.    In this case we have two networks, a tenant network 10.10.1.0/24 named private and an external network 192.168.122.0/24 named public. The instance mycirros has the ip of 10.10.1.102. In order to communicate with other instances or devices outside the tenant network 10.10.1.0/24 an L3 router is required. Note that floating ips are only associated to an instance, the instance itself is completely unaware of floating ip. If you were to do an \"ip a\" on an instance you would not see the floating ip. Neutron implements NAT (SNAT/DNAT) in order to forward traffic between instance ip and floating ip using iptables rules. Since these rules can only be configured on a single network node per floating ip we have a problem. If that network node is lost then external connectivity will be interrupted to those instances.  Neutron solves this problem with L3 HA. In L3 HA you configure multiple network nodes and on each the Open vSwitch (OVS) agent plugin. In event that a network node is lost, the HA router will failover to a standy on another node and connectivity will not be interrupted for those instances. Neutron ml2 plugins exist for many different SDN solutions not just OVS. Out-of-the-box OVS is of course provided. If you are using another SDN then you most likely would not care about L3 HA within Neutron since you wouldn't use the L3 agent but rather the SDN plugin and the SDN would provide its own HA solution.  Install and Configure OpenStack Kilo  In this example we will configure two systems running RHEL or CentOS 7.1. We will use RDO (packstack) to deploy OpenStack Kilo. In OpenStack there are different roles a node can have: controller, network, compute and storage. Things can be customized to even break out individual services like keystone. We will deploy one node as a controller, compute and network node. The second node will only be a network node. This is the minimum configuration required to configure L3 HA since you need two network nodes.  [Controller node]   Install RHEL or CentOS 7.1. Ensure name resolution is working  #vi /etc/hosts 192.168.122.81 osp7-ctr1.lab.com osp7-ctr1  Ensure the hostname is set statically.  #hostnamectl set-hostname osp7-ctr1.lab.com  Disable network manager.  #systemctl disable NetworkManager.service  Disable firewalld to make configuration easier.   #systemctl disable firewalld.service  For RHEL systems register with subscription manager.   #subscription-manager register  #subscription-manager subscribe --auto  #subscription-manager repos --disable=*  #subscription-manager repos --enable=rhel-7-server-rpms  #subscription-manager repos --enable=rhel-7-server-openstack-7.0-rpms  Install yum-utils and update the system.   #yum install -y yum-utils  #yum update -y  #reboot [Network node]   Install RHEL or CentOS 7.1. Ensure name resolution is working  #vi /etc/hosts 192.168.122.82 osp7-net1.lab.com osp7-net1  Ensure the hostname is set statically.  #hostnamectl set-hostname osp7-net1.lab.com  Disable network manager.  #systemctl disable NetworkManager.service  Disable firewalld to make configuration easier.   #systemctl disable firewalld.service  For RHEL systems register with subscription manager.   #subscription-manager register  #subscription-manager subscribe --auto  #subscription-manager repos --disable=*  #subscription-manager repos --enable=rhel-7-server-rpms  #subscription-manager repos --enable=rhel-7-server-openstack-7.0-rpms  Install yum-utils and update the system.   #yum install -y yum-utils  #yum update -y  #reboot [Controller node]   Install packstack packages.   #yum install -y openstack-packstack  Create packstack answers file for customizing the installer.  #packstack --gen-answer-file /root/answers.txt  Update the packstack answers file. Add the second host (in this case 192.168.122.82) to list of network hosts.  #vi /root/answers.txt  CONFIG_NETWORK_HOSTS=192.168.122.81,192.168.122.82  CONFIG_KEYSTONE_ADMIN_PW=redhat  CONFIG_HORIZON_SSL=y  CONFIG_PROVISION_DEMO=n  CONFIG_HEAT_INSTALL=y  CONFIG_HEAT_CLOUDWATCH_INSTALL=y  CONFIG_HEAT_CFN_INSTALL=y  CONFIG_SAHARA_INSTALL=y  CONFIG_TROVE_INSTALL=y  CONFIG_CEILOMETER_INSTALL=y  CONFIG_LBAAS_INSTALL=y  Install OpenStack using packstack from RDO.  #packstack --answer-file /root/answers.txt . /root/keystonerc_admin  Check status of openstack services.  #openstack-status [Controller and Network nodes]   Backup the ifcfg-etho script.  #cp /etc/sysconfig/network-scripts/ifcfg-eth0 /root/  Configure external bridge for floating ip networks.  #vi /etc/sysconfig/network-scripts/ifcfg-eth0  DEVICE=eth0  ONBOOT=yes  TYPE=OVSPort  DEVICETYPE=ovs  OVS_BRIDGE=br-ex #vi /etc/sysconfig/network-scripts/ifcfg-br-ex  DEVICE=br-ex  BOOTPROTO=static  ONBOOT=yes  TYPE=OVSBridge  DEVICETYPE=ovs  USERCTL=yes  PEERDNS=yes  IPV6INIT=no  IPADDR=&lt;www.xxx.yyy.zzz&gt;  NETMASK=255.255.255.0  GATEWAY=&lt;GW IP&gt;  DNS1=&lt;DNS IP&gt;  Add the eht0 physical interface to the br-ex bridge in openVswitch for floating IP networks.  #ovs-vsctl add-port br-ex eth0 ; systemctl restart network.service &nbsp;  Configure L3 HA  [Controller node]   Configure common settings and enable L3 HA.  #openstack-config --set /etc/neutron/neutron.conf DEFAULT l3_ha True  #openstack-config --set /etc/neutron/neutron.conf DEFAULT l3_ha_net_cidr 168.254.192.0/18  #openstack-config --set /etc/neutron/neutron.conf DEFAULT max_l3_agents_per_router 3  #openstack-config --set /etc/neutron/neutron.conf DEFAULT min_l3_agents_per_router 2  #openstack-config --set /etc/neutron/neutron.conf DEFAULT dhcp_agents_per_network 2 [Controller node]   Configure ml2 plugin.  #openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 type_drivers flat,vlan,gre,vxlan  #openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 tenant_network_types vxlan,vlan,gre  #openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 mechanism_drivers openvswitch,l2population  #openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 extension_drivers port_security  #openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2_type_flat flat_networks external  #openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2_type_vlan network_vlan_ranges datacenter:1000:2999  #openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2_type_gre tunnel_id_ranges 10:100  #openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2_type_vxlan vni_ranges 10:100  #openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini securitygroup enable_ipset True  #openstack-service restart neutron  Configure OVS Agent on Network node.  [Network node]  #openstack-config --set /etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini ovs bridge_mappings external:br-ex  #openstack-config --set /etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini agent tunnel_types vxlan,gre  #openstack-config --set /etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini agent l2_population True  #openstack-config --set /etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini agent prevent_arp_spoofing True  #openstack-config --set /etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini securitygroup firewall_driver neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver  #openstack-config --set /etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini securitygroup enable_security_group True  Configure L3 Agent on Network nodes.  [Network node]  #openstack-config --set /etc/neutron/l3_agent.ini DEFAULT interface_driver neutron.agent.linux.interface.OVSInterfaceDriver  #openstack-config --set /etc/neutron/l3_agent.ini DEFAULT use_namespaces True  #openstack-config --set /etc/neutron/l3_agent.ini DEFAULT external_network_bridge \"\"  #openstack-config --set /etc/neutron/l3_agent.ini DEFAULT agent_mode legacy  Configure DHCP agent on Network nodes.  [Network node]  #openstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT interface_driver neutron.agent.linux.interface.OVSInterfaceDriver  #openstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT dhcp_driver neutron.agent.linux.dhcp.Dnsmasq  #openstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT enable_isolated_metadata True  Ensure that the metadata IP is correct, should be IP of controller.  #openstack-config --get /etc/neutron/metadata_agent.ini DEFAULT nova_metadata_ip  Check shared secret for metadata agent.  #openstack-config --get /etc/neutron/metadata_agent.ini DEFAULT metadata_proxy_shared_secret  The output should match that of controller node.  #openstack-config --get /etc/nova/nova.conf neutron metadata_proxy_shared_secret  Restart Services.  [Network node]  #systemctl restart openvswitch  #systemctl restart neutron-openvswitch-agent  #systemctl restart neutron-l3-agent  #systemctl restart neutron-dhcp-agent  #systemctl restart neutron-metadata-agent  Configure OVS Agent on compute nodes. Note: since we are running compute on the controller node we already did this but in case you have separate compute nodes these steps would be required.  [Compute node]  #openstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini ovs bridge_mappings datacenter:br-vlan  #openstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini agent tunnel_types vxlan,gre  #openstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini agent l2_population False  #openstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini agentprevent_arp_spoofing True  #openstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini securitygroup firewall_driver neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver  #openstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini securitygroup enable_security_group True  Restart services.  #systemctl restart openvswitch  #systemctl restart neutron-openvswitch-agent  Verify Neutron agents are working.  [Controller node]  #source keystonerc_admin #neutron agent-list +--------------------------------------+--------------------+-------------------+-------+----------------+---------------------------+ | id | agent_type | host | alive | admin_state_up | binary | +--------------------------------------+--------------------+-------------------+-------+----------------+---------------------------+ | 0bc9823c-30d8-479f-9690-2a2a99e6b099 | Open vSwitch agent | osp7-ctr2.lab.com | :-) | True | neutron-openvswitch-agent | | 5d104819-f28f-4a3c-9e18-7dfad70d938d | L3 agent | osp7-ctr1.lab.com | :-) | True | neutron-l3-agent | | 68c8f35c-47d0-4e84-b089-c0c01f0b7570 | L3 agent | osp7-net1.lab.com | :-) | True | neutron-l3-agent | | 995ab293-af3b-4386-b041-eac98359a84f | DHCP agent | osp7-net1.lab.com | :-) | True | neutron-dhcp-agent | | ab98e58a-162c-460f-a34d-a700375d65e4 | Loadbalancer agent | osp7-ctr1.lab.com | :-) | True | neutron-lbaas-agent | | b7ca70c9-0d3d-4ba6-ad73-a7cd12bd29f9 | Open vSwitch agent | osp7-ctr1.lab.com | :-) | True | neutron-openvswitch-agent | | d5220a38-ba15-4e29-850f-bdadafdc72c5 | DHCP agent | osp7-ctr1.lab.com | :-) | True | neutron-dhcp-agent | | dd63e733-ef74-46c7-b59c-5d4684d93d1b | Metadata agent | osp7-net1.lab.com | :-) | True | neutron-metadata-agent | | df9bc78b-558a-44c0-96bc-8dec4c0f5ec0 | Loadbalancer agent | osp7-net1.lab.com | :-) | True | neutron-lbaas-agent | | f343cdb4-3d34-46ac-bd7e-ac1c7e98fa78 | Metadata agent | osp7-ctr1.lab.com | :-) | True | neutron-metadata-agent | +--------------------------------------+--------------------+-------------------+-------+----------------+---------------------------+ Create Networks with HA router  [Controller node]   Create private network.  #neutron net-create private #neutron subnet-create private 10.10.1.0/24 --name private_subnet --allocation-pool start=10.10.1.100,end=10.10.1.200  Create public network. Note: these steps assume the physical network connected to eth0 is 192.168.122.0/24.  #neutron net-create public --router:external #neutron subnet-create public 192.168.122.0/24 --name public_subnet --allocation-pool start=192.168.122.100,end=192.168.122.200 --disable-dhcp --gateway 192.168.122.1  Add a new HA router and configure router interfaces.  #neutron router-create router1 --ha True #neutron router-gateway-set router1 public #neutron router-interface-add router1 private_subnet  Check status of HA router router1 and ensure HA is enabled.  #neutron router-show router1 +-----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +-----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | admin_state_up | True | | distributed | False | | external_gateway_info | {\"network_id\": \"59307306-7592-42c3-bae9-add052644040\", \"enable_snat\": true, \"external_fixed_ips\": [{\"subnet_id\": \"cd2ba99a-6d61-4106-af6a-71a7615fe891\", \"ip_address\": \"192.168.122.100\"}]} | | ha | True | | id | e7d9bf3c-22a7-4413-9e44-c1fb450f1432 | | name | router1 | | routes | | | status | ACTIVE | | tenant_id | e8fa02de88824e85a2b4fcf36c510d60 | +-----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+  Check which network node is hosting the router.  #neutron l3-agent-list-hosting-router router1 +--------------------------------------+-------------------+----------------+-------+----------+ | id | host | admin_state_up | alive | ha_state | +--------------------------------------+-------------------+----------------+-------+----------+ | 68c8f35c-47d0-4e84-b089-c0c01f0b7570 | osp7-net1.lab.com | True | :-) | active | | 5d104819-f28f-4a3c-9e18-7dfad70d938d | osp7-ctr1.lab.com | True | :-) | standby | +--------------------------------------+-------------------+----------------+-------+----------+  Upload a glance image. In this case we will use a Cirros image because it is small and thus good for testing OpenStack.  #glance image-create --name \"Cirros 0.3.4\" --disk-format qcow2 --container-format bare --is-public True --copy http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img  Create a new m1.nano flavor for running Cirros image.  #nova flavor-create m1.nano 42 64 0 1  Create security group and allow all TCP ports.  #nova secgroup-create all \"Allow all tcp ports\" #nova secgroup-add-rule all TCP 1 65535 0.0.0.0/0  Create a private ssh key for connecting to instances remotely.  #nova keypair-add admin  Create admin.pem file and add private key from output of keypair-add command.  #vi /root/admin.pem #chmod 400 /root/admin.pem  List the network IDs.  #neutron net-list  +--------------------------------------+---------+-------------------------------------------------------+  | id | name | subnets |  +--------------------------------------+---------+-------------------------------------------------------+  | d4f3ed19-8be4-4d56-9f95-cfbac9fdf670 | private | 92d82f53-6e0b-4eef-b8b9-cae32cf40457 10.10.1.0/24     |  | 37c024d6-8108-468c-bc25-1748db7f5e8f | public  | 22f2e901-186f-4041-ad93-f7b5ccc30a81 192.168.122.0/24 |  Start an instance and make sure to replace network id from above command.  #nova boot --flavor m1.nano --image \"Cirros 0.3.4\" --nic net-id=e6c4e128-5a86-47a7-b501-737935680090 --key-name admin --security-groups all mycirros  Create a floating IP and assign it to the mycirros instance.  #nova floating-ip-create #nova floating-ip-associate mycirros &lt;FLOATING IP&gt;  Connect to mycirros instance using the private ssh key stored in the admin.pem file.  #ssh -i admin.pem cirros@&lt;FLOATING IP&gt;  Check the HA router namespace on node that is shown as active for router.  #ip netns show  qdhcp-e6c4e128-5a86-47a7-b501-737935680090 qrouter-e7d9bf3c-22a7-4413-9e44-c1fb450f1432 #ip netns exec qrouter-e7d9bf3c-22a7-4413-9e44-c1fb450f1432 ip a 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN   link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00  inet 127.0.0.1/8 scope host lo  valid_lft forever preferred_lft forever  inet6 ::1/128 scope host   valid_lft forever preferred_lft forever 12: ha-e0a8c860-c8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN   link/ether fa:16:3e:ec:02:6d brd ff:ff:ff:ff:ff:ff  inet 168.254.192.1/18 brd 168.254.255.255 scope global ha-e0a8c860-c8  valid_lft forever preferred_lft forever  inet 169.254.0.1/24 scope global ha-e0a8c860-c8  valid_lft forever preferred_lft forever  inet6 fe80::f816:3eff:feec:26d/64 scope link   valid_lft forever preferred_lft forever 13: qr-4435cc42-ea: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN   link/ether fa:16:3e:c1:27:47 brd ff:ff:ff:ff:ff:ff  inet 10.10.1.1/24 scope global qr-4435cc42-ea  valid_lft forever preferred_lft forever  inet6 fe80::f816:3eff:fec1:2747/64 scope link nodad   valid_lft forever preferred_lft forever 14: qg-04e11260-25: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN   link/ether fa:16:3e:bb:5a:9f brd ff:ff:ff:ff:ff:ff  inet 192.168.122.100/24 scope global qg-04e11260-25  valid_lft forever preferred_lft forever  inet 192.168.122.101/32 scope global qg-04e11260-25  valid_lft forever preferred_lft forever  inet6 fe80::f816:3eff:febb:5a9f/64 scope link nodad   valid_lft forever preferred_lft forever Notice the difference between the router namespace for the standby node. The tenant and floating ips are only configured on the active router.  Summary  In this article we have discussed the use case around Neutron L3 HA and seen how to configure L3 HA. In Neutron Layer 3 is responsible for external connectivity to and from instances. Floating IPs using SNAT/DNAT are configured that allow external connectivity for non-routable tenant IPs. Neutron HA router is required to protect against failures on L3 agent node and possible interruption to Floating IP traffic.  The L3 HA router is mainly for OVS based networks, other SDNs will provide their own solutions for L3 HA. I hope you have found this article of use. If you have any feedback, please share.  Happy OpenStacking!  (c) 2016 Keith Tenzer  &nbsp;  &nbsp;  &nbsp;  &nbsp;  ","categories": ["OpenStack"],
        "tags": ["L3 HA","neutron","OpenStack","OVS"],
        "url": "/openstack/openstack-neutron-configuring-l3-ha/",
        "teaser": null
      },{
        "title": "OpenStack Heat and Ansible - Automation Born in the Cloud",
        "excerpt":"  Overview  In this article we will look at how Ansible can be leveraged within OpenStack to provide enhanced capabilities around software deployment. Before we get into the details lets understand the challenge. There are typically two layers of automation: provisioning and deployment. Provisioning is all about the underlying infrastructure a particular application might require. Deployment is about installing and configuring the application after the infrastructure exists. OpenStack Heat is the obvious choice for automating provisioning. Heat integrates with other OpenStack services and provides the brains, that bring OpenStack powered cloud to life. While Heat is great for provisioning infrastructure, software deployment is not one of its strengths and trying to orchestrate complex software deployments can be rather clunky. That is where Ansible comes into play and as you will see in this article, they fit together perfectly.    Ansible has two components: Ansible core and Ansible Tower. Ansible core provides the ansible runtime and allows execution of playbooks (YAML definitions of what is being orchestrated). What is missing in Ansible core is the management layer, that enhances team collaboration, extensibility, scalability and visibility. Beyond management, Ansible Tower provides the ability to drive Ansible dynamically through APIs. This is a key requirement for OpenStack and dynamic infrastructure.  Through callbacks we can trigger Ansible playbook runs from within OpenStack Heat. Ansible Tower dynamically discovers instances running on OpenStack as Heat provisions them. Ansible Tower is then able to run playbooks against newly provisioned instances dynamically. The result is an end-to-end automation process, that deploys an entire application including its infrastructure stack. Roles can and ideally should be separated, between infrastructure provisioning and software deployment. Heat templates control provisioning created often by OpenStack administrators. Ansible playbook controls software deployment managed by devops teams. In this article we will see how all that fits together. We will not only deploy Ansible Tower on OpenStack, but also walk through a deployment of an all-in-one WordPress application. In this scenario OpenStack Heat is used to deploy a CentOS image with a private and floating ip. Ansible Tower is then triggered directly from Heat using an API callback, the instance is discovered within Ansible Tower and the appropriate playbook for deploying the WordPress application is executed.  OpenStack Installation and Configuration  Installing OpenStack is not covered in this article, however to stand-up an OpenStack lab environment based on Liberty follow this guide. If you are using your own environment ensure you follow the configuration steps in the above guide after OpenStack is deployed or pass the correct parameters into Heat templates that are representative of your environment.  [OpenStack]   Add CentOS Cloud Image to Glance.  # curl -O http://cloud.centos.org/centos/7/images/CentOS-7-x86_64-GenericCloud.qcow2 # glance image-create --name \"CentOS_7\" --disk-format qcow2 --container-format bare --visibility public --file CentOS-7-x86_64-GenericCloud.qcow2 Note: if your CentOS image is named differently, you need to update Heat templates below.   Create Flavor for Ansible Tower  # nova flavor-create m2.small 50 4096 20 4  Create Flavor for WordPress Applicaiton  # nova flavor-create m2.tiny 51 1024 10 1 Note: if your flavors are named differently, you need to update Heat templates below.  Setup Ansible Tower on OpenStack  As mentioned, Ansible Tower provides management, reporting and most important API callbacks. It makes Ansible core even more powerful. In this case Tower is used primarily for API callback and dynamic inventory. This allows us to make an API call from Heat upon completion of infrastructure provision that 1) dynamically updates Ansible inventory with newly created instance IPs 2) run playbook on newly created instance through ssh using private key from OpenStack.  There are two options for deploying Ansible Tower in OpenStack: using Heat template I have provided or deploying an instance and manually configuring tower. Both options are documented in this article. Here we are of course using CentOS, however RHEL will work as well assuming you have subscriptions.  [OpenStack]  Option 1: Deploy Ansible Tower from Heat Template  # vi /root/centos-tower.yaml heat_template_version: 2013-05-23 description: CentOS Ansible Tower parameters:   server_name:     type: string     description: Name of server     default: tower   image:     type: string     description: Image used for servers     default: CentOS_7   key_name:     type: string     description: SSH key to connect to the servers     default: admin   flavor:     type: string     description: flavor used by the web servers     default: m2.small   private_net_id:     type: string     default: 431aa0f5-2790-403b-84e0-7cb88b836782     description: Id of the private network for the compute server   private_subnet_id:     type: string     default: d7b6fb94-f083-4347-a75a-8025c06b5a31     description: Id of the private sub network for the compute server   public_net_id:     type: string     default: c55f71f6-5b6c-4c1a-a56e-8420a8652f50     description: Id of the public network for the compute server    resources:   webserver:     type: OS::Nova::Server     properties:       name: { get_param: server_name }       image: { get_param: image }       flavor: { get_param: flavor }       key_name: { get_param: key_name }       networks:         - port: { get_resource: server_port }       user_data: |         #!/bin/bash -v         curl -O http://dl.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-6.noarch.rpm         rpm -ivh epel-release-7-6.noarch.rpm         yum install -y ansible         cd /root         curl -O http://releases.ansible.com/ansible-tower/setup/ansible-tower-setup-2.4.5.tar.gz         tar xvzf ansible-tower-setup-latest.tar.gz         cd ansible-tower-setup-2*         cat &lt;&lt; EOF &gt; tower_setup_conf.yml         admin_password: redhat01         database: internal         munin_password: redhat01         pg_password: redhat01         primary_machine: localhost         redis_password: redhat01         EOF         sed -i 's/Defaults    requiretty/Defaults    !requiretty/g' /etc/sudoers         ./configure -o tower_setup_conf.yml         ./setup.sh    server_port:     type: OS::Neutron::Port     properties:       network_id: { get_param: private_net_id }       fixed_ips:         - subnet_id: { get_param: private_subnet_id }       security_groups:         - all    server_floating_ip:     type: OS::Neutron::FloatingIP     properties:       floating_network_id: { get_param: public_net_id }       port_id: { get_resource: server_port }  outputs:   server_private_ip:     description: IP address of server on private network     value: { get_attr: [ webserver, first_address ] }   server_public_ip:     description: Floating IP address of server on public network     value: { get_attr: [ server_floating_ip, floating_ip_address ] }  Note: Ansible Heat templates are also available on Github.  # heat stack-create infrastructure -f centos-tower.yaml -P \"server_name=infra\"  To monitor the progress of cloud-init you can connect to instance floating ip via ssh and tail the cloud-init log.  # ssh -i admin.pem centos@&lt;Floating IP&gt; $ sudo -i # tail -f /var/log/cloud-init.log Option 2: Deploy Ansible Tower Manually  [OpenStack]   Start Nova instance using CentOS image.  # nova boot --flavor m2.small --image \"CentOS_7\" --nic net-id=92d82f53-6e0b-4eef-b8b9-cae32cf40457 --key-name admin --security-groups all infra  Create Floating IP.  # nova floating-ip-create  Associate Floating IP with instance.  # nova floating-ip-associate infra &lt;FLOATING IP&gt;  Connect to Ansible Tower.  # ssh -i admin.pem centos@&lt;Floating IP&gt; $sudo -i [Ansible Tower]  The installation has changed slightly between Tower 2 and 3. Below are the steps for installing Tower 3.   Configure EPEL Repository.  # curl -O http://dl.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-6.noarch.rpm # rpm -ivh epel-release-7-6.noarch.rpm  Install Ansible.  # yum install -y ansible  Install Ansible Tower.  # tar xvzf ansible-tower-setup-latest.tar.gz # cd ansible-tower-setup-&lt;VERSION&gt;  Configure Ansible Tower.  [root@tower ansible-tower-setup-3.0.2]# vi inventory [primary] localhost ansible_connection=local  [secondary]  [database]  [all:vars] admin_password='redhat01' redis_password='redhat01'  pg_host='' pg_port=''  pg_database='awx' pg_username='awx' pg_password='redhat01'  Setup Ansible Tower.  # ./setup.sh Configure Ansible Tower  Now that Anisble Tower is up and running we need to configure connection to OpenStack and in addition add a playbook for WordPress.  [Ansible Tower]   Open web browser and goto https://&lt;floating ip&gt;     Add license for Tower (settings-&gt;license). If you dont have one you can get an eval here.     Add Credentials for OpenStack environment (settings-&gt;credentials).  Ansible Tower needs to be able to query OpenStack tenant over API to find out what instances exists, IPs, etc.     Optional: Add Credentials for OpenStack key and OS user, in this case centos.  OpenStack uses ssh keys to access instances for a specific user. In this case we are using the CentOS cloud image and it has a built-in user account named centos. When deploying an instance we need to choose a key. In the OpenStack lab configuration we created this key. Your environment will have a different key of course if you didn't follow that guide.  Ansible Tower implements a hierarchy, to decide what remote user should run tasks within a playbook, on a given target instance. A default remote user can be specified in the ansible.cfg. This is however overwritten by any credentials stored within Tower and credentials are overwritten by what is in the playbook.     Create inventory for OpenStack (inventories).  Inventories are basically a collection of host groups. Hosts are grouped together based on a common inventory. In OpenStack this is done at the tenant level so a host group is a group of hosts belonging to a tenant.     Add a inventory group for OSP8 and ensure you enable overwrite and update on launch (inventories-&gt;OpenStack).  These parameters ensure that inventory is updated prior to execution of playbook. Again this is important because in OpenStack you can't statically configure instance IPs and as such dynamic discovery is required prior to running playbooks.     Create a new project (Projects).  In this case we create a project called Examples where the playbooks are stored in Git. The following git URL contains the WordPress playbook in addition to other examples: https://github.com/ktenzer/ansible-examples.  Note: I have not tested the other examples but likely you would need to replace the remote_user with centos and allow user to become root.     Create job template (Job Templates).  Job templates bring everything together. You specify what credentials to use, what inventory to run against and of course choose a playbook. In this case we choose the already created inventory (OpenStack), credentials (OSP8) and project (Examples). From the Examples project we will select the wordpress-ngix_rhel7/site.yml playbook.    Note: Copy the callback URL and the host config key for authorizing the callback, this is required later.  Deploy WordPress Application using Heat and Ansible  Now it is time to see everything work together and watch the magic happen. We will create a Heat template to deploy an all-in-one WordPress application. Using curl, we will make a callback to Ansible Tower in order to deploy WordPress application once the infrastructure is provisioned. Notice the wonderful simplicity? Just a one-liner from Heat to deploy anything from the simplest to most complex application imaginable.  [OpenStack]  # vi /root/centos-wordpress.yaml heat_template_version: 2013-05-23 description: CentOS WordPress All-In-One deployed using Ansible Tower parameters:   server_name:     type: string     description: Name of server   image:     type: string     description: Image used for servers     default: CentOS_7   key_name:     type: string     description: SSH key to connect to the servers     default: admin   flavor:     type: string     description: flavor used by the web servers     default: m2.tiny   private_net_id:     type: string     default: 431aa0f5-2790-403b-84e0-7cb88b836782     description: Id of the private network for the compute server   private_subnet_id:     type: string     default: d7b6fb94-f083-4347-a75a-8025c06b5a31     description: Id of the private sub network for the compute server   public_net_id:     type: string     default: c55f71f6-5b6c-4c1a-a56e-8420a8652f50     description: Id of the public network for the compute server    tower_private_ip:     type: string     default: 10.10.1.108     description: Ansible Tower Private IP   resources:   webserver:     type: OS::Nova::Server     properties:       name: { get_param: server_name }       image: { get_param: image }       flavor: { get_param: flavor }       key_name: { get_param: key_name }       networks:         - port: { get_resource: server_port }       user_data_format: RAW       user_data:         str_replace:           template: |             #!/bin/bash -v             curl -k --data \"host_config_key=5d77be952e6eb0b7509c8c26ebff785d\" https://tower_ip:443/api/v1/job_templates/15/callback/           params:             tower_ip: { get_param: tower_private_ip }    server_port:     type: OS::Neutron::Port     properties:       network_id: { get_param: private_net_id }       fixed_ips:         - subnet_id: { get_param: private_subnet_id }       security_groups:         - all    server_floating_ip:     type: OS::Neutron::FloatingIP     properties:       floating_network_id: { get_param: public_net_id }       port_id: { get_resource: server_port }  outputs:   server_private_ip:     description: IP address of server on private network     value: { get_attr: [ webserver, first_address ] }  Note: you need to replace the curl command with the callback URL and host config key from your environment.   Deploy WordPress application by running Heat template.  The Heat template will deploy instance and required infrastructure, install firewalld and call Ansible Tower via API callback. You will need to provide at minimum server_name and tower_private_ip as input parameters. Feel free to parameterize things to your heats content.  # heat stack-create wordpress -f centos-wordpress.yaml -P \"server_name=wordpress\"-P \"tower_private_ip=10.10.1.108\" Note: you need to get private IP for the Ansible Tower host.  Adding Heat WaitCondition  At this point we have separated infrastructure from application blueprints and yet still have the capability to perform end-to-end deployment through Heat. One thing that is missing however, is a way to notify Heat that Ansible Tower completed with either a success or failure. Heat provides a resource type called WaitCondition for this exact purpose. The WaitCondition resource will cause the Heat stack to wait until further notified or timeout. A status of success or failure can also be sent back using JSON format.  '{\"status\": \"SUCCESS|FAILURE\"}' The WaitCondition resource type generates an endpoint URL and authorization token as output. Below the original Heat template has been modified to add the WaitCondition resource type and send required parameters to Ansible Tower.  [OpenStack]  # vi /root/centos-wordpress-heat.yaml heat_template_version: 2013-05-23 description: CentOS WordPress All-In-One deployed using Ansible Tower parameters:   server_name:     type: string     description: Name of server   image:     type: string     description: Image used for servers     default: CentOS_7   key_name:     type: string     description: SSH key to connect to the servers     default: admin   flavor:     type: string     description: flavor used by the web servers     default: m2.tiny   private_net_id:     type: string     default: 431aa0f5-2790-403b-84e0-7cb88b836782     description: Id of the private network for the compute server   private_subnet_id:     type: string     default: d7b6fb94-f083-4347-a75a-8025c06b5a31     description: Id of the private sub network for the compute server   public_net_id:     type: string     default: c55f71f6-5b6c-4c1a-a56e-8420a8652f50     description: Id of the public network for the compute server    tower_private_ip:     type: string     default: 10.10.1.108     description: Ansible Tower Private IP   resources:   wait_condition:     type: OS::Heat::WaitCondition     properties:       handle: { get_resource: wait_handle }       count: 1       timeout: 1200    wait_handle:     type: OS::Heat::WaitConditionHandle    webserver:     type: OS::Nova::Server     properties:       name: { get_param: server_name }       image: { get_param: image }       flavor: { get_param: flavor }       key_name: { get_param: key_name }       networks:         - port: { get_resource: server_port }       user_data_format: RAW       user_data:         str_replace:           template: |             #!/bin/bash -v             curl -f -k -H 'Content-Type: application/json' -XPOST -d '{\"host_config_key\": \"8217c0a711b3af173be033aab12534f0\", \"extra_vars\": {\"HEAT_ENDPOINT\": \"wait_endpoint\",\"HEAT_TOKEN\": \"wait_token\"}}' https://tower_ip:443/api/v1/job_templates/7/callback/           params:             tower_ip: { get_param: tower_private_ip }             wc_notify: { get_attr: ['wait_handle', 'curl_cli'] }             wait_endpoint: { get_attr: [ wait_handle, endpoint ] }             wait_token: { get_attr: [ wait_handle, token ] }    server_port:     type: OS::Neutron::Port     properties:       network_id: { get_param: private_net_id }       fixed_ips:         - subnet_id: { get_param: private_subnet_id }       security_groups:         - all    server_floating_ip:     type: OS::Neutron::FloatingIP     properties:       floating_network_id: { get_param: public_net_id }       port_id: { get_resource: server_port }  outputs:   server_private_ip:     description: IP address of server on private network     value: { get_attr: [ webserver, first_address ] }   curl_cli:     value: { get_attr: ['wait_handle', 'curl_cli'] }   wait_endpoint:     value: { get_attr: ['wait_handle', 'endpoint'] }   wait_token:     value: { get_attr: ['wait_handle', 'token'] }   wc_data:     value: { get_attr: ['wait_condition', 'data'] }   In order to notify Heat when Ansible playbook has completed two things are required. First handle the incoming parameters and second, using a role, send notification to Heat.  There are several ways to handle parameters in Ansible, for this example I chose to set them globally by adding following to playbook:   vars:         heat_endpoint: \"\"         heat_token: \"\" Using roles in Ansible allows for greater re-usability, roles can be re-used in many playbooks. Below is the new role to support sending notification to Heat.  --- - block:   - name: curl post to Heat      uri:        url: \"\"        method: POST        HEADER_X-Auth-Token: \"\"        HEADER_Content-Type: \"application/json\"        HEADER_Accept: \"application/json\"        body: '{\"status\": \"SUCCESS\"}'        force_basic_auth: yes        status_code: 200        body_format: json   rescue:   - name: curl post to Heat to notify of failure      uri:        url: \"\"        method: POST        HEADER_X-Auth-Token: \"\"        HEADER_Content-Type: \"application/json\"        HEADER_Accept: \"application/json\"        body: '{\"status\": \"FAILURE\"}'        force_basic_auth: yes        status_code: 200        body_format: json In order to take action upon failures within playbook I chose to use blocks. These are basically like try/catch statements. This is critical so that if any tasks within the playbook fail, Heat is notified immediately and shows stack as being failed. In block statement we define what is supposed to happen normally. In rescue statement we define what should happen in case of failure.  Looking at the MariaDB role we see how the rescue block is used to send Heat a message if anything fails. This is implemented in all roles where tasks are executed.  --- # This playbook will install MariaDB and create db user and give permissions.  - block:   - name: Install MariaDB package     yum: name= state=installed     with_items:       - mariadb-server       - MySQL-python       - libselinux-python       - libsemanage-python    - name: Install firewalld     yum: name= state=installed     with_items:       - firewalld    - name: Enable firewalld     service: name=firewalld state=started enabled=yes    - name: Configure SELinux to start mysql on any port     seboolean: name=mysql_connect_any state=true persistent=yes    - name: Create Mysql configuration file     template: src=my.cnf.j2 dest=/etc/my.cnf     notify:     - restart mariadb    - name: Create MariaDB log file     file: path=/var/log/mysqld.log state=touch owner=mysql group=mysql mode=0775    - name: Start MariaDB Service     service: name=mariadb state=started enabled=yes    - name: insert firewalld rule     firewalld: port=/tcp permanent=true state=enabled immediate=yes   rescue:   - name: curl post to Heat to notify of failure     uri:       url: \"\"       method: POST       HEADER_X-Auth-Token: \"\"       HEADER_Content-Type: \"application/json\"       HEADER_Accept: \"application/json\"       body: '{\"status\": \"FAILURE\"}'       force_basic_auth: yes       status_code: 200       body_format: json In order to use the Ansible playbook that supports Heat WaitCondition simply change the wordpress job template you created above to use \"wordpress-nginx_rhel7_heat\" from ansible-examples.    The following behavior should be observed when executing the centos-wordpress-heat.yaml stack.  # heat stack-create wordpress -f centos-wordpress-heat.yaml -P \"server_name=wordpress\"-P \"tower_private_ip=10.10.1.108\" Heat will not complete after the instance is launched but rather wait, for input from Ansible Tower.    Once playbook is started the heat endpoint and authorization token will show up as extra variables.    After the playbook completes Heat will be notified. In this case things completed successfully    Summary  In this article we have discussed how OpenStack Heat and Ansible provide a powerful combination for cloud orchestration. We have also discussed some of the advantages Ansible Tower provides, allowing not only central API integration through callbacks but needed orchestration extensibility, security, management, visibility and role separation. Both OpenStack and Ansible were born in the cloud. Together they provide end-to-end cloud automation and orchestration for traditional as well as cloud-native applications. I am really interested in your feedback and thoughts on this topic so please share? Hopefully you found this article useful.  Happy orchestrating everything with Ansible Tower in the OpenStack cloud!  (c) 2016 Keith Tenzer  ","categories": ["Ansible"],
        "tags": ["Ansible Tower","Heat","OpenStack"],
        "url": "/ansible/openstack-heat-and-ansible-automation-born-in-the-cloud/",
        "teaser": null
      },{
        "title": "Red Hat OpenStack Platform 8 Lab Configuration using OpenStack Director",
        "excerpt":"  Overview  In this article we will look at how to deploy Red Hat OpenStack Platform 8 (Liberty) using Red Hat OpenStack Director. In a previous article we looked at how to deploy Red Hat OpenStack Platform 7 (Kilo). The first release of OpenStack Director was in OpenStack Platform 7 so this is the second release of OpenStack Director.  One of the main areas where distributions of course distinguish themselves is in regards to the installer. As you will see in this article, Red Hat's installer, OpenStack Director is far more than just an installer, it is a lifecycle tool to manage the infrastructure for OpenStack. OpenStack Director is based on the upstream OpenStack foundation project TripleO. At this point, Red Hat is only distribution basing it's installer on TripleO, hopefully that changes soon. All other distributions use either proprietary software or isolated, fragmented communities to build OpenStack installers. Beyond installing OpenStack, lifecycle management is mostly an afterthought. Installing OpenStack is of course the easiest thing you will do, it isn't a big deal anymore. If your serious about OpenStack you will quickly realize things like updates, in-place upgrades, scaling, infrastructure blueprints and support lifecycles are far more critical.    The aim of TripleO is not only to unify installer efforts but also add needed enterprise features to OpenStack deployment ecosystem. As Red Hat does with all upstream projects, OpenStack director also has a community platform called  RDO-Manaer. Below is an illustration of the path to production for OpenStack Director.    TripleO Concepts  Before getting into the weeds, we should understand some basic concepts. First TripleO uses OpenStack to deploy OpenStack. It mainly utilizes Ironic (metal-as-a-service)  for provisioning and Heat for orchestration though other services such as Nova, Glance and Neutron are also used. Under the hood within Heat, puppet is also used for advanced configuration management. TripleO first deploys an OpenStack cloud used to deploy other OpenStack clouds. This is referred to as the undercloud. OpenStack cloud environment deployed from undercloud is known as overcloud. The networking requirement is that all systems share a non-routed provisioning network. TripleO or better said Ironic, uses PXE to boot and install initial OS image (bootstrap). These images are stored in Glance (image-as-a-service). There are different types of nodes or roles a node can have within OpenStack director. Currently OpenStack Director offers control, compute and storage roles. In the future composite or more granular roles will also be possible. For example, you might want to breakout Keystone or even Neutron into their own, more granular composite roles. CEPH storage is completely integrated into OpenStack Director, meaning CEPH can be deployed with OpenStack together using the same tool, OpenStack Director. This obviously makes a lot of sense as we want to scale environment. We simply tell director to increase number of compute or storage nodes and it is automatically taken care of. We can also re-purpose x86 hardware into different roles if our ratios of compute to storage should change.  Environment Setup  In this environment we have the KVM hypervisor host (Laptop), the undercloud (single VM) and overcloud (1 X compute, 1 X controller, 1 X un-provisioned). The undercloud and overcloud are all VMs running on the KVM hypervisor host (Laptop). The KVM hypervisor host is on the 192.168.122.0/24 network and has IP of 192.168.122.1. The undercloud runs on a single VM on the 192.168.122.0/24 external network and 192.168.126.0/24 (provisioning) netowrk. The undercloud has an IP address of 192.168.122.90 (eth0) and 192.168.126.1 (eth1). The overcloud is on the 192.168.126.0/24 (provisioning) and 192.168.122.0/24 (external) network. Each overcloud node has three NICs, 1 on provisioning network and two external network.  OpenStack requires several networks: provisioning, management, external, tenant, public (API), storage and storage management. The recommendation is to have provisioning network on its own (1Gbit ideally) NIC and then bond remaining NICs (10Gbit ideally). All OpenStack networks besides provisioning would then be configured as VLANs on bond. Depending on OpenStack role (control, compute or storage) different networks would be configured on the various roles. LACP is recommended assuming it is supported by network otherwise active/passive bonding would be possible. The diagram below illustrates the configuration of the environment deployed in this article.    The following diagram illustrates the various OpenStack networks and how they are associated to various roles discussed above.    Above image courtesy of Laurent Domb  Deploying Undercloud  In this section we will configure the undercloud. Normally you would deploy OpenStack nodes on baremetal but since this is designed to run on Laptop or in lab, we are using KVM and nested-virtualization. Before beginning install RHEL or CentOS 7.1 on your undercloud VM.  Ensure hostname is static.  undercloud# hostnamectl set-hostname undercloud.lab.com undercloud# systemctl restart network Register to subscription manager and enable appropriate repositories for RHEL.  undercloud# subscription-manager register undercloud# subscription-manager list --available undercloud# subscription-manager attach --pool=8a85f9814f2c669b014f3b872de132b5 undercloud# subscription-manager repos --disable=* undercloud# subscription-manager repos --enable=rhel-7-server-rpms --enable=rhel-7-server-extras-rpms --enable=rhel-7-server-openstack-8-rpms --enable=rhel-7-server-openstack-8-director-rpms --enable rhel-7-server-rh-common-rpms Perform yum update and reboot system.  undercloud# yum update -y &amp;&amp; reboot Install facter and ensure hostname is set properly in /etc/hosts.  undercloud# yum install facter -y undercloud# ipaddr=$(facter ipaddress_eth0) undercloud# echo -e \"$ipaddr\\t\\tundercloud.lab.com\\tundercloud\" &gt;&gt; /etc/hosts Install TripleO packages.  undercloud# yum install -y python-tripleoclient Create a stack user.  undercloud# useradd stack undercloud# echo \"redhat01\" | passwd stack --stdin undercloud# echo \"stack ALL=(root) NOPASSWD:ALL\" | tee -a /etc/sudoers.d/stack undercloud# su - stack Determine network settings for undercloud. For the purpose of this environment you need two virtual networks on KVM hypervisor. One for provisioning and the other for the various overcloud networks (external). The undercloud provisioning network CIDR is 192.168.126.0/24 and the overcloud external network CIDR 192.168.122.0/24. This will of course probably vary in your environment. The reason the undercloud needs to also be on external network is so that it can test a functioning overcloud to ensure deployment is successful. To do this the undercloud communicates using external API endpoints.  [stack@undercloud ~]$ mkdir ~/images [stack@undercloud ~]$ mkdir ~/templates [stack@undercloud ~]$ cp /usr/share/instack-undercloud/undercloud.conf.sample ~/undercloud.conf [stack@undercloud ~]$ vi ~/undercloud.conf  [DEFAULT]  local_ip = 192.168.126.1/24  undercloud_public_vip = 192.168.126.2  undercloud_admin_vip = 192.168.126.3  local_interface = eth1  masquerade_network = 192.168.126.0/24  dhcp_start = 192.168.126.100  dhcp_end = 192.168.126.150  network_cidr = 192.168.126.0/24  network_gateway = 192.168.126.1  discovery_iprange = 192.168.126.130,192.168.126.150  [auth] Install the undercloud.  [stack@undercloud ~]$ openstack undercloud install ############################################################################# Undercloud install complete.  The file containing this installation's passwords is at /home/stack/undercloud-passwords.conf.  There is also a stackrc file at /home/stack/stackrc.  These files are needed to interact with the OpenStack services, and should be secured.  #############################################################################   Verify undercloud.   [stack@undercloud ~]$ source ~/stackrc  [stack@undercloud ~]$ openstack catalog show nova  +-----------+------------------------------------------------------------------------------+  | Field | Value |  +-----------+------------------------------------------------------------------------------+  | endpoints | regionOne                                                                    |  |           | publicURL: http://192.168.126.1:8774/v2/e6649719251f40569200fec7fae6988a     |  |           | internalURL: http://192.168.126.1:8774/v2/e6649719251f40569200fec7fae6988a   |  |           | adminURL: http://192.168.126.1:8774/v2/e6649719251f40569200fec7fae6988a      |  |           |                                                                              |  | name      | nova                                                                         |  | type      | compute                                                                      |  +-----------+------------------------------------------------------------------------------+ Deploying Overcloud  The overcloud is as mentioned a separate cloud from the undercloud. They are not sharing any resources, other than the provisioning network. Over and under sometimes confuse people into thinking the overcloud is sitting on top of undercloud, from networking perspective. This is of course not the case. In reality the clouds are sitting side-by-side from one another. The term over and under really refers to a logical relationship between both clouds. We will do a minimal deployment for the overcloud, 1 X controller and 1 X compute.  Download images from https://access.redhat.com/downloads/content/191/ver=8/rhel---7/8/x86_64/product-software and copy to ~/images.  Extract image tarballs.  [stack@undercloud ~]$ cd ~/images [stack@undercloud ~]$ for tarfile in *.tar; do tar -xf $tarfile; done Upload images to Glance.  [stack@undercloud ~]$ openstack overcloud image upload --image-path /home/stack/images [stack@undercloud images]$ openstack image list +--------------------------------------+------------------------+ | ID | Name | +--------------------------------------+------------------------+ | 657f84b8-c56c-4cc5-9035-d630ebf0555f | bm-deploy-kernel | | 0a650251-174d-4aee-8787-f63fa4a82bc9 | bm-deploy-ramdisk | | 1e1e8b18-5ce4-48ca-a87a-16144a97716f | overcloud-full | | 1608b138-c571-4773-a429-837f0189713a | overcloud-full-initrd | | a0994fdf-10aa-4542-af00-e432c4a1c0e8 | overcloud-full-vmlinuz | +--------------------------------------+------------------------+ Configure DNS for undercloud. The undercloud system is connected to a network 192.168.122.0/24 that provides DNS.  [stack@undercloud images]$ neutron subnet-list +--------------------------------------+------+------------------+--------------------------------------------------------+ | id | name | cidr | allocation_pools | +--------------------------------------+------+------------------+--------------------------------------------------------+ | d6263720-6648-4886-a67c-1ad38b2b0053 | | 192.168.126.0/24 | {\"start\": \"192.168.126.100\", \"end\": \"192.168.126.150\"} | +--------------------------------------+------+------------------+--------------------------------------------------------+ [stack@undercloud ~]$ neutron subnet-update d6263720-6648-4886-a67c-1ad38b2b0053 --dns-nameserver 192.168.126.254 Since we are in nested virtual environment it is necessary to tweak timeouts.  undercloud# sudo su - undercloud# openstack-config --set /etc/nova/nova.conf DEFAULT rpc_response_timeout 600 undercloud# openstack-config --set /etc/ironic/ironic.conf DEFAULT rpc_response_timeout 600 undercloud# openstack-service restart nova  undercloud# openstack-service restart ironic undercloud# exit Create provisioning and external networks on KVM Hypervisor host. Ensure that NAT forwarding is enabled and DHCP is disabled on the external network. The provisioning network should be non-routable and DHCP disabled. The undercloud will handle DHCP services for the provisioning network and other IPs will statically assigned.  [ktenzer@ktenzer ~]$ cat &gt; /tmp/external.xml &lt;&lt;EOF &lt;network&gt;    &lt;name&gt;external&lt;/name&gt;    &lt;forward mode='nat'&gt;       &lt;nat&gt; &lt;port start='1024' end='65535'/&gt;       &lt;/nat&gt;    &lt;/forward&gt;    &lt;ip address='192.168.122.1' netmask='255.255.255.0'&gt;    &lt;/ip&gt; &lt;/network&gt;  [ktenzer@ktenzer ~]$ virsh net-define /tmp/external.xml [ktenzer@ktenzer ~]$ virsh net-autostart external [ktenzer@ktenzer ~]$ virsh net-start external [ktenzer@ktenzer ~]$ cat &gt; /tmp/provisioning.xml &lt;&lt;EOF &lt;network&gt;    &lt;name&gt;provisioning&lt;/name&gt;    &lt;ip address='192.168.126.254' netmask='255.255.255.0'&gt;    &lt;/ip&gt; &lt;/network&gt; [ktenzer@ktenzer ~]$ virsh net-define /tmp/provisioning.xml [ktenzer@ktenzer ~]$ virsh net-autostart provisioning [ktenzer@ktenzer ~]$ virsh net-start provisioning Create VM hulls in KVM using virsh on hypervisor host. You will need to change the disk path to suit your needs.  ktenzer# cd /home/ktenzer/VirtualMachines ktenzer# for i in {1..3}; do qemu-img create -f qcow2 -o preallocation=metadata overcloud-node$i.qcow2 60G; done ktenzer# for i in {1..3}; do virt-install --ram 4096 --vcpus 4 --os-variant rhel7 --disk path=/home/ktenzer/VirtualMachines/overcloud-node$i.qcow2,device=disk,bus=virtio,format=qcow2 --noautoconsole --vnc --network network:provisioning --network network:external --network network:external --name overcloud-node$i --cpu SandyBridge,+vmx --dry-run --print-xml &gt; /tmp/overcloud-node$i.xml; virsh define --file /tmp/overcloud-node$i.xml; done Enable access on KVM hypervisor host so that Ironic can control VMs.  ktenzer# cat &lt;&lt; EOF &gt; /etc/polkit-1/localauthority/50-local.d/50-libvirt-user-stack.pkla [libvirt Management Access] Identity=unix-user:stack Action=org.libvirt.unix.manage ResultAny=yes ResultInactive=yes ResultActive=yes EOF Copy ssh key from undercloud system to KVM hypervisor host for stack user.  undercloud$ ssh-copy-id -i ~/.ssh/id_rsa.pub stack@192.168.122.1 Save the MAC addresses for the provisioning network on the VMs. Ironic needs to know what MAC addresses a node has associated for provisioning network.  [stack@undercloud ~]$ for i in {1..3}; do virsh -c qemu+ssh://stack@192.168.122.1/system domiflist overcloud-node$i | awk '$3 == \"mgmt\" {print $5};'; done &gt; /tmp/nodes.txt [stack@undercloud ~]$ cat /tmp/nodes.txt  52:54:00:31:df:f2 52:54:00:8a:54:62 52:54:00:4b:85:5d Create JSON file for Ironic baremetal node configuration. In this case we are configuring three nodes which are of course the virtual machines we already created. The pm_addr IP is set to IP of the KVM hypervisor host.  [stack@undercloud ~]$ jq . &lt;&lt; EOF &gt; ~/instackenv.json {   \"ssh-user\": \"stack\",   \"ssh-key\": \"$(cat ~/.ssh/id_rsa)\",   \"power_manager\": \"nova.virt.baremetal.virtual_power_driver.VirtualPowerManager\",   \"host-ip\": \"192.168.122.1\",   \"arch\": \"x86_64\",   \"nodes\": [     {       \"pm_addr\": \"192.168.122.1\",       \"pm_password\": \"$(cat ~/.ssh/id_rsa)\",       \"pm_type\": \"pxe_ssh\",       \"mac\": [         \"$(sed -n 1p /tmp/nodes.txt)\"       ],       \"cpu\": \"2\",       \"memory\": \"4096\",       \"disk\": \"60\",       \"arch\": \"x86_64\",       \"pm_user\": \"stack\"     },     {       \"pm_addr\": \"192.168.122.1\",       \"pm_password\": \"$(cat ~/.ssh/id_rsa)\",       \"pm_type\": \"pxe_ssh\",       \"mac\": [         \"$(sed -n 2p /tmp/nodes.txt)\"       ],       \"cpu\": \"4\",       \"memory\": \"2048\",       \"disk\": \"60\",       \"arch\": \"x86_64\",       \"pm_user\": \"stack\"     },     {       \"pm_addr\": \"192.168.122.1\",       \"pm_password\": \"$(cat ~/.ssh/id_rsa)\",       \"pm_type\": \"pxe_ssh\",       \"mac\": [         \"$(sed -n 3p /tmp/nodes.txt)\"       ],       \"cpu\": \"4\",       \"memory\": \"2048\",       \"disk\": \"60\",       \"arch\": \"x86_64\",       \"pm_user\": \"stack\"     }   ]  }  EOF Validate JSON file.  [stack@undercloud ~]$ curl -O https://raw.githubusercontent.com/rthallisey/clapper/master/instackenv-validator.py python instackenv-validator.py -f instackenv.json INFO:__main__:Checking node 192.168.122.1 DEBUG:__main__:Identified virtual node INFO:__main__:Checking node 192.168.122.1 DEBUG:__main__:Identified virtual node DEBUG:__main__:Baremetal IPs are all unique. DEBUG:__main__:MAC addresses are all unique.  -------------------- SUCCESS: instackenv validator found 0 errors   Delete the built-in flavors and create new ones to meet your VM hull specifications for the overcloud.  [stack@undercloud ~]$ for n in control compute ceph-storage; do  nova flavor-delete $n  done [stack@undercloud ~]$ openstack flavor create --id auto --ram 4096 --disk 57 --vcpus 2 --swap 2048 control  [stack@undercloud ~]$ openstack flavor create --id auto --ram 2048 --disk 57 --vcpus 4 --swap 2048 compute  [stack@undercloud ~]$ openstack flavor create --id auto --ram 2048 --disk 57 --vcpus 2 --swap 2048 ceph-storage In Red Hat OpenStack Platform 8 you can either map ironic nodes to a profile manually or have them auto-discovered. Here we will cover both options.  Option 1: Map Ironic nodes to flavors manually.  [stack@undercloud ~]$ openstack flavor set --property \"cpu_arch\"=\"x86_64\" --property \"capabilities:boot_option\"=\"local\" --property \"capabilities:profile\"=\"control\" control  [stack@undercloud ~]$ openstack flavor set --property \"cpu_arch\"=\"x86_64\" --property \"capabilities:boot_option\"=\"local\" --property \"capabilities:profile\"=\"compute\" compute  [stack@undercloud ~]$ openstack flavor set --property \"cpu_arch\"=\"x86_64\" --property \"capabilities:boot_option\"=\"local\" --property \"capabilities:profile\"=\"ceph-storage\" ceph-storage Option 2: map Ironic nodes to flavors automatically during introspection. Previously in RHEL OSP 7 AHC-Tools were used to do this, now they have been replaced with functionality in Ironic.  [stack@undercloud ~]$ vi rules.json  [   {     \"description\": \"Fail introspection for unexpected nodes\",     \"conditions\": [       {         \"op\": \"lt\",         \"field\": \"memory_mb\",         \"value\": 2048       }     ],     \"actions\": [       {         \"action\": \"fail\",         \"message\": \"Memory too low, expected at least 4 GiB\"       }     ]   },   {     \"description\": \"Assign profile for control profile\",     \"conditions\": [       {         \"op\": \"ge\",         \"field\": \"cpus\",         \"value\" : 2      }     ],     \"actions\": [       {         \"action\": \"set-capability\",         \"name\": \"profile\",         \"value\": \"control\"       }     ]   },   {     \"description\": \"Assign profile for compute profile\",     \"conditions\": [       {         \"op\": \"le\",         \"field\": \"cpus\",         \"value\" : 4       }     ],     \"actions\": [       {         \"action\": \"set-capability\",         \"name\": \"profile\",         \"value\": \"compute\"       }     ]   } ]  [stack@undercloud ~]$ openstack baremetal introspection rule import rules.json List the Ironic rules.  [stack@undercloud ~]$ openstack baremetal introspection rule list +--------------------------------------+-----------------------------------------+ | UUID | Description | +--------------------------------------+-----------------------------------------+ | 3760a0ff-62d6-4a59-830a-6d49dbd66f3a | Fail introspection for unexpected nodes | | 83165269-812d-4536-a722-7bbc39d5f567 | Assign profile for control profile | | 738f293c-ad02-4384-ab2d-3ccbfbb4ddca | Assign profile for compute profile | +--------------------------------------+-----------------------------------------+ Ironic at this point only supports IPMI booting and since we are using VMs we need to use ssh_pxe. This can create some issues around NIC discovery and ordering. If you have problems with introspection you can try below fix that ensures the correct interface is associated with provisioning network. This workaround adds a bit of load to undercloud so it is only recommended for lab environments.  [stack@undercloud ~]$ sudo su - undercloud# cat &lt;&lt; EOF &gt; /usr/bin/bootif-fix #!/usr/bin/env bash  while true;         do find /httpboot/ -type f ! -iname \"kernel\" ! -iname \"ramdisk\" ! -iname \"*.kernel\" ! -iname \"*.ramdisk\" -exec sed -i 's|{mac|{net0/mac|g' {} +; done EOF  undercloud# chmod a+x /usr/bin/bootif-fix undercloud# cat &lt;&lt; EOF &gt; /usr/lib/systemd/system/bootif-fix.service [Unit] Description=Automated fix for incorrect iPXE BOOFIF  [Service] Type=simple ExecStart=/usr/bin/bootif-fix  [Install] WantedBy=multi-user.target EOF  undercloud# systemctl daemon-reload undercloud# systemctl enable bootif-fix undercloud# systemctl start bootif-fix undercloud# exit Add nodes to Ironic.  [stack@undercloud ~]$ openstack baremetal import --json instackenv.json  List newly added baremetal nodes.  [stack@undercloud ~]$ ironic node-list +--------------------------------------+------+---------------+-------------+--------------------+-------------+ | UUID | Name | Instance UUID | Power State | Provisioning State | Maintenance | +--------------------------------------+------+---------------+-------------+--------------------+-------------+ | 0fd03a4a-a719-4816-b0cf-a981d7a7cf4d | None | None | None | available | False | | 94efb93b-1983-41f8-b8e4-13a1b55c9ebb | None | None | None | available | False | | 0d6beec6-99ce-40b8-a784-69544ee0a131 | None | None | None | available | False | +--------------------------------------+------+---------------+-------------+--------------------+-------------+ Enable nodes for baremetal to boot locally.  [stack@undercloud ~]$ openstack baremetal configure boot Perform introspection on baremetal nodes. This will discover hardware and configure node roles.  [stack@undercloud ~]$ openstack baremetal introspection bulk start Setting nodes for introspection to manageable... Starting introspection of node: 0fd03a4a-a719-4816-b0cf-a981d7a7cf4d Starting introspection of node: 94efb93b-1983-41f8-b8e4-13a1b55c9ebb Starting introspection of node: 0d6beec6-99ce-40b8-a784-69544ee0a131 Waiting for introspection to finish... Introspection for UUID 0fd03a4a-a719-4816-b0cf-a981d7a7cf4d finished successfully. Introspection for UUID 0d6beec6-99ce-40b8-a784-69544ee0a131 finished successfully. Introspection for UUID 94efb93b-1983-41f8-b8e4-13a1b55c9ebb finished successfully. Setting manageable nodes to available... Node 0fd03a4a-a719-4816-b0cf-a981d7a7cf4d has been set to available. Node 94efb93b-1983-41f8-b8e4-13a1b55c9ebb has been set to available. Node 0d6beec6-99ce-40b8-a784-69544ee0a131 has been set to available. Introspection completed. To check progress of introspection.  [stack@undercloud ~]$ sudo journalctl -l -u openstack-ironic-inspector -u openstack-ironic-inspector-dnsmasq -u openstack-ironic-conductor -f List the Ironic profiles. If introspection worked you should see the nodes are associated to a profile. In this case 1 X control and 2 X compute.  [stack@undercloud ~]$ openstack overcloud profiles list +--------------------------------------+-----------+-----------------+-----------------+-------------------+ | Node UUID | Node Name | Provision State | Current Profile | Possible Profiles | +--------------------------------------+-----------+-----------------+-----------------+-------------------+ | 8cff2527-f8bb-49a4-b755-d037ef8f4c7c | | available | control_profile | | | 2d35278b-2a4e-4736-a899-e0ab8b84c025 | | available | compute_profile | | | 909ef10b-7853-47f1-932a-77a716dd0925 | | available | compute_profile | | +--------------------------------------+-----------+-----------------+-----------------+-------------------+ Clone my OSP 8 Director Heat Tempates from Github.  [stack@undercloud ~]$ git clone https://github.com/ktenzer/openstack-heat-templates.git [stack@undercloud ~]$ cp -r openstack-heat-templates/director/lab/osp8/templates /home/stack Update network environment template  vi templates/network-environment.yaml EC2MetadataIp: 192.168.126.1 ControlPlaneDefaultRoute: 192.168.126.254 ExternalNetCidr: 192.168.122.0/24 ExternalAllocationPools: [{'start': '192.168.122.101', 'end': '192.168.122.200'}] Update nic template for controller  vi templates/nic-configs/controller.yaml ExternalInterfaceDefaultRoute:   default: '192.168.126.254' Deploy overcloud.  [stack@undercloud ~]$ openstack overcloud deploy --templates -e /usr/share/openstack-tripleo-heat-templates/environments/network-isolation.yaml -e ~/templates/network-environment.yaml -e ~/templates/firstboot-environment.yaml --control-scale 1 --compute-scale 1 --control-flavor control --compute-flavor compute --ntp-server pool.ntp.org --neutron-network-type vxlan --neutron-tunnel-types vxlan Overcloud Endpoint: http://192.168.122.101:5000/v2.0 Overcloud Deployed Notice the public endpoint is coming from the external network, not the provisioning network.  Check detail status of Heat resources to monitor status of overcloud deployment.  [stack@undercloud ~]$ heat resource-list -n 5 overcloud Once the OS install is complete on the baremetal nodes, you can follow progress of individual nodes. This is helpful especially for troubleshooting.  [stack@undercloud ~]$ nova list +--------------------------------------+------------------------+--------+------------+-------------+-------------------------+ | ID                                   | Name                   | Status | Task State | Power State | Networks                | +--------------------------------------+------------------------+--------+------------+-------------+-------------------------+ | 507d1172-fc73-476b-960f-1d9bf7c1c270 | overcloud-compute-0    | ACTIVE | -          | Running     | ctlplane=192.168.126.103| | ff0e5e15-5bb8-4c77-81c3-651588802ebd | overcloud-controller-0 | ACTIVE | -          | Running     | ctlplane=192.168.126.102| +--------------------------------------+------------------------+--------+------------+-------------+-------------------------+ [stack@undercloud ~]$ ssh heat-admin@192.168.126.102 overcloud-controller-0$ sudo -i overcloud-controller-0# journalctl -f -u os-collect-config &nbsp;  After deployment completes successfully you will want to copy overcloudrc in directory where you ran overcloud deployment to controller in overcloud uses heat-admin user.  [stack@undercloud ~]$ scp overcloudrc heat-admin@192.168.126.114: Log into overcloud controller and source profile  [stack@undercloud ~]$ ssh -l heat-admin 192.168.126.114 [heat-admin@overcloud-controller-0 ~]$ source overcloudrc [heat-admin@overcloud-controller-0 ~]$ openstack user list +----------------------------------+------------+ | ID | Name | +----------------------------------+------------+ | 1850b8cd6e7543f38fad7c241aa0f3c1 | heat | | 64ca208912b44ef3a5511fc085e9b4da | ceilometer | | 7787c945b9d64ba2b4b8e87faa1551c0 | nova | | 91b0cb7aeee14015b9330335b3dd1e80 | cinderv2 | | b0da49c8262542fabb3b52cc8ceb9536 | glance | | ca9bdbff969745d19f4822ac369c7681 | admin | | e1349a56ac254d5e9c26f7aeb334997d | swift | | fb16f76bfef944b19505471425026063 | cinder | | fb37a29ccac54f449f91b363b10591d8 | neutron | +----------------------------------+------------+ Scaling Overcloud  OpenStack Director makes scaling easy. Simply re-run the deployment and increase the role number. In this case we are adding a second compute node. Both compute and storage nodes can be scaled seamlessly in this fashion.  [stack@undercloud ~]$ openstack overcloud deploy --templates -e /usr/share/openstack-tripleo-heat-templates/environments/network-isolation.yaml -e ~/templates/network-environment.yaml -e ~/templates/firstboot-environment.yaml --control-scale 1 --compute-scale 2 --control-flavor control --compute-flavor compute --ntp-server pool.ntp.org --neutron-network-type vxlan --neutron-tunnel-types vxlan Useful OpenStack Director Tips  If you need to change Ironic node hardware and re-run introspection, below is the correct process to do this cleanly.  for n in `ironic node-list|grep -v \"available\\|manageable\"|awk '/None/ {print $2}'`; do ironic node-set-maintenance $n off ironic node-set-provision-state $n deleted done Update Ironic node profile manually.  ironic node-update &lt;ID&gt; add properties/capabilities=\"profile:&lt;PROFILE&gt;,boot_option:local\" Get admin password from undercloud or overcloud.  [root@overcloud-controller-0 ~]# hiera admin_password Summary  In this article we covered how to deploy Red Hat OpenStack Platform 8 using OpenStack Director. We have looked into some of the concepts behind OpenStack Director and seen the need for lifecycle management within the OpenStack ecosystem. If OpenStack is to become the future virtualization platform in the Enterprise, it will be because TripleO and OpenStack Director paved the way. I hope you found this article useful and encourage any feedback. Let's all grow and share together the opensource way!  Happy OpenStacking!  (c) 2016 Keith Tenzer  &nbsp;  &nbsp;  &nbsp;  ","categories": ["OpenStack"],
        "tags": ["Linux","Cloud"],
        "url": "/openstack/red-hat-openstack-platform-8-lab-configuration-using-openstack-director/",
        "teaser": null
      },{
        "title": "OpenStack Networking 101 for non-Network engineers",
        "excerpt":"  Overview  In this article we will take a deeper look into OpenStack networking and try to understand general networking concepts . We will look at how various networking concepts are implemented within OpenStack and also discuss SDNs, network scalability and HA.    The most complex service within OpenStack is certainly Neutron. Networking principles have not changed, however Neutron provides a lot of new abstractions that make is rather difficult to follow or understand traffic flows. On top of that there are many, many ways to build Network architectures within Neutron and a huge 3rd party ecosystem exists around Neutron that can make things even more confusing.  Networking Basics  You cannot really start a discussion around networking basics without mentioning the OSI model so that is where we will begin as well.    The OSI model identifies 7 layers, for the purposes of Neutron we are primarily concerned with layer 1 (physical), layer 2 (data link ), layer 3 (network) and layer 4 (transport). Ethernet data is transmitted in packets on layer 1. An Ethernet frame is encapsulated within a packet on layer 2.    Ethernet frames have source and destination MAC addresses however do not include routing information. Layer 2 can only broadcast on the local network segment. It does have a place holder for VLAN ID so traffic can be delivered to correct network segment based on VLAN. A VLAN is nothing more than a logical representation of a layer 2 segment.    Each host on the layer 2 network segment can communicate with one another using Ethernet frame and specifying source / destination MAC address. ARP (Address Resolution Protocol) is used to find out the location of a MAC address.    Once a MAC address has been discovered it is cached on the clients and stored in the ARP cache.    Traffic bound for MAC addresses that don't exist on layer 2 network segment must be routed over layer 3. In other words layer 3 simply connects multiple layer 2 networks together.    In this example we have three class C (255.255.255.0) network subnets. Communication between subnets requires layer 3 routing. Communication within subnet uses layer 2 ethernet frame and ARP. ICMP (Internet Control Messaging Protocol) works at layer 3, tools that use ICMP are ping and mtr. Layer 3 traffic traverses networks and each device has a routing table that understands the next hop.    We can look at the routing table and using commands like \"ip route get\", \"traceroute\" and \"tracepath\" we can understand traffic patterns within layer 3 network.  Layer 4 is of course where we get into TCP (Transmission Control Protocol) and UDP (User Datagram Protocol).  TCP is a reliable protocol that ensures flow control, retransmission and ordered delivery of packets. Ports or socket streams are used to uniquely identify applications communicating with one another. Port range that exists is 1 - 65535 with 1-1023 being reserved for system ports. The default ephemeral port range in Linux is 32768 - 61000.  UDP unlike TCP is a connectionless protocol. Since delivery and sequential ordering are not guaranteed, UDP is not a reliable protocol. Common applications important in OpenStack ecosystem that use UDP are DHCP, DNS, NTP and VXLAN.  Network Tunneling  Tunneling allows a network to support a service or protocol that isnt natively supported within network. Tunneling works by encapsulating metadata into IP packet header. It allows for connecting dissimilar networks, encapsulating services like IPV6 in IPV4 and securely connecting non-trusted networks such as is the case with VPNs. Open vSwitch (out-of-box) SDN provided with OpenStack supports following tunneling protocols: GRE,  (Generic Routing Encapsulation) VXLAN (Virtual Extensible LAN) and GENEVE (General Network Virtualization Encapsulation).  Network Namespaces  Linux network namespaces allow for more granular segregation of software-defined networks. Since namespaces are logically segregated there is no overlap of ip ranges. In order to see networking within namespace commands such as ip, ping, tcdump, etc need to be executed within namespace.  To list network namespaces use below command.  # ip netns show  qdhcp-e6c4e128-5a86-47a7-b501-737935680090 qrouter-e7d9bf3c-22a7-4413-9e44-c1fb450f1432 To get list of interfaces use below command.  # ip netns exec qrouter-e7d9bf3c-22a7-4413-9e44-c1fb450f1432 ip a   Network Concepts Applied to OpenStack  Now that we have a basic overview of networking lets see how this is applied to Neutron. First Neutron is software-defined, certainly you need hardware (switches, routers, etc) but Neutron does not concern itself directly with hardware. It is an abstraction that works with layer 2, layer 3 and layer 4. Neutron defines two types of networks: tenant and provider.  Tenant Network  A tenant network is a layer 2 network that exists only within the OpenStack environment. A tenant network spans compute nodes and tenant networks are isolated from one another. A tenant network is not reachable outside the OpenStack environment. The main idea behind tenant networks is to abstract network complexity from consumer. Since tenant networks are isolated you dont have to worry about IP address range conflicts. This allows creating new networks in a simple scalable fashion.  Floating IPs  Neutron creates a abstraction around IP ranges, tenant networks are completely isolated from real physical networks. In OpenStack an instance gets a tenant IP. You can certainly put your tenant networks on physical networks but then you lose a lot of scalability and flexibility, hence most OpenStack deployments use Floating IPs to connect instances to the outside world. Floating IPs are an SNAT/DNAT that get created in iptables of qrouter network namespace. From within instance you will only see tenant IP, not any floating IPs. Floating IPs are only needed for accessing a tenant from outside, for example connecting via ssh.  Provider Network  A provider network connects to a physical network that exists outside of OpenStack. IN this case each instance gets a IP on the external physical network. Floating IPs are not used or needed. From a networking standpoint using provider networks makes things simple but you lose a lot of flexibility and scalability. Each compute node needs a physical connection to each provider network. Usually what most do is create a large bond and due VLAN tagging on bond to access provider networks.  Traffic Flows  Both north/south and east/west traffic flows exist within an OpenStack environment. A north/south traffic flow occurs when traffic is leaving OpenStack environment and its source or destination is a external network. A east/west traffic flow exists when instances within a tenant network or between tenant networks communicate with one another. Traffic between tenant networks to external networks requires layer 3 (unless using provider networks) and that means routing is involved through the Neutron l3-agent. Traffic within tenant network occurs at layer 2 through Neutron l2-agent.  Network Architectures  OpenStack Neutron offers a vast choice of networking architectures. Out-of-the-box the Neutron OVS Reference Architecture or Nova network can be configured. By integrating with 3rd party SDNs (software-defined networks) the l3-agent within Neutron is replaced by the SDN. Using provider networks also bypasses network overlay and VXLAN or GRE encapsulation.  High Availability  In OpenStack you will deploy either the Neutron reference architecture or an external SDN. The Neutron reference architecture uses haproxy to provide HA for the l3-agent running on OpenStack controllers. This of course creates a scalability bottleneck since all routed traffic needs to go through l3-agent and it is running on controllers. I have seen the neutron reference architecture have performance issues around 1000 instances but this can vary depending on workload.  Scalability  As mentioned the l3-agent in Neutron can become a bottleneck. To address this you have two options DVR (Distributed Virtual Routing) or 3rd Party SDN. DVR allows the l3-agent to run on compute nodes and this of course scales a lot better, however this is not really supported in all OpenStack distros and can be very challenging to troubleshoot. The best option to scale network beyond 1000 instances is at a 3rd party SDN. Neutron will still acts as abstraction in front of SDN but you wont need l3-agent, the SDN will handle this with a more scalable solution. Using SDN is also in my opinion a cleaner approach and allows network teams to maintain network control as they did in pre-openstack era. In future openvswitch should get its own SDN controller to be able to offload l3 traffic but this is not quite ready at this time.  Examples  Below we will look at configuring OpenStack to use provider network and Floating IP network.  Prerequisites  Below are some prerequisites you need to implement within your OpenStack environment.  Get CentOS 7.2 Cloud Image.  curl -O http://cloud.centos.org/centos/7/images/CentOS-7-x86_64-GenericCloud.qcow2 glance image-create --name centos72 --visibility public --disk-format qcow2 --container-format bare --file CentOS-7-x86_64-GenericCloud.qcow2 Create Security Group.  # nova secgroup-create all \"Allow all tcp ports\" # nova secgroup-add-rule all TCP 1 65535 0.0.0.0/0 # nova secgroup-add-rule all ICMP -1 -1 0.0.0.0/0 Create a private ssh key for connecting to instances remotely.  # nova keypair-add admin Create admin.pem file and add private key from output of keypair-add command.  # vi /root/admin.pem # chmod 400 /root/admin.pem Example: Provider Network  # neutron net-create external_network --shared --provider:network_type flat --provider:physical_network extnet Configure Provider Network Subnet.  # neutron subnet-create --name public_subnet --allocation-pool=start=192.168.0.200,end=192.168.0.250 --gateway=192.168.0.1 external_network 192.168.0.0/24 --dns-nameserver 8.8.8.8 Enable isolated metadata server. Metadata server is used for injecting cloud-init and part of bootstrapping process. Other option is to setup route in iptables that goes from metadata server to gateway of provider network.  # vi /etc/neutron/dhcp_agent.ini enable_isolated_metadata = True Get the Network Id  # neutron net-list +--------------------------------------+------------------+-----------------------------------------------------+ | id | name | subnets | +--------------------------------------+------------------+-----------------------------------------------------+ | 459f477d-4c67-4800-ad07-adb9b096caf5 | external_network | 84c5e031-ed09-4ec0-86e4-609b27e21efb 192.168.0.0/24 | +--------------------------------------+------------------+-----------------------------------------------------+ Start instance on the provider network  # nova boot --flavor m1.medium --image \"centos72\" --nic net-id=459f477d-4c67-4800-ad07-adb9b096caf5 --key-name admin --security-groups all mycentos Connect to mycentos instance using the private ssh key stored in the admin.pem file. Note: The first floating IP in the range 192.168.122.201.  # ssh -i admin.pem cirros@192.168.122.201 Example: Floating-ip Network  # neutron net-create private # neutron subnet-create private 10.10.1.0/24 --name private_subnet --allocation-pool start=10.10.1.100,end=10.10.1.200 Create public network. Note: these steps assume the physical network connected to eth0 is 192.168.122.0/24.  # neutron net-create public --router:external # neutron subnet-create public 192.168.122.0/24 --name public_subnet --allocation-pool start=192.168.122.100,end=192.168.122.200 --disable-dhcp --gateway 192.168.122.1 Add a new router and configure router interfaces.  # neutron router-create router1 --ha False # neutron router-gateway-set router1 public # neutron router-interface-add router1 private_subnet List the network IDs.  # neutron net-list  +--------------------------------------+---------+-------------------------------------------------------+  | id | name | subnets |  +--------------------------------------+---------+-------------------------------------------------------+  | d4f3ed19-8be4-4d56-9f95-cfbac9fdf670 | private | 92d82f53-6e0b-4eef-b8b9-cae32cf40457 10.10.1.0/24     |  | 37c024d6-8108-468c-bc25-1748db7f5e8f | public  | 22f2e901-186f-4041-ad93-f7b5ccc30a81 192.168.122.0/24 | Start instance on the provider network  # nova boot --flavor m1.medium --image \"centos72\" --nic net-id=459f477d-4c67-4800-ad07-adb9b096caf5 --key-name admin --security-groups all mycentos Create a floating IP and assign it to the mycirros instance.  # nova floating-ip-create # nova floating-ip-associate mycirros &lt;FLOATING IP&gt; Connect to mycentos instance using the private ssh key stored in the admin.pem file. Note: The first floating IP in the range 192.168.122.201.  # ssh -i admin.pem cirros@192.168.122.201 Summary  In this article we looked at basic network concepts and applied them to OpenStack. We saw various network implementations like provider networks and floating ip networks. Finally we implemented these networks in an OpenStack environment. I hope you found this article useful. I think the most challenging aspect of OpenStack is networking. If you have material or additional information please share.  Happy OpenStacking!  (c) 2016 Keith Tenzer  ","categories": ["OpenStack"],
        "tags": ["Linux","neutron","OpenStack"],
        "url": "/openstack/openstack-networking-101-for-non-network-engineers/",
        "teaser": null
      },{
        "title": "OpenShift Enterprise 3.2: all-in-one Lab Environment",
        "excerpt":"  Overview  In this article we will setup a OpenShift Enterprise 3.2 all-in-one configuration. We will also setup the integration with CloudForms that allows additional management of OpenShift environments.  OpenShift has several different roles: masters, nodes, etcd and load balancers. An all-in-one setup means running all service on a single system. Since we are only using a single system a load balancer or ha-proxy won't be configured. If you would like to read more about OpenShift I can recommend the following:   General OpenShift Product Blogs Persistent Storage OpenShift Networking Part I OpenShift Networking Part II    Prerequisites  Configure a VM with following:   RHEL 7.2 2 CPUs 4096 RAM 30GB disk for OS 25GB disk for docker images  # subscription-manager repos --disable=\"*\" # subscription-manager repos \\     --enable=\"rhel-7-server-rpms\" \\     --enable=\"rhel-7-server-extras-rpms\" \\     --enable=\"rhel-7-server-ose-3.2-rpms\" # yum install -y wget git net-tools bind-utils iptables-services bridge-utils bash-completion # yum update -y # yum install -y atomic-openshift-utils # systemctl reboot # yum install -y docker-1.10.3 # vi /etc/sysconfig/docker OPTIONS='--selinux-enabled --insecure-registry 172.30.0.0/16' # cat &lt;&lt;EOF &gt; /etc/sysconfig/docker-storage-setup DEVS=/dev/vdb VG=docker-vg EOF # docker-storage-setup # systemctl enable docker # systemctl start docker # ssh-keygen # ssh-copy-id -i /root/.ssh/id_rsa-pub ose3-master.lab.com DNS Setup  DNS is a requirement for OpenShift Enterprise. In fact most issues you may run into are a result of not having a properly working DNS environment. For OpenShift you can either use dnsmasq or bind. I recommend using bind but in this article I will cover both options.  DNSMASQ  A colleague Ivan Mckinely was nice enough to create an ansible playbook for deploying dnsmasq. To deploy dnsmasq run following steps on OpenShift master.  # git clone https://github.com/ivanthelad/ansible-aos-scripts.git #cd ansible-aos-scripts Edit inventory file and set dns to IP of the system that should be providing DNS. Also ensure nodes and masters have correct IPs for your OpenShift servers. In our case 192.168.122.60 is master, node and DNS.  #vi inventory  [dns] 192.168.122.60 [nodes] 192.168.122.60 [masters] 192.168.122.60 Configure dnsmasq and add wildcard DNS so all hosts with  # vi playbooks/roles/dnsmasq/templates/dnsmasq.conf  strict-order domain-needed local=/lab.com/ bind-dynamic resolv-file=/etc/resolv.conf.upstream no-hosts address=/.cloudapps.lab.com/192.168.122.60 address=/ose3-master.lab.com/192.168.122.60 log-queries Ensure all hosts you want in DNS are also in /etc/hosts. The dnsmasq service reads /etc/hosts upon startup so all entries in hosts file can be queried through DNS.  #vi /etc/hosts  192.168.122.60  ose3-master.lab.com     ose3-master Configure ssh on DNS host  #ssh-keygen #ssh-copy-id -i ~/.ssh/id_rsa.pub ose3-master.lab.com Install dnsmasq via ansible  # ansible-playbook -i inventory playbooks/install_dnsmas.yml If you need to make changes you can edit the /etc/dnsmasq.conf file and restart dnsmasq service. Below is a sample dnsmasq.conf.  # vi /etc/dnsmasq.conf strict-order domain-needed local=/example.com/ bind-dynamic resolv-file=/etc/resolv.conf.upstream no-hosts address=/.apps.lab.com/192.168.122.60 address=/ose3-master.lab.com/192.168.122.60 address=/kubernetes.default.svc/192.168.122.60 log-queries NAMED  Install DNS tools and utilities.  # yum -y install bind bind-utils # systemctl enable named # systemctl start named Set firewall rules using iptables.  # iptables -A INPUT -p tcp -m state --state NEW -m tcp --dport 53 -j ACCEPT # iptables -A INPUT -p udp -m state --state NEW -m udp --dport 53 -j ACCEPT  Save the iptables Using.  # service iptables save iptables: Saving firewall rules to /etc/sysconfig/iptables:[  OK  ] Note: If you are using firewalld you can just enable service DNS using firwall-cmd utility.  Example of zone file for lab.com.  vi /var/named/dynamic/lab.com.zone   $ORIGIN lab.com. $TTL 86400 @ IN SOA dns1.lab.com. hostmaster.lab.com. (  2001062501 ; serial  21600 ; refresh after 6 hours  3600 ; retry after 1 hour  604800 ; expire after 1 week  86400 ) ; minimum TTL of 1 day ; ;  IN NS dns1.lab.com. dns1 IN A 192.168.122.1  IN AAAA aaaa:bbbb::1 ose3-master IN A 192.168.122.60 *.cloudapps 300 IN A 192.168.122.60 Example of named configuration.  # vi /etc/named.conf  // // named.conf // // Provided by Red Hat bind package to configure the ISC BIND named(8) DNS // server as a caching only nameserver (as a localhost DNS resolver only). // // See /usr/share/doc/bind*/sample/ for example named configuration files. //  options {  listen-on port 53 { 127.0.0.1;192.168.122.1; };  listen-on-v6 port 53 { ::1; };  directory \"/var/named\";  dump-file \"/var/named/data/cache_dump.db\";  statistics-file \"/var/named/data/named_stats.txt\";  memstatistics-file \"/var/named/data/named_mem_stats.txt\";  allow-query { localhost;192.168.122.0/24;192.168.123.0/24; };  /*   - If you are building an AUTHORITATIVE DNS server, do NOT enable recursion.  - If you are building a RECURSIVE (caching) DNS server, you need to enable   recursion.   - If your recursive DNS server has a public IP address, you MUST enable access   control to limit queries to your legitimate users. Failing to do so will  cause your server to become part of large scale DNS amplification   attacks. Implementing BCP38 within your network would greatly  reduce such attack surface   */  recursion yes;  dnssec-enable yes;  dnssec-validation yes;  dnssec-lookaside auto;  /* Path to ISC DLV key */  bindkeys-file \"/etc/named.iscdlv.key\";  managed-keys-directory \"/var/named/dynamic\";  pid-file \"/run/named/named.pid\";  session-keyfile \"/run/named/session.key\";  //forward first;  forwarders {  //10.38.5.26;  8.8.8.8;  }; };  logging {  channel default_debug {  file \"data/named.run\";  severity dynamic;  }; };  zone \".\" IN {  type hint;  file \"named.ca\"; };  zone \"lab.com\" IN {  type master;  file \"/var/named/dynamic/lab.com.zone\";  allow-update { none; }; };  //zone \"122.168.192.in-addr.arpa\" IN { // type master; // file \"/var/named/dynamic/122.168.192.db\"; // allow-update { none; }; //};  include \"/etc/named.rfc1912.zones\"; include \"/etc/named.root.key\"; Note: I have left out reverse DNS, PTR records. If you need this you can of course add zone file and set that up but it isn't required for a lab configuration.  Install OpenShift.  Here we are enabling ovs-subnet SDN and setting authentication to use htpasswd. This is the most basic configuration as we are doing an all-in-one setup. For actual deployments you would want multi-master, dedicated nodes and seperate nodes for handling etcd.  ########################## ### OSEv3 Server Types ### ########################## [OSEv3:children] masters nodes etcd  ################################################ ### Set variables common for all OSEv3 hosts ### ################################################ [OSEv3:vars] ansible_ssh_user=root os_sdn_network_plugin_name='redhat/openshift-ovs-subnet' deployment_type=openshift-enterprise openshift_master_default_subdomain=apps.lab.com openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}] openshift_node_kubelet_args={'maximum-dead-containers': ['100'], 'maximum-dead-containers-per-container': ['2'], 'minimum-container-ttl-duration': ['10s'], 'max-pods': ['110'], 'image-gc-high-threshold': ['90'], 'image-gc-low-threshold': ['80']} logrotate_scripts=[{\"name\": \"syslog\", \"path\": \"/var/log/cron\\n/var/log/maillog\\n/var/log/messages\\n/var/log/secure\\n/var/log/spooler\\n\", \"options\": [\"daily\", \"rotate 7\", \"compress\", \"sharedscripts\", \"missingok\"], \"scripts\": {\"postrotate\": \"/bin/kill -HUP `cat /var/run/syslogd.pid 2&gt; /dev/null` 2&gt; /dev/null || true\"}}] openshift_docker_options=\"-log-driver json-file --log-opt max-size=1M --log-opt max-file=3\" openshift_node_iptables_sync_period=5s openshift_master_pod_eviction_timeout=3m osm_controller_args={'resource-quota-sync-period': ['10s']} osm_api_server_args={'max-requests-inflight': ['400']} openshift_use_dnsmasq=true  ############################## ### host group for masters ### ############################## [masters] ose3-master.lab.com  ################################### ### host group for etcd servers ### ################################### [etcd] ose3-master.lab.com  ################################################## ### host group for nodes, includes region info ### ################################################## [nodes] ose3-master.lab.com openshift_schedulable=True Run Ansible playbook to install and configure OpenShift.  # ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/byo/config.yml Configure OpenShift  Create local admin account and enable permissions.  [root@ose3-master ~]#oc login -u system:admin -n default [root@ose3-master ~]#htpasswd -c /etc/origin/master/htpasswd admin [root@ose3-master ~]#oadm policy add-cluster-role-to-user cluster-admin admin [root@ose3-master ~]#oc login -u admin -n default Configure OpenShift image registry. Image streams are stored in registry. When you build application, your application code will be added as a image stream. This enables S2I (Source to Image) and allows for fast build times.  [root@ose3-master ~]#oadm registry --service-account=registry \\ --config=/etc/origin/master/admin.kubeconfig \\ --images='registry.access.redhat.com/openshift3/ose-${component}:${version}' Configure OpenShift router. The OpenShift router is basically an HA-Proxy that sends incoming service requests to node where pod is running.  [root@ose3-master ~]#oadm router router --replicas=1 \\     --credentials='/etc/origin/master/openshift-router.kubeconfig' \\     --service-account=router CloudForms Integration  CloudForms is a cloud management platform. It integrates not only with OpenShift but also other Cloud platforms (OpenStack, Amazon, GCE, Azure) and traditional virtualization platforms (VMware, RHEV, Hyper-V). Since OpenShift is usually running on cloud or traditional virtualization platforms, CloudForms enables true end-to-end visibility. CloudForms provides not only performance metrics, events, smart state analysis of containers (scanning container contents) but also can provide chargeback for OpenShift projects. CloudForms is included in OpenShift subscription for purpose of managing OpenShift. To add OpenShift as provider in CloudForms follow the below steps.  The management-admin project is designed for scanning container images. A container is started in this context and the image to be scanned mounted. List tokens that are configured in management-admin project (this is created at install time).  [root@ose3-master ~]# oc get sa management-admin -o yaml apiVersion: v1 imagePullSecrets: - name: management-admin-dockercfg-ln1an kind: ServiceAccount metadata:  creationTimestamp: 2016-07-24T11:36:58Z  name: management-admin  namespace: management-infra  resourceVersion: \"400\"  selfLink: /api/v1/namespaces/management-infra/serviceaccounts/management-admin  uid: ee6a1426-5192-11e6-baff-001a4ae42e01 secrets: - name: management-admin-token-wx17s - name: management-admin-dockercfg-ln1an Use describe to get token to enable CloudForms to accesss the management-admin project.  [root@ose3-master ~]# oc describe secret management-admin-token-wx17s Name: management-admin-token-wx17s Namespace: management-infra Labels: &lt;none&gt; Annotations: kubernetes.io/service-account.name=management-admin,kubernetes.io/service-account.uid=ee6a1426-5192-11e6-baff-001a4ae42e01  Type: kubernetes.io/service-account-token  Data ==== ca.crt: 1066 bytes namespace: 16 bytes token: eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJtYW5hZ2VtZW50LWluZnJhIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6Im1hbmFnZW1lbnQtYWRtaW4tdG9rZW4td3gxN3MiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoibWFuYWdlbWVudC1hZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImVlNmExNDI2LTUxOTItMTFlNi1iYWZmLTAwMWE0YWU0MmUwMSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDptYW5hZ2VtZW50LWluZnJhOm1hbmFnZW1lbnQtYWRtaW4ifQ.Y0IlcwhHW_CpKyFvk_ap-JMAT69fbIqCjkAbmpgZEUJ587LP0pQz06OpBW05XNJ3cJg5HeckF0IjCJBDbMS3P1W7KAnLrL9uKlVsZ7qZ8-M2yvckdIxzmEy48lG0GkjtUVMeAOJozpDieFClc-ZJbMrYxocjasevVNQHAUpSwOIATzcuV3bIjcLNwD82-42F7ykMn-A-TaeCXbliFApt6q-R0hURXCZ0dkWC-za2qZ3tVXaykWmoIFBVs6wgY2budZZLhT4K9b4lbiWC5udQ6ga2ATZO1ioRg-bVZXcTin5kf__a5u6c775-8n6DeLPcfUqnLucaYr2Ov7RistJRvg Add OpenShift provider to CloudForms using the management-admin service token.    Performance Metrics  OpenShift provides ability to collect performance metrics using Hawkular. This runs as container and uses cassandra to persist the data. CloudForms is able to display capacity and utilization metrics for OpenShift using Hawkular.  Switch to openshift-infra project. This is intended for running infrastructure containers such as hawkular or ELK for logging.  [root@ose3-master ~]# oc project openshift-infra Create service account for metrics-deployer pod.  [root@ose3-master ~]# oc create -f - &lt;&lt;API apiVersion: v1 kind: ServiceAccount metadata:  name: metrics-deployer secrets: - name: metrics-deployer API Enable permissions and set secret.  [root@ose3-master ~]# oadm policy add-role-to-user edit system:serviceaccount:openshift-infra:metrics-deployer [root@ose3-master ~]#oadm policy add-cluster-role-to-user cluster-reader system:serviceaccount:openshift-infra:heapster [root@ose3-master ~]# oc secrets new metrics-deployer nothing=/dev/null Deploy metrics environment for OpenShift.  [root@ose3-master ~]# oc new-app -f /usr/share/openshift/examples/infrastructure-templates/enterprise/metrics-deployer.yaml \\ -p HAWKULAR_METRICS_HOSTNAME=hawkular-metrics.apps.lab.com \\ -p USE_PERSISTENT_STORAGE=false -p MASTER_URL=https://ose3-master.lab.com:8443 CloudForms Container Provider  CloudForms is a cloud management platform. It integrates not only with OpenShift but also other Cloud platforms (OpenStack, Amazon, GCE, Azure) and traditional virtualization platforms (VMware, RHEV, Hyper-V). Since OpenShift is usually running on cloud or traditional virtualization platforms, CloudForms enables true end-to-end visibility. CloudForms provides not only performance metrics, events, smart state analysis of containers (scanning container contents) but also can provide chargeback for OpenShift projects. CloudForms is included in OpenShift subscription for purpose of managing OpenShift. To add OpenShift as provider in CloudForms follow the below steps.  Use management-admin token that is created in management-admin project during install to provide access to CloudForms.  [root@ose3-master ~]# oc describe sa -n management-infra management-admin Name: management-admin Namespace: management-infra Labels: &lt;none&gt;  Mountable secrets: management-admin-token-vr21i  management-admin-dockercfg-5j3m3  Tokens: management-admin-token-mxy4m  management-admin-token-vr21i  Image pull secrets: management-admin-dockercfg-5j3m3 [root@ose3-master ~]# oc describe secret -n management-infra management-admin-token-mxy4m Name: management-admin-token-mxy4m Namespace: management-infra Labels: &lt;none&gt; Annotations: kubernetes.io/service-account.name=management-admin,kubernetes.io/service-account.uid=87f8f4e4-4c0f-11e6-8aca-52540057bf27  Type: kubernetes.io/service-account-token  Data ==== token: eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJtYW5hZ2VtZW50LWluZnJhIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6Im1hbmFnZW1lbnQtYWRtaW4tdG9rZW4tbXh5NG0iLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoibWFuYWdlbWVudC1hZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6Ijg3ZjhmNGU0LTRjMGYtMTFlNi04YWNhLTUyNTQwMDU3YmYyNyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDptYW5hZ2VtZW50LWluZnJhOm1hbmFnZW1lbnQtYWRtaW4ifQ.dN-CmGdSR2TRh1h0qHvwkqnW6TLvhXJtuHX6qY2jsrZIZCg2LcyuQI9edjBhl5tDE6PfOrpmh9-1NKAA6xbbYVJlRz52gnEdtm1PVgvzh8_WnKiQLZu-xC1qRX_YL7ohbglFSf8b5zgf4lBdJbgM_2P4sm1Czhu8lr5A4ix95y40zEl3P2R_aXnns62hrRF9XpmweASGMjooKOHB_5HUcZ8QhvdgsveD4j9de-ZzYrUDHi0NqOEtenBThe5kbEpiWzSWMAkIeC2wDPEnaMTyOM2bEfY04bwz5IVS_IAnrEF7PogejgsrAQRtYss5yKSZfwNTyraAXSobgVa-e4NsWg ca.crt: 1066 bytes namespace: 16 bytes Add OpenShift provider to CloudForms using token.    Configure metrics by supplying the service name exposed by OpenShift.    Choose a container image to scan.    You should see scanning container start in the project management-infra.  [root@ose3-master ~]# oc project management-infra [root@ose3-master ~]# oc get pods NAME READY STATUS RESTARTS AGE manageiq-img-scan-24297 0/1 ContainerCreating 0 12s [root@ose3-master ~]# oc get pods NAME READY STATUS RESTARTS AGE manageiq-img-scan-24297 1/1 Running 0 1m Check image in CloudForms and you should now see an OpenSCAP report and in addition visibility into packages that are actually installed in the container itself.  Compute-&gt;Containers-Container Images-&gt;MySQL    Packages    OpenScap HTML Report    Aggregate Logging  OpenShift Enterprise supports Kibana and the ELK Stack for log aggregation. Any pod and container that log to STDOUT will have all their log messages aggregated. This provides centralized logging for all application components. Logging is completely integrated within OpenShift and the ELK Stack runs of course containerized within OpenShift.  In openshift-infra project create service account for logging and necessary permissions.  [root@ose3-master ~]# oc project openshift-infra [root@ose3-master ~]# oc secrets new logging-deployer nothing=/dev/null [root@ose3-master ~]# oc create -f - &lt;&lt;API apiVersion: v1 kind: ServiceAccount metadata:   name: logging-deployer secrets: - name: logging-deployer API   [root@ose3-master ~]# oc policy add-role-to-user edit --serviceaccount logging-deployer [root@ose3-master ~]# oadm policy add-scc-to-user privileged system:serviceaccount:logging:aggregated-logging-fluentd [root@ose3-master ~]# oadm policy add-cluster-role-to-user cluster-reade system:serviceaccount:logging:aggregated-logging-fluentd Deploy the logging stack. This creates a template or blueprint.  [root@ose3-master ~]# oc new-app logging-deployer-template \\              --param KIBANA_HOSTNAME=kibana.apps.lab.com \\              --param ES_CLUSTER_SIZE=1 \\              --param KIBANA_OPS_HOSTNAME=kibana-ops.apps.lab.com \\              --param PUBLIC_MASTER_URL=https://ose3-master.lab.com:8443 [root@ose3-master ~]# oc get pods NAME READY STATUS RESTARTS AGE logging-deployer-1de06 0/1 Completed 0 3m Once logger deployer is complete, execute the template.  [root@ose3-master ~]# oc new-app logging-support-template   If you don' see containers creating then you need to manually import image streams.  [root@ose3-master ~]#oc import-image logging-auth-proxy:3.2.0 --from registry.access.redhat.com/openshift3/logging-auth-proxy:3.2.0 [root@ose3-master ~]# oc import-image logging-kibana:3.2.0 --from registry.access.redhat.com/openshift3/logging-kibana:3.2.0 [root@ose3-master ~]# oc import-image logging-elasticsearch:3.2.0 --from registry.access.redhat.com/openshift3/logging-elasticsearch:3.2.0 [root@ose3-master ~]# oc import-image logging-fluentd:3.2.0 --from registry.access.redhat.com/openshift3/logging-fluentd:3.2.0 [root@ose3-master ~]#  oc get pods NAME READY STATUS RESTARTS AGE logging-deployer-9lqkt 0/1 Completed 0 15m logging-es-pm7uamdy-2-rdflo 1/1 Running 0 8m logging-kibana-1-e13r3 2/2 Running 0 13m Once ELK Stack is running update deployment so that persistent storage is used (optional). Note: this requires configuring persistent storage and that is explained in a different blog post referenced above.  #vi pvc.json  {      \"apiVersion\": \"v1\",     \"kind\": \"PersistentVolumeClaim\",     \"metadata\": {          \"name\": \"logging-es-1\"     },     \"spec\": {         \"accessModes\": [ \"ReadWriteOnce\" ],             \"resources\": {                 \"requests\": {                     \"storage\": \"10Gi\"                 }             }      } } [root@ose3-master ~]# oc create -f pvc.json [root@ose3-master ~]# oc get dc NAME TRIGGERS LATEST logging-es-pm7uamdy ConfigChange, ImageChange 2 [root@ose3-master ~]# oc volume dc/logging-es-pm7uamdy --add --overwrite --name=elasticsearch-storage --type=persistentVolumeClaim --claim-name=logging-es-1 OpenShift on RHEV or VMware.  If you are running OpenShift on traditional virtualization platform ensure mac spoofing is enabled or allowed. If it isn't the hypervisor will most likely drop packets going outbound from OpenShift. Enabling mac spoofing for RHEV is documented here under configure RHEV for OpenStack (same issue exists when running OpenStack nested in RHEV). For VMware or Hyper-V, I am not sure so just keep this in mind.  Summary  In this article we have seen how to configure an OpenShift 3.2 all-in-one lab environment. We have also seen how install and configuration can be adapted through ansible playbook. We have seen how to configure various DNS options. It should be repeated that most OpenShift problems are a direct result of improper DNS setup! We have seen how to integrate OpenShift in CloudForms and how to configure metrics using hawkular. Finally we even covered configuring log aggregation using containerized ELK stack. If you have any feedback please share.  Happy OpenShifting!  (c) 2016 Keith Tenzer  &nbsp;  &nbsp;  &nbsp;  ","categories": ["OpenStack"],
        "tags": ["CloudForms","Containers","Docker","Kubernetes","OpenShift"],
        "url": "/openstack/openshift-enterprise-3-2-all-in-one-lab-environment/",
        "teaser": null
      },{
        "title": "OpenShift v3: Basic Release Deployment Scenarios",
        "excerpt":"  source: http://snsoftwarelabs.com/  Overview  One of the hardest things companies struggle with today is release management. Of course many methodologies and even more tools or technologies exist, but how do we bring everything together and work across functional boundaries of an organization? A product release involves everyone in the company not just a single team. Many companies struggle with this and the result is a much slower innovation cycle. In the past this used to be something that at least wasn't a deal breaker. Unfortunately that is no longer the case. Today companies live and die by their ability to not only innovate but release innovation. I would say innovating is the easy part, the ability to provide those innovations in a controlled fashion through products and services is the real challenge.    Moving to micro-services architectures and container based technologies such as Docker have simplified or streamlined many technological aspects. Certainly at least providing light at the end of the tunnel, but until OpenShift there wasn't a platform to bring it all together, allowing development and operations teams to work together while still maintaining their areas of focus or control. In this article we will look at three scenarios for handling application deployments within OpenShift that involve both operations and development. Each scenario builds on the other and should give you a good idea of the new possibilities with OpenShift. Also keep in mind, these are basic scenarios and we are just scratching the surface so this should be viewed as a starting point for doing application deployments or release management in the new containerized world.  Scenario 1: Development leveraging customized image from Operations  Typically operations teams will want to control application runtime environment. Ensure that the application runtime environment meets all security policies, provides needed capabilities and is updated on regular basis.  Development teams want to focus on innovation through application functionality, stability and capabilities.  OpenShift allows both teams to focus on their core responsibility while also providing a means to integrate inputs/outputs of various teams into a end-to-end release.  There are many ways to integrate DevOps teams in OpenShift. One simple way is by separating development and operations into different projects and allowing development to their application runtime environment from operations. In this scenario we will see how to do that using a basic ruby hello-world application as example.  Create Projects  Create operations and development projects for our ruby application.  # oc login -u admin # oc new-project ruby-ops # oc new-project ruby-dev Setup Users  Create a user for development and operations.  # htpasswd /etc/origin/master/htpasswd dev # htpasswd /etc/origin/master/htpasswd ops Create ops and dev users  # htpasswd /etc/origin/master/htpasswd dev # htpasswd /etc/origin/master/htpasswd ops Enable permissions.  Create three groups that allow operations to edit the ruby-ops project, allow development to view the ruby-ops project and also edit the ruby-dev project. In addition the ruby-dev project needs permission to pull images from the ruby-ops project.  Create groups and add users to correct groups.  # oadm groups new ops-edit &amp;&amp; oadm groups new dev-view &amp;&amp; oadm groups new dev-edit # oadm groups add-users ops-edit ops &amp;&amp; oadm groups add-users dev-view dev &amp;&amp; \\ oadm groups add-users dev-edit dev Associate groups to projects and setup pull permissions to allow ruby-dev to pull images from ruby-ops.  # oadm policy add-role-to-group edit ops-edit -n ruby-ops &amp;&amp; \\ # oadm policy add-role-to-group view dev-view -n ruby-ops &amp;&amp; \\ # oadm policy add-role-to-group edit dev-edit -n ruby-dev &amp;&amp; \\ # oadm policy add-role-to-group system:image-puller system:serviceaccounts:ruby-dev -n ruby-ops Operations Ruby Environment  As ops user create a ruby runtime image using application test code.  # oc login -u ops # oc project ruby-ops # oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-hello-world.git Application requires database and service name called database.  # oc new-app mysql-ephemeral -p DATABASE_SERVICE_NAME=database # oc env dc database --list | oc env dc ruby-hello-world -e - Development Ruby Environment  As dev user pull the operations ruby runtime image and build using latest code from different Github branch or project.  # oc login -u dev # oc project ruby-dev # oc new-app ruby-ops/ruby-22-centos7:latest~https://github.com/ktenzer/ruby-hello-world.git Application requires database and service name called database.  # oc new-app mysql-ephemeral -p DATABASE_SERVICE_NAME=database # oc env dc database --list | oc env dc ruby-hello-world -e - Scenario 2: Development to Production Promotion  Once development has an application version with required functionality and having passed all tests it can be promoted to other environments. Typically that would be qa, test, integration and eventually production. In this simple example using the ticket-monster application we will promote directly from development to production.  Using similar concepts as describe in scenario 1 this technique relies on production pulling appropriate images from development. In this scenario however we will create deployment configs and within setup a trigger that when an image with particular name/tag is updated in development, promotion to production will occur automatically. Scenario 1 shows how to do this manually, here we do it automatically and in scenario 3 we will see how to do promotion using jenkins that enables building complex pipelines with approval processes.  Create projects and setup pull permissions  # oc new-project ticket-monster-dev # oc new-project ticket-monster-prod # oc policy add-role-to-group system:image-puller system:serviceaccounts:ticket-monster-prod -n ticket-monster-dev Create ticket monster template for development  # vi monster.yaml kind: Template apiVersion: v1 metadata:  name: monster  annotations:  tags: instant-app,javaee  iconClass: icon-jboss  description: |  Ticket Monster is a moderately complex application that demonstrates how  to build modern applications using JBoss web technologies  parameters: - name: GIT_URI  value: git://github.com/kenthua/ticket-monster-ose - name: MYSQL_DATABASE  value: monster - name: MYSQL_USER  value: monster - name: MYSQL_PASSWORD  from: '[a-zA-Z0-9]{8}'  generate: expression   objects: - kind: ImageStream  apiVersion: v1  metadata:  name: monster  - kind: BuildConfig  apiVersion: v1  metadata:  name: monster  spec:  triggers:  - type: Generic  generic:  secret: secret  - type: ImageChange  - type: ConfigChange  strategy:  type: Source  sourceStrategy:  from:  kind: ImageStreamTag  name: jboss-eap64-openshift:latest  namespace: openshift  source:  type: Git  git:  uri: ${GIT_URI}  ref: master  output:  to:  kind: ImageStreamTag  name: monster:latest  - kind: DeploymentConfig  apiVersion: v1  metadata:  name: monster  spec:  replicas: 1  selector:  deploymentConfig: monster  template:  metadata:  labels:  deploymentConfig: monster  name: monster  spec:  containers:  - name: monster  image: monster  ports:  - name: http  containerPort: 8080  - name: jolokia  containerPort: 8778  - name: debug  containerPort: 8787  readinessProbe:  exec:  command:  - /bin/bash  - -c  - /opt/eap/bin/readinessProbe.sh  env:  - name: DB_SERVICE_PREFIX_MAPPING  value: monster-mysql=DB  - name: TX_DATABASE_PREFIX_MAPPING  value: monster-mysql=DB  - name: DB_JNDI  value: java:jboss/datasources/MySQLDS  - name: DB_DATABASE  value: ${MYSQL_DATABASE}  - name: DB_USERNAME  value: ${MYSQL_USER}  - name: DB_PASSWORD  value: ${MYSQL_PASSWORD}  - name: JAVA_OPTS  value: \"-Xmx512m -XX:MaxPermSize=256m -Djava.net.preferIPv4Stack=true -Djboss.modules.system.pkgs=org.jboss.logmanager -Djava.awt.headless=true -Djboss.modules.policy-permissions=true\"  - name: DEBUG  value: \"true\"  triggers:  - type: ImageChange  imageChangeParams:  automatic: true  containerNames:  - monster  from:  kind: ImageStream  name: monster  - kind: DeploymentConfig  apiVersion: v1  metadata:  name: monster-mysql  spec:  triggers:  - type: ImageChange  imageChangeParams:  automatic: true  containerNames:  - monster-mysql  from:  kind: ImageStreamTag  name: mysql:latest  namespace: openshift  replicas: 1  selector:  deploymentConfig: monster-mysql  template:  metadata:  labels:  deploymentConfig: monster-mysql  name: monster-mysql  spec:  containers:  - name: monster-mysql  image: mysql  ports:  - containerPort: 3306  env:  - name: MYSQL_USER  value: ${MYSQL_USER}  - name: MYSQL_PASSWORD  value: ${MYSQL_PASSWORD}  - name: MYSQL_DATABASE  value: ${MYSQL_DATABASE}  - kind: Service  apiVersion: v1  metadata:  name: monster  spec:  ports:  - name: http  port: 8080  selector:  deploymentConfig: monster  - kind: Service  apiVersion: v1  metadata:  name: monster-mysql  spec:  ports:  - port: 3306  selector:  deploymentConfig: monster-mysql  - kind: Route  apiVersion: v1  metadata:  name: monster  spec:  to:  name: monster # oc create -n openshift -f monster.yaml Create template for ticket monster production environment  Below trigger will only deploy production environment when the image stream in development is tagged with monster:prod.  # vi monster-prod.yaml kind: Template apiVersion: v1 metadata:  name: monster-prod  annotations:  tags: instant-app,javaee  iconClass: icon-jboss  description: |  Ticket Monster is a moderately complex application that demonstrates how  to build modern applications using JBoss web technologies. This template  is for \"production deployments\" of Ticket Monster.  parameters: - name: MYSQL_DATABASE  value: monster - name: MYSQL_USER  value: monster - name: MYSQL_PASSWORD  from: '[a-zA-Z0-9]{8}'  generate: expression  objects: - kind: DeploymentConfig  apiVersion: v1  metadata:  name: monster  spec:  replicas: 3  selector:  deploymentConfig: monster  template:  metadata:  labels:  deploymentConfig: monster  name: monster  spec:  containers:  - name: monster  image: monster  ports:  - name: http  containerPort: 8080  - name: jolokia  containerPort: 8778  readinessProbe:  exec:  command:  - /bin/bash  - -c  - /opt/eap/bin/readinessProbe.sh  env:  - name: DB_SERVICE_PREFIX_MAPPING  value: monster-mysql=DB  - name: TX_DATABASE_PREFIX_MAPPING  value: monster-mysql=DB  - name: DB_JNDI  value: java:jboss/datasources/MySQLDS  - name: DB_DATABASE  value: ${MYSQL_DATABASE}  - name: DB_USERNAME  value: ${MYSQL_USER}  - name: DB_PASSWORD  value: ${MYSQL_PASSWORD}  - name: JAVA_OPTS  value: \"-Xmx512m -XX:MaxPermSize=256m -Djava.net.preferIPv4Stack=true -Djboss.modules.system.pkgs=org.jboss.logmanager -Djava.awt.headless=true -Djboss.modules.policy-permissions=true\"  triggers:  - type: ImageChange  imageChangeParams:  automatic: true  containerNames:  - monster  from:  kind: ImageStreamTag  name: monster:prod  namespace: ticket-monster-dev  - kind: DeploymentConfig  apiVersion: v1  metadata:  name: monster-mysql  spec:  triggers:  - type: ImageChange  imageChangeParams:  automatic: true  containerNames:  - monster-mysql  from:  kind: ImageStreamTag  name: mysql:latest  namespace: openshift  replicas: 1  selector:  deploymentConfig: monster-mysql  template:  metadata:  labels:  deploymentConfig: monster-mysql  name: monster-mysql  spec:  containers:  - name: monster-mysql  image: mysql  ports:  - containerPort: 3306  env:  - name: MYSQL_USER  value: ${MYSQL_USER}  - name: MYSQL_PASSWORD  value: ${MYSQL_PASSWORD}  - name: MYSQL_DATABASE  value: ${MYSQL_DATABASE}  - kind: Service  apiVersion: v1  metadata:  name: monster  spec:  ports:  - name: http  port: 8080  selector:  deploymentConfig: monster  - kind: Service  apiVersion: v1  metadata:  name: monster-mysql  spec:  ports:  - port: 3306  selector:  deploymentConfig: monster-mysql  - kind: Route  apiVersion: v1  metadata:  name: monster  spec:  to:  name: monster # oc create -n openshift -f monster-prod.yaml Deploy ticket-monster development environment  Using the UI you can now choose template monster (monster).    Deploy ticket-monster production environment  Deploy ticket-monster production template (monster-prod).    You will notice in development the environment is built and you can access the application using the service URL: http://monster-ticket-monster-dev.apps.lab    If you look at production environment, the database is running and a service endpoint exists however ticket monster application is not running. The production template is as mentioned setup to pull images from development automatically that have a certain name/tag. The production environment also runs scale of 4 for application where development only has scale of 1.  Promote development application version to production  Get the image stream pull spec.  # oc get is monster -o yaml apiVersion: v1 kind: ImageStream metadata:  annotations:  openshift.io/image.dockerRepositoryCheck: 2016-08-09T13:37:47Z  creationTimestamp: 2016-08-09T13:14:53Z  generation: 7  name: monster  namespace: ticket-monster-dev  resourceVersion: \"107170\"  selfLink: /oapi/v1/namespaces/ticket-monster-dev/imagestreams/monster  uid: 42740a3d-5e33-11e6-aa8d-001a4ae42e01 spec:  tags:  - annotations: null  from:  kind: ImageStreamImage  name: monster@sha256:3a48a056a58f50764953ba856d90eba73dd0dfdee10b8cb6837b0fd9461da7f9  generation: 7  importPolicy: {}  name: prod status:  dockerImageRepository: 172.30.139.50:5000/ticket-monster-dev/monster  tags:  - items:  - created: 2016-08-09T13:26:04Z  dockerImageReference: 172.30.139.50:5000/ticket-monster-dev/monster@sha256:3a48a056a58f50764953ba856d90eba73dd0dfdee10b8cb6837b0fd9461da7f9  generation: 1  image: sha256:3a48a056a58f50764953ba856d90eba73dd0dfdee10b8cb6837b0fd9461da7f9  tag: latest Once you have the image stream pull specification (above in bold), tag the image stream monster:prod.  # oc tag monster@sha256:3a48a056a58f50764953ba856d90eba73dd0dfdee10b8cb6837b0fd9461da7f9 monster:prod You can verify that the image stream in fact has a tag prod.  # oc get is NAME DOCKER REPO TAGS UPDATED monster 172.30.139.50:5000/ticket-monster-dev/monster prod,latest 2 minutes ago As soon as the image stream in ticket-monster-dev is tagged with monster:prod it will be deployed to production. As mentioned above the scale in production is 4 instead of 1.    Scenario 3: AB Deployment using Jenkins  In this scenario we will look at how to do a simple AB deployment using Jenkins. This scenario builds off what we learned in scenario 1 and 2. In this scenario we will create a slightly more complex environment with three environments: development, integration and production. We will also build two version of our application, v1 and v2 in dev environment. Using Jenkins we will show how to promote v2 of application from development to integration to production. Finally we will show how to rollback production from v2 to v1 of application. I want to thank a colleague, Torben Jaeger who created a lot of the below content.  Create projects  # oc new-project dev &amp;&amp; \\ oc new-project int &amp;&amp; \\ oc new-project prod Setup pull permissions  Allow int environment to pull images from dev environment and prod to pull images from both dev and int environments.  # oc policy add-role-to-group system:image-puller system:serviceaccounts:int -n dev  # oc policy add-role-to-group system:image-puller system:serviceaccounts:prod -n int  # oc policy add-role-to-group system:image-puller system:serviceaccounts:prod -n dev Setup jenkins in development environment  Using the UI go to the dev project, select add to project and choose jenkins-ephemeral.    Clone Github repository  # git clone https://github.com/ktenzer/openshift-demo.git Update auth tokens for Jenkins  Get Auth token for OpenShift API. This is needed to allow Jenkins to access OpenShift environment. You need to update the auth tokens.  # oc login -u admin # oc whoami -t DMzhKyEN87DZiDYV6i1d8L8NL2e6gFVFPpT5FnozKtU Update the below jenkins jobs and replace authToken and destinationAuthToken using your token from above.  # ls jenkins-jobs/ promote-int.xml promote-prod.xml rel-v2.xml rollback-prod.xml Setup all three environements  Using templates you updated above with your auth token create all environments.  # cd openshift-demo # oc create -f template.json -n dev # oc create -f acceptance.template.json -n int  # oc create -f production.template.json -n prod Deploy nodejs hello-world application in dev environment  Integration is setup to pull from development and production is setup to pull from integration.  # oc new-app -f template.json -n dev This template creates two versions of the application: v1-ab and v2-ab.    Test applications in development  Connecting to v1-ab via http or curl should print \"Hello World!\". Connecting to v2-ab via http or curl should print \"Hello World, welcome to Frankfurt!\".  Deploy v1 from development to integration  Deploy v1 to int using tags. A trigger is set on int (acceptance) template to deploy when the image acceptance:latest is updated. You should see that a pod is started and running version v1 of application.  oc tag v1:latest v1:1.0 -n dev oc tag dev/v1:1.0 acceptance:v1 -n int oc tag acceptance:v1 acceptance:latest -n int Deploy v1 from integration to production  Deploy v1 to prod using tags. A trigger is set on prod template to deploy when the image production:latest is updated.  oc tag int/acceptance:v1 production:v1 -n prod oc tag production:v1 production:latest -n prod Notice the application scale is 4 and the version of code running in prod is v1. Again this is all defined in the template we provided.    Configure Jenkins  You have seen how to promote application versions manually using image tags and triggers in the templates. Next lets get a bit more sophisticated and orchestrate the same things through jenkins using the OpenShift plugin. This time we will promote the version v2 through int and prod environments.  There are jenkins jobs to promote a release from development to integration and also from integration to production. There is a job to build a new version of the v2 application and finally perform a rollback in production.  # curl -k -u admin:password -XPOST -d @jenkins-jobs/rel-v2.xml 'https://jenkins-dev.apps.lab/createItem?name=rel-v2' -H \"Content-Type: application/xml\" # curl -k -u admin:password -XPOST -d @jenkins-jobs/promote-int.xml 'https://jenkins-dev.apps.lab/createItem?name=promote-int' -H \"Content-Type: application/xml\" # curl -k -u admin:password -XPOST -d @jenkins-jobs/promote-prod.xml 'https://jenkins-dev.apps.lab/createItem?name=promote-prod' -H \"Content-Type: application/xml\" # curl -k -u admin:password -XPOST -d @jenkins-jobs/rollback-prod.xml 'https://jenkins-dev.apps.lab/createItem?name=rollback-prod' -H \"Content-Type: application/xml\" Once you create the jobs you can login to jenkins using the url with user: admin password: password.    Optional Step: make change in v2 code and start build  In order for this to work you need to fork below nodejs-ex Github repository and update the template.json with your github URL. You would then need to redeploy the dev environment using new template.   \"source\": {  \"type\": \"Git\",  \"git\": {  \"uri\": \"https://github.com/ktenzer/nodejs-ex.git\",  \"ref\": \"master\"  }, # git clone https://github.com/ktenzer/nodejs-ex # cd nodejs-ex Checkout v2 branch and make commit.  # git checkout v2 # vi index.html Hello World, welcome to Munich! Commit changes.  # git commit -a -m \"updated to munich\" # git push origin v2 Run rhel-v2 build from Jenkins    If you cloned the nodejs-ex repository then you should see your changes in v2 by clicking the URL or using curl.  Promote v2 from development to integration  Run promote-int build from Jenkins. You will see a new pod is started with v2 code next to v1 pod.    Promote v2 from integration to production  Here we will take a deeper look into what is actually happening under the hood.  Using curl you can see how the application is switched from v1 to v2.  # for i in {1..10000};do curl prod-ab.apps.lab; sleep 1;done Run promote-prod build from Jenkins.    &nbsp;  The deployment is started and a v2 pods are started next to v1 pods. At this point application service still is set to v1.    Two v2 pods are running and readiness checks are done to ensure v2 application is responding. The v1 application pods are set to scale down.    All four v2 pods are running and the v1 pods are scaling to 0. We can see that both v1 and v2 are responding to requests while we are in transition.    Only v2 pods are running and the AB deployment of v2 is complete.    Rollback  Now lets assume we aren't satisfied with v2 and want to do a rollback of v1.  Run rollback-prod build from Jenkins.    Here we observe the same thing as in our previous step, only we are switching from v2 to v1.    To rollback the integration environment to v1 run below command.  # oc tag dev/v1:latest acceptance:latest -n int Summary  In this article we have seen how OpenShift simplifies application deployments and integrates with tools such as Jenkins that enable release management. You have many options using OpenShift and we have only really begun to scratched the surface. With Jenkins you can create very complex build pipelines that allow you to not only control but also visualize your application deployment processes. We have looked at one type of common deployment, the AB deployment but there are also other deployment types such as blue-green or canary. In a future article I will take a look at additional deployment models in OpenShift. If you have feedback or experience then please share your thoughts. I hope you have found this article useful and informative.  Happy OpenShifting!  (c) 2016 Keith Tenzer  ","categories": ["OpenShift"],
        "tags": ["Cloud","DevOps","Docker","Kubernetes","OpenShift"],
        "url": "/openshift/openshift-v3-basic-release-deployment-scenarios/",
        "teaser": null
      },{
        "title": "Cloud Systems Management: Satellite 6.2 Getting Started Guide",
        "excerpt":"  Overview  In this article we will look at how to install Satellite 6.2 and configure a base environment. This article builds on a similar article I published for Satellite 6.1. In addition to installing and configuring Satellite we will also look at one of the long awaited new features remote-cmd execution.    If you are coming from Satellite 5 world then you will want to familiarize yourself with the concepts and how they apply in Satellite 6. The biggest change is around how you manage content through stages (life-cycle enviornments) but there is also a lot more.    source: https://www.windowspro.de/sites/windowspro.de/files/imagepicker/6/Red_Hat_Satellite_6_Life_Cycle.png  Features  Satellite 6.2 continues to build on that release and provides the following features:    Automated Workflows -- This includes remote execution, scheduling for remote execution jobs and expanded bootstrap and provisioning options.    Air-gapped security and federation -- Inter-Satellite sync is now available to export RPM content from one Satellite to import into another    Software Management Improvements -- Simplified Smart Variable management is now available.    Capsule improvements -- Users now have deeper insight into Capsule health and overall performance; Capsules are lighter-weight and can be configured to store only the content that has been requested by its clients; and a new Reference Architecture including deploying a Highly Available Satellite Capsule is now available.    Atomic OSTree and containers -- Mirror, provision and manage RHEL Atomic hosts and content with Satellite; mirror container repositories such as Red Hat Registry, DockerHub™ and other 3rd-party sources; and Satellite provides a secure, curated point of entry for container content    Enhanced documentation -- Both new and improved documentation is available. (https://access.redhat.com/documentation/red-hat-satellite/)    New Guides    Virtual Instance Guide (How to configure virt-who)    Hammer CLI Guide (How to use Satellite’s CLI)    Content Management Guide (How to easily manage Satellite’s content )    Quickstart Guide (How to get up and running quickly)           Improved/more user-friendly documentation       User Guide split to make more topical and easier to follow:    Server Administration Guide    Host Configuration Guide    “Cheat Sheets” available for specific topics (Hammer)    Updated Feature Overviews      Prerequisites  In order to install Satellite we need a subscription and of course RHEL 6 or 7.  subscription-manager register subscription-manager list --available subscription-manager attach --pool=934893843989289 subscription-manager repos --disable \"*\" RHEL 6  subscription-manager repos --enable=rhel-6-server-rpms \\ --enable=rhel-server-rhscl-6-rpms \\ --enable=rhel-6-server-satellite-6.2-rpms RHEL 7  subscription-manager repos --enable=rhel-7-server-rpms \\ --enable=rhel-server-rhscl-7-rpms \\ --enable=rhel-7-server-satellite-6.2-rpms Update all packages.  # yum update -y Add Firewall rules.  RHEL 6  # iptables -A INPUT -m state --state NEW -p udp --dport 53 -j ACCEPT \\ &amp;&amp; iptables -A INPUT -m state --state NEW -p tcp --dport 53 -j ACCEPT \\ &amp;&amp; iptables -A INPUT -m state --state NEW -p udp --dport 67 -j ACCEPT \\ &amp;&amp; iptables -A INPUT -m state --state NEW -p udp --dport 69 -j ACCEPT \\ &amp;&amp; iptables -A INPUT -m state --state NEW -p tcp --dport 80 -j ACCEPT \\ &amp;&amp; iptables -A INPUT -m state --state NEW -p tcp --dport 443 -j ACCEPT \\ &amp;&amp; iptables -A INPUT -m state --state NEW -p tcp --dport 5647 -j ACCEPT \\ &amp;&amp; iptables -A INPUT -m state --state NEW -p tcp --dport 8140 -j ACCEPT \\ &amp;&amp; iptables-save &gt; /etc/sysconfig/iptables RHEL 7  # firewall-cmd --add-service=RH-Satellite-6 # firewall-cmd --permanent --add-service=RH-Satellite-6 NTP  # yum install chrony  # systemctl start chronyd  # systemctl enable chronyd SOS  Setting up SOS is a good idea to get faster responses from Red Hat support.  #yum install -y sos DNS Configuration  This is only required if you want to setup an external DNS server. If you use the integrated DNS provided by Satellite you can skip this step.  [root@ipa ]# vi /etc/named.conf  //  // named.conf  //  // Provided by Red Hat bind package to configure the ISC BIND named(8) DNS  // server as a caching only nameserver (as a localhost DNS resolver only).  //  // See /usr/share/doc/bind*/sample/ for example named configuration files.  //  include \"/etc/rndc.key\";  controls {  inet 192.168.0.26 port 953 allow { 192.168.38.27; 192.168.38.26; } keys { \"capsule\"; };  };  options {  listen-on port 53 { 192.168.0.26; };  listen-on-v6 port 53 { ::1; };  directory \"/var/named\";  dump-file \"/var/named/data/cache_dump.db\";  statistics-file \"/var/named/data/named_stats.txt\";  memstatistics-file \"/var/named/data/named_mem_stats.txt\";  //allow-query { localhost; };  //forwarders {  //8.8.8.8;  //};  forwarders { 8.8.8.8; 8.8.4.4; };  /*  - If you are building an AUTHORITATIVE DNS server, do NOT enable recursion.  - If you are building a RECURSIVE (caching) DNS server, you need to enable  recursion.  - If your recursive DNS server has a public IP address, you MUST enable access  control to limit queries to your legitimate users. Failing to do so will  cause your server to become part of large scale DNS amplification  attacks. Implementing BCP38 within your network would greatly  reduce such attack surface  */  recursion yes;  dnssec-enable yes;  dnssec-validation yes;  /* Path to ISC DLV key */  bindkeys-file \"/etc/named.iscdlv.key\";  managed-keys-directory \"/var/named/dynamic\";  pid-file \"/run/named/named.pid\";  session-keyfile \"/run/named/session.key\";  };  logging {  channel default_debug {  file \"data/named.run\";  severity dynamic;  };  };  zone \"0.168.192.in-addr.arpa\" IN {  type master;  file \"/var/named/dynamic/0.168.192-rev\";  //allow-update { 192.168.0.27; };  update-policy {  grant capsule zonesub ANY;  };  };  zone \"lab\" IN {  type master;  file \"/var/named/dynamic/lab.zone\";  //allow-update { 192.168.0.27; };  update-policy {  grant capsule zonesub ANY;  };  };  include \"/etc/named.rfc1912.zones\";  include \"/etc/named.root.key\"; DNS ZONES  [root@ipa ]# cat /var/named/dynamic/  0.168.192-rev 0.168.192-rev.old lab.zone lab.zone.jnl managed-keys.bind testdns.sh  [root@ipa dynamic]# cat /var/named/dynamic/lab.zone  $ORIGIN lab.  $TTL 86400  @ IN SOA dns1.lab. hostmaster.lab. (  2001062501 ; serial  21600 ; refresh after 6 hours  3600 ; retry after 1 hour  604800 ; expire after 1 week  86400 ) ; minimum TTL of 1 day  ;  ;  IN NS dns1.lab.  rhevm IN A 192.168.0.20  rhevh01 IN A 192.168.0.21  osp8 IN A 192.168.0.22  cf IN A 192.168.0.24  ipa IN A 192.168.0.26  sat6 IN A 192.168.0.27  IN AAAA aaaa:bbbb::1  ose-master IN A 192.168.0.25  * 300 IN A 192.168.0.25  ;  ; [root@ipa ]# cat /var/named/dynamic/0.168.192-rev  $TTL 86400 ; 24 hours, could have been written as 24h or 1d  $ORIGIN 0.168.192.IN-ADDR.ARPA.  @ IN SOA dns1.lab. hostmaster.lab. (  2001062501 ; serial  21600 ; refresh after 6 hours  3600 ; retry after 1 hour  604800 ; expire after 1 week  86400 ) ; minimum TTL of 1 day  ; Name servers for the zone - both out-of-zone - no A RRs required  IN NS dns1.lab.  ; server host definitions  20 IN PTR rhevm.lab.  21 IN PTR rhevh01.lab.  22 IN PTR osp8.lab.  24 IN PTR cf.lab.  25 IN PTR ose3-master.lab.  26 IN PTR ipa.lab.  27 IN PTR sat6.lab. Installation  Before we start with manual install, a Solution Architect and colleague of mine, Sebastian Hetze, created an automated setup script that also can integrate IDM. I strongly recommend using this script and contributing to further enhancements.  https://github.com/shetze/hammer-scripts/blob/master/sat62-setup.sh  If you are of course doing this to learn then it is definitely good to walk through manual steps so you understand concepts and what is involved.  #yum install -y satellite With Integrated DNS # satellite-installer --scenario satellite --foreman-admin-username admin --foreman-admin-password redhat01 --foreman-proxy-dns true --foreman-proxy-dns-interface eth0 --foreman-proxy-dns-zone example.com --foreman-proxy-dns-forwarders 8.8.8.8 --foreman-proxy-dns-reverse 0.168.192.in-addr.arpa --foreman-proxy-dhcp true --foreman-proxy-dhcp-interface eth0 --foreman-proxy-dhcp-range \"192.168.0.100 192.168.0.199\" --foreman-proxy-dhcp-gateway 192.168.0.1 --foreman-proxy-dhcp-nameservers 192.168.0.27 --foreman-proxy-tftp true --foreman-proxy-tftp-servername $(hostname) --capsule-puppet true --foreman-proxy-puppetca true Without Integrated DNS # satellite-installer --scenario satellite --foreman-admin-username admin --foreman-admin-password redhat01 --foreman-proxy-dns true --foreman-proxy-dns-interface eth0 --foreman-proxy-dns-zone sat.lab --foreman-proxy-dns-forwarders 192.168.0.26 --foreman-proxy-dns-reverse 0.168.192.in-addr.arpa --foreman-proxy-dhcp true --foreman-proxy-dhcp-interface eth0 --foreman-proxy-dhcp-range \"192.168.0.225 192.168.0.250\" --foreman-proxy-dhcp-gateway 192.168.0.1 --foreman-proxy-dhcp-nameservers 192.168.0.27 --foreman-proxy-tftp true --foreman-proxy-tftp-servername $(hostname) --capsule-puppet true --foreman-proxy-puppetca true Configuration  At this point you should be able to reach the web UI using HTTPS. In this environment the url is https://sat6.lab.com. Next we need to setup the hammer CLI. Configure hammer so that we automatically pass authentication credentials.  mkdir ~/.hammer cat &gt; .hammer/cli_config.yml &lt;&lt;EOF :foreman:     :host: 'https://sat6.lab/'     :username: 'admin'     :password: 'redhat01'  EOF External DNS Configuration  If you setup external DNS then you need to allow Satellite server to update DNS records on your external DNS server.  # vi /etc/foreman-proxy/settings.d/dns.yml --- :enabled: true :dns_provider: nsupdate :dns_key: /etc/rndc.key :dns_server: 192.168.38.2 :dns_ttl: 86400 # systemctl restart foreman-proxy Register Satellite Server in Red Hat Network (RHN).    Assign subscriptions to the Satellite server and download manifest from RHN.    Note: In the next section we will be using the hammer CLI to configure Satellite. In this environment we are using the organization \"Default Organization\", you would probably change this to a more specific organization name. If so you need to first create a new organization.  Upload manifest file to Satellite server.  #hammer subscription upload --organization \"Default Organization\" --file /root/manifest_d586388b-f556-4623-b6a0-9f76857bedbc.zip Create a subnet in Satellite 6 under Infrastructure-&gt;Subnet. In this environment the subnet is 192.168.122.0/24 and we are using external D.    Enable basic repositories.  At minimum you need RHEL Server, Satellite Tools and RH Common.  #hammer repository-set enable --organization \"Default Organization\" --product 'Red Hat Enterprise Linux Server' --basearch='x86_64' --releasever='7Server' --name 'Red Hat Enterprise Linux 7 Server (RPMs)' #hammer repository-set enable --organization \"Default Organization\" --product 'Red Hat Enterprise Linux Server' --basearch='x86_64' --releasever='7Server' --name 'Red Hat Enterprise Linux 7 Server (Kickstart)' #hammer repository-set enable --organization \"Default Organization\" --product 'Red Hat Enterprise Linux Server' --basearch='x86_64' --name 'Red Hat Satellite Tools 6.2 (for RHEL 7 Server) (RPMs)'  #hammer repository-set enable --organization \"Default Organization\" --product 'Red Hat Enterprise Linux Server' --basearch='x86_64' --name 'Red Hat Enterprise Linux 7 Server - RH Common RPMs x86_64 7Server' Enable EPEL repository for 3rd party packages.  #wget -q https://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-7  -O /root/RPM-GPG-KEY-EPEL-7 #hammer gpg create --key /root/RPM-GPG-KEY-EPEL-7  --name 'GPG-EPEL-7' --organization \"Default Organization\" Create a new product for the EPEL repository. In Satellite 6 products are a groupings of external content outside of RHN. Products can contain RPM repositories, Puppet modules or container images.  #hammer product create --name='EPEL 3rd Party Packages' --organization \"Default Organization\" --description 'EPEL 3rd Party Packages' #hammer repository create --name='EPEL 7 - x86_64' --organization \"Default Organization\" --product='EPEL 3rd Party Packages' --content-type='yum' --publish-via-http=true --url=http://dl.fedoraproject.org/pub/epel/7/x86_64/ --checksum-type=sha256 --gpg-key=GPG-EPEL-7  Synchronize the repositories. This will take a while as all of the RPM packages will be downloaded. Note: you can also use the --async option to run tasks in parallel.  #hammer repository synchronize --async --organization \"Default Organization\" --product 'Red Hat Enterprise Linux Server'  --name 'Red Hat Enterprise Linux 7 Server Kickstart x86_64 7Server' #hammer repository synchronize --async --organization \"Default Organization\" --product 'Red Hat Enterprise Linux Server'  --name 'Red Hat Satellite Tools 6.2 for RHEL 7 Server RPMs x86_64' #hammer repository synchronize --async --organization \"Default Organization\" --product 'Red Hat Enterprise Linux Server'  --name 'Red Hat Enterprise Linux 7 Server RPMs x86_64 7Server'  #hammer repository synchronize --async --organization \"Default Organization\" --product 'Red Hat Enterprise Linux Server'  --name 'Red Hat Enterprise Linux 7 Server - RH Common RPMs x86_64 7Server'  #hammer repository synchronize --async --organization \"$ORG\" --product 'EPEL 3rd Party Packages  --name  'EPEL 7 - x86_64' Create life cycles for development and production.  #hammer lifecycle-environment create --organization \"Default Organization\" --description 'Development' --name 'DEV' --label development --prior Library #hammer lifecycle-environment create --organization \"Default Organization\" --description 'Production' --name 'PROD' --label production --prior 'DEV' Create content view for RHEL 7 base.  #hammer content-view create --organization \"Default Organization\" --name 'RHEL7_base' --label rhel7_base --description 'Core Build for RHEL 7'  #hammer content-view add-repository --organization \"Default Organization\" --name 'RHEL7_base' --product 'Red Hat Enterprise Linux Server' --repository 'Red Hat Enterprise Linux 7 Server RPMs x86_64 7Server' #hammer content-view add-repository --organization \"Default Organization\" --name 'RHEL7_base' --product 'Red Hat Enterprise Linux Server' --repository 'Red Hat Satellite Tools 6.2 for RHEL 7 Server RPMs x86_64'  #hammer content-view add-repository --organization \"Default Organization\" --name 'RHEL7_base' --product 'Red Hat Enterprise Linux Server' --repository 'Red Hat Enterprise Linux 7 Server - RH Common RPMs x86_64 7Server'  #hammer content-view add-repository --organization \"Default Organization\" --name 'RHEL7_base' --product 'EPEL 3rd Party Packages'  --repository  'EPEL 7 - x86_64' Publish and promote content view to the environments.  #hammer content-view publish --organization \"Default Organization\" --name RHEL7_base --description 'Initial Publishing' #hammer content-view version promote --organization \"Default Organization\" --content-view RHEL7_base --to-lifecycle-environment DEV #hammer content-view version promote --organization \"Default Organization\" --content-view RHEL7_base --to-lifecycle-environment PROD Add activation keys for both stage environments.  #hammer activation-key create --organization \"Default Organization\" --description 'RHEL7 Key for DEV' --content-view 'RHEL7_base' --unlimited-hosts --name ak-Reg_To_DEV --lifecycle-environment 'DEV' #hammer activation-key create --organization \"Default Organization\" --description 'RHEL7 Key for PROD' --content-view 'RHEL7_base' --unlimited-hosts --name ak-Reg_To_PROD --lifecycle-environment 'PROD' Add subscriptions to activation keys.    Get medium id needed for hostgroup creation.  #hammer medium list ---|----------------------------------------------------------------------------------|--------------------------------------------------------------------------------- ID | NAME | PATH  ---|----------------------------------------------------------------------------------|--------------------------------------------------------------------------------- 1 | CentOS mirror | http://mirror.centos.org/centos/$version/os/$arch  8 | CoreOS mirror | http://$release.release.core-os.net  2 | Debian mirror | http://ftp.debian.org/debian  9 | Default_Organization/Library/Red_Hat_Server/Red_Hat_Enterprise_Linux_7_Server... | http://sat6.lab/pulp/repos/Default_Organization/Library/content/dist/rhel/ser... 4 | Fedora Atomic mirror | http://dl.fedoraproject.org/pub/alt/atomic/stable/Cloud_Atomic/$arch/os/  3 | Fedora mirror | http://dl.fedoraproject.org/pub/fedora/linux/releases/$major/Server/$arch/os/  5 | FreeBSD mirror | http://ftp.freebsd.org/pub/FreeBSD/releases/$arch/$version-RELEASE/  6 | OpenSUSE mirror | http://download.opensuse.org/distribution/$version/repo/oss  7 | Ubuntu mirror | http://archive.ubuntu.com/ubuntu  ---|----------------------------------------------------------------------------------|--------------------------------------------------------------------------------- Create a host group. A host group is a foreman construct and is used for automation of provisioning parameters. A host is provisioned based on its host group. The host group contains kickstart/provisioning templates, OS information, network information, activation keys, parameters, puppet environment and if virtual a compute profile. Note: you will need to change the hostname sat6.lab.com.  #hammer hostgroup create --architecture x86_64 --content-source-id 1 --content-view RHEL7_base --domain lab --lifecycle-environment DEV --locations 'Default Location' --name RHEL7_DEV_Servers --organizations \"Default Organization\" --puppet-ca-proxy sat6.lab --puppet-proxy sat6.lab --subnet VLAN_0 --partition-table 'Kickstart default' --operatingsystem 'RedHat 7.2' --medium-id 9 Add compute resource for RHEV.  # hammer compute-resource create --provider Ovirt --name RHEV --url https://rhevm.lab/api --organizations \"Default Organization\" --locations 'Default Location' --user admin@internal --password redhat01 Satellite 6 Bootstrapping  Satellite bootstrapping is the process for taking an already provisioned system and attaching it to the Satellite server. The minimum process is outlined below:  Install Katello package from Satellite server.  #rpm -Uvh http://sat6.lab.com/pub/katello-ca-consumer-latest.noarch.rpm  Subscribe using activation key.  #subscription-manager register --org=\"Default_Organization\" --activationkey=\"DEV_CORE\" Enable Satellite tools repository and install katello agent.  #yum -y install --enablerepo rhel-7-server-satellite-tools-6.2-rpms katello-agent In addition to get full functionality you would also need to install and configure Puppet. Many customers are also looking for a solution that would gracefully move a system attached to Satellite 5 into Satellite 6. As such there is a bootstrapping script I can recommend from Evgeni Golov (one of our top Satellite Consultants @Red Hat):  https://github.com/buildout/buildout/blob/master/bootstrap/bootstrap.py  Remote Command Execution  A new feature many Satellite 5 customers have been waiting for is remote-cmd execution. This feature allows you to run and schedule commands on clients connected to Satellite 6 server. You can think of this as poor-man's Ansible.  Ensure remote-cmd execution is configured for Satellite capsule.    Note: If you aren't using provisioning template \"Satellite Kickstart Default\" and you upgraded from Satellite 6.1, you will need to re-clone the \"Satellite Kickstart Default\" template and apply your changes. A snippet was added to \"Satellite Kickstart Default\" in order to automatically configure foreman-proxy ssh keys.    For systems that are already provisioned you need to copy foreman-proxy ssh key.  # ssh-copy-id -i /usr/share/foreman-proxy/.ssh/id_rsa_foreman_proxy.pub root@192.168.122.210 Run \"ls\" command for a given client using remote-cmd execution.  # hammer job-invocation create --async --inputs \"command=ls -l /root\" --search-query name=client1.lab.com --job-template \"Run Command - SSH Default\" Run command using input file or script.  # hammer job-invocation create --async --input-files command=/root/script.sh --search-query name=client1.lab.com --job-template \"Run Command - SSH Default\" Run command on multiple hosts.  # hammer job-invocation create --async --inputs \"command=ls -l /root\" --search-query \"name ~ client1.lab.com|client2.lab.com\" --job-template \"Run Command - SSH Default\" List jobs that are running, completed or failed.  # hammer job-invocation list ---|---------------------|-----------|---------|--------|---------|-------|------------------------ ID | DESCRIPTION | STATUS | SUCCESS | FAILED | PENDING | TOTAL | START  ---|---------------------|-----------|---------|--------|---------|-------|------------------------ 4 | Run ls -l /root | succeeded | 1 | 0 | 0 | 1 | 2016-08-23 11:10:50 UTC 3 | Run puppet agent -t | succeeded | 1 | 0 | 0 | 1 | 2016-08-23 10:43:20 UTC 2 | Run puppet agent -t | failed | 0 | 1 | 0 | 1 | 2016-08-23 10:37:18 UTC 1 | Run puppet agent -t | failed | 0 | 1 | 0 | 1 | 2016-08-23 10:19:41 UTC ---|---------------------|-----------|---------|--------|---------|-------|------------------------ Show details of a completed job.  # hammer job-invocation info --id 3 ID: 4 Description: Run ls -l /root Status: succeeded Success: 2 Failed: 0 Pending: 0 Total: 2 Start: 2016-08-23 11:34:05 UTC Job Category: Commands Mode:  Cron line:  Recurring logic ID:  Hosts:   - client1.lab.com  - client2.lab.com Show output from a completed command for a given host. Satellite will show stdout and return code from command.  # hammer job-invocation output --host client1.lab.com --id 4 total 36 -rw-------. 1 root root 4256 Aug 23 10:34 anaconda-ks.cfg -rw-r--r--. 1 root root 21054 Aug 23 10:34 install.post.log -rw-r--r--. 1 root root 54 Aug 23 10:33 install.postnochroot.log Exit status: 0 Summary  In this article we learned how to deploy Satellite 6.2 environment. We looked at some different options regarding DNS. Provided a guideline for getting a basic Satellite 6.2 environment up and running. Finally we looked a bit more into the new feature remote-cmd execution. I hope you found this article useful. If you have anything to share or otherwise feedback please don't be shy.  Happy Satelliting!  (c) 2016 Keith Tenzer  &nbsp;  &nbsp;  &nbsp;  ","categories": ["Satellite"],
        "tags": ["Foreman","Katello","Puppet","Satellite 6"],
        "url": "/satellite/cloud-systems-management-satellite-6-2-getting-started-guide/",
        "teaser": null
      },{
        "title": "OpenStack Mitaka Lab Installation and Configuration Guide",
        "excerpt":"  Overview  In this article we will focus on installing and configuring OpenStack Mitaka using RDO and the packstack installer. RDO is a community platform around Red Hat's OpenStack Platform. It allows you to test the latest OpenStack capabilities on a stable platform such as Red Hat Enterprise Linux (RHEL) or CentOS. This guide will take you through installing the OpenStack Liberty release, configuring networking, security groups, flavors, images and are other OpenStack related services. The outcome is a working OpenStack environment based on the Mitaka release that you can use as a baseline for testing your applications with OpenStack capabilities.    Install and Configure OpenStack Liberty   Install RHEL or CentOS 7.1. Ensure name resolution is working.  # vi /etc/hosts 192.168.122.80 osp9.lab osp9  Set hostname.  # hostnamectl set-hostname osp9.lab  Disable firewalld since this is for a lab environment.  # systemctl disable firewalld # systemctl stop firewalld  Disable NetworkManager, it is still not recommended for Liberty (at least RDO).  # systemctl stop NetworkManager # systemctl disable NetworkManager  For RHEL systems register with subscription manager.  # subscription-manager register # subscription-manager subscribe --auto # subscription-manager repos --disable=* # subscription-manager repos --enable=rhel-7-server-rpms # subscription-manager repos --enable=rhel-7-server-extras-rpms  # subscription-manager repos --enable=rhel-7-server-rh-common-rpms # subscription-manager repos --enable=rhel-7-server-openstack-9-rpms  Install yum-utils and update the system.  # yum install -y yum-utils # yum update -y  Reboot.  # systemctl reboot  Install packstack packages.  # yum install -y openstack-packstack You can install packstack by providing command-line options or using the answers file.  Option 1: Install using command-line options   # packstack --allinone --os-neutron-ovs-bridge-mappings=extnet:br-ex \\  --os-neutron-ovs-bridge-interfaces=br-ex:eth0 \\  --os-neutron-ml2-type-drivers=vxlan,flat \\  --os-heat-install=y --os-heat-cfn-install=y \\  --os-sahara-install=y --os-trove-install=y \\  --os-neutron-lbaas-install=y \\  --keystone-admin-passwd=redhat01 Option 2: Install using answers file   Create packstack answers file for customizing the installer.  # packstack --gen-answer-file /root/answers.txt  Update the packstack answers file and enable other OpenStack services. Note: as of the writing of this guide SSL is not working in combination with Horizon so don't enable SSL.  # vi /root/answers.txt  CONFIG_KEYSTONE_ADMIN_PW=redhat  CONFIG_PROVISION_DEMO=n  CONFIG_HORIZON_SSL=y  CONFIG_HEAT_INSTALL=y  CONFIG_HEAT_CFN_INSTALL=y  CONFIG_HEAT_CLOUDWATCH_INSTALL=y  CONFIG_SAHARA_INSTALL=y  CONFIG_CEILOMETER_INSTALL=y  CONFIG_LBAAS_INSTALL=y  Install OpenStack Liberty using packstack.  # packstack --answer-file /root/answers.txt  Source the keystone admin profile.  # . /root/keystonerc_admin  Check status of openstack services.  # openstack-status  Backup the ifcfg-etho script.  # cp /etc/sysconfig/network-scripts/ifcfg-eth0 /root/  Configure external bridge for floating ip networks.  # vi /etc/sysconfig/network-scripts/ifcfg-eth0  DEVICE=eth0  ONBOOT=yes  TYPE=OVSPort  DEVICETYPE=ovs  OVS_BRIDGE=br-ex # vi /etc/sysconfig/network-scripts/ifcfg-br-ex  DEVICE=br-ex  BOOTPROTO=static  ONBOOT=yes  TYPE=OVSBridge  DEVICETYPE=ovs  USERCTL=yes  PEERDNS=yes  IPV6INIT=no  IPADDR=&lt;www.xxx.yyy.zzz&gt;  NETMASK=255.255.255.0  GATEWAY=&lt;GW IP&gt;  DNS1=&lt;DNS IP&gt;  Add the eht0 physical interface to the br-ex bridge in openVswitch for floating IP networks.  # ovs-vsctl add-port br-ex eth0 ; systemctl restart network.service Configure OpenStack   Create private network.  # neutron net-create private # neutron subnet-create private 10.10.1.0/24 --name private_subnet --allocation-pool start=10.10.1.100,end=10.10.1.200  Create public network. Note: these steps assume the physical network connected to eth0 is 192.168.122.0/24.  # neutron net-create public --router:external # neutron subnet-create public 192.168.122.0/24 --name public_subnet --allocation-pool start=192.168.122.100,end=192.168.122.200 --disable-dhcp --gateway 192.168.122.1  Add a new router and configure router interfaces.  # neutron router-create router1 --ha False # neutron router-gateway-set router1 public # neutron router-interface-add router1 private_subnet  Upload a glance image. In this case we will use a Cirros image because it is small and thus good for testing OpenStack.  # yum install -y wget # wget http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img # glance image-create --name \"Cirros 0.3.4\" --disk-format qcow2 --container-format bare --visibility public --file /root/cirros-0.3.4-x86_64-disk.img  Create a new m1.nano flavor for running Cirros image.  # nova flavor-create m1.nano 42 64 0 1  Create security group and allow all TCP ports.  # nova secgroup-create all \"Allow all tcp ports\" # nova secgroup-add-rule all TCP 1 65535 0.0.0.0/0  Create security group for base access  # nova secgroup-create base \"Allow Base Access\" # nova secgroup-add-rule base TCP 22 22 0.0.0.0/0 # nova secgroup-add-rule base TCP 80 80 0.0.0.0/0 # nova secgroup-add-rule base ICMP -1 -1 0.0.0.0/0  Create a private ssh key for connecting to instances remotely.  # nova keypair-add admin  Create admin.pem file and add private key from output of keypair-add command.  # vi /root/admin.pem # chmod 400 /root/admin.pem  List the network IDs.  # neutron net-list  +--------------------------------------+---------+-------------------------------------------------------+  | id | name | subnets |  +--------------------------------------+---------+-------------------------------------------------------+  | d4f3ed19-8be4-4d56-9f95-cfbac9fdf670 | private | 92d82f53-6e0b-4eef-b8b9-cae32cf40457 10.10.1.0/24     |  | 37c024d6-8108-468c-bc25-1748db7f5e8f | public  | 22f2e901-186f-4041-ad93-f7b5ccc30a81 192.168.122.0/24 |  Start an instance and make sure to replace network id from above command.  # nova boot --flavor m1.nano --image \"Cirros 0.3.4\" --nic net-id=92d82f53-6e0b-4eef-b8b9-cae32cf40457 --key-name admin --security-groups all mycirros  Create a floating IP and assign it to the mycirros instance.  # nova floating-ip-create # nova floating-ip-associate mycirros &lt;FLOATING IP&gt;  Connect to mycirros instance using the private ssh key stored in the admin.pem file. Note: The first floating IP in the range 192.168.122.201.  # ssh -i admin.pem cirros@192.168.122.201 Nova Nested Virtualization  Most OpenStack Lab or test environments will install OpenStack on a hypervisor platform inside virtual machines. I would strongly recommend KVM. If you are running OpenStack on KVM (Nova nested virtualization) make sure to follow these tips and tricks to get the best performance.  Summary  This article was intended as a hands on guide for standing up an OpenStack Mitaka lab environment using RDO. As mentioned RDO is a stable community platform built around Red Hat's OpenStack Platform. It provides the ability to test the latest OpenStack features against either an enterprise platform (RHEL) or community platform (CentOS). Hopefully you found the information in this article useful. If you have anything to add or feedback, feel free to leave your comments.  Happy OpenStacking!  (c) 2016 Keith Tenzer  ","categories": ["OpenStack"],
        "tags": ["Cloud","Linux","mitaka","OpenStack","RDO"],
        "url": "/openstack/openstack-mitaka-lab-installation-and-configuration-guide/",
        "teaser": null
      },{
        "title": "Ceph: the future of Storage",
        "excerpt":"  Overview  Since joining Red Hat in 2015, I have intentionally stayed away from the topic of storage. My background is storage but I wanted to do something else as storage became completely mundane and frankly boring. Why?  Storage hasn't changed much in 20 years. I started my career as a Linux and Storage engineer in 2000 and everything that existed then, exists today. Only things became bigger, faster, cheaper, due to incremental improvements from technologies such as flash. There comes a point however, where minor incremental improvements are no longer good enough and a completely new way of addressing challenges is the only way forward.  I realized in late 2015 that the storage industry is starting a challenging period for all vendors but, didn't really have feeling for when that could lead to real change. I did know that the monolithic storage array, built on proprietary Linux/Unix, with proprietary x86 hardware we all know and love, was a thing of the past. If you think about it storage is a scam today, you get opensource software running on x86 hardware packaged as a proprietary solution that doesn't interoperate with anything else. So you get none of the value of opensource and pay extra for it. I like to think that economics like gravity, eventually always wins.    Challenges  There are many challenges facing our storage industry but I will focus on three: cost, scale and agility.  Cost  Linux has become a great equalizer in storage and fueled storage startups over the past 3-5 years, allowing them to quickly build innovative products that challenge traditional vendors. Storage companies not building on Linux, are forced to be operating system companies in addition to storage companies, a distinct disadvantage. Their R&amp;D costs to maintain proprietary storage operating systems reduce overall value, slow innovation and in the end, increase costs. In addition most storage platforms today have high vendor lock-in, offering little choice. You usually can't choose hardware and therefore you are often paying a premium for standard x86. Disks are a great example, typically they cost twice what you would pay through Amazon.  \"Oh but our disks are rigorously tested and have a mean time between failure of xxxxxxx so the cost is justified.\"  No it isn't! We need a new way of thinking, what about if disks failures weren't a big deal, didn't cause impact and storage system automatically adjusted?  While these points may be interesting, at the end of the day, cost comes down to one thing. Everyone is measured against Amazon S3. You are either cheaper than S3 (3 cents per GB per Month) or you have some explaining to do. If we consider small or medium environments, it may be doable with hyper-converged storage, but as soon as scale and complexity come into play (multiple use-cases with block, file and object) those costs explode. If we are talking about large environments, forget it. Even if the cost could get down to S3 range, things wouldn't scale across the many use-cases in a large enterprise.  Scale  Scalability is a very complex problem to solve. Sure everything scales until it doesn't but jokes aside, we have reached a point where scalability needs to be applied more generally, especially in storage. Storage arrays today are use-case driven. Each customer has many use-cases and the larger the customer the more use-cases. This means many types of dissimilar storage systems. While a single storage system may scale to some degree, many storage systems together don't. Customers are typically stuck in a 3-year cycle of forklift upgrades, because storage systems don't truly scale-out they only scale-up.  Agility  The result of many storage appliances and arrays is technology sprawl, which in turn creates technology debt. Storage vendors today don't even have a vision for data management within their own ecosystem, no less interoperating with other vendors. Everything is completely fragmented to point where a few use-cases equals a new storage system.  Storage systems today require a fairly low entry cost for management overhead, but as the environment grows and scales those costs increase, reducing agility. They don't stay consistent, how could they when a unified data management strategy is missing.  Storage systems are designed to prevent failure at all costs, they certainly don't anticipate failure. At scale of course, we have more failures, this in turn correlates to more time spent keeping the lights on. The goal of every organization is to reduce that and maximize time spent on innovation. Automation suffers and as such, it becomes increasingly harder to build blueprints around storage. There is just too much variation.  I could certainly go on and there are other challenges to discuss, but I think you get the picture. We have reached the dead-end of storage.  Why Software-defined?  As mentioned, the main problem I see today is for every use-case, there is a storage array or appliance. The startups are all one-trick ponies solving only a few use cases. Traditional storage vendors on the other hand throw different storage systems at each use-case, cobble them together in some UI and call that a solution. You end up with no real data management strategy and that it is truly what is needed. I laugh when I hear vendors talking about data lakes. If you buy into storage array mindset, you end up at same place, a completely unmanageable environment at scale, where operational costs are not linear but constantly going up. Welcome to most storage environments today.  As complexity increases you reach a logical point where abstractions are needed. Today storage not only needs to provide file, block and object but also needs to interoperate with large ecosystem of vendors, cloud providers and applications. Decoupling the data management software from the hardware is the logical next step. This is the same thing we have already observed with server virtualization and are observing in networking with NFV. The economics of cost and advantages of decoupling hardware and software simply make sense. Organizations have been burned over and over, making technology decisions that later are replaced or reverted because newer better technologies become available in other platforms. Software-defined storage allows an easy introduction of new technologies, without having to purchase new storage system because your old storage was designed before that technology was invented. Finally storage migrations. Aren't we tired of always migrating data when changing storage systems? A common data management platform using common x86 hardware and Linux could do away with storage migrations forever.  Why Ceph?  Ceph has become the defacto standard for software-defined storage. Currently the storage industry is at beginning of major disruption period where software-defined storage will drive out traditional proprietary storage systems. Ceph is of course opensource, which enables a rich ecosystem of vendors that provide storage systems based on Ceph. The software-defined world is not possible without opensource and doing things the opensource way.  Ceph delivers exactly what is needed to disrupt the storage industry. Ceph provides a unified scale-out storage system based on common x86 hardware, is self healing and not only anticipates failures but expects them. Ceph does away with storage migrations and since hardware is decoupled, gives you choice of when to deploy new hardware technologies.  Since Ceph can be purchased separately from hardware, you have choice, not only whom you buy Ceph from (Red Hat, Suse, Mirantis, Unbuntu, etc) but also, whom you purchase hardware from (HP, DELL, Fujitsu, IBM, etc). In addition you can even buy ceph together with hardware for an integrated appliance (SanDisk, Fujitsu, etc). You have choice and are free from vendor lock-in.  Ceph is extremely cost efficient. Even the most expensive all-flash, integrated solutions are less than S3 (3 cents per GB per Month). If you really want to go cheap, you can purchase off-the-shelf commodity hardware from companies like SuperMicro and still get enterprise Ceph from Red Hat, Suse, Ubuntu, etc while being a lot cheaper than S3.  Ceph scales. One example I will give is Cern 30 PB test. Ceph can be configured to optimize different workloads such as block, file and object. You can create storage pools and decide to co-locate journals, or put journals on SSDs for optimal performance. Ceph allows you to tune your storage to specific use-cases, while maintaining unified approach. In tech-preview is a new feature called bluestore. This allows Ceph to completely bypass file-system layer and store data directly on raw devices. This will greatly increase performance and there is a ton of optimizations planned after that, this is just the beginning!  Ceph enables agility. Providing unified storage system that supports all three storage types: file, block and object. Ceph provides a unified storage management layer for anything you can present as a disk device. Finally Ceph simplifies management, it is the same level of effort to manage a 10 node Ceph cluster or a 100 node Ceph cluster, running costs remain linear.  Below is a diagram showing how Ceph addresses file, block and object storage using a unified architecture built around RADOS.    source: http://docs.ceph.com/docs/hammer/architecture  Summary  In this article we spent time discussing current storage challenges and the value of not just software-defined storage but also Ceph. We can't keep doing what we have been doing for last 20+ years in storage industry. The economics of scale have brought down barriers and paved the way for a software-defined world. Storage is only the next logical boundry. Ceph being an OpenSource project is already the defacto software-defined standard and is in position to become the key beneficiary as software-defined storage becomes more mainstream. If you are interested in Ceph I will be producing some how-to guides soon, so stay tuned. Please feel free to debate whether you agree or disagree with my views. I welcome all feedback.  (c) 2016 Keith Tenzer  ","categories": ["Storage"],
        "tags": ["Ceph","Software-defined-storage"],
        "url": "/storage/ceph-the-future-of-storage/",
        "teaser": null
      },{
        "title": "Ceph 1.3 Lab Installation and Configuration Guide",
        "excerpt":"  Overview  In this article we will setup a Ceph 1.3 cluster for purpose of learning or a lab environment.  &nbsp;  Ceph Lab Environment  For this environment you will need three VMs (ceph1, ceph2 and ceph3). Each should have 20GB root disk and 100GB data disk. Ceph has three main components: Admin console, Monitors and OSDs.  Admin console - UI and CLI used for managing Ceph cluster. In this environment we will install on ceph1.  Monitors - Monitor health of Ceph cluster. One or more monitors forms a paxos part-time parliment, providing extreme reliability and durability of cluster membership. Monitors maintain the various maps: monitor, osd, placement group (pg) and crush. Monitors will be installed on ceph1, ceph2 and ceph3.  OSDs - Object storage daemon handles storing data, recovery, backfilling, rebalancing and replication. OSDs sit on top of a disk / filesystem. Bluestore enables OSDs to bypass filesystem but is not an option in Ceph 1.3. An OSD will be installed on ceph1, ceph2 and ceph3.    On All Cephs nodes.  #subscription-manager repos --disable=* #subscription-manager repos --enable=rhel-7-server-rpms #subscription-manager repos --enable=rhel-7-server-rpms --enable=rhel-7-server-rhceph-1.3-calamari-rpms --enable=rhel-7-server-rhceph-1.3-installer-rpms --enable=rhel-7-server-rhceph-1.3-tools-rpms Configure firewalld.  sudo systemctl start firewalld sudo systemctl enable firewalld sudo firewall-cmd --zone=public --add-port=80/tcp --permanent sudo firewall-cmd --zone=public --add-port=2003/tcp --permanent sudo firewall-cmd --zone=public --add-port=4505-4506/tcp --permanent sudo firewall-cmd --zone=public --add-port=6789/tcp --permanent sudo firewall-cmd --zone=public --add-port=6800-7300/tcp --permanent sudo firewall-cmd --reload Configure NTP.  yum -y install ntp systemctl enable ntpd.service systemctl start ntpd Ensure NTP is scychronozing.  ntpq -p remote refid st t when poll reach delay offset jitter  ==============================================================================  +privatewolke.co 131.188.3.222 2 u 12 64 1 26.380 -2.334 2.374  +ridcully.episod 148.251.68.100 3 u 11 64 1 26.626 -2.425 0.534  *s1.kelker.info 213.172.96.14 2 u 12 64 1 26.433 -6.116 1.030  sircabirus.von- .STEP. 16 u - 64 0 0.000 0.000 0.000 Create ceph user for deployer.  #useradd ceph #passwd ceph #cat &lt;&lt; EOF &gt;/etc/sudoers.d/ceph ceph ALL = (root) NOPASSWD:ALL Defaults:ceph !requiretty EOF #chmod 0440 /etc/sudoers.d/ceph #su - ceph #ssh-key-gen #ssh-copy-id ceph@ceph1 #ssh-copy-id ceph@ceph2 #ssh-copy-id ceph@ceph3 Set SELinux to permissive. Ceph 2.0 now supports SELinux but for 1.3 it was not possible out-of-box.  #vi /etc/selinux/config  SELINUXTYPE=permissive Create ceph-config dir.   mkdir ~/ceph-config cd ~/ceph-config  On Monitors.  #subscription-manager repos --enable=rhel-7-server-rhceph-1.3-mon-rpms #yum update -y On OSD Nodes.  #subscription-manager repos --enable=rhel-7-server-rhceph-1.3-osd-rpms #yum update -y &nbsp;     On admin node (ceph1).  Setup Admin Console and Calamari.  #sudo yum -y install ceph-deploy calamari-server calamari-clients   #sudo calamari-ctl initialize #su - ceph [ceph@ceph1 ceph-config]$cd ~/ceph-config Create Ceph Cluster.  #ceph-deploy new ceph1 ceph2 ceph3 Deploy Ceph monitors and OSDs.  [ceph@ceph1 ceph-config]$sudo ceph-deploy install --mon ceph1 ceph2 ceph3 [ceph@ceph1 ceph-config]$sudo ceph-deploy install --osd ceph1 ceph2 ceph3 [ceph@ceph1 ceph-config]$sudo ceph-deploy mon create ceph1 ceph2 ceph3 [ceph@ceph1 ceph-config]$sudo ceph-deploy gatherkeys ceph1 Connect Ceph monitors to Calamari.  [ceph@ceph1 ceph-config]$sudo ceph-deploy calamari connect --master ceph1.lab ceph1 ceph2 ceph3 [ceph@ceph1 ceph-config]$sudo ceph-deploy install --cli ceph1 [ceph@ceph1 ceph-config]$sudo ceph-deploy admin ceph1 Check Ceph quorum status.  [ceph@ceph1 ceph-config]$sudo ceph quorum_status --format json-pretty  {  \"election_epoch\": 6,  \"quorum\": [  0,  1,  2  ],  \"quorum_names\": [  \"ceph1\",  \"ceph2\",  \"ceph3\"  ],  \"quorum_leader_name\": \"ceph1\",  \"monmap\": {  \"epoch\": 1,  \"fsid\": \"188aff9b-7da5-46f3-8eb8-465e014a472e\",  \"modified\": \"0.000000\",  \"created\": \"0.000000\",  \"mons\": [  {  \"rank\": 0,  \"name\": \"ceph1\",  \"addr\": \"192.168.0.31:6789\\/0\"  },  {  \"rank\": 1,  \"name\": \"ceph2\",  \"addr\": \"192.168.0.32:6789\\/0\"  },  {  \"rank\": 2,  \"name\": \"ceph3\",  \"addr\": \"192.168.0.33:6789\\/0\"  }  ]  } } Set crush tables to optimal.  [ceph@ceph1 ceph-config]$sudo ceph osd crush tunables optimal Configure OSDs  Prepare and Active OSDs together  [ceph@ceph1 ceph-config]$sudo ceph-deploy osd create ceph1:vdb ceph2:vdb ceph3:vdb OR  [ceph@ceph1 ceph-config]$sudo ceph-deploy disk zap ceph1:vdb ceph2:vdb ceph3:vdb [ceph@ceph1 ceph-config]$sudo ceph-deploy osd prepare ceph1:vdb ceph2:vdb ceph3:vdb [ceph@ceph1 ceph-config]$sudo ceph-deploy osd activate ceph1:vdb1 ceph2:vdb1 ceph3:vdb1 Connect Calamari to Ceph nodes.  [ceph@ceph1 ceph-config]$sudo ceph-deploy calamari connect --master ceph1.lab ceph1 ceph2 ceph3 Tips and Tricks  Remove OSD from Ceph  [ceph@ceph1 ~]$sudo ceph osd out osd.0 [ceph@ceph1 ~]$sudo ceph osd crush remove osd.0 [ceph@ceph1 ~]$sudo ceph auth del osd.0 [ceph@ceph1 ~]$sudo ceph osd down 0 [ceph@ceph1 ~]$sudo ceph osd rm 0 Ceph Placement Group Calculation for Pool   OSDs * 100 / Replicas PGs should always be power of two 62, 128, 256, etc  Re-deploy Ceph  In case at anytime you want to start over you can run below commands to uninstall Ceph. This of course deletes any data so be careful.  [ceph@ceph1 ~]$sudo service ceph restart osd.3 [ceph@ceph1 ~]$sudo ceph-deploy purge &lt;ceph-node&gt; [&lt;ceph-node&gt;] Summary  In this article we installed a Ceph cluster on virtual machines. We deployed the cluster, setup monitors and configured OSDs.  This environment should provide the basis for a journey into software-defined storage and Ceph. The economics of scale have brought down barriers and paved the way for a software-defined world. Storage is only the next logical boundry. Ceph being an OpenSource project is already the defacto software-defined standard and is in position to become the key beneficiary of software-defined storage. I hope you found the information in this article of use, please share your experiences.  Happy Cephing!  (c) 2016 Keith Tenzer  &nbsp;  &nbsp;  ","categories": ["Storage"],
        "tags": ["ceph","software-defined-storage"],
        "url": "/storage/ceph-1-3-lab-installation-and-configuration-guide/",
        "teaser": null
      },{
        "title": "OpenStack: Integrating Ceph as Storage Backend",
        "excerpt":"  Overview  In this article we will discuss why Ceph is Perfect fit for OpenStack. We will see how to integrate three prominent OpenStack use cases with Ceph: Cinder (block storage), Glance (images) and Nova (VM virtual disks).  Integrating Ceph with OpenStack Series:   Integrating Ceph with OpenStack Cinder, Glance and Nova Integrating Ceph with Swift Integrating Ceph with Manila  Ceph provides unified scale-out storage, using commodity x86 hardware, that is self-healing and intelligently anticipates failures. It has become the defacto standard for software-defined storage. Ceph being an OpenSource project has enabled many vendors the ability to provide Ceph based software-defined storage systems. Ceph is not just limited to Companies like Red Hat, Suse, Mirantis, Ubuntu, etc. Integrated solutions from SanDisk, Fujitsu, HP, Dell, Samsung and many more exist today. There are even large-scale community built environments, Cern comes to mind, that provide storage services for 10,000s of VMs.    Ceph is by no means limited to OpenStack, however this is where Ceph started gaining traction. Looking at latest OpenStack user survey, Ceph is by a large margin the clear leader for OpenStack storage. Page 42 in OpenStack April 2016 User Survey reveals Ceph is 57% of OpenStack storage. The next is LVM (local storage) with 28% followed by NetApp with 9%. If we remove LVM, Ceph leads any other storage company by 48%, that is incredible. Why is that?  There are several reasons but I will give you my top three:   Ceph is a scale-out unified storage platform. OpenStack needs two things from storage: ability to scale with OpenStack itself and do so regardless of block (Cinder), File (Manila) or Object (Swift). Traditional storage vendors need to provide two or three different storage systems to achieve this. They don't scale the same and in most cases only scale-up in never-ending migration cycles. Their management capabilities are never truly integrated across broad spectrum of storage use-cases. Ceph is cost-effective. Ceph leverages Linux as an operating system instead of something proprietary. You can choose not only whom you purchase Ceph from, but also where you get your hardware. It can be same vendor or different. You can purchase commodity hardware, or even buy integrated solution of Ceph + Hardware from single vendor. There are even hyper-converged options for Ceph that are emerging (running Ceph services on compute nodes). Ceph is OpenSource project just like OpenStack. This allows for a much tighter integration and cross-project development. Proprietary vendors are always playing catch-up since they have secrets to protect and their influence is usually limited in Opensource communities, especially in OpenStack context.  Below is an architecture Diagram that shows all the different OpenStack components that need storage. It shows how they integrate with Ceph and how Ceph provides a unified storage system that scales to fill all these use cases.    source: Red Hat Summit 2016  If you are interested in more topics relating to ceph and OpenStack I recommend following: http://ceph.com/category/ceph-and-openstack/  Ok enough talking about why Ceph and OpenStack are so great, lets get our hands dirty and see how to hook it up!  If you don't have a Ceph environment you can follow this article on how to set one up quickly.  Glance Integration  Glance is the image service within OpenStack. By default images are stored locally on controllers and then copied to compute hosts when requested. The compute hosts cache the images but they need to be copied again, every time an image is updated.  Ceph provides backend for Glance allowing images to be stored in Ceph, instead of locally on controller and compute nodes. This greatly reduces network traffic for pulling images and increases performance since Ceph can clone images instead of copying them. In addition it makes migrating between OpenStack deployments or concepts like multi-site OpenStack much simpler.  Install ceph client used by Glance.  [root@osp9 ~]# yum install -y python-rbd Create Ceph user and set home directory to /etc/ceph.  [root@osp9 ~]# mkdir /etc/ceph [root@osp9 ~]# useradd ceph [root@osp9 ~]# passwd ceph Add ceph user to sudoers.  cat &lt;&lt; EOF &gt;/etc/sudoers.d/ceph ceph ALL = (root) NOPASSWD:ALL Defaults:ceph !requiretty EOF On Ceph admin node.  Create Ceph RBD Pool for Glance images.  [ceph@ceph1 ~]$ sudo ceph osd pool create images 128 Create keyring that will allow Glance access to pool.  [ceph@ceph1 ~]$ sudo ceph auth get-or-create client.images mon 'allow r' osd 'allow class-read object_prefix rdb_children, allow rwx pool=images' -o /etc/ceph/ceph.client.images.keyring Copy the keyring to /etc/ceph on OpenStack controller.  [ceph@ceph1 ~]$ scp /etc/ceph/ceph.client.images.keyring root@osp9.lab:/etc/ceph Copy /etc/ceph/ceph.conf configuration to OpenStack controller.  [ceph@ceph1 ~]$ scp /etc/ceph/ceph.conf root@osp9.lab:/etc/ceph Set permissions so Glance can access Ceph keyring.  [root@osp9 ~(keystone_admin)]# chgrp glance /etc/ceph/ceph.client.images.keyring [root@osp9 ~(keystone_admin)]#chmod 0640 /etc/ceph/ceph.client.images.keyring Add keyring file to Ceph configuration.  [root@osp9 ~(keystone_admin)]# vi /etc/ceph/ceph.conf [client.images] keyring = /etc/ceph/ceph.client.images.keyring Create backup of original Glance configuration.  [root@osp9 ~(keystone_admin)]# cp /etc/glance/glance-api.conf /etc/glance/glance-api.conf.orig Update Glance configuration.  [root@osp9 ~]# vi /etc/glance/glance-api.conf [glance_store] stores = glance.store.rbd.Store default_store = rbd rbd_store_pool = images rbd_store_user = images rbd_store_ceph_conf = /etc/ceph/ceph.conf Restart Glance.  [root@osp9 ~(keystone_admin)]# systemctl restart openstack-glance-api Download Cirros images and add it into Glance.  [root@osp9 ~(keystone_admin)]# wget http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img Convert QCOW2 to RAW. It is recommended for Ceph to always use RAW format.  [root@osp9 ~(keystone_admin)]#qemu-img convert cirros-0.3.4-x86_64-disk.img cirros-0.3.4-x86_64-disk.raw Add image to Glance.  [root@osp9 ~(keystone_admin)]#glance image-create --name \"Cirros 0.3.4\" --disk-format raw --container-format bare --visibility public --file cirros-0.3.4-x86_64-disk.raw +------------------+--------------------------------------+ | Property | Value | +------------------+--------------------------------------+ | checksum | ee1eca47dc88f4879d8a229cc70a07c6 | | container_format | bare | | created_at | 2016-09-07T12:29:23Z | | disk_format | qcow2 | | id | a55e9417-67af-43c5-a342-85d2c4c483f7 | | min_disk | 0 | | min_ram | 0 | | name | Cirros 0.3.4 | | owner | dd6a4ed994d04255a451da66b68a8057 | | protected | False | | size | 13287936 | | status | active | | tags | [] | | updated_at | 2016-09-07T12:29:27Z | | virtual_size | None | | visibility | public | +------------------+--------------------------------------+ Check that glance image exists in Ceph.  [ceph@ceph1 ceph-config]$ sudo rbd ls images a55e9417-67af-43c5-a342-85d2c4c483f7 [ceph@ceph1 ceph-config]$ sudo rbd info images/a55e9417-67af-43c5-a342-85d2c4c483f7 rbd image 'a55e9417-67af-43c5-a342-85d2c4c483f7':  size 12976 kB in 2 objects  order 23 (8192 kB objects)  block_name_prefix: rbd_data.183e54fd29b46  format: 2  features: layering, striping  flags:  stripe unit: 8192 kB  stripe count: 1 Cinder Integration  Cinder is the block storage service in OpenStack. Cinder provides an abstraction around block storage and allows vendors to integrate by providing a driver. In Ceph, each storage pool can be mapped to a different Cinder backend. This allows for creating storage services such as gold, silver or bronze. You can decide for example that gold should be fast SSD disks that are replicated three times, while silver only should be replicated two times and bronze should use slower disks with erasure coding.  Create Ceph pool for cinder volumes.  [ceph@ceph1 ~]$ sudo ceph osd pool create volumes 128 Create keyring to grant cinder access.  [ceph@ceph1 ~]$ sudo ceph auth get-or-create client.volumes mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rx pool=images' -o /etc/ceph/ceph.client.volumes.keyring Copy keyring to OpenStack controllers.  [ceph@ceph1 ~]$ scp /etc/ceph/ceph.client.volumes.keyring root@osp9.lab:/etc/ceph Create file that contains just the authentication key on OpenStack controllers.  [ceph@ceph1 ~]$ sudo ceph auth get-key client.volumes |ssh osp9.lab tee client.volumes.key Set permissions on keyring file so it can be accessed by Cinder.  [root@osp9 ~(keystone_admin)]# chgrp cinder /etc/ceph/ceph.client.volumes.keyring [root@osp9 ~(keystone_admin)]# chmod 0640 /etc/ceph/ceph.client.volumes.keyring Add keyring to Ceph configuration file on OpenStack controllers.  [root@osp9 ~(keystone_admin)]#vi /etc/ceph/ceph.conf  [client.volumes] keyring = /etc/ceph/ceph.client.volumes.keyring Give KVM Hypervisor access to Ceph.  [root@osp9 ~(keystone_admin)]# uuidgen |tee /etc/ceph/cinder.uuid.txt Create a secret in virsh so KVM can access Ceph pool for cinder volumes.  [root@osp9 ~(keystone_admin)]#vi /etc/ceph/cinder.xml  &lt;secret ephemeral=\"no\" private=\"no\"&gt;  &lt;uuid&gt;ce6d1549-4d63-476b-afb6-88f0b196414f&lt;/uuid&gt;  &lt;usage type=\"ceph\"&gt;  &lt;name&gt;client.volumes secret&lt;/name&gt;  &lt;/usage&gt; &lt;/secret&gt; [root@osp9 ceph]# virsh secret-define --file /etc/ceph/cinder.xml [root@osp9 ~(keystone_admin)]# virsh secret-set-value --secret ce6d1549-4d63-476b-afb6-88f0b196414f --base64 $(cat /etc/ceph/client.volumes.key) Add Ceph backend for Cinder.  [root@osp9 ~(keystone_admin)]#vi /etc/cinder/cinder.conf  [rbd] volume_driver = cinder.volume.drivers.rbd.RBDDriver rbd_pool = volumes rbd_ceph_conf = /etc/ceph/ceph.conf rbd_flatten_volume_from_snapshot = false rbd_max_clone_depth = 5 rbd_store_chunk_size = 4 rados_connect_timeout = -1 glance_api_version = 2 rbd_user = volumes rbd_secret_uuid = ce6d1549-4d63-476b-afb6-88f0b196414f Restart Cinder service on all controllers.  [root@osp9 ~(keystone_admin)]# openstack-service restart cinder Create a cinder volume.  [root@osp9 ~(keystone_admin)]# cinder create --display-name=\"test\" 1 +--------------------------------+--------------------------------------+ | Property | Value | +--------------------------------+--------------------------------------+ | attachments | [] | | availability_zone | nova | | bootable | false | | consistencygroup_id | None | | created_at | 2016-09-08T10:58:17.000000 | | description | None | | encrypted | False | | id | d251bb74-5c5c-4c40-a15b-2a4a17bbed8b | | metadata | {} | | migration_status | None | | multiattach | False | | name | test | | os-vol-host-attr:host | None | | os-vol-mig-status-attr:migstat | None | | os-vol-mig-status-attr:name_id | None | | os-vol-tenant-attr:tenant_id | dd6a4ed994d04255a451da66b68a8057 | | replication_status | disabled | | size | 1 | | snapshot_id | None | | source_volid | None | | status | creating | | updated_at | None | | user_id | 783d6e51e611400c80458de5d735897e | | volume_type | None | +--------------------------------+--------------------------------------+   List new cinder volume  [root@osp9 ~(keystone_admin)]# cinder list +--------------------------------------+-----------+------+------+-------------+----------+-------------+ | ID | Status | Name | Size | Volume Type | Bootable | Attached to | +--------------------------------------+-----------+------+------+-------------+----------+-------------+ | d251bb74-5c5c-4c40-a15b-2a4a17bbed8b | available | test | 1 | - | false | | +--------------------------------------+-----------+------+------+-------------+----------+-------------+ List cinder volume in ceph.  [ceph@ceph1 ~]$ sudo rbd ls volumes volume-d251bb74-5c5c-4c40-a15b-2a4a17bbed8b [ceph@ceph1 ~]$ sudo rbd info volumes/volume-d251bb74-5c5c-4c40-a15b-2a4a17bbed8b rbd image 'volume-d251bb74-5c5c-4c40-a15b-2a4a17bbed8b':  size 1024 MB in 256 objects  order 22 (4096 kB objects)  block_name_prefix: rbd_data.2033b50c26d41  format: 2  features: layering, striping  flags:  stripe unit: 4096 kB  stripe count: 1 Integrating Ceph with Nova Compute  Nova is the compute service within OpenStack. Nova stores virtual disks images associated with running VMs by default, locally on Hypervisor under /var/lib/nova/instances. There are a few drawbacks to using local storage on compute nodes for virtual disk images:   Images are stored under root filesystem. Large images can cause filesystem to fill up, thus crashing compute nodes. A disk crash on compute node could cause loss of virtual disk and as such a VM recovery would be impossible.  Ceph is one of the storage backends that can integrate directly with Nova. In this section we will see how to configure that.  [ceph@ceph1 ~]$ sudo ceph osd pool create vms 128 Create authentication keyring for Nova.  [ceph@ceph1 ~]$ sudo ceph auth get-or-create client.nova mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=vms, allow rx pool=images' -o /etc/ceph/ceph.client.nova.keyring Copy keyring to OpenStack controllers.  [ceph@ceph1 ~]$ scp /etc/ceph/ceph.client.nova.keyring root@osp9.lab:/etc/ceph Create key file on OpenStack controllers.  [ceph@ceph1 ~]$ sudo ceph auth get-key client.nova |ssh osp9.lab tee client.nova.key Set permissions on keyring file so it can be accessed by Nova service.  [root@osp9 ~(keystone_admin)]# chgrp nova /etc/ceph/ceph.client.nova.keyring [root@osp9 ~(keystone_admin)]# chmod 0640 /etc/ceph/ceph.client.nova.keyring Ensure the required rpm packages are installed.  [root@osp9 ~(keystone_admin)]# yum list installed python-rbd ceph-common Loaded plugins: product-id, search-disabled-repos, subscription-manager Installed Packages ceph-common.x86_64 1:0.94.5-15.el7cp @rhel-7-server-rhceph-1.3-mon-rpms python-rbd.x86_64 1:0.94.5-15.el7cp @rhel-7-server-rhceph-1.3-mon-rpms Update Ceph configuration.  [root@osp9 ~(keystone_admin)]#vi /etc/ceph/ceph.conf  [client.nova] keyring = /etc/ceph/ceph.client.nova.keyring Give KVM access to Ceph.  [root@osp9 ~(keystone_admin)]# uuidgen |tee /etc/ceph/nova.uuid.txt Create a secret in virsh so KVM can access Ceph pool for cinder volumes.  [root@osp9 ~(keystone_admin)]#vi /etc/ceph/nova.xml  &lt;secret ephemeral=\"no\" private=\"no\"&gt; &lt;uuid&gt;c89c0a90-9648-49eb-b443-c97adb538f23&lt;/uuid&gt; &lt;usage type=\"ceph\"&gt; &lt;name&gt;client.volumes secret&lt;/name&gt; &lt;/usage&gt; &lt;/secret&gt; [root@osp9 ~(keystone_admin)]# virsh secret-define --file /etc/ceph/nova.xml [root@osp9 ~(keystone_admin)]# virsh secret-set-value --secret c89c0a90-9648-49eb-b443-c97adb538f23 --base64 $(cat /etc/ceph/client.nova.key) Make backup of Nova configuration.  [root@osp9 ~(keystone_admin)]# cp /etc/nova/nova.conf /etc/nova/nova.conf.orig Update Nova configuration to use Ceph backend.  [root@osp9 ~(keystone_admin)]#vi /etc/nova/nova.conf force_raw_images = True disk_cachemodes = writeback  [libvirt] images_type = rbd images_rbd_pool = vms images_rbd_ceph_conf = /etc/ceph/ceph.conf rbd_user = vms rbd_secret_uuid = c89c0a90-9648-49eb-b443-c97adb538f23 Restart Nova services.  [root@osp9 ~(keystone_admin)]# systemctl restart openstack-nova-compute List Neutron networks.  [root@osp9 ~(keystone_admin)]# neutron net-list +--------------------------------------+---------+-----------------------------------------------------+ | id | name | subnets | +--------------------------------------+---------+-----------------------------------------------------+ | 4683d03d-30fc-4dd1-9b5f-eccd87340e70 | private | ef909061-34ee-4b67-80a9-829ea0a862d0 10.10.1.0/24 | | 8d35a938-5e4f-46a2-8948-b8c4d752e81e | public | bb2b65e7-ab41-4792-8591-91507784b8d8 192.168.0.0/24 | +--------------------------------------+---------+-----------------------------------------------------+ Start ephemeral VM instance using Cirros image that was added in the steps for Glance.  [root@osp9 ~(keystone_admin)]# nova boot --flavor m1.small --nic net-id=4683d03d-30fc-4dd1-9b5f-eccd87340e70 --image='Cirros 0.3.4' cephvm +--------------------------------------+-----------------------------------------------------+ | Property | Value | +--------------------------------------+-----------------------------------------------------+ | OS-DCF:diskConfig | MANUAL | | OS-EXT-AZ:availability_zone | | | OS-EXT-SRV-ATTR:host | - | | OS-EXT-SRV-ATTR:hypervisor_hostname | - | | OS-EXT-SRV-ATTR:instance_name | instance-00000001 | | OS-EXT-STS:power_state | 0 | | OS-EXT-STS:task_state | scheduling | | OS-EXT-STS:vm_state | building | | OS-SRV-USG:launched_at | - | | OS-SRV-USG:terminated_at | - | | accessIPv4 | | | accessIPv6 | | | adminPass | wzKrvK3miVJ3 | | config_drive | | | created | 2016-09-08T11:41:29Z | | flavor | m1.small (2) | | hostId | | | id | 85c66004-e8c6-497e-b5d3-b949a1666c90 | | image | Cirros 0.3.4 (a55e9417-67af-43c5-a342-85d2c4c483f7) | | key_name | - | | metadata | {} | | name | cephvm | | os-extended-volumes:volumes_attached | [] | | progress | 0 | | security_groups | default | | status | BUILD | | tenant_id | dd6a4ed994d04255a451da66b68a8057 | | updated | 2016-09-08T11:41:33Z | | user_id | 783d6e51e611400c80458de5d735897e | +--------------------------------------+-----------------------------------------------------+ Wait until the VM is active.  [root@osp9 ceph(keystone_admin)]# nova list +--------------------------------------+--------+--------+------------+-------------+---------------------+ | ID | Name | Status | Task State | Power State | Networks | +--------------------------------------+--------+--------+------------+-------------+---------------------+ | 8ca3e74e-cd52-42a6-acec-13a5b8bda53c | cephvm | ACTIVE | - | Running | private=10.10.1.106 | +--------------------------------------+--------+--------+------------+-------------+---------------------+ List images in Ceph vms pool. We should now see the image is stored in Ceph.  [ceph@ceph1 ~]$ sudo rbd -p vms ls 8ca3e74e-cd52-42a6-acec-13a5b8bda53c_disk Troubleshooting  Unable to delete Glance Images stored in Ceph RBD  [root@osp9 ceph(keystone_admin)]# nova image-list +--------------------------------------+--------------+--------+--------+ | ID | Name | Status | Server | +--------------------------------------+--------------+--------+--------+ | a55e9417-67af-43c5-a342-85d2c4c483f7 | Cirros 0.3.4 | ACTIVE | | | 34510bb3-da95-4cb1-8a66-59f572ec0a5d | test123 | ACTIVE | | | cf56345e-1454-4775-84f6-781912ce242b | test456 | ACTIVE | | +--------------------------------------+--------------+--------+--------+ [root@osp9 ceph(keystone_admin)]# rbd -p images snap unprotect cf56345e-1454-4775-84f6-781912ce242b@snap [root@osp9 ceph(keystone_admin)]# rbd -p images snap rm cf56345e-1454-4775-84f6-781912ce242b@snap [root@osp9 ceph(keystone_admin)]# glance image-delete cf56345e-1454-4775-84f6-781912ce242b Summary  In this article we discussed how OpenStack and Ceph fit perfectly together. We discussed some of the use cases around glance, cinder and nova. Finally we went through steps to integrate Ceph with those use cases. I hope you enjoyed the article and found the information useful. Please share your feedback.  Happy Cephing!  (c) 2016 Keith Tenzer  &nbsp;  ","categories": ["OpenStack"],
        "tags": ["Storage","Ceph","Cinder"],
        "url": "/2016/09/12/openstack-integrating-ceph-as-storage-backend/",
        "teaser": null
      },{
        "title": "Enterprise Container Platform in the Cloud: OpenShift on Azure secured by Azure AD",
        "excerpt":"    Overview  This article is a collaboration from Rolf Masuch (Microsoft) and Keith Tenzer (Red Hat). It is based on our work together in the field with enterprise customers.  In this article we will explore how to deploy a production ready OpenShift enterprise container platform on the Microsoft Azure Cloud. The entire deployment is completely automated using Ansible and ARM (Azure Resource Manager). Everything is template driven using APIs. The bennefit of this approach is the ability to build-up and tear-down a complete OpenShift environment in the Azure cloud before your coffee gets cold.  Since OpenShift already uses Ansible as its installation and configuration management tool, it made sense to stick with Ansible as opposed to using other tools such as Power Shell. A Red Hat colleague, Ivan McKinley created an Ansible playbook that builds out all the required Azure infrastructure components and integrates the existing OpenShift installation playbook. The result is an optimally configure OpenShift environment on the Azure Cloud. We have used this recipe to deploy real production Environments for customers and it leverages both Microsoft as well as Red Hat best practices.    You can access and contribute improvements to the Ansible playbook under Ivan's Github repository:  https://github.com/ivanthelad/ansible-azure  The following, related articles might also be of Interest in case you want a basic understanding of OpenShift.   OpenShift v3.2 All-in-one Lab Environment OpenShift Basic Release Deployment Scenarios  The pre-requisites for deploying OpenShift on Azure are a valid OpenShift subscription and a valid Azure subscription.   If you don't already have a OpenShift Subscription you can purchase one or get an eval by talking to your partner or Red Hat account manager. If you don't already have a Microsoft Azure Subscription you can start one here.  Deploying to Azure  Install Fedora 24 workstation for use as deployment workstation. You need very recent versions of python 2.7 (2.7.12) and unfortunately it isn't available in RHEL or CentOS at writing of this article so we used Fedora.  Install Python and Dependencies  # sudo yum install python # sudo yum install python-pip # sudo dnf install python-devel # sudo dnf install redhat-rpm-config # sudo dnf install openssl-devel Install Azure CLI  # sudo dnf install npm # sudo npm install azure-cli -g # sudo pip install --upgrade pip # sudo pip install azure==2.0.0rc5 Authenticate Azure CLI  [ktenzer@ktenzer ansible-azure]$ azure login  info: Executing command login  \\info: To sign in, use a web browser to open the page https://aka.ms/devicelogin. Enter the code CB8P5ZCKP to authenticate.  -info: Added subscription Pay-As-You-Go  info: Added subscription ITS - RedHat Openshift  info: Setting subscription \"Pay-As-You-Go\" as default  +  info: login command OK List Azure Resource Groups  In order to list resource groups you need your Azure subscription id. You can view this by logging into Azure portal with your user.  [ktenzer@ktenzer ansible-azure]$ azure group list --subscription &lt;subscription id&gt;  info: Executing command group list  + Listing resource groups  data: Name Location Provisioning State Tags:  data: ------------- ---------- ------------------ -----  data: OpenShift_POC westeurope Succeeded null  data: Shared westeurope Succeeded null  info: group list command OK Install Ansible Core  # sudo dnf install ansible Clone OpenShift Azure Ansible Playbooks  # git clone https://github.com/ivanthelad/ansible-azure.git Update Playbook parameters  # cd ansible-azure # cp group_vars/all_example group_vars/all # vi group_vars/all resource_group_name: &lt;new resource group name&gt; ## Azure AD user. ad_username: &lt;Azure user e.g. keith.tenzer@domain.onmicrosoft.com&gt; ### Azure AD password ad_password: &lt;Azure Password&gt; #### Azure Subscription ID subscriptionID: \"&lt;subscription id from Azure&gt;\" ## user to login to the jump host. this user will only be created on the jumphost adminUsername: &lt;username e.g. ktenzer&gt; ## user pwd for jump host ## Password for the jump host adminPassword: &lt;password&gt; ##### Public key for jump host ### Access to environment only allowed through jumphost sshkey: &lt;ssh key e.g. cat /home/ktenzer/.ssh/id_rsa.pub&gt;  # see https://azure.microsoft.com/en-us/documentation/articles/cloud-services-sizes-specs/ ### Size for the master master_vmSize: Standard_DS3_v2 #master_vmSize: Standard_D2_v2 #master_vmSize: Standard_D1_v2  ### Size for the nodes node_vmSize: Standard_DS3_v2 #node_vmSize: Standard_D2_v2 #node_vmSize: Standard_D1_v2  #### Region to deploy in region: westeurope  ## docker info docker_storage_device: /dev/sdc create_vgname: docker_vg filesystem: 'xfs' create_lvsize: '80%FREE' #create_lvsize: '2g'  #### subscription information rh_subscription_user: &lt;Red Hat Subscription User&gt; rh_subscription_pass: &lt;Red Hat Subscription Password&gt; openshift_pool_id: &lt;Red Hat Subscription Pool Id&gt;  ########### list of node ########### ### Warning, you currently cannot create more infra nodes #### ### this will change in the future ### You can add as many nodes as you want ##################################### jumphost:  jumphost1:  name: jumphost1  tags:  region: westeurope  zone: jumphost  stage: jumphost  masters:  master1:  name: master1  tags:  region: westeurope  zone: infra  stage: none  master2:  name: master2  tags:  region: westeurope  zone: infra  stage: none  master3:  name: master3  tags:  region: westeurope  zone: infra  stage: none  infranodes:  infranode1:  name: infranode1  tags:  region: westeurope  zone: infra  stage: dev nodes:  node1:  name: node1  tags:  region: westeurope  zone: app  stage: dev  node2:  name: node2  tags:  region: westeurope  zone: app  stage: dev Run Ansible Playbook  # ansible-playbook --forks=50 -i inventory.azure playbooks/setup_multimaster.new.yml Connect to OpenShift environment  In order to connect to OpenShift environment you need to access jump box. The public IP of jumpbox will be set during playbook run, simply look at outputs to get the public IP for jumpbox.  Connect to jumphost  # ssh -i /home/ktenzer/.ssh/id_rsa.pub ktenzer@&lt;jumphost public IP&gt; Connect to master1  There are three masters and you can connect and manage environment from any of them.  [ktenzer@jumphost1 ~]$ ssh master1 Login as built-in system:admin user  [ktenzer@master1 ~]$ oc login -u system:admin List OpenShift nodes  [ktenzer@master1 ~]$ oc get nodes NAME                                                       STATUS AGE infranode1.KgsZ98734738nshjdsj2.ax.internal.cloudapp.net   Ready  34d master1.KgsZ98734738nshjdsj2.ax.internal.cloudapp.net      Ready 34d master2.KgsZ98734738nshjdsj2.ax.internal.cloudapp.net      Ready 34d master3.KgsZ98734738nshjdsj2.ax.internal.cloudapp.net      Ready 34d node1.KgsZ98734738nshjdsj2.ax.internal.cloudapp.net        Ready 34d node2.KgsZ98734738nshjdsj2.ax.internal.cloudapp.net        Ready 34d The deployment defaults to a highly available three master node OpenShift cluster. It also deploys three nodes, one for infrastructure and other two for applications. Ideally you would want a second infrastructure node so infrastructure services such as routing, image registry, logging and metrics are also highly available. Adding additional nodes is simply a matter of updating the playbook and re-running it. In addition changing OpenShift configuration is also just matter of updating playbook and re-running it.  In order to access the UI get the public URL from the master configuration file. When accessing the public URL traffic is balanced across all three OpenShift master servers.  [root@master1 ~]# cat /etc/origin/master/master-config.yaml |grep masterPublicURL  masterPublicURL: https://master-&lt;Resource Group&gt;.westeurope.cloudapp.azure.com:8443 Azure AD Authentication  OpenShift like most platforms has two layers that govern access to the environment. Authentication grants a user access to the platform and authorization gives a user privileges within the environment. OpenShift supports many authentication providers. In the case of Azure AD openId is used for authentication and OpenShift offloads authentication entirely to Azure AD. Any user permitted to access the OpenShift authentication application in Azure AD is permitted to login to the OpenShift environment. Once a user in Azure AD authenticates to OpenShift, privileges can be given at project level to allow the user access within the environment. We will first configure Azure AD by setting up an authentication application and then allowing access within Azure. Once that is configured we will go through steps to allow OpenShift to use the Azure AD authentication application. Finally we will show how user management works within OpenShift when using Azure AD as authentication provider.  Configure Azure AD Application for OpenShift  The following steps describe how to create an Azure WebApp as your authentication barrier, before accessing the OpenShift Admin portal.  Since you already have installed OpenShift into Azure the normal disclaimers about getting a subscription do not apply here. In case you are just reading through and want to do more in Azure this is the link: http://www.Azure.com  Create Application  To create the application, log in to the Azure portal https://portal.azure.com and click on the plus symbol (1), then on Web + mobile (2) and finally on Web App (3).    This will open another part on the screen that needs to be filled with further details about the new Web App. These parts are called blades and are used in the Azure portal to present details and configuration options. The first box takes (1) the application name that will become part of the DNS Name of the application. It needs to be unique within the namespace of azurewebsites.net. Your subscription in the following dropdown should be pre-filled. The next option, Resource Group, should be set to“Use existing” from the dropdown (2) and should point to the resource group where your OpenShift installation resides. Special attention is needed for the App Service plan and its location (3). The default location may need adjustment. Click on the tile to configure your service plan. After that click on create at the bottom of the blade to create your Web App. You may want to tick the checkbox “Pin to dashboard” for easier access later.    Enable Authentication for Application  When your application has been deployed click on it to access its blade and continue with the configuration by clicking on Authentication/Authorization in the settings.    The right side of the blade will change and you can click on the button for App Service Authentication (1) to turn it on. The dropdown for the Authentication providers should show “Log in with Azure Active Directory” (2). Click on the tile below (3) to configure the Azure Active Directory details.    In the Azure Active Directory Settings dialog select the Management mode option (1) “Express”. The Current Active Directory (2) should read your own Azure Active Directory name. In the lower part of the dialog the button “Create New AD App” is pre-selected and you can provide an additional application name under “Create App”. By default it can be the same as the Web App name. After that click on OK at the bottom of the blade.      Don’t forget to click on Save at the top of the blade.    Now that the Web App part is finished you can close all the blades and return to the dashboard. The next steps are done in the new Azure Active Directory blade.    Configure Azure AD for Application  When you click on it the new blade should open with your existing Azure Active Directory details and the tile App registrations should read “1”. Click on that tile to continue.    Click on Endpoints in order to retrieve the Azure Tenant Id. It is the number after the https://login.windows.net part. Take note of the tenant id as it is needed in the OpenShift configuration. Click on the Application you configured in the step before to access its details and configuration options.    In the settings blade of your app click on Properties (1) to access the applications Application ID. You need this information as well to configure OpenShift to make use of the Azure AD authentication.      Close the Properties blade and click on Keys (2) in the Settings blade to create an individual access key. This key is also needed for the OpenShift configuration. Add a key description (1), select a duration of one or two years or even unlimited live time under (2) and click on save. Pay special attention to the warning displayed about the key that is now shown in the value (4). Copy that key and verify that it is stored properly somewhere else before leaving that blade!      In general, when you add the gathered information into OpenShift, all access to the URL of the installation will now be verified through Azure Active Directory authentication. In case you want to limit the usage to a certain group of users or even individual users you can configure this also but currently only in the Classic Azure portal under https://manage.windowsazure.com.  You may have to sign out and sign in again to access the portal. Once you have signed in, scroll to the Azure Active directory icon and click on the name of your Azure AD to access the dashboard.    Enable User Access to Applications Azure AD  Click on APPLICATIONS (1), search (2) for your application in case there are more than one and click on your applications name (3) to go to its properties.    In the properties click on “CONFIGURE” and toggle the button “USER ASSIGNMENT REQUIRED TO ACCESS APP” to “YES” and click on “SAVE” at the bottom of the page.      Now that the application is toggled users and/or groups need to be assigned to it. You configure that under “USERS AND GROUPS” on the same page.     Select one of your users, click on assign at the bottom of the page and confirm the dialog. Verify the access by opening a new browser window in private mode and navigate to the OpenShift URL. You should be prompted with an authentication dialog from your Azure AD.        Configure OpenShift for Azure AD Authentication  OpenShift supports many identity providers. In this case we are using OpenId to authenticate users against Azure AD. In order to configure Azure AD authentication in OpenShift the following information from Azure AD is required:   Tenant Id - The tenant id from any of the endpoint URLs Client Id - The Application Id from Azure AD Application Client Secret Key - The key that was created for the Application in Azure AD    Configure OpenId Provider  This information should have been captured from the steps above. On all three OpenShift masters edit the master configuration and add openId provider.  [root@master1 ~]# vi /etc/origin/master/master-config.yaml identityProviders:  - name: openId  challenge: false  login: true  mappingMethod: claim  provider:  apiVersion: v1  kind: OpenIDIdentityProvider  clientID: &lt;client id&gt;  clientSecret: &lt;client secret key&gt;  claims:  id:  - sub  preferredUsername:  - preferred_username  name:  - name  email:  - email  urls:  authorize: https://login.microsoftonline.com/&lt;tenant id&gt;/oauth2/authorize  token: https://login.microsoftonline.com/&lt;tenant id&gt;/oauth2/token Restart OpenShift on all masters.  [root@master1 ~]# systemctl restart atomic-openshift-master-api [root@master1 ~]# systemctl restart atomic-openshift-master-controllers [root@master2 ~]# systemctl restart atomic-openshift-master-api [root@master2 ~]# systemctl restart atomic-openshift-master-controllers [root@master3 ~]# systemctl restart atomic-openshift-master-api [root@master3 ~]# systemctl restart atomic-openshift-master-controllers Login as Azure AD user  Using the GUI  Verify the access by opening a new browser window in private mode and navigate to the OpenShift URL https://master-Resource-Group.westeurope.cloudapp.azure.com:8443. You should be prompted with an authentication dialog of your Azure AD.  Using the CLI  [root@master1 ~]# oc login -u keith.tenzer@mydomain.onmicrosoft.com -n default  Login failed (401 Unauthorized)  You must obtain an API token by visiting https://master-Resource_Group.westeurope.cloudapp.azure.com:8443/oauth/token/request  [root@master1 ~]# oc login --token=0h9tKLlTicyAr5dYIgW5xiejiMVHIvltEuX6LLVW8CY --server=https://master-Resource-Group.westeurope.cloudapp.azure.com:8443  Logged into \"https://master-&lt;resource group&gt;.westeurope.cloudapp.azure.com:8443\" as \"G374shPb01h2jwssdjhGhghsghGHJSK\" using the token provided. Notice that the user is actually known to OpenShift is \"G374shPb01h2jwssdjhGhghsghGHJSK\" not  keith.tenzer@mydomain.onmicrosoft.com  List OpenShift users  Once Azure AD users login they are automatically added to OpenShift.  [root@master ~]# oc get users  NAME                                        UID                                   FULL NAME    IDENTITIES  G374shPb01h2jwssdjhGhghsghGHJSK             1245fec9f-598f-1656-12as-123g4g936742 Keith Tenzer openId:G374shPb01h2jwssdjhGhghsghGHJSK List OpenShift identities  [root@master ~]# oc get identity NAME                                               IDP    NAME                                       IDP USER NAME                   USER NAME                                   USER UID openId:G374shPb01h2jwssdjhGhghsghGHJSK             openId FGnyqZlyGFzNCI99feUypSdEoNapBAJ03tg5MWlDSZ G374shPb01h2jwssdjhGhghsghGHJSK 1245fec9f-598f-1656-12as-123g4g936742 Give Azure AD user cluster-admin permission  [root@master ~]# oadm policy add-cluster-role-to-user cluster-admin \"G374shPb01h2jwssdjhGhghsghGHJSK\" Give user permission to project  [root@master ~]# oadm policy add-role-to-user admin \"G374shPb01h2jwssdjhGhghsghGHJSK\" -n test Remove Azure AD user from OpenShift  Delete user  [root@master ~]# oc delete user G374shPb01h2jwssdjhGhghsghGHJSK Delete identity  [root@master ~]# oc delete identity openId:G374shPb01h2jwssdjhGhghsghGHJSK Summary  In this article we have detailed how to standup a production-ready OpenShift environment automatically, before your coffee gets cold in the Azure Cloud. We have seen how to integrate OpenShift into Azure AD for authentication. There is no doubt, container platforms are the future for applications. Container technology enables application to be developed and deployed at much faster speeds, greatly improving a organizations ability to innovate and connect to it's market segment. Many enterprises are interested in getting their hands on these technologies and evaluating them rather quickly. There is no faster way then standing up OpenShift in the Azure cloud! We hope you found this article interesting and look forward to your feedback.  Happy OpenShifting in the Azure Cloud!  (c) 2016 Keith Tenzer and Rolf Masuch  ","categories": ["Azure","OpenShift"],
        "tags": ["Cloud","Containers","Microsoft","OpenShift","Red Hat"],
        "url": "/azure/openshift/enterprise-container-platform-in-the-cloud-openshift-on-azure-secured-by-azure-ad/",
        "teaser": null
      },{
        "title": "OpenShift Enterprise 3.3: all-in-one Lab Environment with Jenkins Build Pipeline",
        "excerpt":"  Overview  In this article we will setup a OpenShift Enterprise 3.3 all-in-one configuration. We will also configure OpenShift router, registry, aggregate logging, metrics, CloudForms integration and finally an integrated jenkins build pipeline.  OpenShift has several different roles: masters, nodes, etcd and load balancers. An all-in-one setup means running all service on a single system. Since we are only using a single system, a load balancer or ha-proxy won’t be configured. If you would like to read more about OpenShift I can recommend the following:   General OpenShift Product Blogs Persistent Storage OpenShift Networking Part I OpenShift Networking Part II    Prerequisites  Configure a VM with following:   RHEL 7.2 2 CPUs 4096 RAM 30GB disk for OS 25GB disk for docker images  Register valid subscription  # subscription-manager register # subscription-manager attach --pool=843298293829382 # subscription-manager repos --disable=\"*\" #subscription-manager repos \\     --enable=\"rhel-7-server-rpms\" \\     --enable=\"rhel-7-server-extras-rpms\" \\     --enable=\"rhel-7-server-ose-3.3-rpms\" Install required tools  # yum install -y wget git net-tools bind-utils iptables-services bridge-utils bash-completion Update  # yum update -y Install OpenShift tools  # yum install -y atomic-openshift-utils Restart OpenShift master  # systemctl reboot Configure Docker  # yum install -y docker-1.10.3 Enable Docker daemon to pull from OpenShift registry  # vi /etc/sysconfig/docker OPTIONS='--selinux-enabled --insecure-registry 172.30.0.0/16' Setup Docker storage for OpenShift registry  Note: we will use the second disk for configuring docker storage.  # cat &lt;&lt;EOF &gt; /etc/sysconfig/docker-storage-setup DEVS=/dev/vdb VG=docker-vg EOF # docker-storage-setup Enable and start Docker daemon  # systemctl enable docker # systemctl start docker Setup ssh access without password for Ansible  # ssh-keygen # ssh-copy-id -i /root/.ssh/id_rsa.pub ose3-master.lab.com DNS Setup  DNS is a requirement for OpenShift Enterprise. In fact most issues you may run into are a result of not having a properly working DNS environment. For OpenShift you can either use dnsmasq or bind. I recommend using dnsmasq but in this article I will cover both options.  Note: Since OpenShift also has skydns for providing DNS services to containers running on port 53 you need to setup dnsmasq or bind on a separate system. The OpenShift environment should resolv to that DNS server.  Option 1: DNSMASQ  A colleague Ivan Mckinely was nice enough to create an ansible playbook for deploying dnsmasq. To deploy dnsmasq run following steps on OpenShift master.  # git clone https://github.com/ivanthelad/ansible-aos-scripts.git #cd ansible-aos-scripts Edit inventory file and set dns to IP of the system that should be providing DNS  Also ensure nodes and masters have correct IPs for your OpenShift servers. In our case 192.168.122.60 is master, node and DNS.  #vi inventory # ip of DNS server [dns] 192.168.122.59 # ip of OpenShift nodes [nodes] 192.168.122.60 # ip of OpenShift masters [masters] 192.168.122.60 Configure dnsmasq and add wildcard DNS so all hosts with  # vi playbooks/roles/dnsmasq/templates/dnsmasq.conf  strict-order domain-needed local=/lab.com/ bind-dynamic resolv-file=/etc/resolv.conf.upstream no-hosts address=/.cloudapps.lab.com/192.168.122.60 address=/ose3-master.lab.com/192.168.122.60 address=/dns.lab.com/192.168.122.59 log-queries Ensure all hosts you want in DNS are also in /etc/hosts  The dnsmasq service reads /etc/hosts upon startup so all entries in hosts file can be queried through DNS.  #vi /etc/hosts 192.168.122.60  ose3-master.lab.com     ose3-master Install dnsmasq via ansible  # ansible-playbook -i inventory playbooks/install_dnsmas.yml If you need to make changes you can edit the /etc/dnsmasq.conf file and restart dnsmasq service.  Below is a sample dnsmasq.conf  # vi /etc/dnsmasq.conf strict-order domain-needed local=/example.com/ bind-dynamic resolv-file=/etc/resolv.conf.upstream no-hosts address=/.apps.lab.com/192.168.122.60 address=/ose3-master.lab.com/192.168.122.60 address=/ose3-master.lab.com/192.168.122.60 address=/dns.lab.com/192.168.122.59 address=/kubernetes.default.svc/192.168.122.60 log-queries Option 2: NAMED  Install DNS tools and utilities  # yum -y install bind bind-utils # systemctl enable named # systemctl start named Set firewall rules using iptables  # iptables -A INPUT -p tcp -m state --state NEW -m tcp --dport 53 -j ACCEPT # iptables -A INPUT -p udp -m state --state NEW -m udp --dport 53 -j ACCEPT  Save the iptables Using  # service iptables save iptables: Saving firewall rules to /etc/sysconfig/iptables:[  OK  ] Note: If you are using firewalld you can just enable service DNS using firwall-cmd utility.  Example of zone file for lab.com  vi /var/named/dynamic/lab.com.zone   $ORIGIN lab.com. $TTL 86400 @ IN SOA dns1.lab.com. hostmaster.lab.com. (  2001062501 ; serial  21600 ; refresh after 6 hours  3600 ; retry after 1 hour  604800 ; expire after 1 week  86400 ) ; minimum TTL of 1 day ; ;  IN NS dns1.lab.com. dns1 IN A 192.168.122.1  IN AAAA aaaa:bbbb::1 ose3-master IN A 192.168.122.60 *.cloudapps 300 IN A 192.168.122.60 Example of named configuration  # vi /etc/named.conf  // // named.conf // // Provided by Red Hat bind package to configure the ISC BIND named(8) DNS // server as a caching only nameserver (as a localhost DNS resolver only). // // See /usr/share/doc/bind*/sample/ for example named configuration files. //  options {  listen-on port 53 { 127.0.0.1;192.168.122.1; };  listen-on-v6 port 53 { ::1; };  directory \"/var/named\";  dump-file \"/var/named/data/cache_dump.db\";  statistics-file \"/var/named/data/named_stats.txt\";  memstatistics-file \"/var/named/data/named_mem_stats.txt\";  allow-query { localhost;192.168.122.0/24;192.168.123.0/24; };  /*   - If you are building an AUTHORITATIVE DNS server, do NOT enable recursion.  - If you are building a RECURSIVE (caching) DNS server, you need to enable   recursion.   - If your recursive DNS server has a public IP address, you MUST enable access   control to limit queries to your legitimate users. Failing to do so will  cause your server to become part of large scale DNS amplification   attacks. Implementing BCP38 within your network would greatly  reduce such attack surface   */  recursion yes;  dnssec-enable yes;  dnssec-validation yes;  dnssec-lookaside auto;  /* Path to ISC DLV key */  bindkeys-file \"/etc/named.iscdlv.key\";  managed-keys-directory \"/var/named/dynamic\";  pid-file \"/run/named/named.pid\";  session-keyfile \"/run/named/session.key\";  //forward first;  forwarders {  //10.38.5.26;  8.8.8.8;  }; };  logging {  channel default_debug {  file \"data/named.run\";  severity dynamic;  }; };  zone \".\" IN {  type hint;  file \"named.ca\"; };  zone \"lab.com\" IN {  type master;  file \"/var/named/dynamic/lab.com.zone\";  allow-update { none; }; };  //zone \"122.168.192.in-addr.arpa\" IN { // type master; // file \"/var/named/dynamic/122.168.192.db\"; // allow-update { none; }; //};  include \"/etc/named.rfc1912.zones\"; include \"/etc/named.root.key\"; Install OpenShift  OpenShift is installed and managed through Ansible. You have two options to install. Either advanced installation where you configure Ansible playbook or basic installation that simply runs a vanilla playbook with default options. I recommend always doing an advanced install and as such will not cover basic installation.  Configure the inventory  Note: If you have different dns names then change items in bold below.  # vi /etc/ansible/hosts ########################## ### OSEv3 Server Types ### ########################## [OSEv3:children] masters nodes etcd  ################################################ ### Set variables common for all OSEv3 hosts ### ################################################ [OSEv3:vars] ansible_ssh_user=root os_sdn_network_plugin_name='redhat/openshift-ovs-subnet' deployment_type=openshift-enterprise openshift_master_default_subdomain=apps.lab.com openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}] openshift_node_kubelet_args={'maximum-dead-containers': ['100'], 'maximum-dead-containers-per-container': ['2'], 'minimum-container-ttl-duration': ['10s'], 'max-pods': ['110'], 'image-gc-high-threshold': ['90'], 'image-gc-low-threshold': ['80']} logrotate_scripts=[{\"name\": \"syslog\", \"path\": \"/var/log/cron\\n/var/log/maillog\\n/var/log/messages\\n/var/log/secure\\n/var/log/spooler\\n\", \"options\": [\"daily\", \"rotate 7\", \"compress\", \"sharedscripts\", \"missingok\"], \"scripts\": {\"postrotate\": \"/bin/kill -HUP `cat /var/run/syslogd.pid 2&gt; /dev/null` 2&gt; /dev/null || true\"}}] openshift_docker_options=\"--log-opt max-size=1M --log-opt max-file=3\" openshift_node_iptables_sync_period=5s openshift_master_pod_eviction_timeout=3m osm_controller_args={'resource-quota-sync-period': ['10s']} osm_api_server_args={'max-requests-inflight': ['400']} openshift_use_dnsmasq=false  ############################## ### host group for masters ### ############################## [masters] ose3-master.lab.com  ################################### ### host group for etcd servers ### ################################### [etcd] ose3-master.lab.com  ################################################## ### host group for nodes, includes region info ### ################################################## [nodes] ose3-master.lab.com openshift_schedulable=True Run playbook to install OpenShift  # ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/byo/config.yml Configure OpenShift  Once OpenShift is installed we need to configure an admin user and also setup the router and registry.  Create local admin account and enable permissions  # oc login -u system:admin -n default # htpasswd -c /etc/origin/master/htpasswd admin # oadm policy add-cluster-role-to-user cluster-admin admin # oc login -u admin -n default Configure OpenShift registry  Image streams and Docker images are stored in registry. When you build application, your application code will be added as a image stream. This enables S2I (Source to Image) and allows for fast build times.  #oadm registry --service-account=registry \\ --config=/etc/origin/master/admin.kubeconfig \\ --images='registry.access.redhat.com/openshift3/ose-${component}:${version}' Note: normally you would want to setup registry in HA configuration.  Configure OpenShift router  The OpenShift router is basically an HA-Proxy that sends incoming requests to node where pod/container are running.  Note: normally you would want to setup the router in an HA configuration.  #oadm router router --replicas=1 \\     --credentials='/etc/origin/master/openshift-router.kubeconfig' \\     --service-account=router Optional: CloudForms Integration  CloudForms is a cloud management platform. It integrates not only with OpenShift but also other Cloud platforms (OpenStack, Amazon, GCE, Azure) and traditional virtualization platforms (VMware, RHEV, Hyper-V). Since OpenShift is usually running on cloud or traditional virtualization platforms, CloudForms enables true end-to-end visibility. CloudForms provides not only performance metrics, events, smart state analysis of containers (scanning container contents) but also can provide chargeback for OpenShift projects. CloudForms is included in OpenShift subscription for purpose of managing OpenShift. To add OpenShift as provider in CloudForms follow the below steps.  The management-infra project in OpenShift is designed for scanning container images. A container is started in this context and the image to be scanned is mounted. A service account management-admin exists and should be used to provide CloudForms access.  Note: scanning images is CPU intensive so it is important to ensure the management-admin project schedules pods/containers on infrastructure nodes.  List tokens that are configured in management-infra project (this is created at install time).  # oc project management-infra # oc get sa management-admin -o yaml apiVersion: v1 imagePullSecrets: - name: management-admin-dockercfg-ln1an kind: ServiceAccount metadata:  creationTimestamp: 2016-07-24T11:36:58Z  name: management-admin  namespace: management-infra  resourceVersion: \"400\"  selfLink: /api/v1/namespaces/management-infra/serviceaccounts/management-admin  uid: ee6a1426-5192-11e6-baff-001a4ae42e01 secrets: - name: management-admin-token-wx17s - name: management-admin-dockercfg-ln1an Use describe to get token to enable CloudForms to accesss the management-admin project.  # oc describe secret management-admin-token-wx17s Name: management-admin-token-wx17s Namespace: management-infra Labels:  Annotations: kubernetes.io/service-account.name=management-admin,kubernetes.io/service-account.uid=ee6a1426-5192-11e6-baff-001a4ae42e01  Type: kubernetes.io/service-account-token  Data ==== ca.crt: 1066 bytes namespace: 16 bytes token: eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJtYW5hZ2VtZW50LWluZnJhIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6Im1hbmFnZW1lbnQtYWRtaW4tdG9rZW4td3gxN3MiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoibWFuYWdlbWVudC1hZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImVlNmExNDI2LTUxOTItMTFlNi1iYWZmLTAwMWE0YWU0MmUwMSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDptYW5hZ2VtZW50LWluZnJhOm1hbmFnZW1lbnQtYWRtaW4ifQ.Y0IlcwhHW_CpKyFvk_ap-JMAT69fbIqCjkAbmpgZEUJ587LP0pQz06OpBW05XNJ3cJg5HeckF0IjCJBDbMS3P1W7KAnLrL9uKlVsZ7qZ8-M2yvckdIxzmEy48lG0GkjtUVMeAOJozpDieFClc-ZJbMrYxocjasevVNQHAUpSwOIATzcuV3bIjcLNwD82-42F7ykMn-A-TaeCXbliFApt6q-R0hURXCZ0dkWC-za2qZ3tVXaykWmoIFBVs6wgY2budZZLhT4K9b4lbiWC5udQ6ga2ATZO1ioRg-bVZXcTin5kf__a5u6c775-8n6DeLPcfUqnLucaYr2Ov7RistJRvg Add OpenShift provider to CloudForms using the management-admin service token.    Optional: CloudForms Container Provider  CloudForms is a cloud management platform. It integrates not only with OpenShift but also other Cloud platforms (OpenStack, Amazon, GCE, Azure) and traditional virtualization platforms (VMware, RHEV, Hyper-V). Since OpenShift is usually running on cloud or traditional virtualization platforms, CloudForms enables true end-to-end visibility. CloudForms provides not only performance metrics, events, smart state analysis of containers (scanning container contents) but also can provide chargeback for OpenShift projects. CloudForms is included in OpenShift subscription for purpose of managing OpenShift. To add OpenShift as provider in CloudForms follow the below steps.  Get token for CloudForms Access  Use management-admin token that is created in management-admin project during install to provide access to CloudForms.  # oc describe sa -n management-infra management-admin Name: management-admin Namespace: management-infra Labels:   Mountable secrets: management-admin-token-vr21i  management-admin-dockercfg-5j3m3  Tokens: management-admin-token-mxy4m  management-admin-token-vr21i  Image pull secrets: management-admin-dockercfg-5j3m3 # oc describe secret -n management-infra management-admin-token-mxy4m Name: management-admin-token-mxy4m Namespace: management-infra Labels:  Annotations: kubernetes.io/service-account.name=management-admin,kubernetes.io/service-account.uid=87f8f4e4-4c0f-11e6-8aca-52540057bf27  Type: kubernetes.io/service-account-token  Data ==== token: eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJtYW5hZ2VtZW50LWluZnJhIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6Im1hbmFnZW1lbnQtYWRtaW4tdG9rZW4tbXh5NG0iLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoibWFuYWdlbWVudC1hZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6Ijg3ZjhmNGU0LTRjMGYtMTFlNi04YWNhLTUyNTQwMDU3YmYyNyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDptYW5hZ2VtZW50LWluZnJhOm1hbmFnZW1lbnQtYWRtaW4ifQ.dN-CmGdSR2TRh1h0qHvwkqnW6TLvhXJtuHX6qY2jsrZIZCg2LcyuQI9edjBhl5tDE6PfOrpmh9-1NKAA6xbbYVJlRz52gnEdtm1PVgvzh8_WnKiQLZu-xC1qRX_YL7ohbglFSf8b5zgf4lBdJbgM_2P4sm1Czhu8lr5A4ix95y40zEl3P2R_aXnns62hrRF9XpmweASGMjooKOHB_5HUcZ8QhvdgsveD4j9de-ZzYrUDHi0NqOEtenBThe5kbEpiWzSWMAkIeC2wDPEnaMTyOM2bEfY04bwz5IVS_IAnrEF7PogejgsrAQRtYss5yKSZfwNTyraAXSobgVa-e4NsWg ca.crt: 1066 bytes namespace: 16 bytes Add OpenShift provider to CloudForms using token    Configure metrics by supplying the service name exposed by OpenShift    Choose a container image to scan    Check for scanning container  You should see scanning container start in the project management-infra.  [root@ose3-master ~]# oc project management-infra [root@ose3-master ~]# oc get pods NAME READY STATUS RESTARTS AGE manageiq-img-scan-24297 0/1 ContainerCreating 0 12s [root@ose3-master ~]# oc get pods NAME READY STATUS RESTARTS AGE manageiq-img-scan-24297 1/1 Running 0 1m Check image in CloudForms  You should now see an OpenSCAP report and in addition visibility into packages that are actually installed in the container itself.  Compute-&gt;Containers-Container Images-&gt;MySQL    Packages    OpenScap HTML Report    Optional: Performance Metrics  OpenShift provides ability to collect performance metrics using Hawkular. This runs as container and uses cassandra to persist the data. CloudForms is able to display capacity and utilization metrics for OpenShift using Hawkular.  Switch to openshift-infra project  [root@ose3-master ~]# oc project openshift-infra Create service account for metrics-deployer pod  [root@ose3-master ~]# oc create -f - &lt; Enable permissions and set secret  [root@ose3-master ~]# oadm policy add-role-to-user edit system:serviceaccount:openshift-infra:metrics-deployer [root@ose3-master ~]#oadm policy add-cluster-role-to-user cluster-reader system:serviceaccount:openshift-infra:heapster [root@ose3-master ~]# oc secrets new metrics-deployer nothing=/dev/null Deploy metrics environment for OpenShift  [root@ose3-master ~]# oc new-app -f /usr/share/openshift/examples/infrastructure-templates/enterprise/metrics-deployer.yaml \\ -p HAWKULAR_METRICS_HOSTNAME=hawkular-metrics.apps.lab.com \\ -p USE_PERSISTENT_STORAGE=false -p MASTER_URL=https://ose3-master.lab.com:8443 Add the metrics URL to the OpenShift master config file  # vi /etc/origin/master/master-config.yaml assetConfig: metricsPublicURL: \"https://hawkular-metrics.apps.lab.com/hawkular/metrics\" Restart OpenShift Master  # systemctl restart atomic-openshift-master Cleanup  # oc delete all,sa,templates,secrets,pvc --selector=\"metrics-infra\" # oc delete sa,secret metrics-deployer Optional: Aggregate Logging  OpenShift Enterprise supports Kibana and the ELK Stack for log aggregation. Any pod and container that log to STDOUT will have all their log messages aggregated. This provides centralized logging for all application components. Logging is completely integrated within OpenShift and the ELK Stack runs of course containerized within OpenShift.  In openshift-infra project create service account for logging and necessary permissions.  Switch to logging project  # oc project logging Create service accounts, roles and bindings  # oc new-app logging-deployer-account-template Setup permissions  # oadm policy add-cluster-role-to-user oauth-editor \\        system:serviceaccount:logging:logging-deployer # oadm policy add-scc-to-user privileged  \\     system:serviceaccount:logging:aggregated-logging-fluentd # oadm policy add-cluster-role-to-user cluster-reader \\     system:serviceaccount:logging:aggregated-logging-fluentd Create configmap  # oc create configmap logging-deployer \\  --from-literal kibana-hostname=kibana.lab.com \\  --from-literal public-master-url=https://ose3-master.lab.com:8443 \\  --from-literal es-cluster-size=1 \\  --from-literal es-instance-ram=2G Create secret  # oc secrets new logging-deployer nothing=/dev/null Deploy aggregate logging  # oc new-app logging-deployer-template Option 1:Enable fluentd on specific nodes  # oc label node/node.example.com logging-infra-fluentd=true Option 2: Enable fluentd all nodes  # oc label node --all logging-infra-fluentd=true Cleanup  # oc new-app logging-deployer-template --param MODE=uninstall  Optional: Jenkins Build Pipeline  As of OpenShift 3.3 Jenkins build pipelines are integrated within OpenShift. This means you can not only execute but also observe and configure build pipelines without leaving OpenShift. In OpenShift 3.3 this feature is technology preview.  Before we build a pipleline we need to create some projects and deploy an application. For this example we have prepared a basic helloworld nodejs application. The application has two versions both are available via branches in Github. Using the build pipeline an end-to-end application upgrade and rollout will be demonstrated. The application will be deployed across three stages: development, integration and production. The development stage has both versions of the application while integration and test have one version, either v1 or v2. In development and integration the application will be scaled to only a single pod. Production is using scaling of 4 pods.  Create Projects  # oc new-project dev # oc new-project int # oc new-project prod Switch to dev project  # oc project dev Build v1 of nodejs application  # oc new-app --name v1 https://github.com/ktenzer/nodejs-ex.git # oc expose service v1 Build v2 of nodejs application  # oc new-app --name v1 https://github.com/ktenzer/nodejs-ex.git#v2 # oc expose service v2 Enable Jenkins build pipelines in OpenShift  # vi /etc/origin/tech-preview/pipelines.js window.OPENSHIFT_CONSTANTS.ENABLE_TECH_PREVIEW_FEATURE.pipelines = true; # vi /etc/origin/master/master-config.yaml jenkinsPipelineConfig:   autoProvisionEnabled: true   templateNamespace: openshift   templateName: jenkins-ephemeral   serviceName: jenkins assetConfig:   extensionScripts:     - /etc/origin/tech-preview/pipelines.js Restart OpenShift master  # systemctl restart atomic-openshift-master Setup permissions for build pipeline  In this case we have three projects. Both the integration and production projects need to pull images from development. In addition jenkins service account in development where jenkins will run needs access to both integration and production projects.  # oc policy add-role-to-user edit system:serviceaccount:dev:jenkins -n prod # oc policy add-role-to-user edit system:serviceaccount:dev:jenkins -n int # oc policy add-role-to-user edit system:serviceaccount:dev:jenkins -n dev # oc policy add-role-to-user system:image-puller system:serviceaccount:int:default -n dev # oc policy add-role-to-user system:image-puller system:serviceaccount:prod:default -n dev Reconsile roles  In case you did an upgrade from OpenShift roles might need to be reconsiled. If you have problems or issues I would recommend these steps.  # oadm policy reconcile-cluster-roles --confirm # oadm policy reconcile-cluster-role-bindings --confirm # oadm policy reconcile-sccs --confirm Create Jenkins Build Pipeline  # vi pipeline.json {     \"kind\": \"List\",     \"apiVersion\": \"v1\",     \"metadata\": {},     \"items\": [{         \"kind\": \"BuildConfig\",         \"apiVersion\": \"v1\",         \"metadata\": {             \"name\": \"nodejs-pipeline-master\",             \"labels\": {                 \"app\": \"nodejs-integration\"             },             \"annotations\": {                 \"pipeline.alpha.openshift.io/uses\": \"[{\\\"name\\\": \\\"master\\\", \\\"namespace\\\": \\\"\\\", \\\"kind\\\": \\\"DeploymentConfig\\\"}]\"             }         },         \"spec\": {             \"triggers\": [{                 \"type\": \"GitHub\",                 \"github\": {                     \"secret\": \"EgXVqyOOobmMzjVzQHSh\"                 }             }, {                 \"type\": \"Generic\",                 \"generic\": {                     \"secret\": \"bz6uJc9u-0-58EoYKgL3\"                 }             }],             \"source\": {                 \"type\": \"Git\",                 \"git\": {                     \"uri\": \"https://github.com/ktenzer/nodejs-ex.git\",                     \"ref\": \"master\"                 }             },             \"strategy\": {                 \"type\": \"JenkinsPipeline\",                 \"jenkinsPipelineStrategy\": {                     \"jenkinsfilePath\": \"jenkins-pipeline.dsl\"                 }             }         }     }, {         \"kind\": \"BuildConfig\",         \"apiVersion\": \"v1\",         \"metadata\": {             \"name\": \"nodejs-pipeline-v2\",             \"labels\": {                 \"app\": \"nodejs-integration\"             },             \"annotations\": {                 \"pipeline.alpha.openshift.io/uses\": \"[{\\\"name\\\": \\\"v2\\\", \\\"namespace\\\": \\\"\\\", \\\"kind\\\": \\\"DeploymentConfig\\\"}]\"             }         },         \"spec\": {             \"triggers\": [{                 \"type\": \"GitHub\",                 \"github\": {                     \"secret\": \"EgXVqyOOobmMzjVzQHSh\"                 }             }, {                 \"type\": \"Generic\",                 \"generic\": {                     \"secret\": \"bz6uJc9u-0-58EoYKgL3\"                 }             }],             \"source\": {                 \"type\": \"Git\",                 \"git\": {                     \"uri\": \"https://github.com/ktenzer/nodejs-ex.git\",                     \"ref\": \"v2\"                 }             },             \"strategy\": {                 \"type\": \"JenkinsPipeline\",                 \"jenkinsPipelineStrategy\": {                     \"jenkinsfilePath\": \"jenkins-pipeline.dsl\"                 }             }         }     }] }  # oc create -f pipeline.json Setup Pipeline for nodejs application master branch  Here we setup v1 of application in development project. Under Build-&gt;pipelines select the nodejs-pipeline-master. On right select actions-&gt;edit. You will need to change \"Jenkins Type\" to inline. Once you have selected inline copy/paste below Jenkins DSL:  node { stage 'build' openshiftBuild(buildConfig: 'v1', showBuildLogs: 'true') stage 'deploy development' openshiftVerifyDeployment(deploymentConfig: 'v1') stage 'promote to int' openshiftTag(alias: 'false', apiURL: '', authToken: '', destStream: 'helloworld', destTag: 'v1', destinationAuthToken: '', destinationNamespace: 'int', namespace: 'dev', srcStream: 'v1', srcTag: 'latest', verbose: 'false') openshiftTag(alias: 'false', apiURL: '', authToken: '', destStream: 'acceptance', destTag: 'latest', destinationAuthToken: '', destinationNamespace: 'int', namespace: 'int', srcStream: 'helloworld', srcTag: 'v1', verbose: 'false') stage 'deploy int' openshiftVerifyDeployment(namespace: 'int', deploymentConfig: 'acceptance') openshiftScale(namespace: 'int', deploymentConfig: 'acceptance',replicaCount: '1') stage 'promote to production' openshiftTag(alias: 'false', apiURL: '', authToken: '', destStream: 'helloworld', destTag: 'v1', destinationAuthToken: '', destinationNamespace: 'prod', namespace: 'int', srcStream: 'helloworld', srcTag: 'v1', verbose: 'false') openshiftTag(alias: 'false', apiURL: '', authToken: '', destStream: 'production', destTag: 'latest', destinationAuthToken: '', destinationNamespace: 'prod', namespace: 'prod', srcStream: 'helloworld', srcTag: 'v1', verbose: 'false') stage 'deploy production' openshiftVerifyDeployment(namespace: 'prod', deploymentConfig: 'production') openshiftScale(namespace: 'prod', deploymentConfig: 'production', replicaCount: '4') } Setup Pipeline for nodejs application v2 branch  Here we setup v2 of application in development project. Under Build-&gt;pipelines select the nodejs-pipeline-v2. On right select actions-&gt;edit. You will need to change \"Jenkins Type\" to inline. Once you have selected inline copy/paste below Jenkins DSL:  node { stage 'build' openshiftBuild(buildConfig: 'v2', showBuildLogs: 'true') stage 'deploy development' openshiftVerifyDeployment(deploymentConfig: 'v2') stage 'promote to int' openshiftTag(alias: 'false', apiURL: '', authToken: '', destStream: 'helloworld', destTag: 'v2', destinationAuthToken: '', destinationNamespace: 'int', namespace: 'dev', srcStream: 'v2', srcTag: 'latest', verbose: 'false') openshiftTag(alias: 'false', apiURL: '', authToken: '', destStream: 'acceptance', destTag: 'latest', destinationAuthToken: '', destinationNamespace: 'int', namespace: 'int', srcStream: 'helloworld', srcTag: 'v2', verbose: 'false') stage 'deploy int' openshiftVerifyDeployment(namespace: 'int', deploymentConfig: 'acceptance') openshiftScale(namespace: 'int', deploymentConfig: 'acceptance',replicaCount: '1') stage 'promote to production' openshiftTag(alias: 'false', apiURL: '', authToken: '', destStream: 'helloworld', destTag: 'v2', destinationAuthToken: '', destinationNamespace: 'prod', namespace: 'int', srcStream: 'helloworld', srcTag: 'v2', verbose: 'false') openshiftTag(alias: 'false', apiURL: '', authToken: '', destStream: 'production', destTag: 'latest', destinationAuthToken: '', destinationNamespace: 'prod', namespace: 'prod', srcStream: 'helloworld', srcTag: 'v2', verbose: 'false') stage 'deploy production' openshiftVerifyDeployment(namespace: 'prod', deploymentConfig: 'production') openshiftScale(namespace: 'prod', deploymentConfig: 'production', replicaCount: '4') } Optional: OpenShift Upgrade from 3.2  If you have an OpenShift 3.2 environment and want to try the integrated jenkins build pipleline follow the steps below.  OpenShift Upgrade  subscription-manager repos --disable=\"rhel-7-server-ose-3.2-rpms\" \\     --enable=\"rhel-7-server-ose-3.3-rpms\"\\     --enable=\"rhel-7-server-extras-rpms\" yum clean all yum update atomic-openshift-utils # ansible-playbook -i /etc/ansible/hosts /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-cluster/upgrades/v3_3/upgrade.yml # systemctl reboot Update images  # NS=openshift;for img in `oc get is -n ${NS}|awk '{print $1;}'|grep -v NAME`; do oc import-image -n ${NS} $img;done Verify  # oc get nodes  NAME STATUS AGE  ose3-master.lab.com Ready 115d # oc get -n default dc/docker-registry -o json | grep \\\"image\\\"  \"image\": \"openshift3/ose-docker-registry:v3.3.1.3\", # oc get -n default dc/router -o json | grep \\\"image\\\"  \"image\": \"openshift3/ose-haproxy-router:v3.3.1.3\", Upgrade Aggregate Logging  # oc project logging # oc apply -n openshift -f \\     /usr/share/openshift/examples/infrastructure-templates/enterprise/logging-deployer.yaml # oc process logging-deployer-account-template | oc apply -f - # oadm policy add-cluster-role-to-user oauth-editor \\        system:serviceaccount:logging:logging-deployer Upgrade Cluster Metrics  To upgrade metrics I recommend just deleting and re-creating. Before doing so you should prune the existing images so the new 3.3 images are downloaded.  Summary  In this article we have seen how to configure an OpenShift 3.3 all-in-one lab environment. We have also seen how install and configuration can be adapted through ansible playbook. We have seen how to configure various DNS options required by OpenShift. It should be repeated that most OpenShift problems are a direct result of improper DNS setup! We have seen how to integrate OpenShift in CloudForms, setup aggregate logging and how to configure metrics using hawkular. Finally we have seen how to create a Jenkins build pipeline using a nodejs application across three stages. As always if you have any feedback please share.  Happy OpenShifting!  (c) 2016 Keith Tenzer  ","categories": ["OpenShift"],
        "tags": ["Containers","Docker","Jenkins","Kubernetes","OpenShift"],
        "url": "/openshift/openshift-enterprise-3-3-all-in-one-lab-environment-with-jenkins-build-pipeline/",
        "teaser": null
      },{
        "title": "Deploying OpenShift Enterprise from Ansible Tower",
        "excerpt":"    Overview  In this article we will look at how to use Ansible Tower to deploy and manage OpenShift environments. OpenShift of course uses Ansible as its deployment and configuration tool already. While that is great, using Tower provides several major advantages:   UI for OpenShift deployment and configuration management Secure store for credentials RBAC and ability to delegate different responsibilities for OpenShift deployments Easy to visualize and manage multiple OpenShift environments and even versions of OpenShift History, audit trail and detailed logging in central location for all OpenShift environments and deployments    Prepare OpenShift Environment  In this example we will be doing an all-in-one deployment of OpenShift. The following steps should be done on OpenShift masters and nodes. Again here we just have one node since it is an all-in-one.  CONFIGURE A VMs WITH FOLLOWING:   RHEL 7.2 2 CPUs 4096 RAM 30GB disk for OS 25GB disk for docker images  REGISTER VALID SUBSCRIPTION  # subscription-manager register # subscription-manager attach --pool=843298293829382 # subscription-manager repos --disable=\"*\" #subscription-manager repos \\     --enable=\"rhel-7-server-rpms\" \\     --enable=\"rhel-7-server-extras-rpms\" \\     --enable=\"rhel-7-server-ose-3.3-rpms\" INSTALL REQUIRED TOOLS  # yum install -y wget git net-tools bind-utils iptables-services bridge-utils bash-completion UPDATE  # yum update -y RESTART OPENSHIFT MASTER  # systemctl reboot CONFIGURE DOCKER  # yum install -y docker-1.10.3 ENABLE DOCKER DAEMON TO PULL FROM OPENSHIFT REGISTRY  # vi /etc/sysconfig/docker OPTIONS='--selinux-enabled --insecure-registry 172.30.0.0/16' SETUP DOCKER STORAGE FOR OPENSHIFT REGISTRY  Note: we will use the second disk for configuring docker storage.  # cat &lt;&lt;EOF &gt; /etc/sysconfig/docker-storage-setup DEVS=/dev/vdb VG=docker-vg EOF # docker-storage-setup ENABLE AND START DOCKER DAEMON  # systemctl enable docker # systemctl start docker Import OpenShift inventory into Ansible Tower  These steps should be done directly on the host running Ansible Tower.  Create Inventory in Ansible Tower  Under inventories add a new inventory.    Create directors for OpenShift inventory  # mkdir /root/ose3 Setup ansible-hosts file  # vi /root/ose3/ansible-hosts ########################## ### OSEv3 Server Types ### ########################## [OSEv3:children] masters nodes etcd  ############################## ### host group for masters ### ############################## [masters] ose3-master2.lab.com  ################################### ### host group for etcd servers ### ################################### [etcd] ose3-master2.lab.com  ################################################## ### host group for nodes, includes region info ### ################################################## [nodes] ose3-master2.lab.com openshift_schedulable=True Create directory for group_vars  Note: this is required because Tower import tool does not yet support [groupname:vars] directly in inventory file.  # mkdir /root/ose3/group_vars Setup OpenShift parameters using group_vars file  # vi /root/ose3/group_Vars/OSEv3 ansible_ssh_user: root os_sdn_network_plugin_name: 'redhat/openshift-ovs-subnet' deployment_type: openshift-enterprise openshift_master_default_subdomain: apps.lab.com openshift_master_identity_providers: [{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}] openshift_node_kubelet_args: {'maximum-dead-containers': ['100'], 'maximum-dead-containers-per-container': ['2'], 'minimum-container-ttl-duration': ['10s'], 'max-pods': ['110'], 'image-gc-high-threshold': ['90'], 'image-gc-low-threshold': ['80']} logrotate_scripts: [{\"name\": \"syslog\", \"path\": \"/var/log/cron\\n/var/log/maillog\\n/var/log/messages\\n/var/log/secure\\n/var/log/spooler\\n\", \"options\": [\"daily\", \"rotate 7\", \"compress\", \"sharedscripts\", \"missingok\"], \"scripts\": {\"postrotate\": \"/bin/kill -HUP `cat /var/run/syslogd.pid 2&gt; /dev/null` 2&gt; /dev/null || true\"}}] openshift_docker_options: \"--log-opt max-size=1M --log-opt max-file=3\" openshift_node_iptables_sync_period: 5s openshift_master_pod_eviction_timeout: 3m osm_controller_args: {'resource-quota-sync-period': ['10s']} osm_api_server_args: {'max-requests-inflight': ['400']} openshift_use_dnsmasq: false Import OpenShift inventory  # tower-manage inventory_import --source=/root/ose --inventory-name=\"OSE_3.3\" --overwrite --overwrite-vars After import is complete you should see inventory. Under OSE_3.3 inventory, a group called OSEv3 should be visible. If you edit the OSEv3 group you should see the variables used to drive OpenShift deployment. Here you can easily change things in order to update or change OpenShift deployment.    Under the OSEv3 group you should see all the OpenShift server groups and under those the actual systems.    Configure Ansible Tower  Create Project in Tower  Under projects add a new project. Add Github URL to ansible-openshift project. Ensure you add the correct branch. OpenShift v3.3 correlates to branch release-1.3. You should add a separate project for every release.  Note: make sure you check what version of ansible-openshift correlates to version of OpenShift you want to deploy!    Add credentials for OpenShift nodes  Under settings-&gt;credentials add new credentials called OSE.  Note: In this example I added the root user and password but you can use non-root user or ssh keys instead of password. In fact there is already a group_var parameter to use sudo.    Add job template  Under job templates add a new template. Select the inventory, project and machine credentials. Select playbooks/byo/config.yaml for the playbook.    Deploy OpenShift  To deploy or update your OpenShift deployment you simply need to run the playbook from Tower by clicking the rocket next to your job template.    You can follow the deployment status by looking at the job in Tower.    Summary  In this article we looked at how to deploy OpenShift using Ansible Tower. The default method for deploying and managing OpenShift is Ansible Core. Tower however gives you a lot of advantages providing central management, credentials store, RBAC, maintain multiple versions or multiple OpenShift environments and of course the more you do with ansible the more sense it makes to start using Tower. I hope you found this article informative and interesting. Looking forward to hearing your thoughts and feedback.  Happy OpenShifting!  (c) 2016 Keith Tenzer  ","categories": ["OpenShift"],
        "tags": ["Ansible","Tower","Automation","OpenShift","Kubernetes"],
        "url": "/openshift/deploying-openshift-enterprise-from-ansible-tower/",
        "teaser": null
      },{
        "title": "Red Hat Ceph Storage 2.0 Lab + Object Storage Configuration Guide",
        "excerpt":"  Overview  Ceph has become the defacto standard for software-defined storage. Ceph is 100% opensource, built on open standards and as such is offered by many vendors not just Red Hat. If you are new to Ceph or software-defined storage, I would recommend the following article before proceeding to understand some high-level concepts:  Ceph - the future of storage  In this article we will configure a Red Hat Ceph 2.0 cluster and set it up for object storage. We will configure RADOS Gateway (RGW), Red Hat Storage Console (RHCS) and show how to configure the S3 and Swift interfaces of the RGW. Using python we will access both the S3 and Swift interfaces.  If you are interested in configuring Ceph for OpenStack see the following article:  OpenStack - Integrating Ceph as Storage Backend    Prerequisites  Ceph has a few different components to be aware of: monitors (mons), storage or osd nodes (osds), Red Hat Storage Console (RHSC), RHSC agents, Calamari, clients and gateways.  Monitors - maintain maps (crush, pg, osd, etc) and cluster state. Monitors use Paxos to establish consensus.  Storage or OSD Node - Provides one or more OSDs. Each OSD represents a disk and has a running daemon process controlled by systemctl. There are two types of disks in Ceph: data and journal. The journal enable Ceph to commit small writes quickly and guarantees atomic compound operations. Journals can be collocated with data on same disks or separate. Splitting journals out to SSDs provides higher performance for certain use cases such as block.  Red Hat Storage Console (optional) - UI and dashboard that can monitor multiple clusters and monitor not only Ceph but Gluster as well.  RHSC Agents (optional) - Each monitor and osd node runs an agent that reports to the RHSC.  Calamari (optional) - Runs on one of the monitors to get statistics on ceph cluster and provides rest endpoint. RHSC talks to calamari.  Clients - Ceph provides an RBD (RADOS Block Device) client for block storage, CephFS for file storage and a fuse client as well. The RADOS GW itself can be viewed as a Ceph client. Each client requires authentication if cephx is enabled. Cephx is based on kerberos.  Gateways (optional) - Ceph is based on RADOS (Reliable Atomic Distributed Object Store). The RADOS Gateway is a web server that provides s3 and swift endpoints and sends those requests to Ceph via RADOS. Similarily there is an ISCSI Gateway that provides ISCSI target to clients and talks to Ceph via RADOS. Ceph itself is of course an object store that supports not only object but file and block clients as well.  Red Hat recommends at minimum three monitors and 10 storage nodes. All of which should be physical not virtual machines. For the gateways and RHSC, VMs can be used. Since the purpose of this article is about building a lab environment we are doing everything on just three VMs. The VMs should be configured as follows with Red Hat Enterprise Linux (7.2 or 7.3):   ceph1: 4096 MB RAM, 2 Cores, 30GB root disk, 2 X 100 GB data disk, 192.168.122.81/24. ceph2: 4096 MB RAM, 2 Cores, 30GB root disk , 2 X 100 GB data disk, 192.168.122.82/24. ceph3: 4096 MB RAM, 2 Cores, 30GB root disk, 2 X 100 GB data disk, 192.168.122.83/24.  Note: this entire environment runs on my 12GB thinkpad laptop. If memory is tight you can cut ceph2 down to 2048MB RAM.  The roles will be devided across the nodes as follows:   Ceph1: RHSC, Rados Gateway, Monitor and OSD Ceph2: Calamari, Monitor and OSD Ceph3: Monitor and OSD    Install Ceph Cluster  Register subscription and enable repositories.  # subscription-manager register # subscription-manager list --available # subscription-manager attach --pool=8a85f981weuweu63628333293829 # subscription-manager repos --disable=* # subscription-manager repos --enable=rhel-7-server-rpms # subscription-manager repos --enable=rhel-7-server-rhceph-2-mon-rpms # subscription-manager repos --enable=rhel-7-server-rhceph-2-osd-rpms # subscription-manager repos --enable=rhel-7-server-rhscon-2-agent-rpms Note: If you are using centos you will need to install ansible and get the ceph-ansible playbooks from github.  Disable firewall  Since this is a lab environment we can make life a bit easier. If you are interested in enabling firewall then follow official documentation here.  #systemctl stop firewalld #systemctl disable firewalld Configure NTP  Time synchronization is absolutely critical for Ceph. Make sure it is reliable.  # yum install -y ntp # systemctl enable ntpd # systemctl start ntpd Test to ensure ntp is working properly.  # ntpq -p Update hosts file  If dns is working you can skip this step.  #vi /etc/hosts 192.168.122.81 ceph1.lab.com ceph1 192.168.122.82 ceph2.lab.com ceph2 192.168.122.83 ceph3.lab.com ceph3 Create Ansible User  Ceph 2.0 now uses ansible to deploy, configure and update. A user is required that has sudo permissions.  # useradd ansible # passwd ansible #cat &lt;&lt; EOF &gt; /etc/sudoers.d/ansible ansible ALL = (root) NOPASSWD:ALL Defaults:ansible !requiretty EOF Enable repositories for RHSC  # subscription-manager repos --enable=rhel-7-server-rhscon-2-installer-rpms # subscription-manager repos --enable=rhel-7-server-rhscon-2-main-rpms Install Ceph-Ansible  # yum install -y ceph-ansible Setup ssh keys for ansible user  # su - ansible $ ssh-keygen $ ssh-copy-id ceph1 $ ssh-copy-id ceph2 $ ssh-copy-id ceph3 $ mkdir ~/ceph-ansible-keys Update Ansible Hosts file  $ sudo vi /etc/ansible/hosts  [mons] ceph1 ceph2 ceph3  [osds] ceph1 ceph2 ceph3  [rgws] ceph1 ceph3 Update Ansible Group Vars  The ceph configuration is maintained by group vars. By default samples are provided. We need to copy these and then update them. For this deployment we need to update group vars for all, mons and osds.  You can find these group var files in github.  $ cd /usr/share/ceph-ansible/group_vars Update general group vars  $ cp all.sample all $ vi all fetch_directory: /home/ansible/ceph-ansible-keys cluster: ceph ceph_stable_rh_storage: true ceph_stable_rh_storage_cdn_install: true generate_fsid: true cephx: true monitor_interface: eth0 journal_size: 1024 public_network: 192.168.122.0/24 cluster_network: \"\" osd_mkfs_type: xfs osd_mkfs_options_xfs: -f -i size=2048 radosgw_frontend: civetweb radosgw_civetweb_port: 8080 radosgw_keystone: false Update monitor group vars  $ cp mons.sample mons Update osd group vars  $ cp osds.sample osds $ vi osds osd_auto_discovery: true journal_collocation: true Update rados gateway group vars  We are just going with defaults here so no changes.  $ cp rgws.sample rgws Run Ansible playbook  $ cd /usr/share/ceph-ansible $ sudo cp site.yml.sample site.yml $ ansible-playbook site.yml -vvvv If everything is successful you should see message similar to below. If something fails simple fix problem and re-run playbook till it succeeds.  PLAY RECAP ********************************************************************  ceph1 : ok=370 changed=17 unreachable=0 failed=0  ceph2 : ok=286 changed=14 unreachable=0 failed=0  ceph3 : ok=286 changed=13 unreachable=0 failed=0 Check Ceph Health  You should see HEALTH_OK. If it is not ok then you can run \"ceph health detail\" to get more information.  $ sudo ceph -s cluster 1e0c9c34-901d-4b46-8001-0d1f93ca5f4d health HEALTH_OK monmap e1: 3 mons at {ceph1=192.168.122.81:6789/0,ceph2=192.168.122.82:6789/0,ceph3=192.168.122.83:6789/0} election epoch 6, quorum 0,1,2 ceph1,ceph2,ceph3 osdmap e14: 3 osds: 3 up, 3 in flags sortbitwise pgmap v26: 104 pgs, 6 pools, 1636 bytes data, 171 objects 103 MB used, 296 GB / 296 GB avail 104 active+clean Configure erasure coded pool for RADOSGW  By default the pool default.rgw.data.root contains data for a RADOSGW and it is configured for replication not erasure coding. In order to change to erasure coding you need to delete the pool and re-create it. For object storage we usually recommend erasure coding as it is much more efficient and brings down costs.  # ceph osd pool delete default.rgw.data.root default.rgw.data.root --yes-i-really-really-mean-it Ceph supports many different erasure coding schemes.  # ceph osd erasure-code-profile ls default k4m2 k6m3 k8m4 The default profile is 2+1. Since we only have three nodes this is the only profile that could actually work so we will use that.  # ceph osd erasure-code-profile get default k=2 m=1 plugin=jerasure technique=reed_sol_van Create new erasure coded pool using 2+1.  # ceph osd pool create default.rgw.data.root 128 128 erasure default Here we are creating 128 placement groups for the pool. This was calculated using the pg calculation tool: https://access.redhat.com/labs/cephpgc/  Configure rados gateway s3 user  The rados gateway was installed and configured on ceph1 per ansible however a user needs to be created for s3. This is not part of Ansible installation by default.  #radosgw-admin user create --uid=\"s3user\" --display-name=\"S3user\"  {  \"user_id\": \"s3user\",  \"display_name\": \"S3user\",  \"email\": \"\",  \"suspended\": 0,  \"max_buckets\": 1000,  \"auid\": 0,  \"subusers\": [],  \"keys\": [  {  \"user\": \"s3user\",  \"access_key\": \"PYVPOGO2ODDQU24NXPXZ\",  \"secret_key\": \"pM1QULv2YgAEbvzFr9zHRwdQwpQiT9uJ8hG6JUZK\"  }  ],  \"swift_keys\": [],  \"caps\": [],  \"op_mask\": \"read, write, delete\",  \"default_placement\": \"\",  \"placement_tags\": [],  \"bucket_quota\": {  \"enabled\": false,  \"max_size_kb\": -1,  \"max_objects\": -1  },  \"user_quota\": {  \"enabled\": false,  \"max_size_kb\": -1,  \"max_objects\": -1  },  \"temp_url_keys\": []  } Test S3 Access  In order to test s3 access we will use a basic python script that uses the boto library.  # pip install boto Create script and update with information from above. The script is located in github.  # cd /root # vi s3upload.py #!/usr/bin/python  import sys  import boto import boto.s3.connection from boto.s3.key import Key  access_key = 'PYVPOGO2ODDQU24NXPXZ' secret_key = 'pM1QULv2YgAEbvzFr9zHRwdQwpQiT9uJ8hG6JUZK' rgw_hostname = 'ceph1' rgw_port = 8080 local_testfile = '/tmp/testfile' bucketname = 'mybucket'   conn = boto.connect_s3( \taws_access_key_id = access_key, \taws_secret_access_key = secret_key, \thost = rgw_hostname, \tport = rgw_port, \tis_secure=False, \tcalling_format = boto.s3.connection.OrdinaryCallingFormat(), \t)  def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = '#'):     percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))     filledLength = int(length * iteration // total)     bar = fill * filledLength + '-' * (length - filledLength)     print('\\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix))     if iteration == total:          print()  def percent_cb(complete, total):     printProgressBar(complete, total)  bucket = conn.create_bucket('mybucket') for bucket in conn.get_all_buckets(): \tprint \"{name}\\t{created}\".format( name = bucket.name, created = bucket.creation_date,)  bucket = conn.get_bucket(bucketname)   k = Key(bucket) k.key = 'my test file' k.set_contents_from_filename(local_testfile, cb=percent_cb, num_cb=20)  Change permissions and run script  # chmod 755 s3upload.py #./s3upload.py Watch the 'ceph -s' command.  # watch ceph -s Every 2.0s: ceph -s Thu Feb 2 19:09:58 2017  cluster 1e0c9c34-901d-4b46-8001-0d1f93ca5f4d  health HEALTH_OK  monmap e1: 3 mons at {ceph1=192.168.122.81:6789/0,ceph2=192.168.122.82:6789/0,ceph3=192.168.122.83:6789/0}  election epoch 36, quorum 0,1,2 ceph1,ceph2,ceph3  osdmap e102: 3 osds: 3 up, 3 in  flags sortbitwise  pgmap v1543: 272 pgs, 12 pools, 2707 MB data, 871 objects  9102 MB used, 287 GB / 296 GB avail  272 active+clean  client io 14706 kB/s wr, 0 op/s rd, 32 op/s wr Configure rados gateway swift user  In order to enable access to object store using swift you need to create a sub-user or nested user for swift access. This user is created under already existing user. We will use the s3user already created. From outside the swift user is it's own user.  # radosgw-admin subuser create --uid=s3user --subuser=s3user:swift --access=full { \"user_id\": \"s3user\", \"display_name\": \"S3user\", \"email\": \"\", \"suspended\": 0, \"max_buckets\": 1000, \"auid\": 0, \"subusers\": [ { \"id\": \"s3user:swift\", \"permissions\": \"full-control\" } ], \"keys\": [ { \"user\": \"s3user\", \"access_key\": \"PYVPOGO2ODDQU24NXPXZ\", \"secret_key\": \"pM1QULv2YgAEbvzFr9zHRwdQwpQiT9uJ8hG6JUZK\" } ], \"swift_keys\": [ { \"user\": \"s3user:swift\", \"secret_key\": \"vzo0KErmx5I9zaE3Y7bIOGGbJaECpJmNtNikFEYh\" } ], \"caps\": [], \"op_mask\": \"read, write, delete\", \"default_placement\": \"\", \"placement_tags\": [], \"bucket_quota\": { \"enabled\": false, \"max_size_kb\": -1, \"max_objects\": -1 }, \"user_quota\": { \"enabled\": false, \"max_size_kb\": -1, \"max_objects\": -1 }, \"temp_url_keys\": [] } Generate keys  This is done by default but in case you want to generate keys you can do so at anytime.  # radosgw-admin key create --subuser=s3user:swift --key-type=swift --gen-secret Test swift access  We will use python again but this time for swift cli which is written in python.  # pip install --upgrade setuptools # pip install python-swiftclient List buckets using swift  # swift -A http://192.168.122.81:8080/auth/1.0 -U s3user:swift -K 'DvvYI2uzd9phjHNTa4gag6VkWCrX29M17A0mATRg' list If things worked then you should see bucket called 'mybucket'.  Configure Red Hat Storage Console (RHSC)  Next we will configure the storage console and import the existing cluster. Ansible does not take care of setting up RHSC.  Install RHSC  # yum install -y rhscon-core rhscon-ceph rhscon-ui Configure RHSC  # skyring-setup Would you like to create one now? (yes/no): yes  Username (leave blank to use 'root'):  Email address:  Password:  Password (again):  Superuser created successfully.  Installing custom SQL ...  Installing indexes ...  Installed 0 object(s) from 0 fixture(s)  ValueError: Type carbon_var_lib_t is invalid, must be a file or device type  Created symlink from /etc/systemd/system/multi-user.target.wants/carbon-cache.service to /usr/lib/systemd/system/carbon-cache.service.  Job for httpd.service failed because the control process exited with error code. See \"systemctl status httpd.service\" and \"journalctl -xe\" for details.  Please enter the FQDN of server [ceph1.lab.com]:  Skyring uses HTTPS to secure the web interface.  Do you wish to generate and use a self-signed certificate? (Y/N): y  -------------------------------------------------------  Now the skyring setup is ready!  You can start/stop/restart the server by executing the command  systemctl start/stop/restart skyring  Skyring log directory: /var/log/skyring  URLs to access skyring services  - http://ceph1.lab.com/skyring  - http://ceph1.lab.com/graphite-web  -------------------------------------------------------  Done! Once installation is complete you can access RHSC via web browser.  https://192.168.122.81 user: admin password: admin Configure RHSC Agent  Each ceph monitor and osd node requires an RHSC agent.  Install agent  Likely ansible took care of this but doesn't hurt to check.  # yum install -y rhscon-agent # curl 192.168.122.81:8181/setup/agent/ | bash Setup calamari server  The calamari server runs on one of the monitor nodes. It's purpose is to collect cluster health, events and statistics. In this case we will run the calamari server on ceph2.  # yum install calamari-server # calamari-ctl clear --yes-i-am-sure Gotta love the --yes-i-am-sure flags.  # calamari-ctl initialize --admin-username admin --admin-password admin --admin-email ktenzer@redhat.com Accept nodes in RHSC  https://  Click on tasks icon at top right and accept all hosts.    Once accepting hosts you should see them available.    Import cluster in RHSC  Go to cluster and import a cluster, ensure to select the monitor node running the calamari server. In this case ceph 2.  Note: we could have also deployed Ceph using RHSC that is what new cluster does but that is no fun,    Click import to start the import process.    Cluster now should be visible in RHSC.    RHSC dashboard shows a high-level glimpse of ceph cluster.    Summary  In this article we saw how to deploy a ceph 2.0 cluster from scratch using VMs that can run on your laptop. We enabled object storage through ragosgw and configured both s3 as well as swift access. Finally we setup the Red Hat Storage Console (RHSC) to provide insight into our ceph cluster. This article provides you a good starting point for your journey into ceph and the future of storage which is software-defined as well as object based. Unlike all other storage systems ceph is the only one that is opensource, built on open standards and truly unified (object, block, file). Ceph is supported by many vendors and can run on any x86 hardware, even commodity. What else is the more to say?  Happy Cephing!  (c) 2017 Keith Tenzer  ","categories": ["Ceph"],
        "tags": ["Ceph","ceph-ansible","object storage","radosgw","software-defined storage"],
        "url": "/ceph/red-hat-ceph-storage-2-0-lab-object-storage-configuration-guide/",
        "teaser": null
      },{
        "title": "Storage for Containers Overview - Part I",
        "excerpt":"  Overview  This is a six part series dedicated to container storage. The article series is a collaboration between Daniel Messer (Technical Marketing Manager Storage @RedHat), Keith Tenzer (Solutions Architect @RedHat) and Kapil Arora (Cloud Platform Architect @NetApp). The focus of this article is an overview on storage for containers. In this article we will focus on laying out fundamentals critical to any container storage discussion. In addition we will go into some details on the various solutions that exist today.   Storage for Containers Overview – Part I Storage for Containers using Gluster – Part II Storage for Containers using Container Native Storage – Part III Storage for Containers using Ceph – Part IV Storage for Containers using NetApp ONTAP NAS – Part V Storage for Containers using NetApp SolidFire – Part VI    The Basics  Containers have been around for a really, long time. They first appeared in UNIX systems in early 2000's and have been in Linux since 2007. These containers acted more like virtual machines and provided additional efficiencies but really weren't ground-breaking. In addition they were rather complicated to setup for the average user. When most people talk about containers these days, they are referring to Docker containers. Docker greatly simplified using containers by taking the existing isolation facilities in the Linux kernel (cgroups, IPC/network/file system namespace) and hiding these behind a simple command, \"docker run\". In addition Docker provided a container format, that allows application services to package themselves in a container that in theory will run on any Linux system with a Docker daemon.  Docker enables fast, iterative application development and portability. That is why it's one of the main technologies that enables DevOps. Not only providing additional simplicity in dealing with containers of the past but rather, fundamentally changing the way we build, package, deploy and run applications. It has become the base technology for packaging micro-services.  Docker containers in regards to storage, have their own \"view\" of a file system since the filesystem itself is a Linux namespace. When you launch a Red Hat Enterprise Linux (RHEL) 7 container and list the contents of \"/\" you see what appears to be a normal root filesystem.  [root@rhel7-workstation ~]# docker run -it registry.access.redhat.com/rhel7 bash Unable to find image 'registry.access.redhat.com/rhel7:latest' locally Trying to pull repository registry.access.redhat.com/rhel7 ... latest: Pulling from registry.access.redhat.com/rhel7 154dc369ca0d: Pull complete e6b5b6e3c142: Pull complete Digest: sha256:822cfa544c7c51d8bca1675dfd7ef5b5aaa205e222617f787868516eca2c6acc [root@20ca20ac05a5 /]# ls -ahl / total 4.0K dr-xr-xr-x. 18 root root 260 Mar 3 11:01 . dr-xr-xr-x. 18 root root 260 Mar 3 11:01 .. -rwxr-xr-x. 1 root root 0 Mar 3 11:01 .dockerenv lrwxrwxrwx. 1 root root 7 Feb 22 17:24 bin -&gt; usr/bin dr-xr-xr-x. 2 root root 6 Mar 10 2016 boot drwxr-xr-x. 5 root root 380 Mar 3 11:01 dev drwxr-xr-x. 49 root root 4.0K Mar 3 11:01 etc drwxr-xr-x. 2 root root 6 Feb 22 17:26 home lrwxrwxrwx. 1 root root 7 Feb 22 17:24 lib -&gt; usr/lib lrwxrwxrwx. 1 root root 9 Feb 22 17:24 lib64 -&gt; usr/lib64 drwx------. 2 root root 6 Feb 22 17:23 lost+found drwxr-xr-x. 2 root root 6 Mar 10 2016 media drwxr-xr-x. 2 root root 6 Mar 10 2016 mnt drwxr-xr-x. 2 root root 6 Mar 10 2016 opt dr-xr-xr-x. 289 root root 0 Mar 3 11:01 proc dr-xr-x---. 3 root root 154 Feb 22 17:30 root drwxr-xr-x. 12 root root 172 Feb 22 17:30 run lrwxrwxrwx. 1 root root 8 Feb 22 17:24 sbin -&gt; usr/sbin drwxr-xr-x. 2 root root 6 Mar 10 2016 srv dr-xr-xr-x. 13 root root 0 Mar 3 10:51 sys drwxrwxrwt. 7 root root 132 Feb 22 17:26 tmp drwxr-xr-x. 13 root root 155 Feb 22 17:24 usr drwxr-xr-x. 18 root root 238 Feb 22 17:25 var The container root filesystem's main purpose is for supplying an independent application runtime (at the minimum glibc). Everything else is the Linux Kernel itself, which is of course shared amoung all containers. You can write to this file system and even do destructive things like erasing /etc/passwd however it will stay inside the container. The next time you restart the container everything will be as it was before your changes. Docker implements a layered approach for container storage.  https://access.redhat.com/webassets/avalon/d/Red_Hat_Enterprise_Linux_Atomic_Host-7-Overview_of_Containers_in_Red_Hat_Systems-en-US/images/440aa214d60c93edebd6b4522fffe54f/docker_structure.png  When we issue a \"docker run\" command it starts the platform image. In this example, a minimal RHEL Operating environment inside the container root filesystem. This is read-only. When the container is started, a writeable layer is added on top. This records only the delta information. Compare this user experience to storage snapshots. Imagine if you had a read-only file system mounted from a snapshot and a writeable layer added on top transparently. It will have it's own lineage.  In Docker the writeable layer is discarded once the container terminates. The only way in this case to persist a change is to stop the container, commit the stopped instance to the local list of images and create another instance from that. That is Docker storage in a nutshell.  Docker originally used the \"Union File System\" to layer images and provide a coherent view of that to the container. Over time however, other implementations emerged like OverlayFS, auFS and those based on the existing file systems with integrated snapshot capabilities like btrfs. Fedora, CentOS and RHEL are implementing this entirely within device-mapper these days.  Why do we care about this? Many applications are certified or tested with a specific runtime i.e. RHEL 7. Before it meant you needed not only the runtime but the entire RHEL 7 OS. It meant our IT infrastructure teams had to support N variants and standardization was impossible. Imagine the shipping industry before shipping containers came along? That is IT today, little portability and standardization. Now with Docker we have a much more flexible, portable means for containing applications that allows complete standardization. If we standardize, we can automate and if we achieve that we have met some of the principles required by DevOps.  The two most important things to understand from a Docker storage perspective in terms of storage are:   Docker container images are ephemeral. When the running container is terminated (which happens very frequently in the container world) the storage inside the container is lost. Every overlay filesystem is very slow for write performance.  These are also the reasons many stayed away from data-intensive workloads in containers in the beginning. Since however there are new solutions for properly handling storage but first impressions die slowly so let us bust some of these myths.  Container Storage Myths  Myth 1: Containerized Applications should be stateless  This really comes from what we learned about Docker storage and platforms such as CloudFoundry that are container-based but only handle stateless applications.  The true value of containers is to enable IT industrialization, meaning a standardized, automated method for building, deploying, upgrading and rolling out new application versions across dissimilar infrastructure platforms. Why should this be limited to just stateless service? Why not include all the entire application stack including databases, middleware, messaging, etc.  Solutions for managing storage and as such providing applications a place to store state exist in both Docker and Kubernetes. We will look into these solutions further in this article.  Myth 2: Persisting Data in Docker Containers is Slow  This is true if you write directly to the Docker container image and as mentioned that should be avoided. As an alternative Docker provides ability to mount a directory on the container host in the container. This called a bind-mount and usually is as performant as normal file systems. As such any locally available file system storage can be used as local sub-directories of the host's root file system, NFS, SAN volumes formatted with xfs, basically anything.  Myth 3: SAN is the Best Choice for Storage  SAN is actually the worst choice besides using the Docker container itself for storage. SAN is expensive, highly complicated and requires a sophisticated cluster locking mechanism to handle access to LUNs in shared environment. SAN was not designed to fan out thousands of individual LUNs for containers at scale either. Forget SAN and forget repeating what we have done the last 20 years in storage. It doesn't fit anymore in the containerized world.  Myth 4: Persisting Data in Containers Requires Shared Storage  Not true anymore. At least not always. Containers enable you to bring compute and storage close together and using local storage is a viable option depending on the application. Many applications are moving from structured SQL databases such as Oracle, MariaDB, PostgreSQL, etc to unstructured NoSQL databases such as CouchBase, Mongo, Cassandra, etc. These database engines bring their own replication and scale-out mechanism. This has a huge effect on storage. NoSQL databases don't need shared storage, they are sharded and re-balance data within a cluster as changes occur. That is to say the database is providing data management. Here local storage may be faster and certainly cheaper than shared storage. Certainly though, the requirements for external storage remain. With data-intensive workloads running in containers the limited local storage capacity of the container host via SATA or SAS is quickly exhausted. Most notably as well the performance is limited to how many storage devices you can put in a single server.  Myth 5: Storage should be Centrally Managed for Containers  The main point of container technology is as enabler to DevOps. What is goal of DevOps? To build, deploy and run applications consistently and with high-degree of automation across all defined application stages (Development-&gt;Test-&gt;QA-&gt;Production).  This is what is meant when we refer to continuous deployment and integration. In order for this to work production teams, such as a storage team, need to be firmly integrated in the process. If a DevOps team needs to bother the storage team or open a ticket for something each time they have storage needs, we won't won't leverage the agility or speed that DevOps promises.  The goal is to allow the container platform to be able to dynamically orchestrate the infrastructure around the application, as needed, including storage. DevOps teams should be given quotas and when they require resources like storage capacity they simply get it, if it fits their quota. The container platform should communicate with the underlying storage and provision storage on the fly. This is known as dynamic provisioning and it is supported by Kubernetes, a very popular and in fact the defacto open-source standard container orchestration engine.  Container Orchestration and Storage  Docker provided us a standard packaging format, application portability and became the enabler technology for building microservices. While it is great to build, package and run applications using Docker, a lot was still missing. Docker is just a technology, not a platform. In order to operate complex applications and handle upgrades or rollouts of many Docker images across many hosts, an orchestration layer was required.  Fortunately there were some very smart people, you guessed it, at Google, who agreed that this is indeed a great idea. In fact, Google has been using containers for more than a decade, long before Docker was born. They implemented orchestration in an internal project called Borg, open-sourced those ideas under a new open-source project called Kubernetes. Today Red Hat and Google are the two main contributors behind Kubernetes. You can see the code contributors on stackalytics. Kubernetes is also the underpinning technology along with Docker used in Red Hat OpenShift Container Platform.  Kubernetes is a container orchestration engine that supports stateful applications and database workloads with storage orchestration. An application owner or developer can simply state that his application or particular micro-service needs storage of a certain capacity. Kubernetes accepts this request and takes care of the rest, from provisioning to ensuring the storage is available as a local file system mount wherever the container happens to run. Let's look at how this works in a bit more detail.  PVs, PVCs, and Storage Classes  Everything in Kubernetes including a storage request, is an object that is described by YAML or JSON. Kubernetes enables infrastructure-as-code so just like code, infrastructure blueprints that provide application requirement can be stored in simple files sitting in a source-code management system like Git.  Kubernetes has a few concepts for managing storage: PersistentVolumes (PVs), PersistentVolumeClaims (PVCs) and Storage Classes. A PVC is a means of requesting storage capacity. It results simply in a mapping of a PV, (the object representing storage) to a specific container or group of containers. Container storage requests are fulfilled if a free PV is available and meets the requirements defined in the PVC. If that occurs a container is granted a PVC and that PV which maps to a filesystem on the container host will be bind-mounted to the container itself. The PVC ensures that wherever a container starts it always gets it's correct volume and if that condition isn't met the container can't start. Storage Classes are providers of PVs that PVCs can explicitly reach out to. They can be thought of as similar to storage tiers. In addition modern storage classes enable dynamic provisioning. This means that once a PVC is issued to such a storage class, storage is provisioned on the storage system, mounted on the host where the container is starting, a PV is created, PVC reserves the PV and then that PV mountpoint is bind-mounted inside the container. In the beginning, especially with NFS, PVs needed to be manually pre-provisioned by admins and there was no concept of dynamic provisioning.  Let's see how this looks like with simple example for creating a PV from Kubernetes documentation. Below a static PV is being created that maps to an NFS server and the export must already exist.    apiVersion: v1   kind: PersistentVolume   metadata:     name: pv0003     annotations:       volume.beta.kubernetes.io/storage-class: \"slow\"   spec:     capacity:       storage: 5Gi     accessModes:       - ReadWriteOnce     persistentVolumeReclaimPolicy: Recycle     nfs:       path: /tmp       server: 172.17.0.2 In this example we see the PV represents provisioned storage, an NFS share /tmp on the host 172.17.0.2. We give the PV a name, provide an access mode and set its capacity. The reclaim policy defines what happens when the PVC is deleted and the PV is returned to the free pool. Recycle will delete the data before the PV is freed using \"rm -rf\". There are other options and even plugins for defining how to recycle data. These can also be defined by the storage class.  How do we \"claim\" this storage from a user perspective? Let's take a simple example of a PVC from the Kubernetes documentation.  kind: PersistentVolumeClaim apiVersion: v1 metadata:   name: myclaim spec:   accessModes:     - ReadWriteOnce   resources:     requests:       storage: 8Gi In this example we have requested 8 GiB of storage. \"ReadWriteOnce\" tells Kubernetes that this storage or volume can only be mounted by a single container host in read-write mode (in contrast to \"ReadWriteMany\" where it can be mounted from multiple containers on different hosts). A \"PersistentVolumeClaim\" (PVC) as mentioned is simply a request for a \"PersistentVolume\" of a certain kind. The above mentioned PVC would not match the PV defined on the NFS server because that is too small (only 5Gi). You can see that manual PV provisioning is very inefficient.  The example above just requests storage but doesn't do anything with it. The example below shows how to request a PVC from the perspective of a Pod. We haven't talked about pods but they are a construct in Kubernetes and contain one or more containers. A pod is a logical grouping of containers. Containers within a pod run co-located, co-scheduled and share the same resource context. Usually we deal with single-container pods unless there is reason for tightly coupling containers.  kind: Pod apiVersion: v1 metadata:   name: mypod spec:   containers:     - name: myfrontend       image: dockerfile/nginx       volumeMounts:       - mountPath: \"/var/www/html\"         name: mypd   volumes:     - name: mypd       persistentVolumeClaim:         claimName: myclaim Without going into more details on how to tell Kubernetes to launch container-based applications in so called \"pods\" let's focus on the essence. We are requesting Kubernetes to launch an instance of the nginx container image with a volume mount. This is storage from the host made available on /var/www/html inside the nginx container.  We specify that the pod should use a volume (called \"mypd\") as the backing storage for this mount. The volume in turn is tied to our request in Kubernetes via the PVC \"myclaim\".  Simply put, we request storage, put a name to that request and then launch our app with a bind-mounted volume backed by that storage request.  Storage on Auto-Pilot with Kubernetes  Without dynamic provisioning, PVs need to be created manually by storage admins. This incredibly slows down DevOps teams, breaks automation and forces teams to operate like they have the last 20+ years, slowly. Fortunately dynamic provisioning was added to Kubernetes. The idea is that the storage class or provider should simply know how to provision storage on-the fly when a request comes in.  From the consumer perspective, nothing changes. But life get's dramatically easier for the Ops storage team. Instead of pre-provisioning NFS-based PVs (like above example), they pick a mature storage technology that knows how to do dynamic provisioning.  Below is an example of a storage-class for GlusterFS.  apiVersion: storage.k8s.io/v1beta1 kind: StorageClass metadata:   name: fast provisioner: kubernetes.io/glusterfs parameters:   resturl: \"http://127.0.0.1:8081\"   restauthenabled: \"true\"   restuser: \"admin\"   secretNamespace: \"default\"   secretName: \"heketi-secret\" Don't worry about the details of the syntax. Once a storage class is introduced that features a dynamic storage provisioner, the storage lifecycle will be completely automated.  PVCs referring to that particular storage class will get their PV objects created on-demand in a completely transparent fashion, with no human intervention. Likewise when a containers PVC is removed, the PV and underlying storage is automatically unprovisioned. The PVC in the previous example requesting storage from class \"fast\" would have behaved exactly like that.  Container Storage Solutions  You have seen above some of the storage fundamentals in Kubernetes and OpenShift. It covers most stateful applications storage needs including exclusive storage access (standalone databases) and shared storage access (content stores, streamin apps, analytics apps).  Kubernetes supports a lot of storage technologies, each with their own features, advantages and disadvantages.   NFS (you already heard about)  static provisioner, manually and statically pre-provisioned, inefficient space allocation ubiquitous, easy to set up in PoCs, well understood, good for tests supports ReadWriteOnce and ReadWriteMany   Ceph RBD  dynamic provisioner, Ceph block devices are automatically created, presented to the host, formatted and presented (mounted into) to the container excellent when running Kubernetes on top of OpenStack where Ceph is the #1 storage does not support ReadWriteMany   GCE Persistent Disk / AWS EBS / AzureDisk  dynamic provisioner, block devices are requested via the provider API, then automatically presented to the instance running Kubernetes/OpenShift and the container, formatted etc does not support ReadWriteMany performance may be problematic on small capacities ( &lt;100GB, typical for PVCs)   AWS EFS / AzureFile  dynamic provisioner, filesystems are requested via the provider API, mounted on the container host and then bind-mounted to the app container supports ReadWriteMany usually quite expensive   CephFS  same as RBD but already a filesystem, a shared one too supports ReadWriteMany excellent when running Kubernetes on top of OpenStack with Ceph   GlusterFS  dynamic provisioner supports ReadWriteOnce available on-premise and in public cloud with lower TCO than public cloud providers Filesystem-as-a-Service   NetApp  Currently tech-preview dynamic provisioner called trident supports ReadWriteOnce (block or file-based), ReadWriteMany (file-based), ReadOnlyMany (file-based) Requires NetApp Data OnTap or Solid Fire Storage    Getting closer to the application  Traditionally storage has always been something that sits external to the application and it's supporting infrastructure. Storage contains valuable data, it is the crown-jewel of every business. Data however also has gravity or inertia (if you want to be accurate in terms of physics). Simply put, the more data you have the harder it becomes to move, especially if your application infrastructure is tightly coupled to the storage backend. This also means that data created in the cloud will likely stay there, whereas data that has been created on-premise you guessed it, won't move.  Moving away from external storage systems will reduce the amount of variety or complexity we need to understand in order to deal with storage. When storage gets closer to your application, here a set containers, you are heightening the abstraction level and it is becoming easier to replicate the stack across providers. It's about having a common denominator when dealing with storage that you can always rely on regardless of what infrastructure platform is used (on-prem, AWS, GCE, Azure, OpenStack, etc).  Container-Native Storage  We can now build a common denominator like this with Container-Native Storage (CNS).  The idea behind it is simple, Kubernetes is an orchestration solution for distributed, containerized applications following a micro-service architecture. GlusterFS is a distributed software architecture split out in multiple smaller services running inside containers. Each node or selected nodes in Kubernetes has a container that runs GlusterFS. A daemon-set is used in Kubernetes to ensure that a GlusterFS container is always running on the correct nodes that are providing physical disks. Each GlusterFS container consumes the local storage (SATA, SSDs, NVMe, etc) and creates a cluster-wide distributed file-based storage system.  Gluster on Kubernetes can expand and shrink with the growth of the Kubernetes cluster and the application workloads on top of it. Availability, distribution, connectivity and health of Gluster is all managed seamlessly by Kubernetes native orchestration capabilities. Kubernetes dynamic provisioner maintains the lifecycle of storage requested by users via PVCs entirely in the background, from initial provisioning, to growth and eventual decommissioning. Since the containers are immutable there is no configuration and upgrades are handled same as other containerized applications, online, in rolling fashion using concept of the Kubernetes service. Finally, a storage architecture that doesn't create more work for storage teams and reduces complexity by providing a common software-defined storage layer.  This is a really powerful as storage itself has been elevated from something deep-down, chained to physical or virtual services to something that runs on the application platform itself. Kubernetes can run anywhere. OpenShift runs everywhere where Red Hat Enterprise Linux (RHEL) runs and from now on, this is also true for storage. While Gluster is the first storage platform to be provided as container-native storage, other software-defined storage platforms could go this route in the future.  Summary  In this article we have discussed the fundamentals behind storing data in the containerized world. Docker has become the standard format for container images. Kubernetes has become the standard for orchestrating Docker container images across 1000s of hosts. Kubernetes allows for running both stateless and stateful applications. It integrates with various storage platforms through storage classes. OpenShift is an enterprise build, deployment and run-time container platform based on Kubernetes that utilizes Docker (or any OCI-compatible runtime for that matter).  Various storage solutions exist for containers such as local storage, external storage and Container-Native Storage. Integrating storage with Kubernetes and enterprise container platform OpenShift means, enabling DevOps teams to look at storage as a omnipresent utility that they can dynamically provision without any knowledge of the storage subsystem. Container Native Storage (CNS) allows storage teams to provide storage for containers on-premise, off-premise, virtualized or bare-metal with automated management and deployment. This greatly offloads storage teams from manual provisioning and decommissioning tasks. It reduces cost of providing storage, avoids lock-in into the cloud-provider storage and allows storage to be part of the DevOps process just like everything else. We hope you found this article of interest and look forward to your feedback.  This is the first part of a six part series so stay tuned! Up until now we have discussed a lot of fundamental concepts and reasoning but no real hands-on. In the coming articles this will definitely change :)  Happy Stateful Applications running as Containers!  (c) 2017 Keith Tenzer  ","categories": ["OpenShift"],
        "tags": ["Docker","Kubernetes","Persistent Storage","software-defined storage","Storage"],
        "url": "/openshift/storage-for-containers-overview-part-i/",
        "teaser": null
      },{
        "title": "OpenShift Enterprise 3.4: all-in-one Lab Environment",
        "excerpt":"  Overview  In this article we will setup a OpenShift Enterprise 3.4 all-in-one configuration.  OpenShift has several different roles: masters, nodes, etcd and load balancers. An all-in-one setup means running all service on a single system. Since we are only using a single system a load balancer or ha-proxy won't be configured. If you would like to read more about OpenShift I can recommend the following:   General OpenShift Product Blogs Persistent Storage OpenShift Networking Part I OpenShift Networking Part II Aggregate Logging, Metrics, CloudForms and Jenkins Integration Deployment Scenarios OpenShift on Azure    Prerequisites  Configure a VM with following:   RHEL 7.3 2 CPUs 4096 RAM 30GB disk for OS 25GB disk for docker images  # subscription-manager repos --disable=\"*\" # subscription-manager repos \\     --enable=\"rhel-7-server-rpms\" \\     --enable=\"rhel-7-server-extras-rpms\" \\     --enable=\"rhel-7-server-ose-3.4-rpms\" # yum install wget git net-tools bind-utils iptables-services bridge-utils bash-completion # yum update -y # yum install -y atomic-openshift-utils # yum install atomic-openshift-excluder atomic-openshift-docker-excluder # atomic-openshift-excluder unexclude # yum install -y docker # vi /etc/sysconfig/docker OPTIONS='--selinux-enabled --insecure-registry 172.30.0.0/16' # cat &lt; /etc/sysconfig/docker-storage-setup DEVS=/dev/vdb VG=docker-vg EOF # docker-storage-setup # systemctl enable docker # systemctl start docker # ssh-keygen # ssh-copy-id -i /root/.ssh/id_rsa-pub ose3-master.lab.com #vi /etc/hosts  192.168.122.60  ose3-master.lab.com     ose3-master # systemctl reboot Install OpenShift.  Here we are enabling ovs-subnet SDN and setting authentication to use htpasswd. This is the most basic configuration as we are doing an all-in-one setup. For actual deployments you would want multi-master, dedicated nodes and seperate nodes for handling etcd.  #Create an OSEv3 group that contains the masters and nodes groups [OSEv3:children] masters nodes  # Set variables common for all OSEv3 hosts [OSEv3:vars] # SSH user, this user should allow ssh based auth without requiring a password ansible_ssh_user=root  # If ansible_ssh_user is not root, ansible_become must be set to true #ansible_become=true  deployment_type=openshift-enterprise  # uncomment the following to enable htpasswd authentication; defaults to DenyAllPasswordIdentityProvider openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]  # host group for masters [masters] ose3-master.lab.com  # host group for nodes, includes region info [nodes] ose3-master.lab.com openshift_schedulable=True Run Ansible playbook to install and configure OpenShift.  # ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/byo/config.yml Configure OpenShift  Create local admin account and enable permissions.  [root@ose3-master ~]#oc login -u system:admin -n default [root@ose3-master ~]#htpasswd -c /etc/origin/master/htpasswd admin [root@ose3-master ~]#oadm policy add-cluster-role-to-user cluster-admin admin [root@ose3-master ~]#oc login -u admin -n default Configure OpenShift image registry. Image streams are stored in registry. When you build application, your application code will be added as a image stream. This enables S2I (Source to Image) and allows for fast build times.  [root@ose3-master ~]#oadm registry --service-account=registry \\ --config=/etc/origin/master/admin.kubeconfig \\ --images='registry.access.redhat.com/openshift3/ose-${component}:${version}' Configure OpenShift router. The OpenShift router is basically an HA-Proxy that sends incoming service requests to node where pod is running.  [root@ose3-master ~]#oadm router router --replicas=1 \\     --credentials='/etc/origin/master/openshift-router.kubeconfig' \\     --service-account=router Summary  In this article we have seen how to configure an OpenShift 3.4 all-in-one lab environment. We have also seen how install and configuration can be adapted through ansible playbook. This environment is intended to be for a Lab and as such no best practices are given in regards to OpenShift. If you have any feedback please share.  Happy OpenShifting!  (c) 2017 Keith Tenzer  ","categories": ["OpenShift"],
        "tags": ["Containers","Docker","Kubernetes","OpenShift"],
        "url": "/openshift/openshift-enterprise-3-4-all-in-one-lab-environment/",
        "teaser": null
      },{
        "title": "Deploying CloudForms in the Azure Cloud",
        "excerpt":"  Overview  In this article we will deploy the CloudForms appliance in the Azure cloud. CloudForms is a cloud management platform based on the opensource project manageiq. Red Hat bought manageiq a few years back and opensourced the software. Originally it was designed to manage VMware but over the years has expanded to many additional traditional as well as cloud platforms. You can use this article as reference for both CloudForms and ManageIQ.  CloudForms can connect to many cloud providers such as RHEV (Red Hat Enterprise Virtualization), VMware, Hyper-V, OpenStack, Amazon Web Services (AWS), Google Cloud Engine (GCE) and Azure. Large organizations don't have one cloud but many and in addition typically have on-premise, off-premise as well as public. All of these various platforms creates a lot of complexity if not managed right. CloudForms can create a bridge between traditional (mode 1) and cloud native (mode 2) workloads, offering applications a path to switch between these modes. In addition, CloudForms allows an IT organization to act as a cloud broker between the various public platforms. Finally CloudForms can be used to automatically deploy and manage applications across the various cloud platforms. Businesses have choice in platform, flexibility and speed while IT can manage it all centrally applying common policies or rule-sets regardless of where workloads are running.    Prepare  Red Hat provides CloudForms as a appliance. The appliances each have various formats, depending on the platform. For Microsft Hyper-V and Azure, Red Hat provides a Virtual Hard Disk (VHD). The vhd is a dynamic disk. Azure unfortunately does not support dynamic disks but only fixed disks. In order to import CloudForms appliance in Azure we need to convert the appliance vhd to fixed disk. In addition the vhd will be fixed size of around 40GB. To prevent having to upload 40GB and just the actual data which is closer to 2GB we will use several tools. You can of course use Powershell, using the Azure cmdlets. If you are a Linux guy like me though, that isn't an option. Thankfully Microsoft has provided a tool written in Go that works great for uploading disks to Azure. In addition Microsoft provides a CLI similar to functionality of Powershell written in python.  Convert VHD from dynamic to fixed  The first question you might have is why? Well Red Hat doesn't want you to have to download a 40GB image so they provide a dynamic disk. In the next steps we will take that image, convert to fixed disk and upload to Azure, ignoring the zero'ed blocks.  Convert image to raw using qemu tools  # qemu-img convert -f vpc -O raw cfme-azure-5.7.0.17-1.x86_64.vhd cfme-azure-5.7.0.17-1.raw Calculate fixed image size  $sudo rawdisk=\"cfme-azure-5.7.0.17-1.raw\" $sudo vhddisk=\"cfme-azure-5.7.0.17-1.vhd\" $sudo MB=$((1024*1024)) $sudo rounded_size=$((($size/$MB + 1)*$MB)) $sudo size=$(qemu-img info -f raw --output json \"$rawdisk\" | gawk 'match($0, /\"virtual-size\": ([0-9]+),/, val) {print val[1]}') $sudo rounded_size=$((($size/$MB + 1)*$MB)) Resize image  $sudo qemu-img resize -f raw \"$rawdisk\" $rounded_size $sudo qemu-img convert -f raw -o subformat=fixed,force_size -O vpc \"$rawdisk\" \"$vhddisk\" Download Azure VHD Tools  As mentioned the Azure VHD tools are written in Go so you need to first install Go. I installed version 1.7.4.  $ gunzip go1.7.4.linux-amd64.tar.gz  $ tar xvf go1.7.4.linux-amd64.tar  $ cd go Export Environment Parameters   $ mkdir $HOME/work  $ export GOPATH=$HOME/work  $ export PATH=$PATH:$GOPATH/bin Install VHD Tools   $ go get github.com/Microsoft/azure-vhd-utils Upload the CloudForms Fixed Disk to Azure  $ ./azure-vhd-utils upload --localvhdpath /home/ktenzer/cfme-azure-5.7.0.17-1.vhd --stgaccountname &lt;storage account&gt; --stgaccountkey &lt;storage key&gt; --containername templates --blobname cfme-azure-5.7.0.17-1.vhd --parallelism 8 Once the upload completes you can deploy the CloudForms Appliance in Azure. In order to do this we will use the Azure CLI which is python based.  Install Python and dependencies  $ sudo yum install python $ sudo yum install python-pip $ sudo yum install python-devel $ sudo yum install openssl-devel $ sudo yum install npm Install Azure CLI  $ npm install azure-cli -g $ sudo npm install azure-cli -g $ sudo pip install --upgrade pip $ sudo pip install azure==2.0.0rc5 Deploy CloudForms Appliance  Using the Azure CLI deploy CloudForms Appliance  $ azure vm image create cfme-azure-5.7.0.17-1 --blob-url https://premiumsadpdhlose2disk.blob.core.windows.net/templates/cfme-azure-5.7.0.17-1.vhd --os Linux /home/ktenzer/cfme-azure-5.7.0.17-1.vhd Note: you can also use the Azure portal UI to create VM once the image is uploaded.  Configure CloudForms in Azure  Once the CloudForms Appliance is deployed you can access it using username/password or ssh-key depending on what you chose when creating VM in Azure.  $ ssh ktenzer@&lt;CloudForms Appliance IP&gt; Run the appliance console  $ sudo appliance_console  Welcome to the CFME Virtual Appliance.  To modify the configuration, use a web browser to access the management page.  Hostname: CFME IP Address: 10.0.0.9 Netmask: 255.255.255.0 Gateway: 10.0.0.1 Primary DNS: 168.63.129.16 Secondary DNS:  Search Order: v1tt5gv0hqjudfkkftdccsm4cc.ax.internal.cloudapp.net MAC Address: 00:0d:3a:28:0c:27 Timezone: Europe/Berlin Local Database Server: running (primary) CFME Server: running CFME Database: localhost Database/Region: vmdb_production / 1 External Auth: not configured CFME Version: 5.7.0.17   Press any key to continue. Advanced Setting  1) Set DHCP Network Configuration 2) Set Static Network Configuration 3) Test Network Configuration 4) Set Hostname 5) Set Timezone 6) Set Date and Time 7) Restore Database From Backup 8) Configure Database 9) Configure Database Replication 10) Configure Database Maintenance 11) Logfile Configuration 12) Configure Application Database Failover Monitor 13) Extend Temporary Storage 14) Configure External Authentication (httpd) 15) Update External Authentication Options 16) Generate Custom Encryption Key 17) Harden Appliance Using SCAP Configuration 18) Stop EVM Server Processes 19) Start EVM Server Processes 20) Restart Appliance 21) Shut Down Appliance 22) Summary Information 23) Quit  Choose the advanced setting: Verify the network configuration, hostname and timezone.  Configure database  Configure Database  Database Operation  1) Create Internal Database 2) Create Region in External Database 3) Join Region in External Database 4) Reset Configured Database  Choose the database operation: Create an internal database, choosing default options  Once database has been created and instantiated start the EVM processes from main menu. Once that completes you can access the CloudForms Appliance UI.  https://&lt;CloudForms Appliance IP&gt; username: admin password: smartvm Summary  In this article we explored how to deploy the CloudForms appliance in the Azure cloud. CloudForms can provide insights into many cloud providers and provide a single-pane for administrating an organizations various cloud platforms. This enables an IT organization to take the role of cloud broker, providing an organization flexibility between various platforms without giving up control and oversight. I hope you found this article of use. If you have additional feedback or comments please let me know.  Happy Clouding in Azure!  (c) 2017 Keith Tenzer  ","categories": ["CloudForms"],
        "tags": ["Azure","Cloud"],
        "url": "/cloudforms/deploying-cloudforms-in-the-azure-cloud/",
        "teaser": null
      },{
        "title": "Storage for Containers using Gluster – Part II",
        "excerpt":"  Overview  This article is a collaboration between Daniel Messer (Technical Marketing Manager Storage @RedHat) and Keith Tenzer (Solutions Architect @RedHat).   Storage for Containers Overview – Part I Storage for Containers using Gluster – Part II Storage for Containers using Container Native Storage – Part III Storage for Containers using Ceph – Part IV Storage for Containers using NetApp ONTAP NAS – Part V Storage for Containers using NetApp SolidFire – Part VI  Gluster as Container-Ready Storage (CRS)  In this article we will look at one of the first options of storage for containers and how to deploy it. Support for GlusterFS has been in Kubernetes and OpenShift for some time. GlusterFS is a good fit because it is available across all deployment options: bare-metal, virtual, on-premise and public cloud. The recent addition of GlusterFS running in a container will be discussed later in this series.  GlusterFS is a distributed filesystem at heart with a native protocol (GlusterFS) and various other protocols (NFS, SMB,...). For integration with OpenShift, nodes will use the native protocol via FUSE to mount GlusterFS volumes on the node itself and then have them bind-mount'ed into the target containers. OpenShift/Kubernetes has a native provisioner that implements requesting, releasing and (un-)mounting GlusterFS volumes.    CRS Overview  On the storage side there is an additional component managing the cluster at the request of OpenShift/Kubernetes called \"heketi\". This is effectively a REST API for GlusterFS and also ships a CLI version. In the following steps we will deploy heketi among 3 GlusterFS nodes, use it to deploy a GlusterFS storage pool, connect it to OpenShift and use it to provision storage for containers via PersistentVolumeClaims. In total we will deploy 4 Virtual Machines. One for OpenShift (lab setup) and three for GlusterFS.  Note: your system should have at least a quad-core CPU, 16GB RAM and 20 GB of free disk space.  Deploying OpenShift  At first you will need an OpenShift deployment. An All-in-One deployment in a VM is sufficient, instructions can be found in the \"OpenShift Enterprise 3.4 all-in-one Lab Environment\" article.  Make sure your OpenShift VM can resolve external domain names. Edit /etc/dnsmasq.conf and add the following line to use Google DNS:  server=8.8.8.8 Restart it:  # systemctl restart dnsmasq # ping -c1 google.com Deploying Gluster  For GlusterFS at least 3 VMs are required with the following specs:   RHEL 7.3 2 CPUs 2 GB RAM 30 GB disk for OS 10 GB disk for GlusterFS bricks  It is necessary to provide local name resolution for the 3 VMs and the OpenShift VM via a common /etc/hosts file.  For example (feel free to adjust the domain and host names to your environment):  # cat /etc/hosts 127.0.0.1      localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1            localhost localhost.localdomain localhost6 localhost6.localdomain6 172.16.99.144  ocp-master.lab ocp-master 172.16.128.7   crs-node1.lab crs-node1 172.16.128.8   crs-node2.lab crs-node2 172.16.128.9   crs-node3.lab crs-node3 Execute the following steps on all 3 GlusterFS VMs:  # subscription-manager repos --disable=\"*\" # subscription-manager repos --enable=rhel-7-server-rpms If you have a GlusterFS subscription you can use it and enable the rh-gluster-3-for-rhel-7-server-rpms repository.  If you don't you can use the unsupported GlusterFS community repositories for testing via EPEL:  # yum -y install http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm # rpm --import http://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-7 Create a file named glusterfs-3.10.repo in /etc/yum.repos.d/  [glusterfs-3.10] name=glusterfs-3.10 description=\"GlusterFS 3.10 Community Version\" baseurl=https://buildlogs.centos.org/centos/7/storage/x86_64/gluster-3.10/ gpgcheck=0 enabled=1 Verify the repository is active:  # yum repolist You should now be able to install GlusterFS  # yum -y install glusterfs-server A couple of basic TCP ports need to be opened for GlusterFS peers to communicate and provide storage to OpenShift:  # firewall-cmd --add-port=24007-24008/tcp --add-port=49152-49664/tcp --add-port=2222/tcp # firewall-cmd --runtime-to-permanent Now we are ready to start the GlusterFS daemon:  # systemctl enable glusterd # systemctl start glusterd That's it. GlusterFS is up and running. The rest of the configuration will be done via heketi.  Install heketi on one of the GlusterFS VMs:  [root@crs-node1 ~]# yum -y install heketi heketi-client  Update for for EPEL users  If you don't have a Red Hat Gluster Storage subscription you will get heketi from EPEL. At the time of writing this is version 3.0.0-1.el7 from October 2016 which is not working with OpenShift 3.4. You will need to update to a more current version:  [root@crs-node1 ~]# yum -y install wget [root@crs-node1 ~]# wget https://github.com/heketi/heketi/releases/download/v4.0.0/heketi-v4.0.0.linux.amd64.tar.gz [root@crs-node1 ~]# tar -xzf heketi-v4.0.0.linux.amd64.tar.gz [root@crs-node1 ~]# systemctl stop heketi [root@crs-node1 ~]# cp heketi/heketi* /usr/bin/ [root@crs-node1 ~]# chown heketi:heketi /usr/bin/heketi* Create a file in /etc/systemd/system/heketi.service for the updated syntax of the v4 heketi binary:  [Unit] Description=Heketi Server  [Service] Type=simple WorkingDirectory=/var/lib/heketi EnvironmentFile=-/etc/heketi/heketi.json User=heketi ExecStart=/usr/bin/heketi --config=/etc/heketi/heketi.json Restart=on-failure StandardOutput=syslog StandardError=syslog  [Install] WantedBy=multi-user.target [root@crs-node1 ~]# systemctl daemon-reload [root@crs-node1 ~]# systemctl start heketi  Heketi will use SSH to configure GlusterFS on all nodes. Create an SSH key pair and copy the public key to all 3 nodes (including the first node you are logged on):  [root@crs-node1 ~]# ssh-keygen -f /etc/heketi/heketi_key -t rsa -N '' [root@crs-node1 ~]# ssh-copy-id -i /etc/heketi/heketi_key.pub root@crs-node1.lab [root@crs-node1 ~]# ssh-copy-id -i /etc/heketi/heketi_key.pub root@crs-node2.lab [root@crs-node1 ~]# ssh-copy-id -i /etc/heketi/heketi_key.pub root@crs-node3.lab [root@crs-node1 ~]# chown heketi:heketi /etc/heketi/heketi_key* The only thing left is to configure heketi to use SSH. Edit the /etc/heketi/heketi.json to look like the below (changed parts highlighted are underlined):  {    \"_port_comment\":\"Heketi Server Port Number\",    \"port\":\"8080\",    \"_use_auth\":\"Enable JWT authorization. Please enable for deployment\",    \"use_auth\":false,    \"_jwt\":\"Private keys for access\",    \"jwt\":{       \"_admin\":\"Admin has access to all APIs\",       \"admin\":{          \"key\":\"My Secret\"       },       \"_user\":\"User only has access to /volumes endpoint\",       \"user\":{          \"key\":\"My Secret\"       }    },    \"_glusterfs_comment\":\"GlusterFS Configuration\",    \"glusterfs\":{       \"_executor_comment\":[          \"Execute plugin. Possible choices: mock, ssh\",          \"mock: This setting is used for testing and development.\",          \" It will not send commands to any node.\",          \"ssh: This setting will notify Heketi to ssh to the nodes.\",          \" It will need the values in sshexec to be configured.\",          \"kubernetes: Communicate with GlusterFS containers over\",          \" Kubernetes exec api.\"       ],       \"executor\":\"ssh\",       \"_sshexec_comment\":\"SSH username and private key file information\",       \"sshexec\":{          \"keyfile\":\"/etc/heketi/heketi_key\",          \"user\":\"root\",          \"port\":\"22\",          \"fstab\":\"/etc/fstab\"       },       \"_kubeexec_comment\":\"Kubernetes configuration\",       \"kubeexec\":{          \"host\":\"https://kubernetes.host:8443\",          \"cert\":\"/path/to/crt.file\",          \"insecure\":false,          \"user\":\"kubernetes username\",          \"password\":\"password for kubernetes user\",          \"namespace\":\"OpenShift project or Kubernetes namespace\",          \"fstab\":\"Optional: Specify fstab file on node. Default is /etc/fstab\"       },       \"_db_comment\":\"Database file name\",       \"db\":\"/var/lib/heketi/heketi.db\",       \"_loglevel_comment\":[          \"Set log level. Choices are:\",          \" none, critical, error, warning, info, debug\",          \"Default is warning\"       ],       \"loglevel\":\"debug\"    } } Finished. heketi will listen on port 8080, let's make sure the firewall allows that:  # firewall-cmd --add-port=8080/tcp # firewall-cmd --runtime-to-permanent Now restart heketi:  # systemctl enable heketi # systemctl restart heketi Test if it's running:  # curl http://crs-node1.lab:8080/hello Hello from Heketi Good. Time to put heketi to work. We will use it to configure our GlusterFS storage pool. The software is already running on all our VMs but it's unconfigured. To change that to a functional storage system we will describe our desired GlusterFS storage pool in a topology file, like below:  # vi topology.json {   \"clusters\": [     {       \"nodes\": [         {           \"node\": {             \"hostnames\": {               \"manage\": [                 \"crs-node1.lab\"               ],               \"storage\": [                 \"172.16.128.7\"               ]             },             \"zone\": 1           },           \"devices\": [             \"/dev/sdb\"           ]         },         {           \"node\": {             \"hostnames\": {               \"manage\": [                 \"crs-node2.lab\"               ],               \"storage\": [                 \"172.16.128.8\"               ]             },             \"zone\": 1           },           \"devices\": [             \"/dev/sdb\"           ]         },         {           \"node\": {             \"hostnames\": {               \"manage\": [                 \"crs-node3.lab\"               ],               \"storage\": [                 \"172.16.128.9\"               ]             },             \"zone\": 1           },           \"devices\": [             \"/dev/sdb\"           ]         }       ]     }   ] } Despite the formatting the file is relatively simple. It basically tells heketi to create a 3 node cluster with each node being known by a FQDN, an IP address and with at least one spare block device which will be used as a GlusterFS brick.  Now feed this file to heketi:  # export HEKETI_CLI_SERVER=http://crs-node1.lab:8080 # heketi-cli topology load --json=topology.json Creating cluster ... ID: 78cdb57aa362f5284bc95b2549bc7e7d  Creating node crs-node1.lab ... ID: ffd7671c0083d88aeda9fd1cb40b339b  Adding device /dev/sdb ... OK  Creating node crs-node2.lab ... ID: 8220975c0a4479792e684584153050a9  Adding device /dev/sdb ... OK  Creating node crs-node3.lab ... ID: b94f14c4dbd8850f6ac589ac3b39cc8e  Adding device /dev/sdb ... OK Now heketi has configured a 3 node GlusterFS storage pool. Easy! You can see that the 3 VMs have successfully formed what's called a Trusted Storage Pool in GlusterFS  [root@crs-node1 ~]# gluster peer status Number of Peers: 2  Hostname: crs-node2.lab Uuid: 93b34946-9571-46a8-983c-c9f128557c0e State: Peer in Cluster (Connected) Other names: crs-node2.lab  Hostname: 172.16.128.9 Uuid: e3c1f9b0-be97-42e5-beda-f70fc05f47ea State: Peer in Cluster (Connected) Now back to OpenShift!  Integrating Gluster with OpenShift  For integration in OpenShift two things are needed: a dynamic Kubernetes Storage Provisioner and a StorageClass. The provisioner ships out of the box with OpenShift. It does the actual heavy lifting of attaching storage to containers. The StorageClass is an entity that users in OpenShift can make PersistentVolumeClaims against, which will in turn trigger a provisioner to implement the actual provisioning and represent the result as Kubernetes PersistentVolume (PV).  Like everything else in OpenShift the StorageClass is simply defined as a YAML file:  # cat crs-storageclass.yaml kind: StorageClass apiVersion: storage.k8s.io/v1beta1 metadata:  name: container-ready-storage  annotations:  storageclass.beta.kubernetes.io/is-default-class: \"true\" provisioner: kubernetes.io/glusterfs parameters:  resturl: \"http://crs-node1.lab:8080\"  restauthenabled: \"false\" Our provisioner is kubernetes.io/glusterfs and we make it point to our heketi instance. We name the class \"container-ready-storage\" and at the same time make it the default StorageClass for all PersistentVolumeClaims that do not explicitly specify one.  Create the StorageClass for your GlusterFS pool:  # oc create -f crs-storageclass.yaml Using Gluster with OpenShift  Let's look at how we would use GlusterFS in OpenShift. First create a playground project on the OpenShift VM:  # oc new-project crs-storage --display-name=\"Container-Ready Storage\" To request storage in Kubernetes/OpenShift a PersistentVolumeClaim (PVC) is issued. It's a simple object describing at the minimum how much capacity we need and in which access mode it should be supplied (non-shared, shared, read-only). It's usually part of an application template but let's just create a standalone PVC:  # cat crs-claim.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata:  name: my-crs-storage  namespace: crs-storage spec:  accessModes:  - ReadWriteOnce  resources:  requests:  storage: 1Gi Issue the claim:  # oc create -f crs-claim.yaml Watch the PVC being processed and fulfilled with a dynamically created volume in OpenShift:  # oc get pvc NAME             STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE my-crs-storage   Bound     pvc-41ad5adb-107c-11e7-afae-000c2949cce7   1Gi        RWO           58s Great! You have now storage capacity available for use in OpenShift without any interaction with the storage system directly. Let's look at the volume that got created:  # oc get pv/pvc-41ad5adb-107c-11e7-afae-000c2949cce7 Name:\t\tpvc-41ad5adb-107c-11e7-afae-000c2949cce7 Labels:\t\t StorageClass:\tcontainer-ready-storage Status:\t\tBound Claim:\t\tcrs-storage/my-crs-storage Reclaim Policy:\tDelete Access Modes:\tRWO Capacity:\t1Gi Message: Source:     Type:\t\tGlusterfs (a Glusterfs mount on the host that shares a pod's lifetime)     EndpointsName:\tgluster-dynamic-my-crs-storage     Path:\t\tvol_85e444ee3bc154de084976a9aef16025     ReadOnly:\t\tfalse The volume has been created specifically according to the design specifications in the PVC. In the PVC we did not explicitly specify which StorageClass we wanted to use because the GlusterFS StorageClass using heketi was defined as the system-wide default.  What happened in the background was that when the PVC reached the system, our default StorageClass reached out to the GlusterFS Provisioner with the volume specs from the PVC. The provisioner in turn communicates with our heketi instance which facilitates the creation of the GlusterFS volume, which we can trace in it's log messages:  [root@crs-node1 ~]# journalctl -l -u heketi.service ... Mar 24 11:25:52 crs-node1.lab heketi[2598]: [heketi] DEBUG 2017/03/24 11:25:52 /src/github.com/heketi/heketi/apps/glusterfs/volume_entry.go:298: Volume to be created on cluster e Mar 24 11:25:52 crs-node1.lab heketi[2598]: [heketi] INFO 2017/03/24 11:25:52 Creating brick 9e791b1daa12af783c9195941fe63103 Mar 24 11:25:52 crs-node1.lab heketi[2598]: [heketi] INFO 2017/03/24 11:25:52 Creating brick 3e06af2f855bef521a95ada91680d14b Mar 24 11:25:52 crs-node1.lab heketi[2598]: [heketi] INFO 2017/03/24 11:25:52 Creating brick e4daa240f1359071e3f7ea22618cfbab ... Mar 24 11:25:52 crs-node1.lab heketi[2598]: [sshexec] INFO 2017/03/24 11:25:52 Creating volume vol_85e444ee3bc154de084976a9aef16025 replica 3 ... Mar 24 11:25:53 crs-node1.lab heketi[2598]: Result: volume create: vol_85e444ee3bc154de084976a9aef16025: success: please start the volume to access data ... Mar 24 11:25:55 crs-node1.lab heketi[2598]: Result: volume start: vol_85e444ee3bc154de084976a9aef16025: success ... Mar 24 11:25:55 crs-node1.lab heketi[2598]: [asynchttp] INFO 2017/03/24 11:25:55 Completed job c3d6c4f9fc74796f4a5262647dc790fe in 3.176522702s ... Success! In just about 3 seconds the GlusterFS pool was configured and provisioned a volume. The default as of today is replica 3, which means the data will be replicated across 3 bricks (GlusterFS speak for backend storage) of 3 distinct nodes. The process is orchestrated via heketi on behalf of OpenShift.  You can see this information on the volume from GlusterFS perspective:  [root@crs-node1 ~]# gluster volume list vol_85e444ee3bc154de084976a9aef16025 [root@crs-node1 ~]# gluster volume info vol_85e444ee3bc154de084976a9aef16025  Volume Name: vol_85e444ee3bc154de084976a9aef16025 Type: Replicate Volume ID: a32168c8-858e-472a-b145-08c20192082b Status: Started Snapshot Count: 0 Number of Bricks: 1 x 3 = 3 Transport-type: tcp Bricks: Brick1: 172.16.128.8:/var/lib/heketi/mounts/vg_147b43f6f6903be8b23209903b7172ae/brick_9e791b1daa12af783c9195941fe63103/brick Brick2: 172.16.128.9:/var/lib/heketi/mounts/vg_72c0f520b0c57d807be21e9c90312f85/brick_3e06af2f855bef521a95ada91680d14b/brick Brick3: 172.16.128.7:/var/lib/heketi/mounts/vg_67314f879686de975f9b8936ae43c5c5/brick_e4daa240f1359071e3f7ea22618cfbab/brick Options Reconfigured: transport.address-family: inet nfs.disable: on Notice how the volume name in GlusterFS corresponds to the \"path\" of the Kubernetes Persistent Volume in OpenShift.  Alternatively you can also use the OpenShift UI to provision storage, which allows you to conveniently select among all known StorageClasses in the system:      Let's make this a little more interesting and run a workload on OpenShift.  On our OpenShift VM still being in the crs-storage project:  # oc get templates -n openshift You should see a nice list of application and database templates for easy consumption in OpenShift to get your app development project kickstarted.  We will use  MySQL to demonstrate how to host a stateful application on OpenShift with persistent and elastic storage. The mysql-persistent template includes a PVC of 1G for the MySQL database directory. For demonstration purposes all default values are fine.  # oc process mysql-persistent -n openshift | oc create -f - Wait for the deployment to finish. You can observe the progress in the UI or via  # oc get pods NAME            READY     STATUS    RESTARTS   AGE mysql-1-h4afb   1/1       Running   0          2m Nice. This template created a service, secrets, a PVC and a pod. Let's use it (your pod name will differ):  # oc rsh mysql-1-h4afb You have successfully attached to the MySQL pod. Let's connect to the database:  sh-4.2$ mysql -u $MYSQL_USER -p$MYSQL_PASSWORD -h $HOSTNAME $MYSQL_DATABASE Conveniently all vital configuration like MySQL credentials, database name, etc are part of environment variables in the pod template and hence available in the pod as shell environment variables too. Let's create some data:  mysql&gt; show databases; +--------------------+ | Database           | +--------------------+ | information_schema | | sampledb           | +--------------------+ 2 rows in set (0.02 sec)  mysql&gt; \\u sampledb Database changed mysql&gt; CREATE TABLE IF NOT EXISTS equipment (     -&gt;     equip_id int(5) NOT NULL AUTO_INCREMENT,     -&gt;     type varchar(50) DEFAULT NULL,     -&gt;     install_date DATE DEFAULT NULL,     -&gt;     color varchar(20) DEFAULT NULL,     -&gt;     working bool DEFAULT NULL,     -&gt;     location varchar(250) DEFAULT NULL,     -&gt;     PRIMARY KEY(equip_id)     -&gt;     ); Query OK, 0 rows affected (0.13 sec)  mysql&gt; INSERT INTO equipment (type, install_date, color, working, location)     -&gt; VALUES     -&gt; (\"Slide\", Now(), \"blue\", 1, \"Southwest Corner\"); Query OK, 1 row affected, 1 warning (0.01 sec)  mysql&gt; SELECT * FROM equipment; +----------+-------+--------------+-------+---------+------------------+ | equip_id | type  | install_date | color | working | location         | +----------+-------+--------------+-------+---------+------------------+ |        1 | Slide | 2017-03-24   | blue  |       1 | Southwest Corner | +----------+-------+--------------+-------+---------+------------------+ 1 row in set (0.00 sec) This means the database is functional. Great!  Do you want to see where the data is stored? Easy! Look at the mysql volume that got created as part of the template:  # oc get pvc/mysql NAME      STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE mysql     Bound     pvc-a678b583-1082-11e7-afae-000c2949cce7   1Gi        RWO           11m # oc describe pv/pvc-a678b583-1082-11e7-afae-000c2949cce7 Name:\t\tpvc-a678b583-1082-11e7-afae-000c2949cce7 Labels:\t\t StorageClass:\tcontainer-ready-storage Status:\t\tBound Claim:\t\tcrs-storage/mysql Reclaim Policy:\tDelete Access Modes:\tRWO Capacity:\t1Gi Message: Source:     Type:\t\tGlusterfs (a Glusterfs mount on the host that shares a pod's lifetime)     EndpointsName:\tgluster-dynamic-mysql     Path:\t\tvol_6299fc74eee513119dafd43f8a438db1     ReadOnly:\t\tfalse Note the path to GlusterFS volume name vol_6299fc74eee513119dafd43f8a438db1.  Return to one of your GlusterFS VMs and issue:  # gluster volume info vol_6299fc74eee513119dafd43f8a438db  Volume Name: vol_6299fc74eee513119dafd43f8a438db1 Type: Replicate Volume ID: 4115918f-28f7-4d4a-b3f5-4b9afe5b391f Status: Started Snapshot Count: 0 Number of Bricks: 1 x 3 = 3 Transport-type: tcp Bricks: Brick1: 172.16.128.7:/var/lib/heketi/mounts/vg_67314f879686de975f9b8936ae43c5c5/brick_f264a47aa32be5d595f83477572becf8/brick Brick2: 172.16.128.8:/var/lib/heketi/mounts/vg_147b43f6f6903be8b23209903b7172ae/brick_f5731fe7175cbe6e6567e013c2591343/brick Brick3: 172.16.128.9:/var/lib/heketi/mounts/vg_72c0f520b0c57d807be21e9c90312f85/brick_ac6add804a6a467cd81cd1404841bbf1/brick Options Reconfigured: transport.address-family: inet nfs.disable: on You can see how the data is replicated across 3 GlusterFS bricks. Let's pick one of them (ideally the host you are logged on to and look at the directory contents):  # ll /var/lib/heketi/mounts/vg_67314f879686de975f9b8936ae43c5c5/brick_f264a47aa32be5d595f83477572becf8/brick total 180300 -rw-r-----. 2 1000070000 2001       56 Mar 24 12:11 auto.cnf -rw-------. 2 1000070000 2001     1676 Mar 24 12:11 ca-key.pem -rw-r--r--. 2 1000070000 2001     1075 Mar 24 12:11 ca.pem -rw-r--r--. 2 1000070000 2001     1079 Mar 24 12:12 client-cert.pem -rw-------. 2 1000070000 2001     1680 Mar 24 12:12 client-key.pem -rw-r-----. 2 1000070000 2001      352 Mar 24 12:12 ib_buffer_pool -rw-r-----. 2 1000070000 2001 12582912 Mar 24 12:20 ibdata1 -rw-r-----. 2 1000070000 2001 79691776 Mar 24 12:20 ib_logfile0 -rw-r-----. 2 1000070000 2001 79691776 Mar 24 12:11 ib_logfile1 -rw-r-----. 2 1000070000 2001 12582912 Mar 24 12:12 ibtmp1 drwxr-s---. 2 1000070000 2001     8192 Mar 24 12:12 mysql -rw-r-----. 2 1000070000 2001        2 Mar 24 12:12 mysql-1-h4afb.pid drwxr-s---. 2 1000070000 2001     8192 Mar 24 12:12 performance_schema -rw-------. 2 1000070000 2001     1676 Mar 24 12:12 private_key.pem -rw-r--r--. 2 1000070000 2001      452 Mar 24 12:12 public_key.pem drwxr-s---. 2 1000070000 2001       62 Mar 24 12:20 sampledb -rw-r--r--. 2 1000070000 2001     1079 Mar 24 12:11 server-cert.pem -rw-------. 2 1000070000 2001     1676 Mar 24 12:11 server-key.pem drwxr-s---. 2 1000070000 2001     8192 Mar 24 12:12 sys You can see the MySQL database directory here. This is how it's stored in GlusterFS backend and presented to the MySQL container as a bind-mount. If you check your mount table on the OpenShift VM you will see the GlusterFS mount.  Summary  What we have done here is create a simple but functional GlusterFS storage pool outside of OpenShift. This pool can grow and shrink independently of the applications. The entire lifecycle of this pool is managed by a simple front-end known as heketi which only needs manual intervention when the deployment grows. For daily provisioning operations it's API is used via OpenShifts dynamic provisioner, eliminating the need for Developers to interact with Infrastructure teams directly.  This is how we bring storage into the DevOps world - painless, and available directly via developer tooling of the OpenShift PaaS system.  GlusterFS and OpenShift run across all foot-prints: bare-metal, virtual, private and public cloud (Azure, Google Cloud, AWS...) ensuring application portability and avoiding cloud provider lock-in.  Happy Glustering your Containers!  (c) 2017 Keith Tenzer  ","categories": ["OpenShift"],
        "tags": ["CRS","Docker","Gluster","heketi","Kubernetes","Persistent Storage","software-defined storage","Storage"],
        "url": "/openshift/storage-for-containers-using-gluster-part-ii/",
        "teaser": null
      },{
        "title": "OpenStack 10 (Newton) Lab Installation and Configuration Guide",
        "excerpt":"  Overview  In this article we will focus on installing and configuring OpenStack Newton using RDO and the packstack installer. RDO is a community platform around Red Hat's OpenStack Platform. It allows you to test the latest OpenStack capabilities on a stable platform such as Red Hat Enterprise Linux (RHEL) or CentOS. This guide will take you through installing the OpenStack Liberty release, configuring networking, security groups, flavors, images and are other OpenStack related services. The outcome is a working OpenStack environment based on the Newton release that you can use as a baseline for testing your applications with OpenStack capabilities.    Install and Configure OpenStack Newton   Install RHEL or CentOS 7.2 or 7.3. Ensure name resolution is working.  # vi /etc/hosts 192.168.122.80 osp10.lab osp10  Set hostname.  # hostnamectl set-hostname osp10.lab  Disable firewalld since this is for a lab environment.  # systemctl disable firewalld # systemctl stop firewalld  Disable NetworkManager, it is still not recommended for Liberty (at least RDO).  # systemctl stop NetworkManager # systemctl disable NetworkManager  For RHEL systems register with subscription manager.  # subscription-manager register # subscription-manager list --available # subscription-manager attach --pool=&lt;pool id&gt; # subscription-manager repos --disable=* # subscription-manager repos --enable=rhel-7-server-rpms # subscription-manager repos --enable=rhel-7-server-rh-common-rpms # subscription-manager repos --enable=rhel-7-server-extras-rpms # subscription-manager repos --enable=rhel-7-server-openstack-10-rpms # subscription-manager repos --enable=rhel-7-server-openstack-10-devtools-rpms  Install yum-utils and update the system.  # yum install -y yum-utils # yum update -y  Reboot.  # systemctl reboot  Install packstack packages.  # yum install -y openstack-packstack You can install packstack by providing command-line options or using the answers file.  Option 1: Install using command-line options   # packstack --allinone --os-neutron-ovs-bridge-mappings=extnet:br-ex \\  --os-neutron-ovs-bridge-interfaces=br-ex:eth0 \\  --os-neutron-ml2-type-drivers=vxlan,flat \\  --os-heat-install=y --os-heat-cfn-install=y \\  --os-neutron-lbaas-install=y --keystone-admin-passwd=redhat01 Option 2: Install using answers file   Create packstack answers file for customizing the installer.  # packstack --gen-answer-file /root/answers.txt  Update the packstack answers file and enable other OpenStack services. Note: as of the writing of this guide SSL is not working in combination with Horizon so don't enable SSL.  # vi /root/answers.txt  CONFIG_KEYSTONE_ADMIN_PW=redhat  CONFIG_PROVISION_DEMO=n  CONFIG_HEAT_INSTALL=y  CONFIG_HEAT_CFN_INSTALL=y  CONFIG_HEAT_CLOUDWATCH_INSTALL=y  CONFIG_CEILOMETER_INSTALL=y  CONFIG_LBAAS_INSTALL=y  Install OpenStack Liberty using packstack.  # packstack --answer-file /root/answers.txt  Source the keystone admin profile.  # . /root/keystonerc_admin  Backup the ifcfg-etho script.  # cp /etc/sysconfig/network-scripts/ifcfg-eth0 /root/  Configure external bridge for floating ip networks.  # vi /etc/sysconfig/network-scripts/ifcfg-eth0 DEVICE=eth0 ONBOOT=yes TYPE=OVSPort DEVICETYPE=ovs OVS_BRIDGE=br-ex # vi /etc/sysconfig/network-scripts/ifcfg-br-ex DEVICE=br-ex BOOTPROTO=static ONBOOT=yes TYPE=OVSBridge DEVICETYPE=ovs USERCTL=yes PEERDNS=yes IPV6INIT=no IPADDR= NETMASK=255.255.255.0 GATEWAY= DNS1=  Add the eht0 physical interface to the br-ex bridge in openVswitch for floating IP networks.  # ovs-vsctl add-port br-ex eth0 ; systemctl restart network.service Configure OpenStack   Create private network.  # neutron net-create private # neutron subnet-create private 10.10.1.0/24 --name private_subnet --allocation-pool start=10.10.1.100,end=10.10.1.200  Create public network. Note: these steps assume the physical network connected to eth0 is 192.168.122.0/24.  # neutron net-create public --router:external # neutron subnet-create public 192.168.122.0/24 --name public_subnet --allocation-pool start=192.168.122.100,end=192.168.122.200 --disable-dhcp --gateway 192.168.122.1  Add a new router and configure router interfaces.  # neutron router-create router1 --ha False # neutron router-gateway-set router1 public # neutron router-interface-add router1 private_subnet  Upload a glance image. In this case we will use a Cirros image because it is small and thus good for testing OpenStack.  # yum install -y wget # wget http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img # glance image-create --name \"Cirros 0.3.4\" --disk-format qcow2 --container-format bare --visibility public --file /root/cirros-0.3.4-x86_64-disk.img  Create a new m1.nano flavor for running Cirros image.  # nova flavor-create m1.nano 42 64 0 1  Create security group and allow all TCP ports.  # nova secgroup-create all \"Allow all tcp ports\" # nova secgroup-add-rule all TCP 1 65535 0.0.0.0/0  Create security group for base access  # nova secgroup-create base \"Allow Base Access\" # nova secgroup-add-rule base TCP 22 22 0.0.0.0/0 # nova secgroup-add-rule base TCP 80 80 0.0.0.0/0 # nova secgroup-add-rule base ICMP -1 -1 0.0.0.0/0  Create a private ssh key for connecting to instances remotely.  # nova keypair-add admin  Create admin.pem file and add private key from output of keypair-add command.  # vi /root/admin.pem # chmod 400 /root/admin.pem  List the network IDs.  # neutron net-list  +--------------------------------------+---------+-------------------------------------------------------+  | id | name | subnets |  +--------------------------------------+---------+-------------------------------------------------------+  | d4f3ed19-8be4-4d56-9f95-cfbac9fdf670 | private | 92d82f53-6e0b-4eef-b8b9-cae32cf40457 10.10.1.0/24     |  | 37c024d6-8108-468c-bc25-1748db7f5e8f | public  | 22f2e901-186f-4041-ad93-f7b5ccc30a81 192.168.122.0/24 |  Start an instance and make sure to replace network id from above command.  # nova boot --flavor m1.nano --image \"Cirros 0.3.4\" --nic net-id=92d82f53-6e0b-4eef-b8b9-cae32cf40457 --key-name admin --security-groups all mycirros  Create a floating IP and assign it to the mycirros instance.  # nova floating-ip-create # nova floating-ip-associate mycirros  Connect to mycirros instance using the private ssh key stored in the admin.pem file. Note: The first floating IP in the range 192.168.122.201.  # ssh -i admin.pem cirros@192.168.122.201 Nova Nested Virtualization  Most OpenStack lab or test environments will install OpenStack on a hypervisor platform inside virtual machines. I would strongly recommend KVM. If you are running OpenStack on KVM (Nova nested virtualization) make sure to follow these tips and tricks to get the best performance.  Summary  This article was intended as a hands on guide for standing up an OpenStack Newton lab environment using RDO. As mentioned RDO is a stable community platform built around Red Hat's OpenStack Platform. It provides the ability to test the latest OpenStack features against either an enterprise platform (RHEL) or community platform (CentOS). Hopefully you found the information in this article useful. If you have anything to add or feedback, feel free to leave your comments.  Happy OpenStacking!  (c) 2017 Keith Tenzer  ","categories": ["OpenStack"],
        "tags": ["KVM","Linux","Newton","OpenStack","RDO"],
        "url": "/openstack/openstack-10-newton-lab-installation-and-configuration-guide/",
        "teaser": null
      },{
        "title": "OpenStack Manila Integration with Ceph",
        "excerpt":"  Overview  In this article we will configure OpenStack Manila using CephFS as a storage backend. OpenStack Manila is an OpenStack project providing file services. Manila is storage backend agnostic and you can have many different kinds of storage backends, similar to Cinder. CephFS is a POSIX-Compliant file system that uses the Ceph storage cluster to store data. CephFS works by providing a Metadata Server (MDS) that collectively manages filesystem namespaces. It also coordinates access to Ceph Object Storage Damones (OSDs). Ceph MDS has two modes: active or passive. There are several documented active/passive MDS configurations and multi-mds or active/active MDS that can be configured when a single MDS becomes a bottleneck. Clients can mount CephFS filesystems using the ceph-fuse client or kernel kernel driver.  Integrating Ceph with OpenStack Series:   Integrating Ceph with OpenStack Cinder, Glance and Nova Integrating Ceph with Swift Integrating Ceph with Manila  Prerequisites  The following are required to configure OpenStack Manila with CephFS:   Already configured Ceph cluster (Jewel or higher). See here to setup Ceph cluster. Already configured OpenStack (Mitaka or higher). See here to setup OpenStack.    Configure CephFS  [All Ceph Nodes]  Add repository for ceph tools.  # subscription-manager repos --enable=rhel-7-server-rhceph-2-tools-rpms [Ceph Ansible Node]  Update Ansible inventory file and add metadata server.  #vi /etc/host/ansible ... [mdss] ceph2 ... Note: For non-lab environment you definitely wan't to configure multiple MDS servers. One is active and additional servers are passive.  Run Ansible.  # su - ansible $ cd /usr/share/ceph-ansible $ ansible-playbook site.yml -vvvv PLAY RECAP ********************************************************************  ceph1 : ok=369 changed=1 unreachable=0 failed=0  ceph2 : ok=364 changed=11 unreachable=0 failed=0  ceph3 : ok=369 changed=1 unreachable=0 failed=0 [Ceph Monitor Node]  Check CephFS metadata server.  # ceph mds stat e24: 1/1/1 up {0=ceph2=up:active} Create keyring and cephx authentication key for Manila service.  # read -d '' MON_CAPS &lt;&lt; EOF allow r, allow command \"auth del\", allow command \"auth caps\", allow command \"auth get\", allow command \"auth get-or-create\" EOF  # ceph auth get-or-create client.manila -o manila.keyring \\ mds 'allow *' \\ osd 'allow rw' \\ mon \"$MON_CAPS\" Enable CephFS snapshots.  # ceph mds set allow_new_snaps true --yes-i-really-mean-it Copy Ceph configuration and manila keyring to OpenStack controller running Manila share service.  # scp /etc/ceph/ceph.conf root@192.168.122.80:/etc/ceph # scp manila.keyring root@192.168.122.80:/etc/ceph Configure Manila  [OpenStack Controller]  Install Ceph tools and python-cephs  # subscription-manager repos --enable=rhel-7-server-rhceph-2-tools-rpms # yum install -y ceph-common # yum install python-cephfs Change permissions on Ceph configuration and keyring for manila.  # chown manila /etc/ceph/manila.keyring # chown manila /etc/ceph/ceph.conf Update Ceph configuration.  # vi /etc/ceph/ceph.conf ... [client.manila] client mount uid = 0 client mount gid = 0 log file = /var/log/manila/ceph-client.manila.log admin socket = /var/run/ceph/ceph-$name.$pid.asok keyring = /etc/ceph/manila.keyring  ... Update Manila configuration.  &nbsp;  # vi /etc/manila/manila.conf ... enabled_share_protocols = NFS,CIFS,CEPHFS enabled_share_backends = generic,cephfs  [cephfs] driver_handles_share_servers = False share_backend_name = cephfs share_driver = manila.share.drivers.cephfs.cephfs_native.CephFSNativeDriver cephfs_conf_path = /etc/ceph/ceph.conf cephfs_auth_id = manila cephfs_cluster_name = ceph cephfs_enable_snapshots = True ... Restart Manila services.  # systemctl restart openstack-manila-scheduler # systemctl restart openstack-manila-api # systemctl restart openstack-manila-share Authenticate to Keystong.  # source /root/keystonerc_admin Set share type for CephFS.  # manila type-create cephfstype false +----------------------+--------------------------------------+ | Property | Value | +----------------------+--------------------------------------+ | required_extra_specs | driver_handles_share_servers : False | | Name | cephfstype | | Visibility | public | | is_default | - | | ID | ae7cc121-d8b6-47e5-86ba-36d607df19b0 | | optional_extra_specs | snapshot_support : True | +----------------------+--------------------------------------+ Create Manila share.  # manila create --share-type cephfstype --name cephshare1 cephfs 1 +-----------------------------+--------------------------------------+ | Property | Value | +-----------------------------+--------------------------------------+ | status | creating | | share_type_name | cephfstype | | description | None | | availability_zone | None | | share_network_id | None | | share_server_id | None | | host | | | access_rules_status | active | | snapshot_id | None | | is_public | False | | task_state | None | | snapshot_support | True | | id | c72318fd-3cb2-4c0a-855e-b75c5bd43c6d | | size | 1 | | user_id | 9d592f8a49654e8592de4e69fd15e603 | | name | cephshare1 | | share_type | ae7cc121-d8b6-47e5-86ba-36d607df19b0 | | has_replicas | False | | replication_type | None | | created_at | 2017-03-28T12:33:30.000000 | | share_proto | CEPHFS | | consistency_group_id | None | | source_cgsnapshot_member_id | None | | project_id | 29f6ba825bf5418395919c85874db4a5 | | metadata | {} | +-----------------------------+--------------------------------------+ View the Manila share.  # manila list +--------------------------------------+------------+------+-------------+-----------+-----------+-----------------+-----------------------------+-------------------+ | ID | Name | Size | Share Proto | Status | Is Public | Share Type Name | Host | Availability Zone | +--------------------------------------+------------+------+-------------+-----------+-----------+-----------------+-----------------------------+-------------------+ | 687d642b-3982-4f79-9b32-21ffb0bb54f9 | cephshare1 | 1 | CEPHFS | available | False | cephfstype | osp10.lab.com@cephfs#cephfs | nova | +--------------------------------------+------------+------+-------------+-----------+-----------+-----------------+-----------------------------+-------------------+ Show Manila export location.  # manila share-export-location-list cephshare1 +--------------------------------------+--------------------------------------------------------------------------------------------------------------------+-----------+ | ID | Path | Preferred | +--------------------------------------+--------------------------------------------------------------------------------------------------------------------+-----------+ | a85a1653-ce6e-4c01-ad05-17b9c41ad241 | 192.168.122.81:6789,192.168.122.82:6789,192.168.122.83:6789:/volumes/_nogroup/b1464433-f10e-458f-b120-b9b41d3f0083 | False | +--------------------------------------+--------------------------------------------------------------------------------------------------------------------+-----------+ Accessing Shares  [OpenStack Controller]  In order to provide access to Manila shares create user.  # manila access-allow cephshare1 cephx keith Create a new keyring for user keith to authenticate via cephx from OpenStack controller running Manila share service.  # ceph --name=client.manila --keyring=/etc/ceph/manila.keyring \\ auth get-or-create client.keith -o keith.keyring Next we will start an RHEL 7.3 instance and access the Manila share. The instance needs to have both the ceph-fuse client, the ceph configuration and user keyring file (for keith) to mount the share.  Start a RHEL instance on OpenStack.  Note: depending on how you setup OpenStack and if you followed guide above you will need to change things in below. Regardless of setup make sure you change net-id to that of your private network.  # nova boot --flavor m1.small --image \"RHEL 7.3\" --nic net-id=332c2dca-a005-42ca-abf2-54637c56bacf --key-name admin --security-groups all myrhel Add floating ip.  Note: floating ip is required and instance needs access to Ceph management network. This can be achieved by adding Ceph management network to OpenStack as public network and assigning floating ip on that network to instance.  # nova floating-ip-create # nova floating-ip-associate myrhel 192.168.122.109 Copy Ceph configuration and keyring for user keith from OpenStack controller to instance.  # scp -i admin.pem /etc/ceph/ceph.conf cloud-user@192.168.122.109: # scp -i admin.pem keith.keyring cloud-user@192.168.122.109: SSH to instance.  # ssh -i admin.pem cloud-user@192.168.122.109 Install Fuse Client.  $ sudo subscription-manager repos --enable=rhel-7-server-rpms $ sudo subscription-manager repos --enable=rhel-7-server-rhceph-2-tools-rpms $ sudo yum install -y ceph-fuse Using ceph-fuse client mount the volume.  $ sudo ceph-fuse /mnt/cephfs --id=keith \\ --conf=/home/cloud-user/ceph.conf --keyring=/home/cloud-user/keith.keyring \\ --client-mountpoint=/volumes/_nogroup/b1464433-f10e-458f-b120-b9b41d3f0083   List mounted file systems and we should see /mnt/ceph. $ df -k Filesystem 1K-blocks Used Available Use% Mounted on /dev/vda1 20956772 1209676 19747096 6% / devtmpfs 922260 0 922260 0% /dev tmpfs 941864 0 941864 0% /dev/shm tmpfs 941864 16680 925184 2% /run tmpfs 941864 0 941864 0% /sys/fs/cgroup tmpfs 188376 0 188376 0% /run/user/1000 ceph-fuse 1048576 0 1048576 0% /mnt/cephfs Resetting MDS Server  [Ceph Monitor]  If your MDS server becomes degraded and you don't have a standby or backup you may need to either reset the journal or repair.  In this case we will show how to reset the MDS journal. Note: resetting the journal gets rid of all MDS metadata so be careful.  # ceph -s  cluster 1e0c9c34-901d-4b46-8001-0d1f93ca5f4d  health HEALTH_ERR  mds rank 0 is damaged  mds cluster is degraded ... Reset Journal.  # cephfs-journal-tool journal reset Set MDS to repaired.  # ceph mds repaired 0 Check MDS status.  # ceph mds stat e46: 1/1/1 up {0=ceph2=up:active} Summary  In this article we configured OpenStack Manila to use the CephFS storage backend. Ceph is the perfect fit for OpenStack storage as it is a unified distributed software-defined storage system that scales with OpenStack. Ceph provides all storage access methods such as block (Cinder, Nova, Glance), file (Manila) and object (S3/Swift/Glance). As such Ceph can satisfy all OpenStack storage needs in a single unified, easy to manage system. Hopefully you found this article of use. Please let me know any and all feedback.  Happy Manilaing!  (c) 2017 Keith Tenzer  ","categories": ["OpenStack"],
        "tags": ["Ceph","CephFS","Manila","OpenStack"],
        "url": "/openstack/openstack-manila-integration-with-ceph/",
        "teaser": null
      },{
        "title": "Storage for Containers using Container Native Storage – Part III",
        "excerpt":"  Overview  In this article we will focus on a new area of storage for containers called Container Native Storage. This article is a collaboration between Daniel Messer (Technical Marketing Manager Storage @RedHat) and Keith Tenzer (Sr. Solutions Architect @RedHat).   Storage for Containers Overview – Part I Storage for Containers using Gluster – Part II Storage for Containers using Container Native Storage – Part III Storage for Containers using Ceph – Part IV Storage for Containers using NetApp ONTAP NAS – Part V Storage for Containers using NetApp SolidFire – Part VI  So What is Container Native Storage?  Essentially container native storage or CNS is a hyper-converged approach to storage and compute in the context of containers. Each container host or node supplies both compute and storage allowing storage to be completely integrated or absorbed by the platform itself. A software-defined storage system running in containers on the platform, consumes disks provided by nodes and provides a cluster-wide storage abstraction layer. The software-defined storage system then provides capabilities such as high-availability, dynamic provisioning and general storage management. This enables DevOps from a storage perspective, allows storage to grow with the container platform and delivers a high level of efficiency.    Container Native Storage Overview  CNS is realized by containerizing GlusterFS in a Docker container. This is deployed on a running OpenShift or Kubernetes cluster using native container orchestration provided by the platform. To provide integration with the storage provisioning framework an additional management component is added to GlusterFS called heketi. It serves as an API and CLI front-end for storage lifecyle operations. In addition it supports multiple deployment scenarios. Heketi runs containerized along side the GlusterFS pods on OpenShift or Kubernetes cluster. The entire deployment of this infrastructure is automated using a utility called cns-deploy.  Container Native Storage Pre-requisites  There are several container orchestration technologies such as Docker Swarm, Marathon (Mesos), Diego (CloudFoundry) and Kubernetes (OpenShift). Container Native Storage applies to Kubernetes and OpenShift as Kubernetes supports both stateless and stateful applications. In order to use CNS you need an OpenShift cluster. To setup OpenShift you can follow this article. As the article focuses on an all-in-one setup, meaning a single VM you will need to make a few minor changes.  Create minimum of three VMs  Each VM should have following spec:   RHEL 7.2 or 7.3 2 vCPUs 4GB RAM 30 GB Root Disk 25 GB Docker Disk 3 x 20GB CNS Disks  Update OpenShift Ansible Inventory  Since the article only deploys a single VM we need to change the inventory to reflect 3 nodes.  ... # host group for masters [masters] ose3-master.lab.com  # host group for nodes, includes region info [nodes] ose3-master.lab.com openshift_schedulable=True ose3-node1.lab.com ose3-node2.lab.com ... Once OpenShift deployment is complete you should see three nodes that are ready.  # oc get nodes NAME STATUS AGE ose3-master.lab.com Ready 5m ose3-node1.lab.com Ready 5m ose3-node2.lab.com Ready 5m Install and Configure Container Native Storage  These steps should be done on OpenShift master node(s).  Enable repository for CNS  subscription-manager repos --enable=rh-gluster-3-for-rhel-7-server-rpms Install CNS Tools  yum install cns-deploy heketi-client Update Firewall Rules  On all OpenShift nodes the firewall rules need to be updated.  # vi /etc/sysconfig/iptables ... -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 24007 -j ACCEPT -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 24008 -j ACCEPT -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 2222 -j ACCEPT -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m multiport --dports 49152:49664 -j ACCEPT ... # systemctl reload iptables Create Project in OpenShift for CNS  oc new-project storage-project Enable deployment of priviledged containers  # oadm policy add-scc-to-user privileged -z default # oadm policy add-scc-to-user privileged -z router # oadm policy add-scc-to-user privileged -z default Update dnsmasq for Router  On OpenShift master we need to setup external resolution to the CNS nodes. If you have a real DNS server this should be done there.  # vi /etc/dnsmasq.conf ... address=/.apps.lab.com/192.168.122.61 ... # systectl restart dnsmasq Add localhost as nameserver  # vi /etc/resolv.conf ... nameserver 127.0.0.1 ... Create Configuration Template for CNS  The template identifies all nodes in the CNS cluster. It also identifies which devices should be used. CNS requires at a minimum three nodes. Data will be by default replicated three times hence why three nodes are required.  # cp /usr/share/heketi/topology-sample.json vi /usr/share/heketi/topology.json # vi /usr/share/heketi/topology.json {     \"clusters\": [         {             \"nodes\": [                 {                     \"node\": {                         \"hostnames\": {                             \"manage\": [                                 \"ose3-master.lab.com\"                             ],                             \"storage\": [                                 \"192.168.122.61\"                             ]                         },                         \"zone\": 1                     },                     \"devices\": [                         \"/dev/vdc\",                         \"/dev/vdd\",                         \"/dev/vde\"                     ]                 },                 {                     \"node\": {                         \"hostnames\": {                             \"manage\": [                                 \"ose3-node1.lab.com\"                             ],                             \"storage\": [                                 \"192.168.122.62\"                             ]                         },                         \"zone\": 2                     },                     \"devices\": [                         \"/dev/vdc\",                         \"/dev/vdd\",                         \"/dev/vde\"                     ]                 },                 {                     \"node\": {                         \"hostnames\": {                             \"manage\": [                                 \"ose3-node2.lab.com\"                             ],                             \"storage\": [                                 \"192.168.122.63\"                             ]                         },                         \"zone\": 2                     },                     \"devices\": [                         \"/dev/vdc\",                         \"/dev/vdd\",                         \"/dev/vde\"                     ]                 }             ]         }     ] }  Deploy Container Native Storage  # cns-deploy -n storage-project -g /usr/share/heketi/topology.json  Welcome to the deployment tool for GlusterFS on Kubernetes and OpenShift.  Before getting started, this script has some requirements of the execution  environment and of the container platform that you should verify.  The client machine that will run this script must have:  * Administrative access to an existing Kubernetes or OpenShift cluster  * Access to a python interpreter 'python'  * Access to the heketi client 'heketi-cli'  Each of the nodes that will host GlusterFS must also have appropriate firewall  rules for the required GlusterFS ports:  * 2222 - sshd (if running GlusterFS in a pod)  * 24007 - GlusterFS Daemon  * 24008 - GlusterFS Management  * 49152 to 49251 - Each brick for every volume on the host requires its own  port. For every new brick, one new port will be used starting at 49152. We  recommend a default range of 49152-49251 on each host, though you can adjust  this to fit your needs.  In addition, for an OpenShift deployment you must:  * Have 'cluster_admin' role on the administrative account doing the deployment  * Add the 'default' and 'router' Service Accounts to the 'privileged' SCC  * Have a router deployed that is configured to allow apps to access services  running in the cluster  Do you wish to proceed with deployment?  [Y]es, [N]o? [Default: Y]: Y  Multiple CLI options detected. Please select a deployment option.  [O]penShift, [K]ubernetes? [O/o/K/k]: O  Using OpenShift CLI.  NAME STATUS AGE  storage-project Active 4m  Using namespace \"storage-project\".  template \"deploy-heketi\" created  serviceaccount \"heketi-service-account\" created  template \"heketi\" created  template \"glusterfs\" created  node \"ose3-master.lab.com\" labeled  node \"ose3-node1.lab.com\" labeled  node \"ose3-node2.lab.com\" labeled  daemonset \"glusterfs\" created  Waiting for GlusterFS pods to start ... OK  service \"deploy-heketi\" created  route \"deploy-heketi\" created  deploymentconfig \"deploy-heketi\" created  Waiting for deploy-heketi pod to start ... OK  % Total % Received % Xferd Average Speed Time Time Time Current  Dload Upload Total Spent Left Speed  100 17 100 17 0 0 864 0 --:--:-- --:--:-- --:--:-- 894  Creating cluster ... ID: 4bfec05e6fa80e5178c4314bec238786  Creating node ose3-master.lab.com ... ID: f95eabc360cddd6f5c6419094c1ae085  Adding device /dev/vdc ... OK  Adding device /dev/vdd ... OK  Adding device /dev/vde ... OK  Creating node ose3-node1.lab.com ... ID: 82fa6bf3a37dffa4376c77935f37d44a  Adding device /dev/vdc ... OK  Adding device /dev/vdd ... OK  Adding device /dev/vde ... OK  Creating node ose3-node2.lab.com ... ID: c26872fc64f2408f2ddea664698e3964  Adding device /dev/vdc ... OK  Adding device /dev/vdd ... OK  Adding device /dev/vde ... OK  Saving heketi-storage.json  secret \"heketi-storage-secret\" created  endpoints \"heketi-storage-endpoints\" created  service \"heketi-storage-endpoints\" created  job \"heketi-storage-copy-job\" created  deploymentconfig \"deploy-heketi\" deleted  route \"deploy-heketi\" deleted  service \"deploy-heketi\" deleted  pod \"deploy-heketi-1-z8ite\" deleted  job \"heketi-storage-copy-job\" deleted  secret \"heketi-storage-secret\" deleted  service \"heketi\" created  route \"heketi\" created  deploymentconfig \"heketi\" created  Waiting for heketi pod to start ... OK  % Total % Received % Xferd Average Speed Time Time Time Current  Dload Upload Total Spent Left Speed  100 17 100 17 0 0 2766 0 --:--:-- --:--:-- --:--:-- 2833  heketi is now running. List Pods in Storage Project  Once deployment is complete you should see three GlusterFS pods, the heketi pod (CNS management) and a router.  # oc get pods -o wide  NAME READY STATUS RESTARTS AGE IP NODE  glusterfs-eedk4 1/1 Running 0 4m 192.168.122.63 ose3-node2.lab.com  glusterfs-kyrz1 1/1 Running 0 4m 192.168.122.62 ose3-node1.lab.com  glusterfs-y6w8n 1/1 Running 0 4m 192.168.122.61 ose3-master.lab.com  heketi-1-zq0ie 1/1 Running 0 2m 10.129.0.10 ose3-master.lab.com  storage-project-router-1-nnobe 1/1 Running 0 8m 192.168.122.61 ose3-master.lab.com Setup Heketi CLI  Heketi is a management tool for CNS. We need to export the path to the server in order to use the CLI.  # export HEKETI_CLI_SERVER=$(oc describe svc/heketi | grep \"Endpoints:\" | awk '{print \"http://\"$2}') # echo $HEKETI_CLI_SERVER http://10.129.0.10:8080 Show CNS Topology  # heketi-cli topology info  Cluster Id: 4bfec05e6fa80e5178c4314bec238786  Volumes:  Name: heketidbstorage  Size: 2  Id: e64a8b64f58bf5248afdb1db34ba420f  Cluster Id: 4bfec05e6fa80e5178c4314bec238786  Mount: 192.168.122.61:heketidbstorage  Mount Options: backup-volfile-servers=192.168.122.62,192.168.122.63  Durability Type: replicate  Replica: 3  Snapshot: Disabled  Bricks:  Id: 2504dbb5b0b9fd38c3c8eaa25c19e6e0  Path: /var/lib/heketi/mounts/vg_4b315e3d01f3398ea371cc3ec44a46ab/brick_2504dbb5b0b9fd38c3c8eaa25c19e6e0/brick  Size (GiB): 2  Node: f95eabc360cddd6f5c6419094c1ae085  Device: 4b315e3d01f3398ea371cc3ec44a46ab  Id: 30fea25c05c3c7b252590b81c3f38369  Path: /var/lib/heketi/mounts/vg_001e9e13cf06727862b157283b22051d/brick_30fea25c05c3c7b252590b81c3f38369/brick  Size (GiB): 2  Node: c26872fc64f2408f2ddea664698e3964  Device: 001e9e13cf06727862b157283b22051d  Id: d7c2b9e7b80ed2726309ad516dd253cf  Path: /var/lib/heketi/mounts/vg_4f8745833e2577ff9a1eb302d9811551/brick_d7c2b9e7b80ed2726309ad516dd253cf/brick  Size (GiB): 2  Node: 82fa6bf3a37dffa4376c77935f37d44a  Device: 4f8745833e2577ff9a1eb302d9811551  Nodes:  Node Id: 82fa6bf3a37dffa4376c77935f37d44a  State: online  Cluster Id: 4bfec05e6fa80e5178c4314bec238786  Zone: 2  Management Hostname: ose3-node1.lab.com  Storage Hostname: 192.168.122.62  Devices:  Id:26333a53457037df86243d164d280f07 Name:/dev/vdc State:online Size (GiB):29 Used (GiB):0 Free (GiB):29  Bricks:  Id:4f8745833e2577ff9a1eb302d9811551 Name:/dev/vde State:online Size (GiB):29 Used (GiB):2 Free (GiB):27  Bricks:  Id:d7c2b9e7b80ed2726309ad516dd253cf Size (GiB):2 Path: /var/lib/heketi/mounts/vg_4f8745833e2577ff9a1eb302d9811551/brick_d7c2b9e7b80ed2726309ad516dd253cf/brick  Id:c1520ae2b0adbf0fec0b0ffd5fd5a0f7 Name:/dev/vdd State:online Size (GiB):29 Used (GiB):0 Free (GiB):29  Bricks:  Node Id: c26872fc64f2408f2ddea664698e3964  State: online  Cluster Id: 4bfec05e6fa80e5178c4314bec238786  Zone: 2  Management Hostname: ose3-node2.lab.com  Storage Hostname: 192.168.122.63  Devices:  Id:001e9e13cf06727862b157283b22051d Name:/dev/vde State:online Size (GiB):29 Used (GiB):2 Free (GiB):27  Bricks:  Id:30fea25c05c3c7b252590b81c3f38369 Size (GiB):2 Path: /var/lib/heketi/mounts/vg_001e9e13cf06727862b157283b22051d/brick_30fea25c05c3c7b252590b81c3f38369/brick  Id:705d793971aeb2c3315ea674af0aace1 Name:/dev/vdd State:online Size (GiB):29 Used (GiB):0 Free (GiB):29  Bricks:  Id:cc542ecd46d872a8db41819f2f9f69fe Name:/dev/vdc State:online Size (GiB):29 Used (GiB):0 Free (GiB):29  Bricks:  Node Id: f95eabc360cddd6f5c6419094c1ae085  State: online  Cluster Id: 4bfec05e6fa80e5178c4314bec238786  Zone: 1  Management Hostname: ose3-master.lab.com  Storage Hostname: 192.168.122.61  Devices:  Id:4b315e3d01f3398ea371cc3ec44a46ab Name:/dev/vdd State:online Size (GiB):29 Used (GiB):2 Free (GiB):27  Bricks:  Id:2504dbb5b0b9fd38c3c8eaa25c19e6e0 Size (GiB):2 Path: /var/lib/heketi/mounts/vg_4b315e3d01f3398ea371cc3ec44a46ab/brick_2504dbb5b0b9fd38c3c8eaa25c19e6e0/brick  Id:dc37c4b891c0268f159f1b0b4b21be1e Name:/dev/vde State:online Size (GiB):29 Used (GiB):0 Free (GiB):29  Bricks:  Id:fab4c9f1f82010164a26ba162411211a Name:/dev/vdc State:online Size (GiB):29 Used (GiB):0 Free (GiB):29  Bricks: Using CNS in OpenShift  In order to create persistent volumes using dynamic provisioning a storage class must be created. Storage classes in Kubernetes provide Kubernetes (OpenShift) with access and permissions to the storage system. A plugin exists for the storage provider (in this case GlusterFS) that knows how to provision and reclaim storage.  Create Storage Class  A storage class just like anything in Kubernetes is an object defined by YAML or JSON.  # vi /root/glusterfs-storage-class.yaml  apiVersion: storage.k8s.io/v1beta1  kind: StorageClass  metadata:  name: glusterfs-container  provisioner: kubernetes.io/glusterfs  parameters:  resturl: \"http://10.129.0.10:8080\"  restuser: \"admin\"  secretNamespace: \"default\"  secretName: \"heketi-secret\" Create storage class from YAML using oc command  # oc create -f /root/glusterfs-storage-class.yaml Setup Secret  Secrets are used in OpenShift to grant access to services, in this case CNS.  Create password.  # echo -n \"mypassword\" | base64 bXlwYXNzd29yZA== Create a secret object for CNS  # vi /root/glusterfs-secret.yaml  apiVersion: v1  kind: Secret  metadata:  name: heketi-secret  namespace: default  data:  key: bXlwYXNzd29yZA==  type: kubernetes.io/glusterfs Create secret using YAML file.  # oc create -f glusterfs-secret.yaml Create Persistent Volume Claim Using CLI  Persistent volume claim is what developers issue when they need storage. It binds a persistent volume to a Pod in OpenShift. In our case since CNS supports dynamic provisioning creating a claim will also create the volume and as such provision storage.  #vi /root/glusterfs-pvc-1.yaml {   \"kind\": \"PersistentVolumeClaim\",   \"apiVersion\": \"v1\",   \"metadata\": {     \"name\": \"claim1\",     \"annotations\": {         \"volume.beta.kubernetes.io/storage-class\": \"glusterfs-container\"     }   },   \"spec\": {     \"accessModes\": [       \"ReadWriteOnce\"     ],     \"resources\": {       \"requests\": {         \"storage\": \"4Gi\"       }     }   } } Show Persistent Volume Claim  # oc get pvc NAME STATUS VOLUME CAPACITY ACCESSMODES AGE claim1 Bound pvc-6b4599fa-0813-11e7-a395-525400c9c97e 4Gi RWO 13s Show Persistent Volume  # oc get pv NAME CAPACITY ACCESSMODES RECLAIMPOLICY STATUS CLAIM REASON AGE pvc-6b4599fa-0813-11e7-a395-525400c9c97e 4Gi RWO Delete Bound storage-project/claim1 2m Using Persistent Volume Claim  Once a developer has a claim they simply need to add the ClaimName to their Pod YAML file. This can also be done easily via the GUI as we will see shortly. Below is an example using CLI.  apiVersion: v1 kind: Pod metadata:   name: busybox spec:   containers:     - image: busybox       command:         - sleep         - \"3600\"       name: busybox       volumeMounts:         - mountPath: /usr/share/busybox           name: mypvc   volumes:     - name: mypvc       persistentVolumeClaim:         claimName: claim1 Create Persistent Volume Claim Using GUI  Under the project in OpenShift choose 'Storage' and 'create storage' button on far right. Specify name, access mode and size. Click 'create' and OpenShift will create the persistent volume and persistent volume claim.    Once the persistent volume claim is created it will show up under storage tab of OpenShift project.    At this point you can use the persistent volume claim for any Pods.  Another way to do things is to deploy application from pre-defined template. Such templates exist out-of-the-box in OpenShift. Here we will deploy mariadb using the 'mariadb-persistent' template.  Create new project 'mariadb'.    Type in 'mariadb' in search field of catalog to narrow down choices. Select 'MariadDB (Persistent'.    Select defaults to launch mariadb Pod on next screen. The Pod will stay in pending state because it's persistent volume is not mapped to a storage class. This can be of course updated in the storage class itself. To specify a default cluster-wide storage class update the annotations section of the storage class.  ... annotations:   storageclass.beta.kubernetes.io/is-default-class: true ...   Under storage select the persistent volume claim 'mariadb'. On the right under actions select 'edit yaml'. Add following to persistent volume claim yaml.  ... annotations:  volume.beta.kubernetes.io/storage-class: glusterfs-container ...   Once we click save on next screen we should see our volume is bound after a few moments. This means that OpenShift provisioned a volume from CNS.    Finally our mariadb Pod should now be able to start.    CNS Deep Dive  Now that we have seen how to use CNS in context of OpenShift let us take a closer look into the storage itself and understand how persistent volumes are mapped to GlusterFS volumes.  Get Glusterfs Volume  If we look at the YAML for the persistent volume (pv) we can get the GlusterFS volume.  # oc get pv pvc-acbade81-0818-11e7-a395-525400c9c97e -o yaml  apiVersion: v1  kind: PersistentVolume  metadata:  annotations:  pv.beta.kubernetes.io/gid: \"2001\"  pv.kubernetes.io/bound-by-controller: \"yes\"  pv.kubernetes.io/provisioned-by: kubernetes.io/glusterfs  volume.beta.kubernetes.io/storage-class: glusterfs-container  creationTimestamp: 2017-03-13T18:12:59Z  name: pvc-acbade81-0818-11e7-a395-525400c9c97e  resourceVersion: \"10271\"  selfLink: /api/v1/persistentvolumes/pvc-acbade81-0818-11e7-a395-525400c9c97e  uid: b10085a3-0818-11e7-a395-525400c9c97e  spec:  accessModes:  - ReadWriteOnce  capacity:  storage: 1Gi  claimRef:  apiVersion: v1  kind: PersistentVolumeClaim  name: mariadb  namespace: my-ruby  resourceVersion: \"10262\"  uid: acbade81-0818-11e7-a395-525400c9c97e  glusterfs:  endpoints: gluster-dynamic-mariadb  path: vol_094f7fc95d623fdc88c72aa5cb303b24  persistentVolumeReclaimPolicy: Delete  status:  phase: Bound Connect to Glusterfs Node  Under the project 'storage-project' get list of the GlusterFS pods. These are the pods the cns-deploy stood up as part of the initial install. They are part of a Kubernetes DaemonSet with a node selector. All nodes with the label storagenode=glusterfs will be part of this DaemonSet and hence run a GlusterFS pod. This allows for easy expansion of additional pods later.  # oc project storage-project  Already on project \"storage-project\" on server \"https://ose3-master2.lab.com:8443\". # oc get pods NAME READY STATUS RESTARTS AGE glusterfs-eedk4 1/1 Running 0 1h glusterfs-kyrz1 1/1 Running 0 1h glusterfs-y6w8n 1/1 Running 0 1h heketi-1-zq0ie 1/1 Running 0 1h storage-project-router-1-nnobe 1/1 Running 0 1h Choose one of the glusterfs nodes and connect to Pod using 'oc' command.  # oc exec -ti glusterfs-eedk4 /bin/sh List Gluster Volumes  From a GlusterFS node we can list volumes. We can see the volume name and mountpoint highlighted in bold below.  sh-4.2# gluster volume info all  Volume Name: heketidbstorage  Type: Replicate  Volume ID: 17779abc-870d-4f4f-9e29-60eea6d5e01e  Status: Started  Number of Bricks: 1 x 3 = 3  Transport-type: tcp  Bricks:  Brick1: 192.168.122.63:/var/lib/heketi/mounts/vg_001e9e13cf06727862b157283b22051d/brick_30fea25c05c3c7b252590b81c3f38369/brick  Brick2: 192.168.122.61:/var/lib/heketi/mounts/vg_4b315e3d01f3398ea371cc3ec44a46ab/brick_2504dbb5b0b9fd38c3c8eaa25c19e6e0/brick  Brick3: 192.168.122.62:/var/lib/heketi/mounts/vg_4f8745833e2577ff9a1eb302d9811551/brick_d7c2b9e7b80ed2726309ad516dd253cf/brick  Options Reconfigured:  performance.readdir-ahead: on  Volume Name: vol_094f7fc95d623fdc88c72aa5cb303b24  Type: Replicate  Volume ID: e29be8b3-b733-4c2e-a536-70807d948fd6  Status: Started  Number of Bricks: 1 x 3 = 3  Transport-type: tcp  Bricks:  Brick1: 192.168.122.63:/var/lib/heketi/mounts/vg_cc542ecd46d872a8db41819f2f9f69fe/brick_818bd64213310df8f7fa6b05734d882d/brick  Brick2: 192.168.122.62:/var/lib/heketi/mounts/vg_c1520ae2b0adbf0fec0b0ffd5fd5a0f7/brick_be228a22ac79112b7474876211e0686f/brick  Brick3: 192.168.122.61:/var/lib/heketi/mounts/vg_fab4c9f1f82010164a26ba162411211a/brick_635253e7ef1e8299b993a273fa808cf6/brick  Options Reconfigured:  performance.readdir-ahead: on  Volume Name: vol_e462bd9fa459d0ba088198892625e00d  Type: Replicate  Volume ID: 9272b326-bf9c-4a6a-b570-d43c6e2cba83  Status: Started  Number of Bricks: 1 x 3 = 3  Transport-type: tcp  Bricks:  Brick1: 192.168.122.63:/var/lib/heketi/mounts/vg_705d793971aeb2c3315ea674af0aace1/brick_165ecbfd4e8923a8efcb8d733a601971/brick  Brick2: 192.168.122.62:/var/lib/heketi/mounts/vg_c1520ae2b0adbf0fec0b0ffd5fd5a0f7/brick_4008026414bf63a9a7c26ac7cd09cf16/brick  Brick3: 192.168.122.61:/var/lib/heketi/mounts/vg_fab4c9f1f82010164a26ba162411211a/brick_003626574ffc4c9c96f22f7cda5ea8af/brick  Options Reconfigured:  performance.readdir-ahead: on  Look for local mount usi  sh-4.2# mount | grep heketi  /dev/mapper/rhel-root on /var/lib/heketi type xfs (rw,relatime,seclabel,attr2,inode64,noquota)  /dev/mapper/vg_001e9e13cf06727862b157283b22051d-brick_30fea25c05c3c7b252590b81c3f38369 on /var/lib/heketi/mounts/vg_001e9e13cf06727862b157283b22051d/brick_30fea25c05c3c7b252590b81c3f38369 type xfs (rw,noatime,seclabel,nouuid,attr2,inode64,logbsize=256k,sunit=512,swidth=512,noquota)  /dev/mapper/vg_705d793971aeb2c3315ea674af0aace1-brick_165ecbfd4e8923a8efcb8d733a601971 on /var/lib/heketi/mounts/vg_705d793971aeb2c3315ea674af0aace1/brick_165ecbfd4e8923a8efcb8d733a601971 type xfs (rw,noatime,seclabel,nouuid,attr2,inode64,logbsize=256k,sunit=512,swidth=512,noquota)  /dev/mapper/vg_cc542ecd46d872a8db41819f2f9f69fe-brick_818bd64213310df8f7fa6b05734d882d on /var/lib/heketi/mounts/vg_cc542ecd46d872a8db41819f2f9f69fe/brick_818bd64213310df8f7fa6b05734d882d type xfs (rw,noatime,seclabel,nouuid,attr2,inode64,logbsize=256k,sunit=512,swidth=512,noquota) List Contents of Mountpoint  Once we have mountpoint we can do an 'ls' to list the contents. Here we can see mariadb files since this is the volume owned by our mariadb Pod.  sh-4.2# ls /var/lib/heketi/mounts/vg_cc542ecd46d872a8db41819f2f9f69fe/brick_818bd64213310df8f7fa6b05734d882d/brick/ aria_log.00000001 aria_log_control ib_logfile0 ib_logfile1 ibdata1 mariadb-1-wwuu4.pid multi-master.info mysql performance_schema sampledb tc.log test Sunmmary  In this article we focused on Container Native Storage (CNS).  We discussed the need for CNS in order to make storage completely integrated into DevOps. This is a very exciting time for storage and CNS is an approach to do storage differently, making storage a first-class DevOps citizen instead of taking traditional storage and bolting it on somehow. We explored OpenShift pre-requisites for CNS as well as installed and configured CNS on OpenShift. Using the CLI and GUI we saw how developers can manage storage enabled by CNS from within OpenShift. Finally we took a bit of a deep-dive to examine how CNS maps to persistent volume claims in OpenShift. Hopefully you found this article interesting and useful. Looking forward to any and all feedback!  Happy Container Native Storaging!  (c) 2017 Keith Tenzer  ","categories": ["OpenShift"],
        "tags": ["Container Native Storage","Docker","Gluster","Kubernetes","OpenShift","Storage"],
        "url": "/openshift/storage-for-containers-using-container-native-storage-part-iii/",
        "teaser": null
      },{
        "title": "OpenStack Swift Integration with Ceph",
        "excerpt":"  Overview  In this article we will configure OpenStack Swift to use Ceph as a storage backend. Object of cloud storage is one of the main services provided by OpenStack. Swift is an object storage protocol and implementation. It has been around for quite a while but is fairly limited (it uses rsync to replicate data, scaling rings can be problematic and it only supports object storage to just mention a few things). OpenStack needs to provide storage for many use cases such as block (Cinder), block (Glance), file (Manila), block (Nova) and object (Swift). Ceph is a distributed software-defined storage system that scales with OpenStack and provides all these use cases. As such it is the defacto standard for OpenStack and is why you see in OpenStack user survey that Ceph is 60% of all OpenStack storage.  OpenStack uses Keystone to store service endpoints for all services. Swift has a Keystone endpoint that authenticates OpenStack tenants to Swift providing object or cloud storage on a per-tenant basis. As mentioned, Ceph provides block, file and object access. In the case of object Ceph provides S3, Swift and NFS interfaces. The RADOS Gateway (RGW) provides object interfaces for Ceph. S3 and Swift users are stored in the RGW. Usually you would want several RADOS GWs in an active/active configuration using a load balancer. OpenStack tenants can be given automatic access and their Keystone tenant ids are automatically configured in the RADOS GW when Swift object storage is accessed from given tenant.  Using Ceph with OpenStack for object storage provides tenants access to cloud storage, integrated with OpenStack using swift and automatically handles authentication of OpenStack tenants. It also provides advantage that external users or tenants (outside of OpenStack) such as application developers can access object storage directly with  protocol of choice: S3, Swift or NFS.  Integrating Ceph with OpenStack Series:   Integrating Ceph with OpenStack Cinder, Glance and Nova Integrating Ceph with Swift Integrating Ceph with Manila  In order to integrate OpenStack Swift with Ceph you need to first follow below prerequisites:   Configure OpenStack environment here Configure Ceph cluster here    OpenStack Configuration  In OpenStack we need to delete the already existing Swift endpoint and add a new one that points to RADOS Gateway (RGW).  [OpenStack Controller]  Delete Existing Swift Endpoint  # openstack endpoint list  +----------------------------------+-----------+--------------+----------------+  | ID | Region | Service Name | Service Type |  +----------------------------------+-----------+--------------+----------------+  | 8a13a475c4904fc3a3b967e3118ea996 | RegionOne | nova | compute |  | d9741ef7fe024cb48566530fb0d69981 | RegionOne | aodh | alarming |  | 3c841904adf84e678d23400d713fed09 | RegionOne | cinderv3 | volumev3 |  | f685c9fa042448cd962828b99f124c4b | RegionOne | cinder | volume |  | 07d77cefad0049b1ae5e1eb6692ebfa1| RegionOne | swift | object-store |  | dd14ff462f4a4bfbb260aa60562ca225 | RegionOne | heat-cfn | cloudformation |  | dbac05cbfb174b048edfb78356532201 | RegionOne | keystone | identity |  | b3f56e1257de4268be9f003936fe32d7 | RegionOne | gnocchi | metric |  | a103977deb8f45389f1b6fa8705fdef1 | RegionOne | neutron | network |  | baa32d390ab64d48a67b74959ff9951e | RegionOne | manila | share |  | 421bc582aefd493ebf2d7377d565954e | RegionOne | cinderv2 | volumev2 |  | e84a94f373dd45efb069f7060d81a166 | RegionOne | glance | image |  | 4200e6b1ae7448369a19bbe7295091e8 | RegionOne | manilav2 | sharev2 |  | b4446a635fc246d093c875805e08225f | RegionOne | ceilometer | metering |  | 014695a1e8f54830ad267f272552fe78 | RegionOne | heat | orchestration |  +----------------------------------+-----------+--------------+----------------+ # openstack endpoint delete 07d77cefad0049b1ae5e1eb6692ebfa1 Note: if endpoint fails to delete you can manually remove it from Keystone database  # mysql keystone MariaDB [keystone]&gt; delete from endpoint where id=\"07d77cefad0049b1ae5e1eb6692ebfa1\"; Create Swift Endpoint  If Swift wasn't previously installed then you also need to create a Swift service in Keystone.  # openstack endpoint create --region RegionOne --publicurl \\  \"http://192.168.122.81:8080/swift/v1\" --adminurl \"http://192.168.122.81:8080/swift/v1\" \\ --internalurl \"http://192.168.122.81:8080/swift/v1\" swift Show the endpoint  # openstack endpoint show d30b81fec7f0406e9c23156745074d6b  +--------------+-------------------------------------+  | Field | Value |  +--------------+-------------------------------------+  | adminurl | http://192.168.122.81:8080/swift/v1 |  | enabled | True |  | id | d30b81fec7f0406e9c23156745074d6b |  | internalurl | http://192.168.122.81:8080/swift/v1 |  | publicurl | http://192.168.122.81:8080/swift/v1 |  | region | RegionOne |  | service_id | bce5f3fdb05e46589d19b915e0e9ac53 |  | service_name | swift |  | service_type | object-store |  +--------------+-------------------------------------+ Ceph Configuration  In Ceph we need to configure the RADOS Gateway. The RGW will authenticate OpenStack tenants accessing the object storage from OpenStack using Keystone. As such we need to configure access to Keystone.  [Ceph Monitors]  # vi /etc/ceph/ceph.conf  ... [client.rgw.ceph3] host = ceph3 keyring = /var/lib/ceph/radosgw/ceph-rgw.ceph3/keyring rgw socket path = /tmp/radosgw-ceph3.sock log file = /var/log/ceph/ceph-rgw-ceph3.log rgw data = /var/lib/ceph/radosgw/ceph-rgw.ceph3 rgw frontends = civetweb port=8080  # Keystone information rgw_keystone_url = http://192.168.122.80:5000 rgw_keystone_admin_user = admin rgw_keystone_admin_password = redhat01 rgw_keystone_admin_tenant = admin rgw_keystone_accepted_roles = admin rgw_keystone_token_cache_size = 10 rgw_keystone_revocation_interval = 300 rgw_keystone_make_new_tenants = true rgw_s3_auth_use_keystone = true #nss_db_path = {path to nss db} rgw_keystone_verify_ssl = false ... Note: In this example we are setting ssl verify to false so we aren't using ssl certificates.  Copy the /etc/ceph/ceph.conf to all nodes in Ceph cluster.  [RGW Nodes]  Restart all RGWs by finding the service name and then restarting on all nodes running RGW.  # systemctl status |grep radosgw  │ └─64133 grep --color=auto radosgw  ├─system-ceph\\x2dradosgw.slice  │ └─ceph-radosgw@rgw.ceph1.service  │ └─49414 /usr/bin/radosgw -f --cluster ceph --name client.rgw.ceph1 --setuser ceph --setgroup ceph # systemctl restart ceph-radosgw@rgw.ceph1.service Validating Swift  Finally now that things are configured we can test access the object store through a tenant in OpenStack. For this I have created a tenant named \"Test\" and a tenant user that has access to \"Test\" project named \"keith\".  [OpenStack Controller]  Show User Info  # openstack user show keith  +------------+----------------------------------+  | Field | Value |  +------------+----------------------------------+  | enabled | True |  | id | ff29eea35c234ae08fbd0c124ae1cdca |  | name | keith |  | project_id | bba8cef1e00744e48ef10d3b68ec8002 |  | username | keith |  +------------+----------------------------------+ Note: pay attention to the project or tenant id.  Upload File to Swift  Authenticate as user keith.  # source keystonerc_keith Upload file /root/answers.txt to a container called config. If container doesn't exist it is created.  # swift upload config /root/answers.txt root/answers.txt List contents of container.  # swift list config root/answers.txt Show contents of container in Horizon.    Show OpenStack Tenant in RADOS Gateway  As mentioned the OpenStack tenant was automatically created as a user in the RGW.  [RGW Nodes]  # radosgw-admin metadata list user  [  \"b22c788f00a14d718fd6b6986987b7c1\",  \"bba8cef1e00744e48ef10d3b68ec8002\",  \"29f6ba825bf5418395919c85874db4a5\",  \"s3user\"  ] List Info for tenant user.  # radosgw-admin user info --uid=\"bba8cef1e00744e48ef10d3b68ec8002\"  {  \"user_id\": \"bba8cef1e00744e48ef10d3b68ec8002\",  \"display_name\": \"Test\",  \"email\": \"\",  \"suspended\": 0,  \"max_buckets\": 1000,  \"auid\": 0,  \"subusers\": [],  \"keys\": [],  \"swift_keys\": [],  \"caps\": [],  \"op_mask\": \"read, write, delete\",  \"default_placement\": \"\",  \"placement_tags\": [],  \"bucket_quota\": {  \"enabled\": false,  \"max_size_kb\": -1,  \"max_objects\": -1  },  \"user_quota\": {  \"enabled\": false,  \"max_size_kb\": -1,  \"max_objects\": -1  },  \"temp_url_keys\": []  } Summary  In this article we learned how to configure OpenStack Swift to use Ceph as a storage backend for tenant cloud storage. We briefly discussed the value of using Ceph instead of Swift for OpenStack object storage. Finally we saw how user management works between Keystone and the RADOS Gateway in OpenStack. I hope you enjoyed this article and found it of use. Comments are more than welcome so don't be shy.  Happy Swifting!  (c) 2017 Keith Tenzer  &nbsp;  &nbsp;  &nbsp;  ","categories": ["OpenStack"],
        "tags": ["Ceph","cloud storage","object storage","OpenStack","Swift"],
        "url": "/openstack/openstack-swift-integration-with-ceph/",
        "teaser": null
      },{
        "title": "Storage for Containers using NetApp ONTAP NAS – Part V",
        "excerpt":"  Overview  In this article we will look at how you can configure and dynamically provision ONTAP NAS Storage for containerized applications in a Kubernetes/OpenShift environment.  This article is a work from Kapil Arora (Cloud Platform Architect @NetApp).   Storage for Containers Overview – Part I Storage for Containers using Gluster – Part II Storage for Containers using Container Native Storage – Part III Storage for Containers using Ceph – Part IV Storage for Containers using NetApp ONTAP NAS – Part V Storage for Containers using NetApp SolidFire – Part VI  NetApp recently released an open source project known as Trident, the first external storage provisioner for Kubernetes leveraging on-premises storage.  Trident enables the use of the new storage class concept in Kubernetes, acting as a provisioning controller that watches for PVCs (persistent volume requests) and creates them on-demand.  This means that when a pod requests storage from a storage class that Trident is responsible for, it will provision a volume that meets those requirements and make it available to the pod in real-time.  To learn more  check-out these  Trident videos.    This is how it works:   The OpenShift/Kubernetes Administrator deploys and configures Trident with NetApp ONTAP NAS and defines StorageClasses and Quotas. The developer creates a PVC  with the respective StorageClass. Trident dynamically provisions storage for the developer.  Installation and Configuration Instructions  Following are the steps to install and configure NetApp ONTAP NAS backend with Trident in your OpenShift environment:  1) Ensure OpenShift 3.4+, or OpenShift Origin 1.4+  Create a service account for trident.  oc create serviceaccount trident oc adm policy add-cluster-role-to-user storage-admin system:serviceaccount:myproject:trident oc adm policy add-scc-to-user anyuid system:serviceaccount:myproject:trident oc adm policy add-role-to-user edit system:serviceaccount:myproject:trident You can find detailed  instructions here.  2) Install 'kubectl' client for Trident install script  curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl chmod +x ./kubectl  sudo mv ./kubectl /usr/local/bin/kubectl  You can find detailed instructions here.  3) Download and untar the Trident installer bundle.  4) Install nfs utils on all your OpenShift nodes if not already installed.  $ sudo yum install -y nfs-utils 5) Configure the ONTAP backend. A sample is provided here sample-input/backend-ontap-nas.json. Copy the edited file to setup/backed.json   https://gist.github.com/kapilarora/ce450bf8840f57ce5f152a45689785aa  6) Execute Trident installer script.  $ ./install_trident.sh --namespace myproject --serviceaccount trident 7) Wait until the deployment is complte. You should see Trident deployment when you execute 'oc get deployment' and trident pod when you execute 'oc get pods'  8) If you face issues, check this troubleshooting session.  9) Register you backend with Trident  $ cat setup/backend.json | kubectl exec -i -- post.sh backend 10) Edit storage-class-nfs-gold.yaml to configure your storage class.  https://gist.github.com/kapilarora/f7c921fca4c4a42c59ac5df89162fdba  11) Create storage class in openshift:  $ oc create -f  storage-class-nfs-gold.yaml 12) Trident is now installed and we have also defined a storage class for ONTAP NAS.  13) Now Storage is dynamically provisioned when we create a PVC. e.g.  Make sure you provide the right ONTAP NFS export policy as per your environment.  https://gist.github.com/kapilarora/565ed2647a4368dd863948dd417e5a65  Application use-case  Let us look at how we can deploy a Wordpress application backed by a MySQL database.  In the below example we use Trident to dynamically provision ONTAP NAS storage for our Wordpress (+ MySQL) application using PVCs and nfs-gold StorageClass.    https://gist.github.com/kapilarora/93eb209182c027db2588a1898e869cc2  NetApp ONTAP  NetApp ONTAP is a unified enterprise storage system which supports both File and Block storage  and comes in many flavours like AFF (All Flash version), FAS (Fabric Attached Storage), ONTAP Select (Virtualized) and ONTAP Cloud (in Public cloud AWS/Azure).     Why NetApp ONTAP?  NetApp ONTAP offers following key benefits for the most demanding applications:      Predictable high performance Consistent low latency and excellent response time Inline efficiency, achieved by inline deduplication and compression Instant, efficient and efficient database clones for QA and dev-test environments Backup and recovery using Snapshot technology Disaster recovery solutions using SnapMirror(enterprise replication technology) Scalability, reliability and non-disruptive operations  Summary  With NetApp Trident dynamic provisioner ONTAP NAS storage can be provisioned on-demand for your containerized applications.  Trident can help you deploy Stateful containerized applications  e.g.  Databases with NetApp ONTAP enterprise storage  References   NetApp Trident GitHub project NetApp thePub website Join NetApp on Slack NetApp Trident and OpenShift Trident Videos  Happy ONTAPing!  (c) 2017 Keith Tenzer  ","categories": ["OpenShift"],
        "tags": ["Containers","Docker","NAS","NetApp","ONTAP","OpenShift","Storage"],
        "url": "/openshift/storage-for-containers-using-netapp-ontap-nas-part-v/",
        "teaser": null
      },{
        "title": "Storage for Containers using NetApp SolidFire– Part VI",
        "excerpt":"  Overview  In this article we will look at how you can configure and dynamically provision NetApp SolidFire Storage for containerized applications in a Kubernetes/OpenShift environment. This article is a work from Kapil Arora (Cloud Platform Architect @NetApp).   Storage for Containers Overview – Part I Storage for Containers using Gluster – Part II Storage for Containers using Container Native Storage – Part III Storage for Containers using Ceph – Part IV Storage for Containers using NetApp ONTAP NAS – Part V Storage for Containers using NetApp SolidFire – Part VI  NetApp recently released an open source project known as Trident, the first external storage provisioner for Kubernetes leveraging on-premises storage.  Trident enables the use of the new storage class concept in Kubernetes, acting as a provisioning controller that watches for PVCs (persistent volume requests) and creates them on-demand.  This means that when a pod requests storage from a storage class that Trident is responsible for, it will provision a volume that meets those requirements and make it available to the pod in real-time.  To learn more  check-out these  Trident videos.    This is how it works:   The OpenShift/Kubernetes Administrator deploys and configures Trident with SolidFire and defines StorageClasses and Quotas. The developer creates a PVC  with the respective StorageClass. Trident dynamically provisions storage for the developer.  Installation and Configuration Instructions  Steps to install and configure NetApp SolidFire backend with Trident in your OpenShift/Kubernetes environment:  1) Ensure OpenShift 3.4+, or OpenShift Origin 1.4+  Create a service account for trident.  oc create serviceaccount trident oc adm policy add-cluster-role-to-user storage-admin system:serviceaccount:myproject:trident oc adm policy add-scc-to-user anyuid system:serviceaccount:myproject:trident oc adm policy add-role-to-user edit system:serviceaccount:myproject:trident You can find detailed  instructions here.  2) Install 'kubectl' client for Trident install script  curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl chmod +x ./kubectl  sudo mv ./kubectl /usr/local/bin/kubectl  You can find detailed instructions here.  3) Download and untar the Trident installer bundle.  4) Install iscsi initiater utils on all your OpenShift nodes if not already installed.  $ sudo yum install iscsi-initiator-utils 5) Create a VAG (Volume Access Group) name 'trident' on your SolidFire cluster.  6) Add IQNs of all OpenShift nodes to the 'trident' VAG.  $ sudo cat /etc/iscsi/initiatorname.iscsi   9) Discover iscsi Targets on each node  $ sudo iscsiadm -m discoverydb -t st -p sf-svip.demo.netapp.com --discover 10) Login to discovered iscsi targets  $ sudo iscsiadm -m node -p sf-svip.demo.netapp.com —-login 11) Enable and start iscsi  $ sudo systemctl enable iscsi $ sudo systemctl start iscsi Follow instructions here for iscsi multipathing setup.  12) Configure the SolidFire backend. A sample is provided here sample-input/backend-solidfire.json. Copy the edited file to setup/backed.json   https://gist.github.com/kapilarora/bf73342961aeb7256e04089c097a6055  https://gist.github.com/kapilarora/38b26138e1facb0c03e06c4bbaa42c12  13) Execute Trident installer script.  $ ./install_trident.sh --namespace myproject --serviceaccount trident 14) Wait until the deployment is complte. You should see Trident deployment when you execute 'oc get deployment' and trident pod when you execute 'oc get pods'  15) If you face issues, check this troubleshooting session.  16) Register you backend with Trident  $ cat setup/backend.json | kubectl exec -i -- post.sh backend 17) Edit storage-class-gold.yaml to configure your storage class.  https://gist.github.com/kapilarora/e339f780a8651624dcabe457a4e2724f  18) Create storage class in kubernetes:  $ oc create -f  storage-class-gold.yaml 19) Trident is now installed and we have also defined a storage class for SolidFire.  20) Now Storage is dynamically provisioned when we create a PVC. e.g.  https://gist.github.com/kapilarora/007266ca364db8dc370f1deadc3ecd08  Application use-case  Let us look at how we can deploy a Wordpress application backed by a MySQl database.  In the below example we have use Trident to dynamically provision SolidFire storage for our Wordpress (+ MySQL) application using PVCs and gold StorageClass.    https://gist.github.com/kapilarora/57e6cdfbd618712958ff8e25baaad438  NetApp SolidFire  NetApp SolidFire is a zero-touch,  all flash, scale-out , block-storage system that provides  guaranteed QoS (Quality of Service) to each and every storage volume.  Why SolidFire?  NetApp SolidFire offers following key benefits for your applications:   Guaranteed QoS: With SolidFire you can guarantee minimum, maximum and burst IOPS to each and every Application Volume. Zero-touch Storage: SolidFire requires minimum possible administration and is literally zero touch infrastructure. It enables your infrastructure teams to not worry about Storage systems and performance and concentrate on automation and business focussed tasks. Scale-out: SolidFire is a true Scale out storage system which scales upto 100 nodes. Speed: SolidFire is a all Flash Storage system to power your most demanding applications. Automation: All SolidFire  operations can be performed using REST based APIs. Always ON global efficiencies and data assurance  Summary  With NetApp Trident dynamic provisioner NetApp SolidFire zero-touch storage can be provisioned on-demand for your containerized applications.  Trident can help you deploy Stateful containerized applications  e.g.  Databases with NetApp SolidFire.  References   NetApp Trident GitHub project NetApp thePub website Join NetApp on Slack NetApp Trident and OpenShift Trident Videos  Happy SolidFireing!  (c) 2017 Keith Tenzer  ","categories": ["OpenShift"],
        "tags": ["Containers","Docker","ISCSI","NetApp","OpenShift","SolidFire","Storage"],
        "url": "/openshift/storage-for-containers-using-netapp-solidfire-part-vi/",
        "teaser": null
      },{
        "title": "Storage for Containers Using Ceph RBD - Part IV",
        "excerpt":"  Overview  In this article we will look at how to integrate Ceph RBD (Rados Block Device) with Kubernetes and OpenShift. Ceph is of course a scale-out software-defined storage system that provides block, file and object storage. It focuses primarily on cloud-storage use cases. Providing storage for Kubernetes and OpenShift is just one of many use cases that fit very well with Ceph.   Storage for Containers Overview – Part I Storage for Containers using Gluster – Part II Storage for Containers using Container Native Storage – Part III Storage for Containers using Ceph – Part IV Storage for Containers using NetApp ONTAP NAS – Part V Storage for Containers using NetApp SolidFire – Part VI    Ceph integrates with Kubernetes and OpenShift using the kubernetes.io/rbd driver or provisioner. As you will see this enables dynamic provisioning, allowing storage to be provisioned automatically when developers or users request it. Essentially a developer or OpenShift user, requests a PVC (Persistent Volume Claim) against a storage-class where the the kubernetes.io/rbd provisionier is configured. A PVC will create a RBD image in a specific RBD pool on Ceph and map it to a PV (Persistent Volume) in OpenShift. Once a container starts, the PV pointing to an RBD image is mounted. RBD client then maps the RBD image to the appropriate OpenShift node and mounts it using desired filesystem (ext4). The wonderful thing is everything happens automatically and the Ceph storage administrators only need to manage OpenShift project quotas for storage. Since each storage-class maps to an RBD pool it is possible to create various storage SLAs (gold, silver, bronze, etc).  Prerequisites  Before going any further an OpenShift 3.4 or higher cluster is required as well as a Ceph 1.3 or higher cluster. Not to worry, you can set this all up using below guides in a few hours and learn more about OpenShift and Ceph. I ran everything required OpenShift (all-in-one) and Ceph (3 node cluster) on my laptop with 12GB RAM.   OpenShift 3.4 all-in-one lab setup Ceph 2.0 lab setup  Configuring Ceph  In Ceph we need to create an RBD pool for OpenShift and also create a Ceph authx keyring to access the Ceph cluster from OpenShift.  [Ceph Monitor]  Create RBD Pool  [ceph@ceph1]$ sudo ceph osd pool create ose 128 Note: if you don't have enough free PGs (placement groups) you can go with 32 or even 16 for lab setups.  Create Keyring  [ceph@ceph1]$ sudo ceph auth get-or-create client.ose mon 'allow r' \\ osd 'allow class-read object_prefix rdb_children, allow rwx pool=ose' \\ -o /etc/ceph/ceph.client.ose.keyring Copy Keyring file to all OpenShift Nodes  [ceph@ceph1]$ scp /etc/ceph/ceph.client.ose.keyring \\ root@192.168.122.60:/etc/ceph Convert Ceph key to base64 for client.admin user  [ceph@ceph1]$ sudo ceph auth get client.admin  exported keyring for client.admin  [client.admin]  key = AQA8nJBYZAQDKxAAuSX4mzY1YODtPU8gzmIufQ==  caps mds = \"allow *\"  caps mon = \"allow *\"  caps osd = \"allow *\" [ceph@ceph1]$  echo AQA8nJBYZAQDKxAAuSX4mzY1YODtPU8gzmIufQ== |base64 QVFBOG5KQllaQVFES3hBQXVTWDRtelkxWU9EdFBVOGd6bUl1ZlE9PQo= Note: save the new base64 key you will need it later.  Convert Ceph key to base64 for client.ose user  [ceph@ceph1]$ sudo ceph auth get client.ose  exported keyring for client.ose  [client.ose]  key = AQDvU+ZYooHxHBAANGVNCfRpA24iYiTtMgt/tQ==  caps mon = \"allow r\"  caps osd = \"allow class-read object_prefix rdb_children, allow rwx pool=ose\" [ceph@ceph1]$ echo AQDvU+ZYooHxHBAANGVNCfRpA24iYiTtMgt/tQ== |base64  QVFEdlUrWllvb0h4SEJBQU5HVk5DZlJwQTI0aVlpVHRNZ3QvdFE9PQo= Note: save the new base64 key you will need it later.  Configure Ceph Storage Class in OpenShift  [OpenShift Master]  Install ceph-common on all OpenShift nodes  [root@ose3-master ~]# yum install -y ceph-common Create /var/run/ceph directory  [root@ose3-master ~]# mkdir /var/run/ceph Create New Project  [root@ose3-master ~]# oc login -u admin [root@ose3-master ~]# oc new-project ceph Create Secret for Ceph client.admin user  The key in the secret should be the Ceph authx key converted to base64.  [root@ose3-master ~]# vi /root/ceph-secret.yaml apiVersion: v1 kind: Secret metadata:   name: ceph-secret   namespace: default data:   key: QVFBOG5KQllaQVFES3hBQXVTWDRtelkxWU9EdFBVOGd6bUl1ZlE9PQo= [root@ose3-master ~]# oc create -f /root/ceph-secret.yaml Note: Ceph admin secret should be in default project  [root@ose3-master ~]# oc get secret ceph-secret -n default  NAME TYPE DATA AGE  ceph-secret Opaque 1 25s Create Secret for Ceph client.ose user  The key in the secret should be the Ceph authx key converted to base64.  [root@ose3-master ~]# vi /root/ceph-secret-user.yaml apiVersion: v1 kind: Secret metadata:   name: ceph-secret data:   key: QVFEdlUrWllvb0h4SEJBQU5HVk5DZlJwQTI0aVlpVHRNZ3QvdFE9PQo= [root@ose3-master ~]# oc create -f /root/ceph-secret-user.yaml Note: Ceph admin secret must be in the project requesting Ceph storage.  Create Storage Class for Ceph  [root@ose3-master ~]# vi /root/ceph-rbd-storage-class.yaml apiVersion: storage.k8s.io/v1beta1 kind: StorageClass metadata:   name: ceph   annotations:     storageclass.beta.kubernetes.io/is-default-class: \"true\" provisioner: kubernetes.io/rbd parameters:   monitors: 192.168.122.81:6789,192.168.122.82:6789,192.168.122.83:6789   adminId: admin   adminSecretName: ceph-secret   adminSecretNamespace: default   pool: ose   userId: ose   userSecretName: ceph-secret-user [root@ose3-master ~]# oc create -f /root/ceph-rbd-storage-class.yaml Dynamic Provisioning using Ceph RBD  Create PVC (Persistent Volume Claim)   Using the storage-class for Ceph we can now create a PVC.  [root@ose3-master ~]# vi /root/ceph-pvc.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata:   name: ceph-claim   annotations:     volume.beta.kubernetes.io/storage-class: ceph spec:   accessModes:     - ReadWriteOnce   resources:     requests:       storage: 2Gi [root@ose3-master ~]# oc create -f /root/ceph-pvc.yaml Examine PVC and PV  The PVC will create a RBD image in the ose RBD pool and map it to a PV.  [root@ose3-master ~]# oc get pvc  NAME STATUS VOLUME CAPACITY ACCESSMODES AGE  ceph-claim Bound pvc-792ec052-1ae6-11e7-9752-52540057bf27 2Gi RWO 3s [root@ose3-master ~]# oc get pv  NAME CAPACITY ACCESSMODES RECLAIMPOLICY STATUS CLAIM REASON AGE  pvc-792ec052-1ae6-11e7-9752-52540057bf27 2Gi RWO Delete Bound ceph/ceph-claim 50s List RBD Images  [root@ose3-master ~]# rbd list -p ose --name client.ose \\ --keyring /etc/ceph/ceph.client.ose.keyring  kubernetes-dynamic-pvc-7930af93-1ae6-11e7-9752-52540057bf27 Dynamic Provisioning using OpenShift UI  In OpenShift UI login in and go to the project ceph. Under \"resources-&gt;storage\" you can view the already created PVC.    Request New PVC  Clicking \"create storage\" you can choose storage class and create a PVC. This is repeating what we already did with PVC.    Under storage we now see both PVCs.    Create MariaDB Persistent Database  In order to use a PVC we need to mount it's PV in a running Pod. In this case we will click \"add to project\", search for MariaDB and select the MariaDB persistent template from the OpenShift services catalog.    On the next screen accept the defaults and create the MariaDB persistent service. Once the MariaDB Pod is deploying, under \"resources-&gt;storage\" you will see the newly created PVC for mariadb.    Finally we see that the MariaDB persistent Pod is started and passed health as well as readiness checks. At this point any data written to this database will be saved in an RBD image residing on a Ceph storage cluster.    Summary  In this article we discussed how Ceph RBD integrates with OpenShift and Kubernetes. We saw how to configure Ceph and OpenShift to use RBD through a storage-class. Finally we observed how developers or users can easily consume Ceph storage within OpenShift. Dynamic storage in OpenShift is a huge shift in how we consume storage. I view this as a new beginning in storage. Finally we are really providing storage-as-a-service!  Happy Cephing in OpenShift!  (c) 2017 Keith Tenzer  &nbsp;  &nbsp;  ","categories": ["OpenShift"],
        "tags": ["Ceph","Containers","Kubernetes","OpenShift","RBD","Storage"],
        "url": "/openshift/storage-for-containers-using-ceph-rbd-part-iv/",
        "teaser": null
      },{
        "title": "Red Hat OpenStack Platform 10 (Newton) Installation and Configuration Guide",
        "excerpt":"  Overview  In this article we will setup an OpenStack environment based off Newton using the Red Hat OpenStack Platform. OpenStack is OpenStack but every distribution differs in what capabilities or technologies are supported and how OpenStack is installed, configured as well as upgraded.  The Red Hat OpenStack Platform uses OpenStack director based on the TripleO (OpenStack on OpenStack) project to install, configure and update OpenStack. Director is a lifecycle management tool for OpenStack. Red Hat's approach is to make OpenStack easy to manage, without compromising on the \"Open\" part of OpenStack. If management of OpenStack can be simpler and the learning curve brought down then it has a real chance to be the next-gen virtualization platform. What company wouldn't want to be able to consume their internal IT resources like using AWS, GCE or Azure if they didn't give up anything to do so? We aren't there yet but Red Hat is making bold strides and as you will see in this article, is on a journey to make OpenStack consumable for everyone!    Red Hat OpenStack Platform  The Red Hat OpenStack platform uses director to build, manage and upgrade Red Hat OpenStack. Director is in fact a minimal OpenStack deployment itself, with everything needed to deploy OpenStack. The main piece outside of the OpenStack core (Nova, Neutron, Glance, Swift and Heat) is Ironic. The Ironic project is focused on baremetal-as-a-service.  Director allows you to add physical nodes to Ironic and assign them OpenStack roles: compute, control, storage, network, etc. Once roles are assigned an OpenStack environment can be deployed, upgraded and even scaled. As mentioned director is a complete life-cycle management tool that uses OpenStack to manage OpenStack.  In this article we will deploy director (undercloud) on a single VM. We will add three baremetal nodes (VMs) and then deploy OpenStack (overcloud) in a minimal configuration (1 controller node and 1 compute node). I am able to run this on a laptop with just 12GB RAM.  Lab Environment  My idea for this configuration was build the most minimal OpenStack environment possible, something that would run on my laptop with just 12GB RAM using Red Hat OpenStack Director. In the end this experiment was successful and the configuration used is as follows:   KVM Hypervisor Physical Laptop: RHEL 7.3, CentOS or Fedora, Dual core, 12 GB RAM and 250GB disk Undercloud VM: RHEL 7.3, 2x vCPUs, 4GB RAM, 1 x NIC 8(provisioning), 1 x NIC (external) and 40GB disk Overcloud Controller VM: RHEL 7.3, 2 x vCPUs, 6GB RAM, 1 x NIC (provisioning), 2 x NICs (external) and 30GB disk Overcloud Compute VM: RHEL 7.3, 2 x vCPU, 4GB RAM, 1 x NIC (provisioning), 2 x NICs (external) and 20GB disk    Networking Setup  In this configuration we are using virtual networks provided by the hypervisor host (my laptop). Create provisioning and external networks on KVM Hypervisor host. Ensure that NAT forwarding is enabled and DHCP is disabled on the external network. We run OpenStack overcloud on the external network. The provisioning network should be non-routable and DHCP disabled. The undercloud will handle DHCP services for the provisioning network and other IPs will be statically assigned.  [Hypervisor]  Create external network for the overcloud.  [ktenzer@ktenzer ~]$ cat &gt; /tmp/external.xml &lt;&lt;EOF &lt;network&gt;    &lt;name&gt;external&lt;/name&gt;    &lt;forward mode='nat'&gt;       &lt;nat&gt; &lt;port start='1024' end='65535'/&gt;       &lt;/nat&gt;    &lt;/forward&gt;    &lt;ip address='192.168.122.1' netmask='255.255.255.0'&gt;    &lt;/ip&gt; &lt;/network&gt;  Note: hypervisor is 192.168.122.1 and reachable via this IP from undercloud.  [ktenzer@ktenzer ~]$ sudo virsh net-define /tmp/external.xml [ktenzer@ktenzer ~]$ sudo virsh net-autostart external [ktenzer@ktenzer ~]$ sudo virsh net-start external Create provisioning network for undercloud.  Note: gateway is 192.168.126.254 as we will use 192.168.126.1 as IP for the VM running our undercloud.  [ktenzer@ktenzer ~]$ cat &gt; /tmp/provisioning.xml &lt;&lt;EOF &lt;network&gt;    &lt;name&gt;provisioning&lt;/name&gt;    &lt;ip address='192.168.126.254' netmask='255.255.255.0'&gt;    &lt;/ip&gt; &lt;/network&gt; [ktenzer@ktenzer ~]$ sudo virsh net-define /tmp/provisioning.xml [ktenzer@ktenzer ~]$ sudo virsh net-autostart provisioning [ktenzer@ktenzer ~]$ sudo virsh net-start provisioning Deploy Undercloud  First install Red Hat Enterprise Linux (RHEL) 7.3 on undercloud VM. Register with subscription manager and configure required RPM repositories for Red Hat OpenStack Platform.  [Undercloud]  [root@director ~]# subscription-manager register [root@director ~]#subscription-manager list --available \\ subscription-manager attach --pool= [root@director ~]# subscription-manager repos --disable=* [root@director ~]# subscription-manager repos --enable=rhel-7-server-rpms \\ --enable=rhel-7-server-extras-rpms \\  --enable=rhel-7-server-rh-common-rpms \\  --enable=rhel-ha-for-rhel-7-server-rpms \\  --enable=rhel-7-server-openstack-10-rpms Update all pacakges and reboot.  [root@director ~]# yum update -y [root@director ~]# systemctl reboot Install Director Packages.  [root@director ~]# yum install -y python-tripleoclient Ensure host is defined in /etc/hosts.  [root@director ~]# vi /etc/hosts  192.168.122.90 ospd.lab.com ospd Create Stack User.  [root@director ~]# useradd stack [root@director ~]# passwd stack  # specify a password Configure user with sudo permissions.  [root@director ~]# echo \"stack ALL=(root) NOPASSWD:ALL\" | tee -a /etc/sudoers.d/stack [root@director ~]# chmod 0440 /etc/sudoers.d/stack Switch to new stack user.  [root@director ~]# su - stack [stack@director ~]$ Create directories for images and templates. Images are used to boot initial systems and provide baseline OS. Templates are used to customize deployment.  [stack@director ~]$ mkdir ~/images [stack@director ~]$ mkdir ~/templates Configure Director using the sample.  [stack@director ~]$ cp /usr/share/instack-undercloud/undercloud.conf.sample \\ ~/undercloud.conf In my environment the 192.168.126.0/24 network is the undercloud network and used for provisioning as well as deploying the overcloud.  [stack@undercloud ~]$ vi ~/undercloud.conf  [DEFAULT]  local_ip = 192.168.126.1/24  undercloud_public_vip = 192.168.126.2  undercloud_admin_vip = 192.168.126.3  local_interface = eth1  masquerade_network = 192.168.126.0/24  dhcp_start = 192.168.126.100  dhcp_end = 192.168.126.150  network_cidr = 192.168.126.0/24  network_gateway = 192.168.126.1  inspection_iprange = 192.168.126.130,192.168.126.99  generate_service_certificate = true  certificate_generation_ca = local Install the undercloud.  [stack@odpd ~]$ openstack undercloud install #############################################################################  Undercloud install complete. The file containing this installation's passwords is at  /home/stack/undercloud-passwords.conf. There is also a stackrc file at /home/stack/stackrc. These files are needed to interact with the OpenStack services, and should be  secured. ############################################################################# Import overcloud images.  [stack@odpd ~]$ source stackrc [stack@odpd ~]$ sudo yum install -y \\ rhosp-director-images rhosp-director-images-ipa [stack@odpd ~]$ cd ~/images $ for i in \\ /usr/share/rhosp-director-images/overcloud-full-latest-10.0.tar \\ /usr/share/rhosp-director-images/ironic-python-agent-latest-10.0.tar; \\ do tar -xvf $i; done [stack@odpd ~]$ openstack overcloud image upload --image-path \\ /home/stack/images/ Configure DNS on undercloud network.  [stack@odpd ~]$ neutron subnet-list +--------------------------------------+------+------------------+--------------------------------------------------------+ | id | name | cidr | allocation_pools | +--------------------------------------+------+------------------+--------------------------------------------------------+ | 294ff536-dc8b-49a3-8327-62d9792d30a6 | | 192.168.126.0/24 | {\"start\": \"192.168.126.100\", \"end\": \"192.168.126.200\"} | +--------------------------------------+------+------------------+--------------------------------------------------------+ [stack@odpd ~]$ neutron subnet-update 294ff536-dc8b-49a3-8327-62d9792d30a6 \\ --dns-nameserver 8.8.8.8 [Hypervisor]  Registering Overcloud Nodes. Create VM hulls in KVM using virsh on hypervisor host.  Note: You will need to change the disk path to suit your needs.  ktenzer$ cd /home/ktenzer/VirtualMachines ktenzer$ sudo for i in {1..3}; do qemu-img create -f qcow2 -o preallocation=metadata overcloud-node$i.qcow2 60G; done ktenzer$ sudo for i in {1..3}; do virt-install --ram 4096 --vcpus 4 --os-variant rhel7 --disk path=/home/ktenzer/VirtualMachines/overcloud-node$i.qcow2,device=disk,bus=virtio,format=qcow2 --noautoconsole --vnc --network network:provisioning --network network:external --network network:external --name overcloud-node$i --cpu SandyBridge,+vmx --dry-run --print-xml &gt; /tmp/overcloud-node$i.xml; virsh define --file /tmp/overcloud-node$i.xml; done [Undercloud]  Copy ssh key from undercloud system to KVM hypervisor host for stack user.  [stack@odpd ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub stack@192.168.122.1 Save the MAC addresses of the NICs used for provisioning.  Note: Ironic needs to know what MAC addresses a node has associated for provisioning network.  [stack@odpd images]$ for i in {1..3}; do virsh \\ -c qemu+ssh://stack@192.168.122.1/system domiflist overcloud-node$i \\ | awk '$3 == \"provisioning\" {print $5};'; done &gt; /tmp/nodes.txt [stack@odpd images]$ cat /tmp/nodes.txt  52:54:00:7e:d8:01 52:54:00:f6:a6:73 52:54:00:c9:b2:84 [stack@undercloud ~]$ jq . &lt;&lt; EOF &gt; ~/instackenv.json {   \"ssh-user\": \"stack\",   \"ssh-key\": \"$(cat ~/.ssh/id_rsa)\",   \"power_manager\": \"nova.virt.baremetal.virtual_power_driver.VirtualPowerManager\",   \"host-ip\": \"192.168.122.1\",   \"arch\": \"x86_64\",   \"nodes\": [     {       \"pm_addr\": \"192.168.122.1\",       \"pm_password\": \"$(cat ~/.ssh/id_rsa)\",       \"pm_type\": \"pxe_ssh\",       \"mac\": [         \"$(sed -n 1p /tmp/nodes.txt)\"       ],       \"cpu\": \"2\",       \"memory\": \"4096\",       \"disk\": \"60\",       \"arch\": \"x86_64\",       \"pm_user\": \"stack\"     },     {       \"pm_addr\": \"192.168.122.1\",       \"pm_password\": \"$(cat ~/.ssh/id_rsa)\",       \"pm_type\": \"pxe_ssh\",       \"mac\": [         \"$(sed -n 2p /tmp/nodes.txt)\"       ],       \"cpu\": \"4\",       \"memory\": \"2048\",       \"disk\": \"60\",       \"arch\": \"x86_64\",       \"pm_user\": \"stack\"     },     {       \"pm_addr\": \"192.168.122.1\",       \"pm_password\": \"$(cat ~/.ssh/id_rsa)\",       \"pm_type\": \"pxe_ssh\",       \"mac\": [         \"$(sed -n 3p /tmp/nodes.txt)\"       ],       \"cpu\": \"4\",       \"memory\": \"2048\",       \"disk\": \"60\",       \"arch\": \"x86_64\",       \"pm_user\": \"stack\"     }   ]  }  EOF Validate introspection configuration.  [stack@odpd ~]$ curl -O  https://raw.githubusercontent.com/rthallisey/clapper/master/instackenv-validator.py Import nodes into Ironic and set them to bootable.  [stack@odpd ~]$ openstack baremetal import --json ~/instackenv.json [stack@odpd ~]$ openstack baremetal configure boot [stack@odpd ~]$ openstack baremetal node list  +--------------------------------------+------+---------------+-------------+--------------------+-------------+  | UUID | Name | Instance UUID | Power State | Provisioning State | Maintenance |  +--------------------------------------+------+---------------+-------------+--------------------+-------------+  | ea61b158-9cbd-46d2-93e9-eadaccb1589b | None | None | power off | available | False |  | 11aaf849-361e-4bda-81f5-74c245f554af | None | None | power off | available | False |  | 275448c1-aa8d-4854-bb3b-bc73e1e1a794 | None | None | power off | available | False |  +--------------------------------------+------+---------------+-------------+--------------------+-------------+ Set nodes to managed.  [stack@odpd ~]$ for node in $(openstack baremetal node list -c UUID \\ -f value) ; do openstack baremetal node manage $node ; done List nodes.  [stack@odpd ~]$ openstack baremetal node list  +--------------------------------------+------+---------------+-------------+--------------------+-------------+  | UUID | Name | Instance UUID | Power State | Provisioning State | Maintenance |  +--------------------------------------+------+---------------+-------------+--------------------+-------------+  | ea61b158-9cbd-46d2-93e9-eadaccb1589b | None | None | power off | manageable | False |  | 11aaf849-361e-4bda-81f5-74c245f554af | None | None | power off | manageable | False |  | 275448c1-aa8d-4854-bb3b-bc73e1e1a794 | None | None | power off | manageable | False |  +--------------------------------------+------+---------------+-------------+--------------------+-------------+ Run introspection against all managed nodes.  Note: Nodes are booted using ramdisk and their hardware inspected. Introspection prepares nodes for deployment into overcloud.  [stack@odpd ~]$ openstack overcloud node introspect --all-manageable \\ --provide   Tag Control Nodes.  Note: tagging nodes allows us to associate a node with a specific role in the overcloud.  [stack@odpd ~]$ openstack baremetal node set \\ --property capabilities='profile:control,boot_option:local' \\ 0e30226f-f208-41d3-9780-15fa5fdabbde Tag Compute Nodes.  [stack@odpd ~]$ openstack baremetal node set \\ --property capabilities='profile:compute,boot_option:local' \\ cd3e3422-e7db-45c7-9645-858503a2cdc8 [stack@odpd ~]$ openstack baremetal node set \\ --property capabilities='profile:compute,boot_option:local' \\ 5078e1c1-fbe5-4d7f-a222-0c0fd32af423 Check Overcloud Profiles.  [stack@odpd ~]$ openstack overcloud profiles list  +--------------------------------------+-----------+-----------------+-----------------+-------------------+  | Node UUID | Node Name | Provision State | Current Profile | Possible Profiles |  +--------------------------------------+-----------+-----------------+-----------------+-------------------+  | 0e30226f-f208-41d3-9780-15fa5fdabbde | | available | control | |  | cd3e3422-e7db-45c7-9645-858503a2cdc8 | | available | compute | |  | 5078e1c1-fbe5-4d7f-a222-0c0fd32af423 | | available | compute | |  +--------------------------------------+-----------+-----------------+-----------------+-------------------+ Deploy Overcloud  There are two ways to deploy overcloud 1) default 2) customize. You will pretty much always want to customize your deployment but for starting out the default method can be a good way to simplify things and rule out potential problems. I recommend always doing default install just to get a baseline working environment and then throwing it away, redeploying with a customized install  [Undercloud]  Option 1: Default Deployment  The default deployment will put the overcloud on the provisioning network. That means you end up with one network hosting both undercloud and overcloud. The external network is not used.  [stack@odpd ~]$ openstack overcloud deploy --templates --control-scale 1 \\ --compute-scale 1 --neutron-tunnel-types vxlan --neutron-network-type vxlan Option 2: Customized Deployment  The really nice thing about director is you have a high degree of customization. In this example we are setting overcloud up on a single 192.168.122.0/24 network. However normally you would have separate networks for OpenStack management, API, public, storage, etc.  Clone my github repository.  [stack@odpd ~]$ git clone https://github.com/ktenzer/openstack-heat-templates.git Copy my templates to your local ~/templates directory.  [stack@odpd ~]$ cp ~/openstack-heat-templates/director/lab/osp10/templates/* ~/templates Deploy overcloud using templates.  [stack@odpd ~]$ openstack overcloud deploy --templates \\ -e /usr/share/openstack-tripleo-heat-templates/environments/network-isolation.yaml \\ -e ~/templates/network-environment.yaml \\  -e /usr/share/openstack-tripleo-heat-templates/environments/low-memory-usage.yaml \\  -e ~/templates/firstboot-environment.yaml --control-scale 1 \\  --compute-scale 1 --control-flavor control \\  --compute-flavor compute --ntp-server pool.ntp.org \\  --neutron-network-type vxlan --neutron-tunnel-types vxlan  2017-04-12 14:46:11Z [overcloud.AllNodesDeploySteps]: CREATE_COMPLETE Stack CREATE completed successfully 2017-04-12 14:46:12Z [overcloud.AllNodesDeploySteps]: CREATE_COMPLETE state changed 2017-04-12 14:46:12Z [overcloud]: CREATE_COMPLETE Stack CREATE completed successfully  Stack overcloud CREATE_COMPLETE  Started Mistral Workflow. Execution ID: 000ecec3-46aa-4e3f-96d9-8a240d34d6aa /home/stack/.ssh/known_hosts updated. Original contents retained as /home/stack/.ssh/known_hosts.old Overcloud Endpoint: http://192.168.122.106:5000/v2.0 Overcloud Deployed List overcloud nodes.  [stack@odpd ~]$ nova list +--------------------------------------+------------------------+--------+------------+-------------+--------------------------+ | ID | Name | Status | Task State | Power State | Networks | +--------------------------------------+------------------------+--------+------------+-------------+--------------------------+ | 1e286764-9334-4ecd-9baf-e37a49a4fbd5 | overcloud-compute-0 | ACTIVE | - | Running | ctlplane=192.168.126.106 | | a21a14f5-94df-4a3a-8629-ba8d851525ff | overcloud-controller-0 | ACTIVE | - | Running | ctlplane=192.168.126.103 | +--------------------------------------+------------------------+--------+------------+-------------+--------------------------+ Connect to overcloud controller from undercloud.  [stack@odpd ~]$ ssh heat-admin@192.168.126.103 [Overcloud Controller]  Get overcloud admin password.  Overcloud parameters generated during deployment such as password are stored in hiera.  [root@overcloud-controller-0 ~]$ sudo -i [root@overcloud-controller-0 ~]# hiera keystone::admin_password HngV6vc4ZP2bZ78ePfgWAvHAh [Undercloud]  Create overcloud keystone source file.  [stack@odpd ~]$ vi overcloudrc export OS_NO_CACHE=True export OS_CLOUDNAME=overcloud export OS_AUTH_URL=http://192.168.122.106:5000/v2.0 export NOVA_VERSION=1.1 export COMPUTE_API_VERSION=1.1 export OS_USERNAME=admin export OS_PASSWORD=HngV6vc4ZP2bZ78ePfgWAvHAh export PYTHONWARNINGS=\"ignore:Certificate has no, ignore:A true SSLContext object is not available\" export OS_TENANT_NAME=admin export PS1='[\\u@\\h \\W]$ ' Source overcloudrc.  [stack@odpd ~]$ source overcloudrc List hypervisor hosts in overcloud.  [stack@odpd ~]$ nova hypervisor-list +----+---------------------------------+-------+---------+ | ID | Hypervisor hostname | State | Status | +----+---------------------------------+-------+---------+ | 1 | overcloud-compute-0.localdomain | up | enabled | +----+---------------------------------+-------+---------+ Troubleshooting Deployment  Let's face it in OpenStack there is a lot that can go wrong. I like this quote from Dirk Wallerstorfer.  \"In short, OpenStack networking is a lot like Venice—there are masquerades and bridges all over the place!\"  -Dirk Wallerstorfer  source: https://www.dynatrace.com/blog/openstack-network-mystery-2-bytes-cost-me-two-days-of-trouble/  [Undercloud]  Red Hat is making it much easier to troubleshoot deployment problems.While the deployment is running you can follow along in Heat by showing nested steps.  [stack@odpd ~]$ heat stack-list --show-nested If for some reason the deployment fails, there is now a command to gather up all the information to make it really easy to find out what happened.  [stack@odpd ~]$ openstack stack failures list --long overcloud Summary  OpenStack is the way of the future for virtualization platforms and I think in the future many traditional virtualization environments will be moving to OpenStack. The choice is simple either they will stay on-premise and become OpenStack or move to public cloud. Of course there will be those that stick with traditional virtualization, there are still lots and lots of mainframes around but clear trend will be to public cloud or OpenStack. The only thing holding OpenStack back is complexity and manageability. Red Hat is focused on making OpenStack simple without losing the \"Open\" in OpenStack. In other words without compromising on what makes OpenStack a great cloud computing platform. As you have seen in this article Red Hat OpenStack Platform is making great strides and the fact that you can setup an OpenStack environment using enterprise production grade tooling on a 12GB RAM laptop is a good sign.  Happy OpenStacking!  (c) 2017 Keith Tenzer  &nbsp;  ","categories": ["OpenStack"],
        "tags": ["Ironic","Newton","OpenStack","OpenStack Director","RHOSP","TripleO"],
        "url": "/openstack/red-hat-openstack-platform-10-newton-installation-and-configuration-guide/",
        "teaser": null
      },{
        "title": "RHV 4.1 Lab Installation and Configuration Guide",
        "excerpt":"  Overview  In this article we will setup a Red Hat Enterprise Virtualization (RHV) environment. RHV is based on upstream opensource projects KVM and Ovirt. RHV is composed of Red Hat Enterprise Linux (RHEL which includes KVM) and Red Hat Enterprise Virtualization Management (RHV-M), based on Ovirt. As of this article the latest version is RHV 4.1.  RHV has of late become very interesting to customers looking for alternatives to VMware. Below are a few reasons why you should be interested in RHV:   100% opensource no proprietary code and no proprietary licencing. Best Hypervisor for running Red Hat Enterprise Linux (RHEL). Integrated and built with RHEL, uses SELinux to secure Hypervisor. RHV leads VMware in SPECvirt performance and price per performance (last results 2013). RHV scales vertically and performs extremely well on 4 or even 8 socket servers. All features are included in RHV subscription, no licensing for extra capabilities. KVM is future proof and is the defacto standard for OpenStack and modern virtualizations platforms.    Preparation  In this configuration I am using two physical servers. Each has a dual-core CPU and 16GB RAM. Specifically I am using two Intel NUCs which make a great, portable lab environments. In my environment I named the Hypervisor hosts rhevh01.lab and rhevh02.lab.  [RHEVH01 and RHEVH02]  Install RHEL 7.3 on both nodes.  Enable subscription and configure repositories.  # subscription-manager register # subscription-manager list --available # subscription-manager attach --pool= # subscription-manager repos --disable=* # subscription-manager repos --enable=rhel-7-server-rpms \\  --enable=rhel-7-server-rhv-4.1-mgmt-agent-rpms Disable firewalld and NetworkManager  # systemctl stop firewalld # systemctl disable firewalld # systemctl stop NetworkManager #systemctl disable NetworkManager Update.  #yum update -y #systemctl reboot Configure NFS.  RHV requires shared storage in order to cluster Hypervisors. In this case we will expose the local storage of both nodes as NFS shares.  # yum install nfs-utils rpcbind # systemctl enable rpcbind # systemctl enable nfs-server # systemctl start rpcbind # systemctl start nfs-server Configure and export NFS share.  # mkdir /usr/share/rhev # chown -R 36:36 /usr/share/rhev # chmod -R 0755 /usr/share/rhev # vi /etc/exports  /usr/share/rhev 192.168.2.0/24(rw) # exportfs -a RHV Installation  There are two different installations approaches for the hypervisor and management. RHV as mentioned earlier consists of the Hypervisor host(s) (RHEL + KVM) and a system running the management software (RHV-M).  Hypervisor Options   Use RHEL Use RHEV-H (Based on RHEL but designed to run as a Hypervisor, only software required for running Hypervisor installed)  RHV-M Options   Install RHV-M on separate VM or physical server. Use Hosted-engine which deploys RHV-M on the actual RHV cluster. In this example we will choose hosted-engine and this is my personal preference because RHV-M automatically gets all the capabilities and benefits of running on RHV, such as HA.  Install hosted engine  [RHEVH01]  Install hosted-engine software and dependencies.  [root@rhevh01 ~]# yum install -y ovirt-hosted-engine-setup [root@rhevh01 ~]# yum install -y rhevm-appliance Install screen for using with setup.  # yum -y install screen Start screen session.  This is used in event you have network connection issue during setup.  # screen Deploy hosted-engine on hypervisor host.  # hosted-engine --deploy &nbsp;  &nbsp;  Select nfs3 for storage and configure OVF using cloud-init  [ INFO ] Stage: Setup validation  --== CONFIGURATION PREVIEW ==--  Bridge interface : eno1  Engine FQDN : rhevm.lab  Bridge name : ovirtmgmt  Host address : rhevh01.lab  SSH daemon port : 22  Firewall manager : iptables  Gateway address : 192.168.0.1  Host name for web application : hosted_engine_1  Storage Domain type : nfs3  Host ID : 1  Image size GB : 50  Storage connection : 192.168.0.21:/usr/share/rhev  Console type : vnc  Memory size MB : 4096  MAC address : 00:16:3e:3c:79:9b  Boot type : disk  Number of CPUs : 2  OVF archive (for disk boot) : /usr/share/ovirt-engine-appliance/rhevm-appliance-4.0.20170307.0-1.el7ev.ova  Appliance version : 4.0.20170307.0-1.el7ev  Restart engine VM after engine-setup: True  CPU Type : model_Haswell-noTSX  Please confirm installation settings (Yes, No)[Yes]:  Note: If deployment fails it may be because of conflict in version of ebtables.  Check if you have ebtables version &gt; 2.0.10-13 (optional).  qpm -qa | grep ebtables # yum downgrade ebtables-2.0.10-13.el7.x86_64 Configure RHV cluster in RHV-M  Once we have installed the hosted-engine we can complete the setup and configure both nodes in a RHV cluster.  [RHV-M Console]  Connect to RHV-M management console.  Note: The hostname of RHV-M not IP needs to be used to access the management console.  http://rhevm.lab/ovirt-engine    Select Datacenter-&gt;Hosts-&gt;New.    RHV-M will configure the second node and bring it into the cluster.    Add iptables rules to allow NFS.  This is specific to this setup as we are cheap and lazy. We are using local disks on Hypervisor hosts for storage.  [RHEVH01 and RHEVH02]  # vi /etc/sysconcfig/iptables  #NFS  -A INPUT -p tcp -m tcp --dport 2049 -j ACCEPT  -A INPUT -p tcp -m tcp --dport 32803 -j ACCEPT  -A INPUT -p tcp -m tcp --dport 32769 -j ACCEPT  -A INPUT -p tcp -m tcp --dport 892 -j ACCEPT  -A INPUT -p udp -m udp --dport 892 -j ACCEPT  -A INPUT -p tcp -m tcp --dport 875 -j ACCEPT  -A INPUT -p udp -m udp --dport 875 -j ACCEPT  -A INPUT -p udp -m udp --dport 662 -j ACCEPT  -A INPUT -p udp -m udp --dport 662 -j ACCEPT Restart iptables.  # systemctl restart iptables Configure Storage Domains  RHV has three types of storage domains: Data, ISO and Export.   Data are for VM disks. ISOs are what they say, for storing ISO images. Export is for OVA or virtual machine templates.  Storage domain's also play key roll in clustering and are used as quorum. One data storage domain will be the master.  Add Data Storage Domain from RHEVH01.  Select Datacenter-&gt;Storage-&gt;New        Add Data Storage Domain from RHEVH02.  Select Datacenter-&gt;Storage-&gt;New    Add ISO Storage Domain from RHEVH01.  Select Datacenter-&gt;Storage-&gt;New    Add Export Storage Domain from RHEVH01.  Select Datacenter-&gt;Storage-&gt;New    If everything worked you should see all the storage domains available.  Troubleshooting  You should never do any of this on a production system! Always work with RH Support. Ok now that we cleared that up lets get into the bowels of RHV.  RHV-M stores state in it's database. It is possible and sometimes needed to access database to cleanup state, especially if you are like me and do something stupid, in this case destroy filesystems housing your RHV storage domains.  As such I have documented how to handle cleaning up any stale storage connections and even how to delete storage domains themselves manually from within database.  [RHV-M Host]  Connect to RHV-M Database.  # source /etc/ovirt-engine/engine.conf.d/10-setup-database.conf # export PGPASSWORD=${ENGINE_DB_PASSWORD} # psql -h localhost -U $ENGINE_DB_USER Show list of storage connections.  engine=&gt; select id, connection from storage_server_connections;  id | connection  --------------------------------------+------------------------------  02953919-feb8-49a5-812f-5d6459984ba3 | 192.168.0.21:/usr/share/data  59037cc9-e0d0-446c-b38c-7e74ed2081cc | 192.168.0.21:/usr/share/rhev  ee0d0921-338b-4ada-9bc9-28dc8978299d | 192.168.0.22:/usr/share/data  (3 rows) Delete a storage connection.  engine=&gt; DELETE from storage_server_connections where id='ee0d0921-338b-4ada-9bc9-28dc8978299d';  DELETE 1 Cleaning up storage domains.  Storage domains all have a connection listed in the storage_server_connections table. In addition they also exist in storage_domain_static and storage_domain_dynamic tables.  List storage connections.  engine=&gt; select id, connection from storage_server_connections;  id | connection  --------------------------------------+--------------------------------  02953919-feb8-49a5-812f-5d6459984ba3 | 192.168.0.21:/usr/share/data  59037cc9-e0d0-446c-b38c-7e74ed2081cc | 192.168.0.21:/usr/share/rhev  4e1f2603-66cf-4b68-b7db-7aadd74d50e7 | 192.168.0.22:/usr/share/data  e38d9d3b-26c2-4c5e-96c6-6ee06a2fb4a6 | 192.168.0.21:/usr/share/iso  6ad3e7ca-305d-44ab-9765-b5c72162def3 | 192.168.0.21:/usr/share/export  (5 rows) Show storage domain ids.  engine=&gt; select id, storage from storage_domain_static;  id | storage  --------------------------------------+--------------------------------------  a4ba370b-4f28-414e-9620-960bff62fce5 | 02953919-feb8-49a5-812f-5d6459984ba3  0e142f0b-e934-4b3c-b6ad-8c53b574e303 | 59037cc9-e0d0-446c-b38c-7e74ed2081cc  d5ed7709-896a-4c33-b758-09ca957844b4 | ee0d0921-338b-4ada-9bc9-28dc8978299d  788f3e56-a11d-4665-b851-50c95ebee5c5 | b7f4fee9-1bb9-414c-8cfa-3b9cbd95578b  c3e101f0-ada3-465d-9411-c04e13d7749f | cc5d9f30-5eba-4a24-9212-11c8dff6248b  7887943f-8860-4079-9679-4530bd525948 | 4e1f2603-66cf-4b68-b7db-7aadd74d50e7  231d4b20-bc8a-41c7-9e58-226ce4aa26fa | e38d9d3b-26c2-4c5e-96c6-6ee06a2fb4a6  30621071-3dba-401d-9605-02a914b4069a | 6ad3e7ca-305d-44ab-9765-b5c72162def3  (8 rows) Delete Storage Domains from dynamic table using id.  engine=&gt; delete from storage_domain_dynamic where id = 'd5ed7709-896a-4c33-b758-09ca957844b4';  DELETE 1  engine=&gt; delete from storage_domain_dynamic where id = '788f3e56-a11d-4665-b851-50c95ebee5c5';  DELETE 1  engine=&gt; delete from storage_domain_dynamic where id = 'c3e101f0-ada3-465d-9411-c04e13d7749f';  DELETE 1  engine=&gt; delete from storage_domain_dynamic where id = '7887943f-8860-4079-9679-4530bd525948';  DELETE 1  Delete Storage Domains from statictable using id.  engine=&gt; delete from storage_domain_static where id = 'd5ed7709-896a-4c33-b758-09ca957844b4';  DELETE 1  engine=&gt; delete from storage_domain_static where id = '788f3e56-a11d-4665-b851-50c95ebee5c5';  DELETE 1  engine=&gt; delete from storage_domain_static where id = 'c3e101f0-ada3-465d-9411-c04e13d7749f';  DELETE 1  engine=&gt; delete from storage_domain_static where id = '7887943f-8860-4079-9679-4530bd525948';  DELETE 1  Show storage domains.  engine=&gt; select id, storage from storage_domain_static;  id | storage  --------------------------------------+--------------------------------------  a4ba370b-4f28-414e-9620-960bff62fce5 | 02953919-feb8-49a5-812f-5d6459984ba3  0e142f0b-e934-4b3c-b6ad-8c53b574e303 | 59037cc9-e0d0-446c-b38c-7e74ed2081cc  231d4b20-bc8a-41c7-9e58-226ce4aa26fa | e38d9d3b-26c2-4c5e-96c6-6ee06a2fb4a6  30621071-3dba-401d-9605-02a914b4069a | 6ad3e7ca-305d-44ab-9765-b5c72162def3  (4 rows) Summary  In this article we discussed some of the advantages of Red Hat Enterprise Virtualization (RHV). We went through steps to build a basic lab environment. Showed how to configure RHV cluster and add storage domains. Finally we looked at some troubleshooting steps using RHV database. Hopefully you found this article informative. Feedback always appreciated!  Happy RHVing!  (c) 2017 Keith Tenzer  ","categories": ["RHEV"],
        "tags": ["KVM","Linux","RHEL","RHEV-M","Virtualization"],
        "url": "/rhev/rhev-4-1-lab-installation-and-configuration-guide/",
        "teaser": null
      },{
        "title": "CloudForms Installation and Configuration Guide for Red Hat Virtualization",
        "excerpt":"  Overview  In this article we will deploy CloudForms 4.2 on Red Hat Enterprise Virtualization (RHV). We will also show how to configure CloudForms in order to properly manage a RHV cluster and it's hosts as well as virtual machines.  Before you begin a RHV cluster is needed. If you haven't set one up, here is a guide on how to build a basic two node RHV cluster.    Install CloudForms  CloudForms is provided by Red Hat as a virtual applicance. Red Hat provides appliances for many infrastructure platforms such as RHV, VMware, Hyper-V, OpenStack, AWS, GCE and Azure. In this case we will download the RHV virtual appliance.  Download Image  Image should be downloaded to the CFME appliance using your RHN login.  https://access.redhat.com/downloads/content/167/ver=/cf-me---4.2/4.2/x86_64/product-software  Add Appliance into RHV Export Domain  In order to import an appliance it needs to be copied to export domain or imported to export domain using CLI.  # cp cfme-rhevm-5.7.2.1-1.x86_64.rhevm.ova /usr/share/export/30621071-3dba-401d-9605-02a914b4069a Once copied to export domain we need to extract the appliance and set permissions.  # cd /usr/share/export/30621071-3dba-401d-9605-02a914b4069a/ # tar xvf cfme-rhevm-5.3-15.x86_64.rhevm.ova # chown -R 36:36 images/ # chown -R 36:36 master/ Import CFME Appliance as Template  Once the appliance is in the export domain we can import it as an appliance using the RHV-M management console. Under export domain and template, choose the import option.      We can provide our CFME template with a new name.    Once import completes the status will change to \"OK\".    Create CloudForms Virtual Machine  Under \"Virtual Machines\", select \"new\" in the RHV-M management console. For template choose the newly imported CFME template.    Add Additional Disk for CloudForms Database  Under CFME \"Virtual Machine\" Disks select \"new\" to add a new disk. I would recommend 30GB but this is based on how much data is to be collected and for how long.    Configure Appliance  Start CFME VM from RHV-M management console. Login as \"root/smartvm\" and run \"appliance_console\" to configure CloudForms.    Configure static network settings    Configure local database    Make sure you create an internal database    Choose a disk, this should be the 30GB disk we added    Choose \"N\" to run standalone database server    Create database region and choose password    Restart the CFME appliance    Below are the basic settings that should be configured    Configure RHV for CloudForms  In order for CloudForms to report capacity &amp; utilization we need to configure access from RHV-M.  Login to RHV-M host using ssh as root  Connect to RHV-M database  # su - postgres As postgres user add a new role for CloudForms. As of this article CloudForms needs superuser but there is an RFE to enable read-only access in future.  $ psql -U postgres CREATE ROLE cfme LOGIN UNENCRYPTED PASSWORD 'redhat01' \\ SUPERUSER VALID UNTIL 'infinity'; \\q &nbsp;  Open firewall ports to allow communication to database  # firewall-cmd --add-port=5432/tcp --permanent # firewall-cmd --reload Ensure PostgreSQL is exposing port and listening to all interfaces  # vi /var/lib/pgsql/data/pg_hba.conf host all all 0.0.0.0/0 md5 # vi /var/lib/pgsql/data/postgresql.conf listen_addresses = '*' Restart PostgreSQL database  # service postgresql reload Configure RHV Provider in CloudForms  Connect to CloudForms management console  Using browser you can simply hit the IP of CloudForms appliance using https. Login with \"admin/smartvm\".    Add RHV Provider  Under providers, choose \"Infrastructure Providers\" and \"new\". Add the default credentials using IP or name of RHV-M host. The user should be admin@internal for RHV-M.  Once that is complete, go to the \"C&amp; U Database\" tab. Here you will enter the IP or name for the RHV-M host and the superuser credentials you created above. In this example \"cfme/redhat01\".  Configure Hypervisors  CloudForms requires access to hypervisors for certain capabilities like \"smart state analysis\". Ability to peer inside VM images in order to for example, check security vulnerabilities. In this environment there are two hypervisor hosts (rhevh01.lab and rhevh02.lab).  Configure RHEVH01  Under RHV infrastructure provider select host rhevh01.lab and configure.    Configure RHEVH02  Under RHV infrastructure provider select host rhevh02.lab and configure.    Enable Capacity and Utilization  On upper far right in CloudForms management console under user, select \"Configuration\". Enable the Capacity and Utilization roles underthe \"Server: EVM\".  C &amp; U    Set SmartProxy Affinity  Under \"Zone: Default Zone\" associate all hosts and datastores.    Enable Capacity and Utilization Collection of RHV Environment  Under \"CFME:Region\" enable collection for all clusters and datastores.    At this point CloudForms is collecting capacity and utilization statistics and after a few hours you should start seeing data under VM. You can also use planning to forecast and model various growth scenarios.  Summary  In this article we installed and configured the CloudForms appliance on an existing RHV cluster. We prepared RHV-M for CloudForms data collection. Configured a new infrastructure provider for RHV in CloudForms. Finally we enabled capacity and utilization for RHV within CloudForms.  Happy CloudForming!  (c) 2017 Keith Tenzer  &nbsp;  ","categories": ["CloudForms"],
        "tags": ["Cloud","cloud management","KVM","ManageIQ","RHEV"],
        "url": "/cloudforms/cloudforms-installation-and-configuration-guide-for-red-hat-enterprise-virtualization/",
        "teaser": null
      },{
        "title": "Explaining OpenStack Cinder Types and Scheduler",
        "excerpt":"  Overview  OpenStack Cinder is responsible for handling block storage in the context of OpenStack. Cinder provides a standard API and interface that allows storage companies to create their own drivers in order to integrate storage capabilities into OpenStack in a consistent way. Each storage pool exposed to OpenStack Cinder is a backend and you can have many storage backends. You can also have many of the same kind of storage backends. In this article we will look at two advanced features Cinder provides: types and the scheduler.  Cinder types essentially allow us to label Cinder storage backends. This allows for building out storage services that have expected characteristics and capabilities. The Cinder driver exposes those storage capabilities to Cinder.  The Cinder scheduler is responsible for deciding where to create Cinder volumes when we have more than one of the same kind of storage backend. This is done by looking at the filter rules in order to identify the most appropriate storage backend. More about filter rules can be found here.    Prerequisites  In order to follow along you will need an OpenStack environment. The easiest thing to do is setup an all-in-one environment with RDO. Those steps are documented here.  If you want to use Ceph and don't have an environment or things setup properly then you can follow below guides:   Ceph setup Ceph OpenStack configuration  Configure Storage Backends  [OpenStack Controller]  Add two disks to OpenStack controller.   For all-in-one setup this is straight-forward. If you have multiple controllers then it needs to be a controller running openstack-cinder-volume service.  Create LVM Storage Backends.  Authenticate to OpenStack using keystone source file.  # source /root/keystonerc_admin Determine appropriate block device names for newly added disks. The below commands can be useful. In this case the two disks are vdb and vdc.  # blkid # lsblk Create physical volumes and volume groups.  # pvcreate /dev/vdb # vgcreate lvm-1 /dev/vdb # pvcreate /dev/vdc # vgcreate lvm-2 /dev/vdc Update Cinder configuration and add LVM backends.  # vi /etc/cinder/cinder.conf default_volume_type = rbd enabled_backends = lvm1,lvm2,rbd  [lvm1] iscsi_helper=lioadm iscsi_ip_address=192.168.122.80 volume_driver=cinder.volume.drivers.lvm.LVMVolumeDriver volumes_dir=/var/lib/cinder/volumes volume_backend_name=lvm volume_group=lvm-1 filter_function = \"volume.size &lt; 5\"  [lvm2] iscsi_helper=lioadm iscsi_ip_address=172.16.7.50 volume_driver=cinder.volume.drivers.lvm.LVMVolumeDriver volume_dir=/var/lib/cinder/volumes volume_backend_name=lvm volume_group=lvm-2 filter_function = \"volume.size &gt;= 5\" Notice we also added filter rules. Since we have multiple lvm backends defined, filter rules will help scheduler decide what backend to use for provisioning Cinder volumes. In this case we have added a filter based on size. Cinder volumes smaller than 5GB will end up on backend lvm1 while Cinder volumes larger than 5GB will end up on backend lvm2.  Restart Cinder Services.  # systemctl restart openstack-cinder-api # systemctl restart openstack-cinder-volume Configure Cinder Types  As mentioned Cinder types are used to create storage services and provide certain capabilities that are exposed by the driver. In this case we are simply labeling the backends however capabilities such as replication, backup and other things are also exposed via cinder types. This depends on the storage backend and the underlying storage capabilities.  Create Cinder Types for backend lvm1.  # openstack volume type create --public LVM1  +---------------------------------+--------------------------------------+  | Field | Value |  +---------------------------------+--------------------------------------+  | description | None |  | id | 7ee9c132-3556-4294-80d5-a87dffaa8db4 |  | is_public | True |  | name | LVM |  | os-volume-type-access:is_public | True |  +---------------------------------+--------------------------------------+ # openstack volume type set --property volume_backend_name=lvm1 LVM1 Create Cinder Types for backend lvm2.  # openstack volume type create --public LVM2 +---------------------------------+--------------------------------------+ | Field | Value | +---------------------------------+--------------------------------------+ | description | None | | id | 7ee9c132-3556-4294-80d5-a87dffaa8db4 | | is_public | True | | name | LVM | | os-volume-type-access:is_public | True | +---------------------------------+--------------------------------------+ # openstack volume type set --property volume_backend_name=lvm2 LVM2 List Cinder backends.  # cinder get-pools  +----------+------------------------+  | Property | Value |  +----------+------------------------+  | name | osp10.lab.com@lvm2#lvm |  +----------+------------------------+  +----------+-----------------------+  | Property | Value |  +----------+-----------------------+  | name | osp10.lab.com@lvm1#lvm |  +----------+-----------------------+ Create Cinder Volumes  Now that multiple lvm backends are configured, types exist and scheduler has filter rules, we can provision Cinder volumes.  Create 3GB Cinder Volume.  # openstack volume create --size 3 3gb-vol --type lvm  +---------------------+--------------------------------------+  | Field | Value |  +---------------------+--------------------------------------+  | attachments | [] |  | availability_zone | nova |  | bootable | false |  | consistencygroup_id | None |  | created_at | 2017-03-29T15:17:58.457813 |  | description | None |  | encrypted | False |  | id | 4d764067-da2a-476c-8f64-a52203e2dfd7 |  | migration_status | None |  | multiattach | False |  | name | 3gb-vol |  | properties | |  | replication_status | disabled |  | size | 3 |  | snapshot_id | None |  | source_volid | None |  | status | creating |  | type | LVM |  | updated_at | None |  | user_id | 9d592f8a49654e8592de4e69fd15e603 |  +---------------------+--------------------------------------+ Create 6GB Cinder Volume.  # openstack volume create --size 6 6gb-vol --type lvm  +---------------------+--------------------------------------+  | Field | Value |  +---------------------+--------------------------------------+  | attachments | [] |  | availability_zone | nova |  | bootable | false |  | consistencygroup_id | None |  | created_at | 2017-03-29T15:18:21.086318 |  | description | None |  | encrypted | False |  | id | 5a39b0c3-fb07-481e-a150-36c0eae1adc3 |  | migration_status | None |  | multiattach | False |  | name | 6gb-vol |  | properties | |  | replication_status | disabled |  | size | 6 |  | snapshot_id | None |  | source_volid | None |  | status | creating |  | type | LVM |  | updated_at | None |  | user_id | 9d592f8a49654e8592de4e69fd15e603 |  +---------------------+--------------------------------------+ List Cinder Volumes.  # openstack volume list  +--------------------------------------+--------------+-----------+------+-------------+  | ID | Display Name | Status | Size | Attached to |  +--------------------------------------+--------------+-----------+------+-------------+  | 5a39b0c3-fb07-481e-a150-36c0eae1adc3 | 6gb-vol | available | 6 | |  | 4d764067-da2a-476c-8f64-a52203e2dfd7 | 3gb-vol | available | 3 | |  +--------------------------------------+--------------+-----------+------+-------------+ Show Details on LVM volumes  # lvs  LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert  volume-4d764067-da2a-476c-8f64-a52203e2dfd7 lvm-1 -wi-a----- 3.00g  volume-5a39b0c3-fb07-481e-a150-36c0eae1adc3 lvm-2 -wi-a----- 6.00g  root rhel -wi-ao---- 91.57g  swap rhel -wi-ao---- 7.88g Using the Cinder volume id we can see that the scheduler properly placed the Cinder volumes according to the filter. In this case the 3GB volumes ended up on lvm-1 backend and the 6GB volume on lvm-2 backend.  Summary  In this article we explained how to build intelligent storage services using OpenStack Cinder. Using Cinder types we can define service levels and capabilities. Using the scheduler we can apply filters that apply intelligent decision making, in order to find the most appropriate storage backend. Finally we provided a basic example using two LVM backends and a filter based on volume size.  Happy OpenStacking!  (c) 2017 Keith Tenzer  &nbsp;  &nbsp;  &nbsp;  ","categories": ["OpenStack"],
        "tags": ["Cinder","KVM","Linux","OpenStack","Storage"],
        "url": "/openstack/explaining-openstack-cinder-types-and-scheduler/",
        "teaser": null
      },{
        "title": "Ansible Tower and Satellite: End to End Automation for the Enterprise",
        "excerpt":"  Overview  In this article we will look at how Ansible Tower and Red Hat Satellite 6 integrate with one another, providing end-to-end automation for the enterprise. Satellite is a systems management tool that combines several popular opensource projects: Foreman (provisioning), Katello (content repository), Pulp (database), Candlepin (subscription management) and Puppet (configuration management). While puppet is directly integrated into Satellite, many organizations would rather use Ansible because of its power, simplicity and ease-of-use.  Ansible Tower integrates with Satellite, allowing organizations to run playbooks against the hierarchy and groups of servers defined in Satellite. Additionally, Ansible Tower can dynamically update its inventories with hosts and their updated facts from Satellite at anytime. Hosts show up in Ansible Tower under the groups defined by Satellite. This allows organizations to use Satellite to define their infrastructure, provision hosts, provide patch management while leveraging Ansible to deploy and manage software configuration. It also allows other teams the ability to run playbooks and automation against the infrastructure defined by Satellite. Personally I am a huge fan of this loose coupling and find this solution much more advantageous than a direct coupling of Ansible in Satellite.    Prerequisites  In order to get up and running, Satellite and Ansible Tower are required. Below are guide to get you going in case you don't have environments.   Satellite 6 Configuration Guide Ansible Tower Configuration Guide  For those that are just using foreman and not Satellite, I haven't tried it but let me know if it works. If not there is a Foreman Ansible dynamic inventory that can be added into Ansible Tower.  Satellite Configuration  In Satellite we need to provision a host or bootstrap an existing host. In this example I have simply setup a host with RHEL 7.3 and then connected it to Satellite environment via bootstrapping.  [On RHEL Host]  Install Katello package from Satellite server.  # rpm -Uvh http://sat6.lab.com/pub/katello-ca-consumer-latest.noarch.rpm  Subscribe using activation key.  # subscription-manager register --org=\"Default_Organization\" --activationkey=\"DEV_CORE\" Enable Satellite tools repository and install katello agent.  # yum -y install --enablerepo rhel-7-server-satellite-tools-6.2-rpms katello-agent Once bootstrapping is complete, the host should show up under Hosts-&gt;All Hosts in Satellite. In this case the host in question is called 'soetest.lab'. Note the list of hosts as when we do an inventory in Ansible we will automagically see them, their groups and facts, all from Satellite.    Ansible Tower Configuration  Now that we have a few hosts configured in Satellite and our example host 'soetest.lab', we can setup Ansible Tower. Again the idea is to use Satellite for provisioning and patch management, then Ansible for everything else while taking advantage of the hierarchy as well as host facts from Satellite.  [Ansible Tower]  Create a new project  A project in tower points to source code management (in this case Git) and provides various options. In this case we checked \"Update on Launch\". This means if a playbook is launched from project (Git repository), Ansible Tower will do SCM update and get latest version of playbook (git pull). Also note a branch can be provided, it is left blank which for Git defaults to the main branch.    Once project is added we can see last update and also the revision.    Add Credentials  Ansible Tower allows for securely storing credentials. There are three different types of credentials: machine, cloud and network. In this case we need machine credentials to connect to our host 'soetest.lab' in order to run playbooks. In addition to dynamically get inventory of hosts from Satellite, including host facts such as IP (which is required), cloud credentials are needed to connect to Satellite. For those of you used to defining static inventories with IPs, the light should now being going on.  Add Machine Credential  Ansible Tower can store either user/password or ssh private key. Most likely you would want to use private key but here I chose to use user/password.    Add Cloud Credential  In order to communicate with Satellite and dynamically inventory hosts from Satellite, cloud credentials for Satellite are required. Ansible Tower supports many cloud platforms out-of-box (VMware, AWS, GCE, Azure, OpenStack, CloudForms and Rackspace).    Once credentials have been defined we can create inventories.  Create Inventory  An inventory is a hierarchy and you can have groups that have sub-groups, it is really flexibly. In this example we create an inventory called Datacenter. Under Datacenter a group is created for all hosts that are part of Satellite environment. Windows systems could be inventoried in a separate group from Satellite servers under Datacenter for example.  Create Datacenter Inventory    Create Satellite Inventory Group under Datacenter  When adding Satellite inventory we would select source \"Red Hat Satellite 6\" and our cloud credential for Satellite. In addition we select \"Update on Launch\". This ensures when a playbook is run against Satellite inventory that the hosts are dynamically updated prior to execution of playbook. Let's say hosts are using DHCP and IPs of our hosts changed. Normally this would be an issue but with dynamic inventory, host IP is updated and Ansible playbook runs without issues.    Once Satellite inventory is added we can click the sync button next to \"Satellite Hosts\". Doing so will manually sync hosts. On the right we then see the hosts, the same hosts we saw under \"All Hosts\" in Satellite.    In addition if we click on host, the facts provided by Satellite are also displayed in YAML or JSON. Any host facts in Satellite can be used directly in our playbooks or as limit rules as we will see. Limit rules allow you to limit hosts within an inventory. In this example we limit the playbook to the Satellite hostgroup \"RHEL_7_Dev_Servers\" which is a parameter provided by the host facts.    If we look under Satellite group we see the groups and hierarchy imported from Satellite. Not just hostgroups but lifecycle environments are also imported.    If we click on \"foreman_hostgroup_rhel_7_dev_servers\" (hostgroup), we see just the one host soetest.lab that is associated to that hostgroup in Satellite. Again these relationships are all defined in Satellite and automatically imported into Ansible Tower.    Create Job Template  In Ansible Tower a job template brings together a playbook located within a project with an inventory and credentials. Here we have selected the playbook http_simple. This playbook will setup a webserver and use information from satellite to setup website (more on this later). In this example we have also set a limit rule to limit the run of the playbook to hosts that are part of the Satellite hostgroup \"RHEL 7 Dev Servers\".    Create Survey  Survey's in Ansible Tower are optional but provide opportunity to ask the user for input. In this example we ask for a username. The username will then be displayed in the website dynamically to show how to use survey parameters inside playbooks.    Run Job Template  By clicking on the rocket, a job template can be manually started. Job templates can also be scheduled and Ansible Tower offers an integrated scheduler that can even balance jobs across multiple Tower servers in HA setup.    After launching job we are prompted for limit. The default is our satellite hostgroup. You can decide if you want to prompt and allow users to change limits.    Once we accept limit we are prompted with our survey. Here we need to enter username. The username has a variable associated \"survey_username\" and we can use this to display or use the input in our playbook.    Once job is started we are brought to the job status. On the left we can see our limit and survey username as extra variables. On the right we see the tasks that were executed in our playbook. We also see summary of tasks where something changed, failed or was unreachable. We see the hosts where the playbook was run, we had just one host since our hostgroup (limit rule) only defined one host. The reports are available for all jobs and we can trace them back to user who ran them. Users running jobs don't need to have access to systems or even have credentials. This is a huge advantage for compliance and auditing.    If we look at jobs, we will see three jobs. Our SCM (Git) was updated, then our host inventory (Satellite) was dynamically updated and finally the playbook ran.    After playbook run completes we can view our website. Here we see the hostname and IP. These are coming from the foreman facts in Satellite. In addition we see our username from the survey.    The implementation for this dynamic web page can be found here or below.  &lt;html&gt;  &lt;head&gt;  &lt;title&gt;Ansible Application&lt;/title&gt;  &lt;/head&gt;  &lt;body&gt;  &lt;/br&gt;  &lt;a href=http:///index.html&gt;Homepage&lt;/a&gt;  &lt;/br&gt; &lt;?php   Print \"Hello, World! I am a web server configured using Ansible and I am : \";  echo exec('hostname');  Print \"&lt;/BR&gt;\";  Print \"My ip address imported from foreman facts is : \";  echo \"''\";  Print \"&lt;/BR&gt;\";  Print \"My username from survey is : \";  echo \"''\";  Print \"&lt;/BR&gt;\"; ?&gt; &lt;/body&gt; &lt;/html&gt; Summary  In this article we have discussed the value for integrating Satellite and Ansible Tower. We have seen how to configure a basic host in Satellite for patch management. Using Ansible Tower we were able to dynamically get not only hosts and their associated groups from Satellite but also the host facts. We were able to run a playbook against a group of hosts in Satellite using a limit rule. Finally using survey and facts we observed how to easily reuse external parameters withing our playbooks.  Happy Automating with Ansible Tower and Satellite!  (c) 2017 Keith Tenzer  ","categories": ["Ansible"],
        "tags": ["Ansible Tower","Automation","Satellite 6"],
        "url": "/ansible/ansible-tower-and-satellite-end-to-end-automation-for-the-enterprise/",
        "teaser": null
      },{
        "title": "Ansible Tower Installation and Configuration Guide",
        "excerpt":"  Overview  In this article we will setup and configure Ansible Tower on Red Hat Enterprise Linux (RHEL). By now unless you are hiding under a rock, you have heard about Ansible. Ansible is quickly becoming the standard automation language used in enterprises for automating everything. Ansible is powerful, simple, easy to learn and these of course are the main reasons it becoming the standard everywhere. Ansible has two components: Ansible core and Ansible Tower. Core provides the Ansible runtime that executes playbooks (yaml files defining tasks and roles) against inventories (group of hosts). Ansible Tower provides management, visibility, job scheduling, credentials, RBAC, auditing / compliance and more. Installing Ansible Tower also installs Ansible core so you kill two birds with one stone.    Prerequisites  Before starting you need to deploy a host or VM with RHEL 7.3. Once the host is configured and on the network, you can register it with subscription-manager and enable the required repositories. For Ansible Tower you need rhel-7-server-rpms, rhel-7-server-extras-rpms and EPEL.  # subscription-manager register # subscription-manager list --available # subscription-manager attach --pool=&lt;pool id&gt; # subscription-manager repos --disable=* # subscription-manager repos --enable=rhel-7-server-rpms \\ --enable=rhel-7-server-extras-rpms Install and configure EPEL repositories.  yum install http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm Install Ansible  # yum install -y ansible Install Ansible Tower  Download latest Ansible Tower release.  # curl -k -O https://releases.ansible.com/awx/setup/ansible-tower-setup-latest.tar.gz Extract tarball.  # gunzip ansible-tower-setup-latest.tar.gz # tar xvf ansible-tower-setup-latest.tar Configure Ansible Tower. Ansible Tower uses an Ansible playbook to deploy itself (what a great idea). As such configuration parameters or groupvars are stored in inventory file.  # cd ansible-tower-setup-3.1.2 # vi inventory  [tower]  localhost ansible_connection=local  [database]  [all:vars]  admin_password='redhat01'  pg_host=''  pg_port=''  pg_database='awx'  pg_username='awx'  pg_password='redhat01'  rabbitmq_port=5672  rabbitmq_vhost=tower  rabbitmq_username=tower  rabbitmq_password='redhat01'  rabbitmq_cookie=cookiemonster  # Needs to be true for fqdns and ip addresses  rabbitmq_use_long_name=false Run Setup.  # ./setup.sh Configure Ansible Tower  Ansible Tower Provides a RESTful API, CLI and UI. To connect to the UI simply open browser using https and point to your Ansible Tower IP or hostname.  https://&lt;Ansible Tower IP or Hostname&gt; Login using the user you configured in the inventory file, in this case admin/redhat01.    Once you are logged in, you need to configure license. Tower license comes in a file, so simply browse to the file and accept the terms. If you don't have a license you can get a trial here.    Summary  In this brief article we went through how to install and configure Ansible Tower. Regardless of if you are new to Ansible or experienced, Tower is definitely worth looking at as it provides a solid management platform for Ansible and makes everything even easier and more powerful!  Happy Ansibleling!  (c) 2017 Keith Tenzer  &nbsp;  ","categories": ["Ansible"],
        "tags": ["Ansible Tower","Automation","Playbook"],
        "url": "/ansible/ansible-tower-installation-and-configuration-guide/",
        "teaser": null
      },{
        "title": "OpenShift Showback Reporting using CloudForms",
        "excerpt":"     Overview  One of the most important capabilities of any platform in today's service driven, pay-as-you-go economy is metering and showback. Without a solid understanding of costs, organizations are in fact unable to provide services. With containers, metering and showback becomes more challenging. If we think about containers simply being processes, then we are basically needing to meter and perform showback at that level of granularity. In addition since OpenShift uses Kubernetes for container orchestration, there are additional concepts that are new. For example, one more more containers run together in what Kubernetes refers to as a Pod. Next Pods are extremely dynamic and their lifetime very short. All of this make metering and showback anything but straight-forward. Thankfully OpenShift and CloudForms have the solution.    OpenShift comes with Hawkular, a highly scalable, flexible, performant metric storage system based on Cassandra. Hawkular runs on OpenShift (as a container) and collects CPU, Memory, Disk and Network utilization for all Pods running in environment.  CloudForms is a multi-cloud management platform that integrates with many platforms including OpenShift. CloudForms is able to read the data from Hawkular, apply user-defined rates and provide report on costs for containers running in Pods on OpenShift. By default CloudForms is able to show these reports per project. Since an application or business will have multiple projects, showback should be ideally done based on a group of projects. In this article we will look at how to create showback reports from a group of interrelated projects in OpenShift using CloudForms.  Pre-requisites  In order to setup showback reports you will need an OpenShift environment and of course CloudForms. I tested with OpenShift 3.5 and CloudForms 4.5.   Deploy CloudForms on Red Hat Enterprise Virtualization Deploy CloudForms on Azure Deploy OpenShift and Configure CloudForms  Label Projects in OpenShift  In OpenShift since orchestration is Kubernetes, everything can be labeled. In this case we want to label our OpenShift projects (Kubernetes namespace) so projects that belong together can be easily identified. For simplicity, we will call a group of OpenShift projects that have same cost center a box. Each box has a number, so it can be identified. Ideally the name of the project should also have the box for quick identification.  In the real-world you will most likely not want to allow developers to create projects directly in OpenShift. Instead you can use CloudForms and provide a service. Here you can ask end-user for a cost center or something to identify themselves and then ensure all projects that are created have that identifier in the name (optional) and label (required). You can also enforce quotas and of course would want to do that to ensure end-user's only get what they pay for.  List all projects that belong together  Here it is easy since they all have Box02 in display name. Again why you would want to optionally add identifier to project name.  [ktenzer@master1 ~]$ oc get projects |grep -i box greetme-poc Box02: greetme-poc Active greetme-norman-dev Box02: greetme-norman-dev Active greetme-prod Box02: greetme-prod Active greetme-workshop Box02: GreetMe-Workshop Active greetme-dev Box02: greetme-dev Active greetme-norman-prod Box02: greetme-norman-prod Active Label all the projects that belong together  In this case we are labeling box02 projects.  [ktenzer@master1 ~]$ oc label namespace greetme-poc box=box02 namespace \"greetme-poc\" labeled [ktenzer@master1 ~]$ oc label namespace greetme-norman-dev box=box02 namespace \"greetme-norman-dev\" labeled [ktenzer@master1 ~]$ oc label namespace greetme-prod box=box02 namespace \"greetme-prod\" labeled [ktenzer@master1 ~]$ oc label namespace greetme-workshop box=box02 namespace \"greetme-workshop\" labeled [ktenzer@master1 ~]$ oc label namespace greetme-dev box=box02 namespace \"greetme-dev\" labeled [ktenzer@master1 ~]$ oc label namespace greetme-norman-prod box=box02 namespace \"greetme-norman-prod\" labeled Map Labels to Tags in CloudForms  Once projects have been labeled in OpenShift we need to map those labels to tags in CloudForms.  Refresh CloudForms Data  Go to the OpenShift provider in CloudForms and refresh relationships.    View Labels  OpenShift projects labels should now be visible in CloudForms. In this case box is the label and on our greetme-prod project it is set to the box02 label.    Map label to tag  Under user-&gt;Configuration you can map labels to tags.    We will create a new mapping and simply map the OpenShift label box to a tag in CloudForms also called box. The mappings don't of course need to match.    Refresh CloudForms Data  Go to the OpenShift provider in CloudForms and refresh relationships. CloudForms needs to now discover the tags.    View Tags  Under the project, smart management tab we can now see the tag box: box02 has been applied.    Create Showback Reports in CloudForms  Once the labels exist in OpenShift and are mapped to tags we can start to generate reports and actually sort by tag. Here we will create two reports. One that shows showback costs for all boxes or tags and other for a specific tag like box02.  Create Showback Rates  Under \"Cloud Intel-&gt;Chargeback&gt;Rates\" you can create your own rates. CloudForms lets you choose the currency and rates. You can either configure fixed or variable for CPU, Memory, Network I/O and Disk I/O. Finally CloudForms also lets you set ranges. You could have the first few CPU cores cost X and the rest cost Y, as example. Both reports will use the same showback rates.    Create Showback Report for all \"box\" tags  This report will show all boxes for our tag box and their cost. Under \"Cloud Intel&gt;Reports-&gt;Custom\" add a new report.    Name the report and select the fields that should be part of the report.    Here we have selected the tag and CPU / Memory used and cost.    Under the \"filter\" tab of the report we need to define the filters. Select all projects and group by the tag box.    To run the report select the report and click queue. You can also generate the report under the preview tab. Below is the output. This report shows cost according to box (a group of OpenShift projects). It is a monthly report that shows total cost per day. In this case we only have box02. Notice the &lt;empty&gt; box. These are projects that are not associated to a box.    Create Showback Report for tag box02  In this report we want to only see projects that have the tag box02. Follow the same steps as above only this time change show costs, to tag and select the tag box02. Grouping should be done by project.    To run the report select the report and click queue. You can also generate the report under the preview tab. Below is the output. This report shows cost according to box02. We see all the projects that are part of box02 and their costs. It is a monthly report that shows total cost per day per project.    Summary  In this article we looked at how OpenShift and CloudForms integrate with one another to provide both metering and showback. This involves labeling OpenShift projects, mapping them to tags in CloudForms and building reports based on user-defined rates. We created a report to provide an overview of all tagged OpenShift projects and a report that shows only the projects of a specific tag. Hopefully this article provides some ideas and you have seen the possibilities that exist today for metering and providing showback of costs in OpenShift using CloudForms.  Happy OpenShifting!  (c) 2017 Keith Tenzer  ","categories": ["CloudForms"],
        "tags": ["Chargeback","Containers","Kubernetes","Metering","OpenShift","Showback"],
        "url": "/cloudforms/openshift-showback-reporting-using-cloudforms/",
        "teaser": null
      },{
        "title": "Keep Your Servers and Run Your Applications Forever with Red Hat Virtualization powered by KVM",
        "excerpt":"  Overview  This article was written by myself and fellow colleague Götz Rieger. Often one of the most challenging problems we are facing today is both absorbing and leading change. Software defined-everything has taken over and is leveling the playing field, de-marginalizing staunch competitive advantages and nothing is safe anymore. Develop great applications and thrive or become irrelevant is the mantra facing many organizations. In such environments it is important to innovate constantly, delivering new capabilities at an ever increasing speed. In order to do so, new practices (DevOps), values (Agile) and of course technology (Containers) are being implemented.   Today it seems almost everyone is focused on \"the new\" software-defined whatever, when in reality change happens at different levels and different speeds. Gartner tried to summarize this with \"mode 1 vs mode 2\" but that trivializes things too far. It comes down to application lifecycles which dictates dependency on change.  What if certain software doesn't need to change? What if it has a purpose and is already doing it's job function? What if the software cannot be ported to a new operating platform? What do you do then? The answer surprisingly, is maybe nothing? Maybe we let those applications live well beyond their intended support lifecycles. Consider the old programs in the Matrix, some found a way to survive and were not killed. These were also some of the most important, powerful programs.   Virtualization has enabled us to let x86 platforms essentially run forever or at least well beyond their support lifecycles (hardware and software). If we consider outdated Cobol applications on UNIX or Windows platforms like NT, XP and 2003; they haven't been supported for years. Applications running on these platforms might are not able to migrate for whatever reason, else they would have already done so. If we think about it, this is in fact a very valid use case for virtualization. There are of course other considerations that are important, like isolation (since these applications are not receiving patches) but assuming that is handled, why not? If it ain't broken and doesn't need to change, why fix it?   In this article we will look at how to run Windows NT Server (an operating system that hasn't been supported since 2004) on KVM and Red Hat Virtualization powered by KVM.     Prerequisites  We found two options to work well, a VMware VM with NT already installed and patched or install NT into a qemu/KVM VM. In order to continue you will need the following:   Red Hat Virtualization (RHV) environment (Here we used RHV 4.1) KVM Windows NT installation medium (ISO) or a VMware virtual disk with pre-installed NT  WARNING: Just to be clear Windows NT is not a supported guest OS for Red Hat Virtualization (RHV). That should be obvious since it isn't supported by Microsoft either.  Option 1: Installing Windows NT ISO  On RHV we didn’t get the Windows NT 4.0 installer from the original boot CD to run without Blue Screening on us, so we had to take a detour. Windows NT 4.0 installs on qemu/KVM using virt-manager (RHEL 7.3 in this case). After installation, NT can be patched up to SP6 (which runs fine on RHV) and then the VM can be migrated to RHV. Remember in those days you couldn’t easily build a slip-stream CD with the updates integrated anyway.  In order to install Windows NT you will need the original installation CD as  ISO image, Service Pack 6 and the Realtek RTL8139 network driver for NT. Of course it goes without saying, you should use a purchased copy of Windows NT.   Convert Windows NT SP 6 to ISO  # mkisofs -o iso winntsp6.iso Convert Realtek Driver to ISO  # mkisofs -o iso rtl8139.iso Create Windows NT Virtual Machine in KVM  Provide the Windows NT Server ISO and configure the Virtual Machine using virt-manager.  Set CPU Model to Pentium  Windows NT (without SP6) can only boot on older CPU generations like Pentium.    Set Virtual Disk to IDE  Windows NT does not have a virtio driver so we need to use IDE interface.    Set Network to RTL8139  Windows NT has drivers for Realtek 8139.    Configure Windows NT Server    Format Partition as NTFS   Select defaults other than NTFS for the partition.    Reboot  Before rebooting change the boot order in virtual machine so that it boots from disk and not cdrom.    Windows NT Setup  Enter license and otherwise select defaults and server role (DC or standalone).    Install Realtek 8139 Driver  Mount the rtl8139.iso in the cdrom that was created above and install driver. Configure IP, Subnet, Gateway and DNS.    Login to Windows NT Server    Install Windows NT Server SP 6  Using ISO created above, mount the Windows NT SP6 ISO and run the installer.  Note: A lesson learned back in the day: If you install the network card driver (or anything else for that matter) after Service Pack installation, just re-install the SP afterwards. This takes care of the dreaded “a service couldn’t be started” error.     Test Networking  Ensure network stack is functional.    Congratulations!!! You just installed plus booted, an Operating System that has been off support since 2004 (over 10 years) and would most definitely not run on any modern hardware that exists today.  Option 2: Importing from VMware  Red Hat provides a tool called virt-v2v for converting from VMware or other virtualization platforms to RHV and KVM. Unfortunately in this case, Windows NT is simply too old and virt-v2v cannot recognize the operating system. Thankfully however the qemu-img command can easily convert a vmdk to qcow2.  Convert VMDK to QCOW2  # qemu-img convert -f vmdk -O qcow2 winnt_server.vmdk winnt_server.qcow2 Once the image is converted you can import into KVM or RHV.  Note: you need to remember to set CPU: Pentium, Virtual Disk Interface: IDE and NIC: RTL8139 after importing and before starting virtual machine.  Importing Windows NT Server in RHV  Now that we have a working Windows NT Server 4 SP6 qcow2 image it is time to import into Red Hat Virtualization (RHV). There are several options but we will explore two.   Importing into RHV directly from KVM (Libvirt) Importing using a script from Richard Jones (Colleague in Virtualization Group @Red Hat)  Option 1: Import using KVM (Libvirt)  RHV has a direct integration and can import virtual machines directly from many virtualization platforms like Libvirt, VMware, Amazon EC2, etc.  Configure KVM (Libvirt) to allow external connections  [On KVM Host]  Enable TCP connections to Libvirt Daemon  #vi /etc/libvirt/libvirtd.conf listen_tls = 0 listen_tcp = 1 tcp_port = \"16509\" auth_tcp = \"none\" #vi /etc/sysconfig/libvirtd LIBVIRTD_ARGS=\"--listen\" Open Libvirt Port on Firewall  firewall-cmd --permanent --add-port=\"16509/tcp\" firewall-cmd --reload Import Virtual Machine   In RHV management console under VM-&gt;Import select KVM as source. The URI should be \"qemu+tcp://IP/system\". Once setting username and password select load to display the virtual machines running on KVM host. Select the Window NT Server virtual machine.    Choose Storage Domain  Storage domains in RHV are similar to VMware datastores, this is where the virtual machines live. Here we will select a desired storage domain.    Once the import process is started you will be notified and can follow the process via the event monitor in RHV.  Option 2: Import Using Script  Richard Jones has created a very useful script that imports qcow2 images directly into RHV via the export domain (a special storage domain for importing templates). We really like this method for qcow2 because it doesn't try to convert the image like virt-v2v (which isn't needed) and you don’t need RHV to have direct access to another virtualization platform. The script and it's commits are stored here. In addition Richard Jones has also provided a blog on using his import script.  Install the Script  Select a host where you would like to install the script. The host shouldn’t be one of your RHV systems (neither RHV-H nor RHV-M). Then simply copy/paste or use curl to get the script from here.  Install Dependencies  # yum install perl-XML-Writer perl-Sys-Guestfs Configure Direct Image Conversion  We don't want to convert image using libvirt but direct.  # export LIBGUESTFS_BACKEND=direct Run Import Script  The script needs to be able to access the export storage domain via NFS. Ensure the host running the export script can mount the NFS share locally and write there. Here the host providing the export domain mount is \"rhevh01.lab\" and the path is \"/usr/share/export\".  # ./import-to-ovirt.pl winnt_server.qcow2 rhevh01.lab:/usr/share/export The script will create the necessary structure and metadata and place things into the export storage domain in RHV.  Import Image from Export Domain  In RHV management under storage select the export storage domain. At the bottom select the tab \"VM Import\". Here you should see the image we just imported.    Congratulations!!! You just successfully imported Windows NT Server into RHV.  Confirm Settings  Before starting the Windows NT virtual machine we will ensure the settings are correct and the VM is configured to present the supported hardware.  Virtual Disk Set to IDE    Set CPU Model  Okay, we are done and ready to start NT! No, wait a second… you might remember we said NT is only able to boot on older CPU families, we choose “Pentium” in virt-manager. Have a look at the CPU settings available for the VM in RHV, no Pentium or other archaeological  interesting CPU families to be seen.  This doesn’t mean they aren’t there, underneath it’s KVM after all, but they are not exposed in the web UI. Fortunately RHV comes with a feature called “VDSM Hooks” that allows to run arbitrary scripts on VM actions and enables us to change almost every feature of a VM.  Configure VDSM Plugin  [On each Hypervisor (RHV-H) Host]  Credit here goes to colleagues Martin Polednik and Michal Skrivanek for providing VDSM hook script.  # vi /usr/libexec/vdsm/hooks/before_vm_start/50_cpufamily #!/usr/bin/python  import os import sys import hooking import traceback  if os.environ.has_key('cpufamily'):  try:     domxml = hooking.read_domxml()   #here we read the VM XML into the domxml variable     vcpu = domxml.getElementsByTagName('cpu')[0] #find and read the CPU definition in the VM XML     vcpu.setAttribute('mode', 'custom')     vcpu.setAttribute('match', 'exact')     vcpu.setAttribute('check', 'partial')     sys.stderr.write('cpufamily: Changing cpu family to: %s\\n' % os.environ['cpufamily'])  #sys.stderr.write is caught by vdsm and logged into vdsm.log for debugging     e = domxml.createElement('model')     e.setAttribute('fallback', 'forbid')     txt = domxml.createTextNode(os.environ['cpufamily'])     e.appendChild(txt)     modelnode = vcpu.getElementsByTagName('model')[0]     vcpu.replaceChild(e,modelnode)      # Set topology sockets to N (example uses N = 1).     topo = vcpu.getElementsByTagName('topology')[0]     topo.setAttribute('sockets', '1') # &lt;- N      # Edit  element's text node.     vcpu = domxml.getElementsByTagName('vcpu')[0]     vcpu.firstChild.nodeValue = 1      hooking.write_domxml(domxml) #and write to the altered domxml  except:     sys.stderr.write('cpufamily: [unexpected error]: %s\\n' % traceback.format_exc())     sys.exit(2)  Make VDSM plugin executable  # chmod +x /usr/libexec/vdsm/hooks/before_vm_start/50_cpufamily How the VDSM Hook Works   Looks if there is a configuration key named “cpufamily” in this VMs configuration. Sets the CPU model to the value of the key. Does some CPU topology corrections.  So what’s missing is the key/value pair for the hook script. This can be set as a custom attribute, inside the VM.   Set Custom Attribute in VM  [On RHV-Manager Host]  # engine-config -s UserDefinedVMProperties='cpufamily=^(Conroe|Penryn|Nehalem|Westmere|SandyBridge|pentium)$' --cver=4.1 This sets the user defined property “cpufamily” and supplies a number of values you can choose from in the web UI.   Note: A list of the supported CPU models can be found on your RHV Hypervisors under /usr/share/libvirt/cpu_map.xml.  Restart RHV-M Engine  # systemctl restart ovirt-engine Set CPU Type to Pentium  In the settings of your NT VM in RHV-M go to “Custom Properties”, you should have a key named “cpufamily”, set it to “pentium”.    NIC Set to RTL8139  The last step is to configure the network card as Realtek 8139 so the network configuration inside the VM works. Select the VM and at bottom there is a tab for network and there you can add or edit the network interface.    &nbsp;  Start Windows NT Virtual Machine on RHV  The moment has arrived where you can finally start the Windows NT virtual machine. I know, you can cut through the excitement with a knife. Again check to ensure network connectivity is working (likely you will need to reconfigure TCP/IP inside virtual machine).    This is exactly what success feels like!!!  Windows NT Server HA  Things have changed a lot since Windows NT era, one thing that has significantly improved is HA. Once Windows NT Server is running we can use migrate option and automatically migrate the virtual machine to another Hypervisor.   Select Hypervisor  Here we see the Windows NT virtual machine running on \"rhevh02.lab\" and will migrate to \"rhevh01.lab\".    Migrate Virtual Machine  During the migration the virtual machine is locked, meaning actions like start and stop are not allowed.     Migration Complete  Viewing the event log in RHV we can see when the migration operation started and completed.    Summary  In this article we discussed one of the least talked about virtualization use cases, extending application and operating system lifecycles well beyond intended support of hardware or software. Using Windows NT Server, an operating system introduced in 1996 that went end-of-life in 2004, we walked through steps of bringing things online in both KVM and Red Hat Virtualization (RHV) powered by KVM. Hopefully this article brings to light the point that if it ain't broken, maybe just maybe you don't need to fix it!  Happy Virtualizing!  (c) 2017 Keith Tenzer  ","categories": ["RHV"],
        "tags": ["KVM","Linux","NT","RHEV","RHV-H","RHV-M","Virtual Machine","Virtualization","Windows"],
        "url": "/rhv/keep-your-servers-and-run-your-applications-forever-with-red-hat-virtualization-powered-by-kvm/",
        "teaser": null
      },{
        "title": "OpenShift 3.6 Fast Track: Everything You Need, Nothing You Don't",
        "excerpt":"  Overview  OpenShift Container Platform 3.6 went GA on August 9, 2017. You can read more about the release and new features here. In this article we will setup a standard non-HA environment that is perfect for PoCs or labs. Before we begin, let's explain OpenShift for those that may be starting their OpenShift journey today. OpenShift is a complete container application build + run-time platform built on Kubernetes (Container Orchestration) and Docker (Container Packaging Format). Organizations looking to adopt containerization for their applications need of course a lot more than just technology, (Kubernetes and Docker), they need a real platform. OpenShift provides a service catalog for containerized applications, huge selection of already certified application runtimes + xPaaS services, a method for building containerized applications (source to image), centralized application logging, metrics, autoscaling, application deployments (Blue-Green, A/B, Canary, Rolling), integrated Jenkins CI/CD pipelines, integrated docker registry, load balancing / routes to containerized apps, multi-tenant SDN, security features (SELinux, secrets, security context), management tooling supporting multiple OpenShift environments (CloudForms), persistent storage (built-in Container Native Storage), automated deployment tooling based on Ansible and much, much more. OpenShift is a platform that runs on any infrastructure, from bare-metal to virtualization to public cloud (Amazon, Google, Microsoft), providing portability across cloud infrastructure for containerized applications. All of these things together is what truly enables organizations to move to DevOps, increase application release cycles, speed up innovation cycles, scale efficiently, gain independence from infrastructure providers and deliver new capabilities faster with more reliability to their customers.    OpenShift Environment  OpenShift has a few different architectures (Single Master, Multiple Master Integrated ETCD or Multiple Master Separate ETCD).  Single Master - This is a non-HA configuration. First there is only a single master which also runs ETCD and a single infrastructure node. The infrastructure node also runs OpenShift services such as docker registry, router, metrics and logging. This is only intended for lab or PoC environments.  Multiple Master Integrated ETCD - This is an HA configuration with three masters and two or more infrastructure nodes. OpenShift control plane as well as critical services are all in active/active HA configuration. ETCD service is running on the three masters.  Multiple Master Separate ETCD - This is an HA configuration with three masters and two or more infrastructure nodes. OpenShift control plane as well as critical services are all in active/active HA configuration. ETCD service is running on three separate servers. Since ETCD maintains state for OpenShift cluster it is critical the service is not only available but provides fast responses and updates. In some environments due to number of objects or size, it is preferred to separate ETCD. This is of course an architectural decision that requires discussion and planning.  Here we have chosen to go with Single Master. Note: when referring to regions and zones, these are just labels that allow us to isolate nodes to different applications / projects. They are not dependent on public cloud regions.    API and management traffic go to the master server. In a multiple master configuration a load balancer would balance traffic across the masters. Application traffic instead goes directly to the infrastructure node. In case of HA configuration,  multiple infrastructure nodes and of course a load balancer in front would be configured. Application domains are in this case directed toward the single infrastructure node in DNS. In case of multiple infrastructure nodes, DNS should point application domains to load balancer. DNS round-robin is not a good option as it will continue to send traffic to infrastructure nodes, even if unavailable.  In this configuration we will create two regions and two zones. OpenShift through labeling lets you tailor the architecture to meet your infrastructure and Affinity / Anti-Affinity requirements. Infrastructure and applications are isolated to specific nodes using the region. Within the application region (Primary) we separate applications between zones test and production. Using the multi-tenant SDN included in OpenShift, traffic between applications in test and production will be restricted or not allowed.  Application persistence is provided by container native storage. Normally we would define special nodes for providing storage, in this case however the application nodes will fill that need. Container Native Storage works by running glusterfs (software-defined scale-out storage system) inside a container on each node. Container Native Storage requires a minimum of three nodes. The glusterfs container consumes local disks, available on node and builds a cluster-wide highly redundant storage system spanning all the nodes. OpenShift provides a storage class for consuming the storage dynamically. Developers can simply ask for storage by size (GB) and is is provided on the fly, dynamically assuming of course quotas aren't overwritten. In this environment storage is also automatically reclaimed based on policies defined by the Developer.  Configure OpenShift Nodes  In OpenShift there are several types of nodes (master, infra, app, storage and etcd). Each type of node has different CPU, memory and storage requirements. The minimum requirements are 8GB of memory for a node but you can get away with far less (if you disable checking in configuration). Since this is a lab setup we will do just that.  For storage we will either use 40GB or 30GB root disk, 15GB for Docker and 100GB for Container Native Storage. Everything is being provided by the virtualization layer, in this case Red Hat Virtualization powered by KVM. Again this could be bare-metal, any virtualization platform or any public cloud provider.  Master Nodes      Infra Nodes      App Nodes      Prepare OpenShift Nodes  Once the infrastructure is complete, a Container Operating System needs to be installed. The Operating System is as important as ever since containers are simply processes that run on Linux. Containers do not provide good isolation like VMs so security features in Operating System like SELinux, Kernel Capabilities, Container Signing, Secured Registries, TLS communications, Secrets and Security Context's are incredibly important. Thankfully you get it all out-of-the-box with OpenShift.  The choice of Operating Systems comes down to RHEL or Atomic. Atomic is a RHEL kernel but is image based (no RPMs), everything that is installed or runs in Atomic must run as container. RHEL allows more flexibility to install other required software but the recommendation is definitely Atomic for OpenShift, especially if additional software is not required (monitoring, etc).  [All Nodes]  Register Repositories  # subscription-manager register # subscription-manager attach --pool=&lt;pool_id&gt; # subscription-manager repos --disable=\"*\" # subscription-manager repos \\  --enable=\"rhel-7-server-rpms\" \\  --enable=\"rhel-7-server-extras-rpms\" \\  --enable=\"rhel-7-server-ose-3.6-rpms\" \\  --enable=\"rhel-7-fast-datapath-rpms\" Install Required Packages  # yum install -y wget git net-tools bind-utils iptables-services \\ bridge-utils bash-completion kexec-tools sos psacct Update Operating System  # yum update -y Configure Docker  # yum install -y docker-1.12.6 # vi /etc/sysconfig/docker OPTIONS='--insecure-registry=172.30.0.0/16 --selinux-enabled --log-driver=journald' Since we are not using Docker Hub (god save you if you are doing that) and the OpenShift Docker registry is internal, we can safely allow insecure registry. Make sure SELinux is enabled, this is a big deal for securing your containers. The log driver should be journald.  cat &lt;&lt;EOF &gt; /etc/sysconfig/docker-storage-setup DEVS=/dev/sdb VG=docker-vg EOF Here we will use the 15GB disk for docker storage. You can use the 'lsblk' command to easily see the device numbers of your disks. Make sure the disk has no partitions.  # docker-storage-setup # systemctl enable docker  # systemctl start docker Name Resolution  Here we have two options, either use DNS or hosts file. Obviously for a real environment you are going with DNS but some people are lazy.  If you are using DNS, create a wildcard pointing from application domain (apps.lab) to the infrastructure node. You can simply create an 'A' record in your named zone (*.apps.lab).  vi /etc/resolv.conf #search lab cluster.local Ensure any search domain is commented out in /etc/resolv.conf, this will break SkyDNS used in OpenShift for internal service registry and resolution. To keep resolv.conf from being updated you can 'chattr +i' the file or update dnsmasq settings.  # systemctl reboot [On Laptop]  In case you are using hosts file, make sure your laptop can resolve the OpenShift master and Hawkular (metrics) as well as Kibana (logging) need to be pointed at infra node. Additionally if you create any applications in OpenShift and expose them via routes, those host names need to go into hosts file and resolve to infra node. Hopefully you see now why a wildcard DNS is recommended.  Example of my Laptop /etc/hosts.  # vi /etc/hosts 192.168.0.30 master1.lab master1 192.168.0.34 hawkular-metrics.apps.lab 192.168.0.34 kibana.apps.lab Configure OpenShift  As mentioned, OpenShift uses Ansible as it's life-cyle management and deployment tool. There are playbooks for installing, upgrading and adding or removing various components (Metrics, Logging, Storage, etc). Normally a Bastion host would be deployed for Ansible. In this case we install Ansible on the master.  [On Master]  Install Ansible and OpenShift Playbooks  # yum install -y atomic-openshift-utils Configure SSH Authorization  # ssh-keygen # for host in master1.lab \\ appnode1.lab \\ appnode2.lab \\ appnode3.lab \\ infranode1.lab; \\ do ssh-copy-id $host; \\ done Configure Deployment Options  The Ansible playbooks require an inventory file that defines the hosts and also configuration options for OpenShift via group vars.  # vi /etc/ansible/hosts # Create an OSEv3 group that contains the masters and nodes groups [OSEv3:children] masters etcd glusterfs nodes  # Set variables common for all OSEv3 hosts [OSEv3:vars] # SSH user, this user should allow ssh based auth without requiring a password ansible_ssh_user=root  # OpenShift Deployment, enterprise of course! openshift_deployment_type=openshift-enterprise  #Set Domain for Apps openshift_master_default_subdomain=apps.lab  # Enable htpasswd authentication; defaults to DenyAllPasswordIdentityProvider openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]  # Set networking to multi-tenant os_sdn_network_plugin_name='redhat/openshift-ovs-multitenant'  # Enable CNS (glusterfs) as default storage provider openshift_storage_glusterfs_namespace=glusterfs openshift_storage_glusterfs_name=storage  # Deploy logging openshift_hosted_logging_deploy=true openshift_hosted_logging_storage_kind=dynamic  #Deploy Metrics openshift_hosted_metrics_deploy=true  #Disable disk and memory checks openshift_disable_check=disk_availability,memory_availability  # host group for masters [masters] master1.lab  # host group for etcd [etcd] master1.lab  # host group for glusterfs [glusterfs] appnode1.lab glusterfs_ip=192.168.0.31 glusterfs_devices='[ \"/dev/sdb\" ]' appnode2.lab glusterfs_ip=192.168.0.32 glusterfs_devices='[ \"/dev/sdb\" ]' appnode3.lab glusterfs_ip=192.168.0.33 glusterfs_devices='[ \"/dev/sdb\" ]'  # host group for nodes, includes region info [nodes] master1.lab openshift_schedulable=False infranode1.lab openshift_schedulable=True openshift_node_labels=\"{'region': 'infra', 'zone': 'default'}\" appnode1.lab openshift_schedulable=True openshift_node_labels=\"{'region': 'primary', 'zone': 'prod'}\" appnode2.lab openshift_schedulable=True openshift_node_labels=\"{'region': 'primary', 'zone': 'prod'}\" appnode3.lab openshift_schedulable=True openshift_node_labels=\"{'region': 'primary', 'zone': 'test'}\" Install OpenShift  Once the hosts and configurations options are set in the inventory file, simply run the playbook. There are many playbooks, even ones that integrate directly with infrastructure platforms like (Amazon, Google, Microsoft). These can also setup VMs and infrastructure components, like load balancers that are required. In our case we will go with BYO (bring your own), basically means we have taken care of infrastructure and it is all ready to go. This is typically what you want for on-premise deployments.  # ansible-playbook -vv /usr/share/ansible/openshift-ansible/playbooks/byo/config.yml PLAY RECAP *****************************************************************************************************************************************************************************************  appnode1.lab : ok=244 changed=12 unreachable=0 failed=0  appnode2.lab : ok=244 changed=12 unreachable=0 failed=0  appnode3.lab : ok=244 changed=12 unreachable=0 failed=0  infranode1.lab : ok=229 changed=12 unreachable=0 failed=0  localhost : ok=13 changed=0 unreachable=0 failed=0  master1.lab : ok=999 changed=199 unreachable=0 failed=0 Success!!! Okay if you are paying attention and know Ansible, the recap shows few tasks actually changed anything. This is because I had an error in my storage configuration due to using wrong disk device, hence my playbook run failed previously. Not a big deal though, I simply fixed the issue in inventory file and re-ran Ansible. This is the beauty, simplicity and power of Ansible. Fail, Fail until you succeed and then you can reproduce it every time successfully.  Verifying OpenShift Environment  Once installation is complete, check to make sure additional services (Storage, Logging and Metrics) are functioning properly.  Create Admin User  [On Master]  Login to the master and create an admin user for external access through UI or CLI.  [root@master1 ~]# oc login -u system:admin -n default [root@master1 ~]# htpasswd -c /etc/origin/master/htpasswd admin [root@master1 ~]# oadm policy add-cluster-role-to-user \\ cluster-admin admin [root@master1 ~]# oc login -u admin -n default GUI  To access GUI simply point browser from laptop at the master on port 8443 (https://master.lab:8443).  Storage  Since we are using Container Native Storage everything is integrated and deployed with installation of OpenShift. Go into the project glusterfs. Under resources select pods. You should see three glusterfs pods (one per app node).    Select one of the glusterfs pods and select terminal. This opens a session directly, in container. Next you can check it's peer status as well as status of glusterfs volumes. Since we have three disks we will end up with three bricks. Data is then replicated and distributed across all three. No need to worry, adding and removing storage is all done through OpenShift dynamically, you should never need to touch anything in glusterfs level unless troubleshooting.    Make Storage Class Default  # oc patch storageclass glusterfs-storage -p '{\"metadata\": {\"annotations\": {\"storageclass.kubernetes.io/is-default-class\": \"true\"}}}' Logging  Open the logging project. Under Kibana you will see a route or URL (https://kibana.apps.lab). This will change depending on what domain was configured in OpenShift. Connect and you will be prompted to login. OpenShift uses elastic search and automatically populates it with projects and users from OpenShift. Users only see logs for projects and applications where they have access.    Metrics  In the logging project you will notice CPU, Memory and Network utilization are shown. This is coming from data collected in Hawkular and a 15 minute sample is shown.    Under the pods you can view more details and see the performance data historically over much longer period of time.    Miscellaneous  In OpenShift 3.6 not only OpenShift itself but also additional services are installed with Ansible. Here is an example of how to install and remove logging.  [On Master]  Install Logging  ansible-playbook -vv /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-cluster/openshift-logging.yml Remove Logging  ansible-playbook -vv /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-cluster/openshift-logging.yml -e openshift_logging_install_logging=False Coolstore  Coolstore is a polygot demo application comprised of many different services that form a shopping car application. It is a great example of the value OpenShift as a platform brings to a microservice orientated architecture. It illustrates how various containerized services can communicate with one another and how services can be independently released.  Clone Git Repo  # git clone https://github.com/jbossdemocentral/coolstore-microservice.git Change Dir  # cd coolstore-microservice/openshift Create Project  # oc new-project coolstore Deploy Coolstore App  # oc process -f coolstore-template.yaml | oc create -f -   &nbsp;  Summary  In this article we introduced OpenShift being a platform for containerized applications and spoke of the value it provides above the underlying technology (Kubernetes / Docker). We discussed various architectures and detailed the design for a lab or PoC configuration. Finally we went through all the steps to configure infrastructure, prepare nodes, configure, install and verify a running OpenShift environment.  Happy OpenShifting!  (c) 2017 Keith Tenzer  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  ","categories": ["OpenShift"],
        "tags": ["Containers","DevOps","Docker","Kubernetes","OpenShift"],
        "url": "/openshift/openshift-3-6-fast-track-everything-you-need-nothing-you-dont/",
        "teaser": null
      },{
        "title": "OpenStack 11 (Ocata) Lab Installation and Configuration Guide",
        "excerpt":"  Overview  In this article we will focus on installing and configuring OpenStack Ocata using RDO and the packstack installer. RDO is a community platform around Red Hat’s OpenStack Platform. It allows you to test the latest OpenStack capabilities on a stable platform such as Red Hat Enterprise Linux (RHEL) or CentOS. This guide will take you through installing the OpenStack Liberty release, configuring networking, security groups, flavors, images and are other OpenStack related services. The outcome is a working OpenStack environment based on the Ocata release that you can use as a baseline for testing your applications with OpenStack capabilities.      Install and Configure OpenStack Ocata  In this configuration we will go with one controller and two compute nodes.  [All Nodes]   Install RHEL or CentOS 7.3 or 7.4. Ensure name resolution is working.  # vi /etc/hosts 192.168.122.40 ospctr.lab ospctr 192.168.122.41 ospcmp1.lab ospcmp1 192.168.122.42 ospcmp2.lab ospcpmp2  Set hostname.  We will have three nodes: ospctrl.lab (controller), ospcmp1.lab (compute) and ospcmp2.lab (compute).  # hostnamectl set-hostname ospctr.lab Note: set hostname for other nodes (ospcmp1.lab and ospcmp2.lab)   Disable firewalld since this is for a lab environment.  # systemctl disable firewalld # systemctl stop firewalld  Disable NetworkManager, it is still not recommended for Liberty (at least RDO).  # systemctl stop NetworkManager # systemctl disable NetworkManager  For RHEL systems register with subscription manager.  # subscription-manager register # subscription-manager list --available # subscription-manager attach --pool=&lt;pool id&gt; # subscription-manager repos --disable=* # subscription-manager repos --enable=rhel-7-server-rpms # subscription-manager repos --enable=rhel-7-server-rh-common-rpms # subscription-manager repos --enable=rhel-7-server-extras-rpms # subscription-manager repos --enable=rhel-7-server-openstack-11-rpms # subscription-manager repos --enable=rhel-7-server-openstack-11-devtools-rpms  Install yum-utils and update the system.  # yum install -y yum-utils # yum update -y  Reboot.  # systemctl reboot [Controller]   Install packstack packages.  # yum install -y openstack-packstack You can install packstack by providing command-line options or using the answers file.  INSTALL USING ANSWERS FILE   Create packstack answers file for customizing the installer.  # packstack --gen-answer-file /root/answers.txt  Update the packstack answers file and enable other OpenStack services. Note: as of the writing of this guide SSL is not working in combination with Horizon so don’t enable SSL.  # vi /root/answers.txt CONFIG_NTP_SERVERS=0.pool.ntp.org,1.pool.ntp.org,2.pool.ntp.org CONFIG_CONTROLLER_HOST=192.168.0.40  CONFIG_COMPUTE_HOSTS=192.168.0.41,192.168.0.42 CONFIG_NETWORK_HOSTS=192.168.0.40 CONFIG_STORAGE_HOST=192.168.0.40 CONFIG_KEYSTONE_ADMIN_PW=redhat CONFIG_PROVISION_DEMO=n CONFIG_HEAT_INSTALL=y CONFIG_HEAT_CFN_INSTALL=y CONFIG_HEAT_CLOUDWATCH_INSTALL=y CONFIG_CEILOMETER_INSTALL=y  CONFIG_LBAAS_INSTALL=y  Install OpenStack Liberty using packstack.  # packstack --answer-file /root/answers.txt  Source the keystone admin profile.  # . /root/keystonerc_admin  Backup the ifcfg-etho script.  # cp /etc/sysconfig/network-scripts/ifcfg-eth0 /root/  Configure external bridge for floating ip networks.  # vi /etc/sysconfig/network-scripts/ifcfg-eth0 DEVICE=eth0 ONBOOT=yes TYPE=OVSPort DEVICETYPE=ovs OVS_BRIDGE=br-ex # vi /etc/sysconfig/network-scripts/ifcfg-br-ex DEVICE=br-ex BOOTPROTO=static ONBOOT=yes TYPE=OVSBridge DEVICETYPE=ovs USERCTL=yes PEERDNS=yes IPV6INIT=no IPADDR= NETMASK=255.255.255.0 GATEWAY= DNS1=  Add the eht0 physical interface to the br-ex bridge in openVswitch for floating IP networks.  # ovs-vsctl add-port br-ex eth0 ; systemctl restart network.service CONFIGURE OPENSTACK   Create private network.  # neutron net-create private # neutron subnet-create private --name private_subnet --allocation-pool start=10.10.1.100,end=10.10.1.200 10.10.1.0/24  Create public network. Note: these steps assume the physical network connected to eth0 is 192.168.122.0/24.  # neutron net-create public --provider:network_type flat --provider:physical_network extnet --router:external # neutron subnet-create public --name public_subnet --allocation-pool start=192.168.0.100,end=192.168.0.200 --disable-dhcp --gateway 192.168.0.1 192.168.0.0/24  Add a new router and configure router interfaces.  # neutron router-create router1 --ha False # neutron router-gateway-set router1 public # neutron router-interface-add router1 private_subnet  Check to ensure network connectivity is working in router network namespace by pinging the external gateway.  # ip netns show qrouter-88dde0ef-22a2-44b1-baa9-304273653bb1 qdhcp-f1582f71-b531-43af-99c1-299b603232fc # ip netns exec qrouter-88dde0ef-22a2-44b1-baa9-304273653bb1 ping 192.168.0.1  Upload a glance image. In this case we will use a Cirros image because it is small and thus good for testing OpenStack.  # yum install -y wget # wget http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img # glance image-create --name \"Cirros 0.3.4\" --disk-format qcow2 --container-format bare --visibility public --file /root/cirros-0.3.4-x86_64-disk.img  Create a new m1.nano flavor for running Cirros image.  # nova flavor-create m1.nano 42 64 0 1  Create security group and allow all TCP ports.  # openstack security group create all --description \"Allow all ports\" # openstack security group rule create --protocol TCP --dst-port 1:65535 --remote-ip 0.0.0.0/0 all # openstack security group rule create --protocol ICMP --remote-ip 0.0.0.0/0 all  Create security group for base access  # openstack security group create base --description \"Allow base ports\" # openstack security group rule create --protocol TCP --dst-port 22 --remote-ip 0.0.0.0/0 base # openstack security group rule create --protocol TCP --dst-port 80 --remote-ip 0.0.0.0/0 base # openstack security group rule create --protocol ICMP --remote-ip 0.0.0.0/0 base  Create a private ssh key for connecting to instances remotely.  # nova keypair-add admin  Create admin.pem file and add private key from output of keypair-add command.  # vi /root/admin.pem # chmod 400 /root/admin.pem  List the network IDs.  # neutron net-list  +--------------------------------------+---------+-------------------------------------------------------+  | id | name | subnets |  +--------------------------------------+---------+-------------------------------------------------------+  | d4f3ed19-8be4-4d56-9f95-cfbac9fdf670 | private | 92d82f53-6e0b-4eef-b8b9-cae32cf40457 10.10.1.0/24     |  | 37c024d6-8108-468c-bc25-1748db7f5e8f | public  | 22f2e901-186f-4041-ad93-f7b5ccc30a81 192.168.122.0/24 |  Start an instance and make sure to replace network id from above command.  # nova boot --flavor m1.nano --image \"Cirros 0.3.4\" --nic net-id=92d82f53-6e0b-4eef-b8b9-cae32cf40457 --key-name admin --security-groups all mycirros # nova list +--------------------------------------+----------+--------+------------+-------------+---------------------+ | ID | Name | Status | Task State | Power State | Networks | +--------------------------------------+----------+--------+------------+-------------+---------------------+ | 18450c0f-3410-4b8a-a35e-2964242f33cb | mycirros | ACTIVE | - | Running | private=10.10.1.108 | +--------------------------------------+----------+--------+------------+-------------+---------------------+  Create a floating IP and assign it to the mycirros instance.  # nova floating-ip-create +--------------------------------------+---------------+-----------+----------+--------+ | Id | IP | Server Id | Fixed IP | Pool | +--------------------------------------+---------------+-----------+----------+--------+ | 4b71690d-d881-4f32-9587-89c814617c74 | 192.168.0.106 | - | - | public | +--------------------------------------+---------------+-----------+----------+--------+ # nova floating-ip-associate mycirros 192.168.0.106  Check the OpenStack router network namespace and you should also see floating ip  #ip netns exec qrouter-88dde0ef-22a2-44b1-baa9-304273653bb1 ip a 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1  link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00  inet 127.0.0.1/8 scope host lo  valid_lft forever preferred_lft forever  inet6 ::1/128 scope host   valid_lft forever preferred_lft forever 15: qg-7b3b000a-6f: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN qlen 1000  link/ether fa:16:3e:3c:e0:49 brd ff:ff:ff:ff:ff:ff  inet 192.168.0.101/24 brd 192.168.122.255 scope global qg-7b3b000a-6f  valid_lft forever preferred_lft forever  inet 192.168.0.106/32 brd 192.168.122.100 scope global qg-7b3b000a-6f  valid_lft forever preferred_lft forever  inet6 fe80::f816:3eff:fe3c:e049/64 scope link   valid_lft forever preferred_lft forever 16: qr-4fe396dd-2d: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN qlen 1000  link/ether fa:16:3e:97:1b:e6 brd ff:ff:ff:ff:ff:ff  inet 10.10.1.1/24 brd 10.10.1.255 scope global qr-4fe396dd-2d  valid_lft forever preferred_lft forever  inet6 fe80::f816:3eff:fe97:1be6/64 scope link   valid_lft forever preferred_lft forever  Connect to mycirros instance using the private ssh key stored in the admin.pem file. Note: The first floating IP in the range 192.168.122.201.  # ssh -i admin.pem cirros@192.168.0.106 $ ping google.com PING google.com (172.217.21.14): 56 data bytes 64 bytes from 172.217.21.14: seq=0 ttl=54 time=37.692 ms 64 bytes from 172.217.21.14: seq=1 ttl=54 time=27.758 ms 64 bytes from 172.217.21.14: seq=2 ttl=54 time=25.640 ms Nova Nested Virtualization  Most OpenStack lab or test environments will install OpenStack on a hypervisor platform inside virtual machines. I would strongly recommend KVM. If you are running OpenStack on KVM (Nova nested virtualization) make sure to follow these tips and tricks to get the best performance.  Summary  This article was intended as a hands on guide for standing up an OpenStack Ocata lab environment using RDO. As mentioned RDO is a stable community platform built around Red Hat’s OpenStack Platform. It provides the ability to test the latest OpenStack features against either an enterprise platform (RHEL) or community platform (CentOS). Hopefully you found the information in this article useful. If you have anything to add or feedback, feel free to leave your comments.  Happy OpenStacking!  (c) 2017 Keith Tenzer  ","categories": ["OpenStack"],
        "tags": ["Cloud","Ocata","OpenStack"],
        "url": "/openstack/openstack-11-ocata-lab-installation-and-configuration-guide/",
        "teaser": null
      },{
        "title": "OpenShift: Accessing External Services using Egress Router",
        "excerpt":"  Overview  Egress traffic is traffic going from OpenShift pods to external systems, outside of OpenShift. There are two main options for enabling egress traffic. Allow access to external systems from OpenShift physical node IPs or use egress router. In enterprise environments egress routers are often preferred. They allow granular access from a specific pod, group of pods or project to an external system or service. Access via node IP means all pods running on a given node can access external systems.    An egress router is a pod that has two interfaces (eth0) and (macvlan0). Eth0 is sitting on the cluster network in OpenShift (internal) and macvlan0 has an IP and gateway from the external physical network. The network team can allow access to external systems using the egress router IP. OpenShift administrators using project level access can assign pods access to the egress router service thus enabling them to access external services. The egress router acts as a bridge between pods and an external system. Traffic going out the egress router, goes via node but instead of having MAC address of node it will have MAC address of the macvlan0 interface inside the egress router.  Configuration  In this configuration we have deployed a simple OpenShift all-in-one environment running on a KVM hypervisor. We have also deployed a second VM running a web server. The KVM hypervisor has two virtual networks 192.168.122.0/24 and 192.168.123.0/24. OpenShift has two network interfaces, eth0 is on 192.168.122.0/24 and eth1 is on 192.168.123.0/24. The Web server has one interface, eth0 on 192.168.123.0/24. To test the egress router we will only allow access to the web server from the source IP of the egress router. Using another pod we will show how to access the web server using the egress router.    [Web Server]  Allow only the IP of the egress router (192.168.123.99) in OpenShift to access the web server on port 80.  # firewall-cmd --permanent --zone=public \\ --add-rich-rule='rule family=\"ipv4\" source address=\"192.168.123.99\" \\ port protocol=\"tcp\" port=\"80\" accept' # firewall-cmd --reload [OpenShift Master]  Create a new project  # oc new-project myproj Configure Security Context  Egress router in legacy mode will run as root so we need to allow root containers. To do this we update the security context.  # vi scc.yaml kind: SecurityContextConstraints apiVersion: v1 metadata:   name: scc-admin allowPrivilegedContainer: true runAsUser:   type: RunAsAny seLinuxContext:   type: RunAsAny fsGroup:   type: RunAsAny supplementalGroups:   type: RunAsAny users: - admin  Note: You can also add groups to the security context.  # oc create -f scc.yaml Deploy Egress router in legacy mode  # vi egress-router.yaml apiVersion: v1 kind: Pod metadata:   name: egress-1   labels:     name: egress-1   annotations:     pod.network.openshift.io/assign-macvlan: \"true\" spec:   containers:   - name: egress-router     image: openshift3/ose-egress-router     securityContext:       privileged: true     env:     - name: EGRESS_SOURCE        value: 192.168.123.99     - name: EGRESS_GATEWAY        value: 192.168.123.1     - name: EGRESS_DESTINATION        value: 192.168.123.91  # oc create -f egress-router.yaml Check Egress pod and ensure it can access web server  # oc get pods NAME READY STATUS RESTARTS AGE egress-1 1/1 Running 0 2h # oc rsh egress-1 Once connected to egress router you will notice it is running as root. This is the difference between legacy mode and init. For troubleshooting and testing it is easier to run in legacy mode and then switch to init mode when things are working.  sh-4.2# curl http://192.168.123.91 Hello World! My App deployed via Ansible V6.  Here we see we can access our web server.  Deploy Egress service  The egress service allows other pods to access external services using the egress router.  # vi egress-service.yaml apiVersion: v1 kind: Service metadata:   name: egress-1 spec:   ports:   - name: http     port: 80   type: ClusterIP   selector:     name: egress-1  # oc create -f efress-service.yaml Deploy Ruby hello world pod  The ruby example pod will be used to access our web server (192.168.123.91) via the egress router. Remember only source IP 192.168.123.99 (egress router) can access web server.  # oc new-app \\ centos/ruby-22-centos7~https://github.com/openshift/ruby-ex.git # oc get pods NAME READY STATUS RESTARTS AGE egress-1 1/1 Running 0 2h ruby-ex-1-wt3q9 1/1 Running 1 15h Here we now see the ruby example pod is running.  Get service for the egress router  # oc get service NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE egress-1 172.30.137.86  80/TCP 2h ruby-ex 172.30.205.126  8080/TCP 15h Check that web server pod cannot access web server directly  # oc rsh ruby-ex-1-wt3q9 sh-4.2$ curl http://192.168.123.91 curl: (7) Failed connect to 192.168.123.91:80; No route to host Check that web server pod can access web server using egress service  sh-4.2$ curl http://172.30.137.86 Hello World! My App deployed via Ansible V6.  sh-4.2$ curl http://egress-1  Hello World! My App deployed via Ansible V6. Configure Egress Router Init Mode  Once egress router is working it is recommend re-configuring using init mode. This ensures that the egress router is not running as root.  apiVersion: v1 kind: Pod metadata:   name: egress-1   labels:     name: egress-1   annotations:     pod.network.openshift.io/assign-macvlan: \"true\"  spec:   initContainers:   - name: egress-router     image: openshift3/ose-egress-router     securityContext:       privileged: true     env:     - name: EGRESS_SOURCE        value: 192.168.123.99     - name: EGRESS_GATEWAY        value: 192.168.123.1     - name: EGRESS_DESTINATION        value: 192.168.123.91     - name: EGRESS_ROUTER_MODE        value: init   containers:   - name: egress-router-wait     image: openshift3/ose-pod  Troubleshooting in Legacy Mode  In order to troubleshoot the egress router it is recommended to run in legacy mode so you have access to the IP space.  View Network Configuration  Below we can see that eth0 has a IP from pod network and macvlan0 has IP on our external network.  # oc rsh egress-1 sh-4.2# ip a 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1     link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00     inet 127.0.0.1/8 scope host lo        valid_lft forever preferred_lft forever     inet6 ::1/128 scope host         valid_lft forever preferred_lft forever 3: eth0@if26: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP      link/ether 0a:58:0a:80:00:51 brd ff:ff:ff:ff:ff:ff link-netnsid 0     inet 10.128.0.81/23 scope global eth0        valid_lft forever preferred_lft forever     inet6 fe80::858:aff:fe80:51/64 scope link         valid_lft forever preferred_lft forever 4: macvlan0@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN      link/ether b6:6d:62:ee:2e:bb brd ff:ff:ff:ff:ff:ff link-netnsid 0     inet 192.168.123.99/32 scope global macvlan0        valid_lft forever preferred_lft forever     inet6 fe80::b46d:62ff:feee:2ebb/64 scope link         valid_lft forever preferred_lft forever  Send ARP requests from egress router to gateway of external network  sh-4.2# arping -I macvlan0 -c 2 192.168.123.91 ARPING 192.168.123.91 from 192.168.123.99 macvlan0 Unicast reply from 192.168.123.91 [52:54:00:7F:0A:4A] 0.944ms Unicast reply from 192.168.123.91 [52:54:00:7F:0A:4A] 0.671ms Sent 2 probes (1 broadcast(s)) Received 2 response(s) View ARP table of OpenShift node  Notice both the egress router (192.168.123.99) and web server (192.168.123.91) show up in arp cache. The egress router is incomplete because the node cannot see the MAC address. This is important and reason you need to enable promiscuous mode if you are running OpenShift on top of a virtualization platform. Otherwise hypervisor will not recognize MAC address and simply drop the packets.  # arp Address                  HWtype  HWaddress           Flags Mask    Iface 10.128.0.71              ether   0a:58:0a:80:00:47   C             tun0 10.128.0.66              ether   0a:58:0a:80:00:42   C             tun0 10.128.0.74              ether   0a:58:0a:80:00:4a   C             tun0 10.128.0.69              ether   0a:58:0a:80:00:45   C             tun0 192.168.123.99                   (incomplete)                      eth1 10.128.0.81              ether   0a:58:0a:80:00:51   C             tun0 10.128.0.72              ether   0a:58:0a:80:00:48   C             tun0 10.128.0.67              ether   0a:58:0a:80:00:43   C             tun0 192.168.122.1            ether   52:54:00:18:40:b7   C             eth0 10.128.0.75              ether   0a:58:0a:80:00:4b   C             tun0 10.128.0.70              ether   0a:58:0a:80:00:46   C             tun0 192.168.123.1            ether   52:54:00:03:3f:fd   C             eth1 192.168.123.91           ether   52:54:00:7f:0a:4a   C             eth1 10.128.0.73              ether   0a:58:0a:80:00:49   C             tun0  Use tcpdump on OpenShift node to analyze packets  While running tcpdump connect to web server through egress router using curl.  # tcpdump -i eth1 -e 14:29:46.550889 b6:6d:62:ee:2e:bb (oui Unknown) &gt; 52:54:00:03:3f:fd (oui Unknown), ethertype IPv4 (0x0800), length 74: 192.168.123.99.55892 &gt; 192.168.123.91.http: Flags [S], seq 2654909605, win 28200, options [mss 1410,sackOK,TS val 11141299 ecr 0,nop,wscale 7], length 0 14:29:46.550991 52:54:00:03:3f:fd (oui Unknown) &gt; 52:54:00:7f:0a:4a (oui Unknown), ethertype IPv4 (0x0800), length 74: 192.168.123.99.55892 &gt; 192.168.123.91.http: Flags [S], seq 2654909605, win 28200, options [mss 1410,sackOK,TS val 11141299 ecr 0,nop,wscale 7], length 0 14:29:46.551107 52:54:00:7f:0a:4a (oui Unknown) &gt; b6:6d:62:ee:2e:bb (oui Unknown), ethertype IPv4 (0x0800), length 74: 192.168.123.91.http &gt; 192.168.123.99.55892: Flags [S.], seq 2459314841, ack 2654909606, win 28960, options [mss 1460,sackOK,TS val 10909669 ecr 11141299,nop,wscale 7], length 0  Notice we now see the MAC address of the egress router (b6:6d:62:ee:2e:bb). We can also see the egress router talking to the web server (192.168.123.91) through the gateway (192.168.123.1).  Troubleshooting in Init Mode  As mentioned for troubleshooting legacy mode is recommended but using init mode the egress router network namespace can still be accessed in order to identify potential problems.  Identify node running egress router  # oc get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE egress-1 1/1 Running 1 1d 10.128.0.98 ocp36.lab.com Get docker id for egress container  # docker ps |grep egress-1  49d42d963169 registry.access.redhat.com/openshift3/ose-egress-router@sha256:30f8aa01c90c9d83934c7597152930a9feff2fe121c04e09fcf478cc42e45d72 \"/bin/sh -c /bin/egre\" 4 minutes ago Up 4 minutes k8s_egress-router_egress-1_myproj_2e61d2c8-ac0c-11e7-994f-5254003fbd93_1  6ba0d22b286d openshift3/ose-pod:v3.6.173.0.21 \"/usr/bin/pod\" 6 minutes ago Up 6 minutes k8s_POD_egress-1_myproj_2e61d2c8-ac0c-11e7-994f-5254003fbd93_2 Inspect egress container and get pid  # docker inspect 49d42d963169 |grep Pid \"Pid\": 5675, \"PidMode\": \"\", \"PidsLimit\": 0, Enter egress container network namespace  # nsenter -n -t 5675 Show egress router network interfaces  Note: be careful with nsenter and track if you are in network namespace of container or not. To leave network namespace type 'exit'.  [root@ocp36 ~]# ip a 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1     link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00     inet 127.0.0.1/8 scope host lo        valid_lft forever preferred_lft forever     inet6 ::1/128 scope host         valid_lft forever preferred_lft forever 3: eth0@if16: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP      link/ether 0a:58:0a:80:00:62 brd ff:ff:ff:ff:ff:ff link-netnsid 0     inet 10.128.0.98/23 scope global eth0        valid_lft forever preferred_lft forever     inet6 fe80::1439:8bff:fe62:f208/64 scope link         valid_lft forever preferred_lft forever 4: macvlan0@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN      link/ether 5e:ee:12:f6:bb:4f brd ff:ff:ff:ff:ff:ff link-netnsid 0     inet 192.168.123.99/32 scope global macvlan0        valid_lft forever preferred_lft forever     inet6 fe80::5cee:12ff:fef6:bb4f/64 scope link         valid_lft forever preferred_lft forever  Ping gateway of egress router  # ping 192.168.123.1 PING 192.168.123.1 (192.168.123.1) 56(84) bytes of data. 64 bytes from 192.168.123.1: icmp_seq=1 ttl=64 time=0.154 ms 64 bytes from 192.168.123.1: icmp_seq=2 ttl=64 time=0.131 ms 64 bytes from 192.168.123.1: icmp_seq=3 ttl=64 time=0.119 ms Access application using curl  sh-4.2# curl http://192.168.123.91  Hello World! My App deployed via Ansible V6. Summary  In this article we discussed the importance of the egress router and how it can be used to allow granular access of external services. We configured an egress router in OpenShift to allow access to an external web server. Finally we looked at how to troubleshoot the egress router.  Happy OpenShifting!  (c) 2017 Keith Tenzer  ","categories": ["OpenShift"],
        "tags": ["Containers","egress","Kubernetes","networking","OpenShift"],
        "url": "/openshift/openshift-accessing-external-services-using-egress-router/",
        "teaser": null
      },{
        "title": "Ansible Getting Started Guide",
        "excerpt":"  Overview  Automation is one of the most critical areas of improvement in most organizations. Today, most companies are in the process or re-inventing themselves in one way or another to add software development capabilities and as such, take full advantage of the digitalization of everything. Software development release cycles are changing in order to release faster. Continuous delivery, where every change is potentially it's own release is becoming the new standard. Infrastructure is following suit, after all, continuous delivery is not about just software changes but all changes and infrastructure plays a key roll. For any of this to work of course, 100% automation is required. To achieve that goal, an automation language that is easy and applicable to development and operations is needed. Ansible is that language and if you are not on-board yet, now is your chance not to miss the train because it is leaving the station. Ansible is easy, Ansible is powerful and Ansible is flexible. This guide will show that and get you up and running with Ansible before your coffee gets cold.    In this article we will dive into Ansible and learn the basics to get up and running faster, creating playbooks the right way. This article will focus on those who are new to Ansible but it may also provide value to those that are already writing their own playbooks.  Pre-requisites   Several VMs or hosts, I recommend 5. One to be Ansible management host and other four managed nodes. RHEL or CentOS 7.3  Ansible Basics   Ansible Engine - The Ansible runtime and CLI which executes playbooks. The engine runs on a management node, from where you want to drive automation. Playbook - A series or group of tasks that automate a process. Playbooks are written in YAML. Inventory - Determines what hosts you want to run playbook against. Inventories can be dynamic or static. Configuration - The configuration for Ansible is stored in ansible.cfg by default under /etc/ansible/ansible.cfg. It determines default settings for Ansible, where the inventory is located and much more. You can have many ansible.cfg files. Role - Ansible way to build re-usable automation components. A role groups together playbooks, templates, variables and everything needed to automate a given process in a pre-packaged unit of work. Ansible Galaxy - Ansible roles provided by the Ansible community. Not only can you get access to 1000s of roles but you can share as well. Ansible Modules - Used in Ansible to perform tasks, install software, enable services, etc. Modules are provided with Ansible (core modules) and also provided by the community. They are mostly written in python which is used by Ansible itself. Ansible Tower - API and UI for Ansible to handle scheduling, reporting, visualization and support teamwork for organizations running many playbooks across many teams.  All exercises and examples should be carried out on the Ansible management node.  Install Ansible  In order to install Ansible you need minimum python 2.6 or greater.  Check Python version.  # yum list installed python Create Ansible user.  # useradd ansible -h /home/ansible # passwd ansible Install Ansible.  $ sudo yum install -y ansible Create Inventory file.  The inventory file stores managed nodes and is used to execute playbooks or commands against a group of hosts. You can have nested groups and add variables but let's just start with a simple inventory.  $ vi inventory  [servers] server1.lab.com server2.lab.com server3.lab.com server4.lab.com ADHOC Commands  Ansible can be used to run adhoc commands against an inventory of servers. We will use Ansible adhoc commands to create a user on our four managed nodes for Ansible that can sudo.  Create Ansible user on managed nodes.  $ ansible -m user -a 'name=ansible shell=/bin/bash generate_ssh_key=yes \\ state=present' -u root --ask-pass -i ./inventory servers The above command uses the Ansible user module to create a new user on the four hosts listed in our inventory group servers. It will use root as ssh user and prompt for root password.  Create a suders file to allow ansible sudo permissions.  $ ansible -m lineinfile -a \"dest=/etc/sudoers.d/ansible \\ line='ansible ALL=(ALL) NOPASSWD: ALL' create=yes state=present\" \\ -u root --ask-pass -i ./inventory servers The above command will use the lineinfile module to create the sudoers file for Ansible and add the line to allow Ansible user sudo permissions.  You may be interested in running other modules. Thankfully Ansible modules are wonderfully documented.  Generate list of all Ansible modules.  $ ansible-doc -l Read documentation for user module.  $ ansible-doc user Now we come to best practice #1. You might discover a module called command or shell. These modules can be used to execute any remote commands. While convenient, always try to find a purpose based module instead of using command or shell. Using specific modules is much cleaner, more readable and should always be the first choice.  Playbook Basics  Now that we understand how inventories work and how to run basic adhoc commands it is time to explore playbooks. In order to run a playbook you need Ansible, an inventory, a remote user and of course the playbook itself. A playbook simply put is just one or more tasks. Above we saw how to execute individual adhoc commands or tasks using our inventory servers. Now let us bring those tasks together in a playbook.  Create playbook.  $ vi add_user.yml --- - name: Add a privileged user for Ansible   hosts: servers    tasks:     - name: Add a user       user:         name: ansible         shell: /bin/bash         generate_ssh_keys: yes         state: present     - name: Configure sudo permission       lineinfile:         dest: /etc/sudoers.d/ansible         line: 'ansible ALL=(ALL) NOPASSWD: ALL'         create: yes         state: present &nbsp;  Now we come to best practice #2. The name parameter is optional, don't be lazy, always name the playbook and all tasks. This is important for reading what was done and also to let other's know what your playbook is supposed to be doing.  You can see how the inventory group is used and also how the tasks are invoked. Again a task just invokes and Ansible module with a set of parameters. Everything is documented in ansible-doc. With just what we have covered here, probably took you 10-15 minutes you can start automating everything with Ansible. Hopefully a light as gone on.  Run playbook.  Assuming you ran the adhoc commands a ansible user already exists that can sudo. If not you need to run the adhoc commands.  $ ansible-playbook add_user.yml -u ansible -b -i ./inventory We can use the newly created Ansible user and the -b option (become root) to elevate permissions.  Variables  Variables allow for dynamic input. In Ansible variables can be defined in many ways such as: playbook itself, inventory file via host or group vars, cli via extra_vars or in a separate vars file that can be included in playbook. Let's take a look at these options in more detail. Instead of hard-coding the username we created above let's explore doing so using the different variable options.  Vars via inventory file.  Variables can be defined for a host or group in the inventory file itself. Here we will define a variable username for the group servers.  Update inventory file and add vars for group servers.  $ vi inventory  [servers] server1.lab.com server2.lab.com server3.lab.com server4.lab.com  [servers:vars] username=ansible Update playbook to use variables.  $ vi add_user.yml --- - name: Add a privileged user for Ansible   hosts: servers    tasks:     - name: Add a user       user:         name: \"\"         shell: /bin/bash         generate_ssh_keys: yes         state: present     - name: Configure sudo permission       lineinfile:         dest: /etc/sudoers.d/         line: 'ansible ALL=(ALL) NOPASSWD: ALL'         create: yes         state: present  Notice that the variable  needs quotes only when it starts a line.  Run playbook.  $ ansible-playbook add_user.yml -u ansible -b -i ./inventory Variables in playbook  Variables can be defined directly in the playbook.  $ vi add_user.yml --- - name: Add a privileged user for Ansible   hosts: servers   vars:     username: ansible    tasks:     - name: Add a user       user:         name: \"\"         shell: /bin/bash         generate_ssh_keys: yes         state: present     - name: Configure sudo permission       lineinfile:         dest: /etc/sudoers.d/         line: 'ansible ALL=(ALL) NOPASSWD: ALL'         create: yes         state: present Run playbook.  $ ansible-playbook add_user.yml -u ansible -b -i ./inventory Variables imported from vars file.  Similar to above example, variables can be defined in a separate file and then imported into playbook.  Create vars file.  $ vi my_vars.yml --- username: ansible Update playbook.  $ vi add_user.yml --- - name: Add a privileged user for Ansible   hosts: servers   vars_files:     - ./my_vars.yml    tasks:     - name: Add a user       user:         name: \"\"         shell: /bin/bash         generate_ssh_keys: yes         state: present     - name: Configure sudo permission       lineinfile:         dest: /etc/sudoers.d/         line: 'ansible ALL=(ALL) NOPASSWD: ALL'         create: yes         state: present The best practice #3 is to use inventory file to define key/value type variables that should be parameterized. Variables that are dynamically generated or utilize nested data structures should use vars_files and be included. Avoid using vars in playbook directly if possible.  Run playbook.  $ ansible-playbook add_user.yml -u ansible -b -i ./inventory Ansible Facts  Each time Ansible is run unless disabled the setup module is also run. The setup module gathers Ansible facts. These are variables that give us valuable information about the managed host and they can be acted upon within playbooks.  Anything from a hosts network, hardware and OS information are gathered. It is also possible to define custom facts that would be gathered.  View ansible facts for a host.  $ ansible -m setup -u ansible -b -i ./inventory servera.lab.com Use ansible facts in playbook.  Here we will print the memory and number of cpu cores in our playbook by adding a new task.  $ vi add_user.yml ---     - name: Print Memory and CPU Cores       debug:         msg: \"Host  has  MB Memory and  CPU Cores.\" --- Run playbook.  $ ansible-playbook add_user.yml -u ansible -b -i ./inventory Controlling Playbooks  We have seen how to use modules to drive tasks and even parameterize them with variables. Next we will understand how to better control tasks within those playbooks.  When Conditional  This acts as an if statement in common programming languages. In Ansible we use when statement to run a task based on a condition being met.  Execute add user only when username is defined.  $ vi add_user.yaml ---     - name: Add a user       user:         name: \"\"         shell: /bin/bash         generate_ssh_keys: yes         state: present       when: username is defined --- Run playbook.  $ ansible-playbook add_user.yml -u ansible -b -i ./inventory Loops  In Ansible loops are very useful when the same task or module need to execute against a list. In our playbook let's update it to take action on many users and thus show how to use a loop.  Update vars file.  $ vi my_vars.yml --- users:   - ansible   - bob   - joe   - keith Update playbook.  $ vi add_user.yml --- - name: Add a privileged user for Ansible   hosts: servers   vars_files:     - ./my_vars.yml    tasks:     - name: Print Memory and CPU Cores        debug:          msg: \"Host  has  MB Memory and  CPU Cores.\"     - name: Add a user       user:         name: \"\"         shell: /bin/bash         generate_ssh_keys: yes         state: present       with_items: \"\"       when: item is defined     - name: Configure sudo permission       lineinfile:         dest: /etc/sudoers.d/         line: 'ansible ALL=(ALL) NOPASSWD: ALL'         create: yes         state: present       with_items: \"\"       when: item is defined Run playbook.  $ ansible-playbook add_user.yml -u ansible -b -i ./inventory Handlers  In order to couple tasks or have a task executed from another task, a handler is required. Here we will look at converting the \"configure sudo permission\" task into a handler that can be triggered by the \"add a user\" task.  Update playbook.  $ vi add_user.yml --- - name: Add a privileged user for Ansible   hosts: servers   vars_files:     - ./my_vars.yml    tasks:     - name: Print Memory and CPU Cores        debug:          msg: \"Host  has  MB Memory and  CPU Cores.\"     - name: Add a user       user:         name: \"\"         shell: /bin/bash         generate_ssh_keys: yes         state: present       with_items: \"\"       when: item is defined       notify:         - 'Configure sudo permission'    handlers:     - name: Configure sudo permission       lineinfile:         dest: /etc/sudoers.d/         line: 'ansible ALL=(ALL) NOPASSWD: ALL'         create: yes         state: present       with_items: \"\"       when: item is defined Now sudo permissions will only be set when a new user is actually added. It is very important to understand behavior with handlers, a notifer will only run when the task made a change.  Run playbook.  $ ansible-playbook add_user.yml -u ansible -b -i ./inventory Tags  It may be desired to only run certain tasks and to control which tasks get run from cli. This is done in Ansible via tags. Lets set a tag which will only run the task that prints memory and cpu info.  Update playbook.  $ vi add_user.yml --- - name: Add a privileged user for Ansible   hosts: servers   vars_files:     - ./my_vars.yml    tasks:     - name: Print Memory and CPU Cores        debug:          msg: \"Host  has  MB Memory and  CPU Cores.\"       tags:         - info     - name: Add a user       user:         name: \"\"         shell: /bin/bash         generate_ssh_keys: yes         state: present       with_items: \"\"       when: item is defined       notify:         - 'Configure sudo permission'    handlers:     - name: Configure sudo permission       lineinfile:         dest: /etc/sudoers.d/         line: 'ansible ALL=(ALL) NOPASSWD: ALL'         create: yes         state: present       with_items: \"\"       when: item is defined Run playbook using tag 'info'.  $ ansible-playbook add_user.yml -u ansible -b -i ./inventory --tags 'info' Blocks  In order to handle errors in Ansible blocks are often used. The block statement defines the main task to run. The rescue statement defines a task that should run if an error is encountered inside the block. Here we will setup error handling for our add user task.  Update playbook.  $ vi add_user.yml --- - name: Add a privileged user for Ansible   hosts: servers   vars_files:     - ./my_vars.yml    tasks:     - name: Print Memory and CPU Cores        debug:          msg: \"Host  has  MB Memory and  CPU Cores.\"       tags:         - info     - block:         - name: Add a user           user:             name: \"\"             shell: /bin/bash             generate_ssh_keys: yes             state: present           with_items: \"\"           when: item is defined           notify:             - 'Configure sudo permission'       rescue:         - name: revert user add           user:              name: \"\"              state: absent            with_items: \"\"            when: item is defined            handlers:     - name: Configure sudo permission       lineinfile:         dest: /etc/sudoers.d/         line: 'ansible ALL=(ALL) NOPASSWD: ALL'         create: yes         state: present       with_items: \"\"       when: item is defined The playbook will now attempt to remove users if an error occurs with the add user task. You can simulate this by adding a new user to my_vars.yml file and running without the -b (become root) option.  Run playbook.  $ ansible-playbook add_user.yml -u ansible -i ./inventory Templates  In Ansible templates are used mostly to parameterize configuration files. Ansible uses the jinja2 templating system. A template module is provided that defines the template and where it should be placed. Variables are automatically substituted for their values when the template is copied to its destination. We will add a new task to the playbook which sets the motd using hostname as a dynamic variable from a motd jinja2 template.  Create a jinja2 template for motd.  $ vi motd.j2 Ansible Rocks!!! This is host . Update playbook.  $ vi add_user.yml --- - name: Add a privileged user for Ansible   hosts: servers   vars_files:     - ./my_vars.yml    tasks:     - name: Configure MOTD using template       template:         src: ./motd.j2         dest: /etc/motd         owner: root         group: root         mode: 0644     - name: Print Memory and CPU Cores        debug:          msg: \"Host  has  MB Memory and  CPU Cores.\"       tags:         - info     - block:         - name: Add a user           user:             name: \"\"             shell: /bin/bash             generate_ssh_keys: yes             state: present           with_items: \"\"           when: item is defined           notify:             - 'Configure sudo permission'       rescue:         - name: revert user add           user:              name: \"\"              state: absent            with_items: \"\"            when: item is defined            handlers:     - name: Configure sudo permission       lineinfile:         dest: /etc/sudoers.d/         line: 'ansible ALL=(ALL) NOPASSWD: ALL'         create: yes         state: present       with_items: \"\"       when: item is defined Run playbook.  $ ansible-playbook add_user.yml -u ansible -b -i ./inventory The playbook will now update motd based on our jinja2 template. The hostname will be dynamically applied based on the variable.  Check MOTD.  $ ssh -l ansible server1 Ansible Rocks!!! This is host server1. Roles  Now that we have a good understanding about playbooks it is time to move on to roles. In Ansible roles allow Ansible to be reusable. The Ansible community provides galaxy which allows community members to share roles. There are 1000s of roles so often if you want to do something in Ansible, there is probably already a roll that exists in galaxy to do so. You can also easily modify roles to fit intended purpose. We now come to best practice #4, don't be lazy, always create roles. Not only will roles allow you to share your playbook packages with others, but will also enforce good structuring , clean code and allow you to re-use role components in ways that were not even considered. We will now restructure our playbook tasks, variables, handlers and templates into a add user role.  Create empty role structure.  The ansible-galaxy command allows lists, installs and removes roles from galaxy. It also generates the empty structure of a role, useful for creating your own roles. Here we will create the empty structure for our new add_user role.  $ ansible-galaxy init --offline -p roles add_user  Create role tasks.  Now we will move the tasks from our playbook into the role. A simply copy/paste and delete of the first four empty spaces of each line will suffice.  $ vi roles/add_user/tasks/main.yml --- - name: Configure MOTD using template   template:     src: ./motd.j2     dest: /etc/motd     owner: root     group: root     mode: 0644 - name: Print Memory and CPU Cores    debug:      msg: \"Host  has  MB Memory and  CPU Cores.\"   tags:     - info - block:     - name: Add a user       user:         name: \"\"         shell: /bin/bash         generate_ssh_keys: yes         state: present       with_items: \"\"       when: item is defined       notify:         - 'Configure sudo permission'   rescue:     - name: revert user add       user:          name: \"\"          state: absent        with_items: \"\"        when: item is defined Create role handlers.  Similar to tasks handlers also have a specific location within a role. Again copy/paste and delete of the first four spaces of each line.  $ vi roles/add_user/handlers/main.yml --- - name: Configure sudo permission   lineinfile:     dest: /etc/sudoers.d/     line: 'ansible ALL=(ALL) NOPASSWD: ALL'     create: yes     state: present   with_items: \"\"   when: item is defined Create role vars.  Variables for a role can be stored under vars or default. In this case we will put them under vars.  $ vi roles/add_users/vars/main.yml ---  users:    - ansible    - bob    - joe    - keith Create role templates.  Templates simply need to be copied to the correct location.  $ cp motd.j2 roles/add_user/templates Create playbook that uses role.  $ vi add_user_using_role.yml --- - name: Add user using role   hosts: servers   roles:     - add_user Run playbook.  $ ansible-playbook add_user_using_role.yml -u ansible -b -i ./inventory The playbook will now execute our role. As you have seen roles provide a pre-defined structuring and packaging of playbook tasks that greatly enhances re-usability.  Summary  In this article we have discussed the need and great opportunity automation provides. We have explored the basics of Ansible and why Ansible has become the standard language of automation. This article attempts to provide a pragmatic approach to learning Ansible, from installation, adhoc commands, creating your first playbook , learning fundamentals and finally putting it all together in a role. The greatest thing you can do for yourself and your organization is to automate everything.  Happy Automating!  (c) 2017 Keith Tenzer  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  ","categories": ["Ansible"],
        "tags": ["automaiton","galaxy","Playbook","roles","yaml"],
        "url": "/ansible/ansible-getting-started-guide/",
        "teaser": null
      },{
        "title": "Ansible Tower Cluster Configuration Guide",
        "excerpt":"  Overview  In this article we will setup and configure an Ansible Tower cluster on Red Hat Enterprise Linux (RHEL). If you are interested in a single all-in-one deployment, I have already documented this here.  Ansible Tower clustering replaces the traditional active/passive with an active/active configuration. It provides not only HA but scalability as well. Ansible Tower has two critical components: Tower instances running API/Scheduler and the database. RabbitMQ is used for communication between the Tower instances.    Tower Instances  Tower instances can be assigned to instance groups and those can be further assigned to resources (Organizations, Inventories and Job Templates). Jobs executed against a resource are assigned to instance group for execution. Isolated instance groups that overcome physical network boundaries are also possible. In this case a minimal Ansible configuration is used on isolated instances so that a Tower instance can execute playbooks in a network zone where there is no direct access to Ansible client nodes. Ansible Tower accesses isolated groups is via ssh.  Tower Database  Ansible Tower uses PostgreSQL for it's internal database. The options are to use a built-in database, installed via playbook during Tower installation or use external PostgreSQL database. The PostgreSQL database can be clustered in active/passive configuration. If this is desired then it is recommended to setup a standalone external PostgreSQL database.  If you are installing the database using the ansible playbook it will be installed in the default location, /var/lib/pgsql/. You need to make sure /var is backed by fast storage and also large enough.  Load Balancer  A load balancer is recommended. You can either terminate SSL at the load balancer and go http from there or pass SSL through load balancer to Tower instances.  Environment  In this configuration we will setup three Ansible Tower instances and a single Tower database using the built-in installer (playbook). The environment will sized to handle 300 forks (total number of parallel processes that can be run).  Tip: Sizing guidelines suggest 1vCPU per 10 forks and 4GB RAM per 100 forks.  Tower Instance Sizing: 10 vCPUs, 4GB RAM, 25GB Local Storage (&gt;= 750 IOPS)  Note: 10GB of 25GB Local Storage must be dedicated to /var  Tower Database Sizing: 10 vCPUs, 16GB RAM, 25GB Local Storage and 150+ Database Storage (&gt;= 1000 IOPS)  Note: for a demo environment you can optionally exclude database storage.    Prerequisites  Before starting you need to deploy four hosts or VM with RHEL 7.3 or 7.4. Once the hosts are configured and on the network, you can register them with subscription-manager and enable the required repositories. For Ansible Tower you need rhel-7-server-rpms, rhel-7-server-extras-rpms and EPEL.  Ports and instances used by Tower are as follows:   80, 443 (normal Tower ports) 22 (ssh) 5432 (database instance - if the database is installed on an external instance, needs to be opened to the tower instances)  Clustering/RabbitMQ ports:   4369, 25672 (ports specifically used by RabbitMQ to maintain a cluster, needs to be open between each instance) 15672 (if the RabbitMQ Management Interface is enabled, this port needs to be opened (optional))  [All Nodes]  # subscription-manager register # subscription-manager list --available # subscription-manager attach --pool=&lt;pool id&gt; # subscription-manager repos --disable=* # subscription-manager repos --enable=rhel-7-server-rpms \\ --enable=rhel-7-server-extras-rpms Install and configure EPEL repositories.  # yum install -y http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm Install Ansible.  # yum install -y ansible Install Ansible Tower  Optional [On towerdb.lab]  Create LVM Volume and Mountpoint for Database.  Postgres writes in 8k pages so we will set blocksize to 8k.  # pvcreate /dev/sdb1 # vgcreate vg_postgres /dev/sdb1 # lvcreate -L 150G vg_postgres -n postgres_disk1 # mkfs.xfs -d su=8k /dev/vg_postgres/psotgres_disk1 [On tower1.lab]  Create Invetory File.  We will use ansible to allow our user (ktenzer) to sudo without password. This is needed for the installer. You can also of course use root to install Tower.  # vi inventory_tower_hosts [servers] tower1.lab tower2.lab tower3.lab towerdb.lab Allow User Privileged Permissions.  # ansible -m lineinfile -a \"dest=/etc/sudoers.d/ktenzer \\ line='ktenzer ALL=(ALL) NOPASSWD: ALL' create=yes state=present\" -u root \\ --ask-pass -i ./inventory_tower_hosts servers Setup ssh keys.  We need to create ssh key and copy it to all nodes for user who will install Tower (in this case ktenzer).  # su - ktenzer ktenzer$ ssh-keygen ktenzer$ for p in tower1.lab tower2.lab tower3.lab towerdb.lab; \\ do ssh-copy-id $p; done &nbsp;  ktenzer$ for p in tower1.lab tower2.lab tower3.lab towerdb.lab; \\ do ssh $p; exit done Download Ansible Tower.  [ktenzer@tower1 ~]$ curl -O http://releases.ansible.com/ansible-tower/setup/ansible-tower-setup-latest.tar.gz Extract Tower tarball.  [ktenzer@tower1 ~]$ gunzip ansible-tower-setup-latest.tar.gz [ktenzer@tower1 ~]$ tar xvf ansible-tower-setup-latest.tar Configure Tower Installation Inventory File.  [ktenzer@tower1 ~]$ cd ansible-tower-setup-3.2.1/ [ktenzer@tower1 ~]$ vi inventory [tower] tower1.lab tower2.lab tower3.lab  [database] towerdb.lab  [all:vars] ansible_become=true  admin_password='redhat01'  pg_host='towerdb.lab' pg_port='5432'  pg_database='tower' pg_username='tower' pg_password='redhat01'  rabbitmq_port=5672 rabbitmq_vhost=tower rabbitmq_username=tower rabbitmq_password=redhat01 rabbitmq_cookie=rabbitmqcookie  # Needs to be true for fqdns and ip addresses rabbitmq_use_long_name=true Run Setup.  [ktenzer@tower1 ~]$ ./setup.sh PLAY RECAP ***************************************************************************************** tower1.lab : ok=118 changed=54 unreachable=0 failed=0 tower2.lab : ok=112 changed=52 unreachable=0 failed=0 tower3.lab : ok=112 changed=52 unreachable=0 failed=0 towerdb.lab : ok=46 changed=18 unreachable=0 failed=0 Configure Ansible Tower  Ansible Tower Provides a RESTful API, CLI and UI. To connect to the UI simply open browser using https and point to your Ansible Tower IP or hostname.  https://&lt;Ansible Tower IP or Hostname&gt; Login using the user you configured in the inventory file, in this case admin/redhat01.    Once you are logged in, you need to install Tower license. Tower license comes in a file, so simply browse to the file and accept the terms. If you don't have a license you can get a trial here.    Verify Tower instances under settings (next to the user, upper right).    Summary  In this article we explained a bit about Tower clustering and a general way to approach sizing. In addition we went through the easy installation steps to deploy a cluster with three Tower instances and a database.  Regardless of if you are new to Ansible or experienced, Tower is definitely worth looking at as it provides a solid management platform for Ansible, making Ansible even easier and more powerful!  Happy Ansibleling!  (c) 2017 Keith Tenzer  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  ","categories": ["Ansible"],
        "tags": ["Automation","Playbook","Tower","yaml"],
        "url": "/ansible/ansible-tower-cluster-configuration-guide/",
        "teaser": null
      },{
        "title": "Containers in Large IT Enterprises",
        "excerpt":"  Overview  As this will be the last article of 2017 I wanted to do something different and get away from my typical how-to guides (rest assured I will continue doing them in 2018). Over the past year, I have engaged in a lot of conversation with many large organizations looking to adopt or increase their container footprint. In this article I will share my thoughts on what I have learned from those discussions. We will discuss the impact of containers in large IT organizations. Understand the difference between container technology and container platform. Look into the integration points a container platform has into the existing IT landscape and finally discuss high-level architectural design ideas.  This article should serve as a good starting point for IT organizations trying to understand how to go about adopting container technology in their organization.    Impact of Containers in Large IT Organizations  Containers are of course just a technology, but it is a technology at the center of the DevOps movement. When talking about the impact of containers we also must discuss DevOps. Currently about 70% of development teams are using agile principles. However only about 30% of operations teams are using those same principles. This is unfortunately a major gap and a huge challenge for the enterprise. To understand things further, we need to take a closer look into how many IT organizations mange service.  A large majority of enterprise IT organizations are using ITIL (a framework for IT service management). One of the key processes in ITIL is of course change management. In many organizations change management is a painful process involving meetings, approvals and getting things done can at times, feel like pulling teeth. This is not the fault of ITIL or even IT but rather the application architectures of the past and how ITIL processes were implemented to deal with those monolithic dragons. Basically for the last 20 years, developers have been throwing code over the wall to their ops colleagues who in turn, had to deploy the code on infrastructure that differed in every way imaginable from that of where application testing occurred. The result was a lot of trial and error and certainly lots of frustration. ITIL however helped safeguard the process of change in such a chaotic environment so that a service could still meet its SLA.  Today developers are not only adopting agile but practices like continuous delivery, creating incredibly fast release cycles (change) and operations simply cannot keep up and even if they could a developer releases a new version in a day, where the ITIL change process (without complete automation) may take multiple days or even weeks. As such, how can a large IT organization move forward given this situation?  Well one view that got a lot of traction from Gartner (Bi-modal IT) is one option. Basically your current IT is for running stable, long running applications that don't change much where the current processes, structure and culture fit (mode 1). A new organization is created that adds capability for fast releasing, innovative, agile applications (mode 2). In fact this is what many organizations have chosen to do and why we now see entire new organizations in enterprises called *something* \"Digitalization\". Makes sense but is this the right approach?  I don't think so, let's be honest, it is very short sighted. We talk a lot about digital transformation, but how is creating a new organization a transformation? Yes it will produce some short-term results and provide a bi-modal capability (if you want to call i that) but at what cost? You now are running two independent IT organizations, that probably don't talk to each other, are completely isolated and services, resources, knowledge between these organizations aren't leveraged. Worse your new bi-modal organization isn't leveraging past 20 years of expertise in running IT. To put it another way, you are forking your IT organization.  Instead of building a new greenfield digital organization, why not take a long-term strategic approach? You won't be able to change everything overnight but at least you can start by creating a new group within your existing IT organization that is building applications using DevOps practices and providing a platform-as-a-service is an integral part to mode 2 service IT could offer.  Container Technology vs Container Platform  When discussing containers it is important to understand difference between technology and platform.  Technology simply applies an action to a human need. To make DevOps work we needed a way to package applications so the application version and its runtime stayed consistent, through the application lifecycle, across potentially varying infrastructure platforms. A platform is a group of technologies that is used as a base from which other applications, processes or additional technologies are developed. In other words a platform is an environment for providing business capabilities and needs that generate business value for an organization, hopefully resulting in additional revenue. Below is a chart that further compares technology to platform focusing on container technology vs container platform.           Capability Technology Platform    Container Packaging (Docker)       Container Orchestration (Kubernetes)      Container Runtime Environment (Operating System)      Centralized Logging      Monitoring &amp; Reporting      Security      Networking      Storage      Performance      Service Catalog      Registry (Container Images)     CI/CD Pipeline and Release Management      While it would seem obvious that if you are not Google, Amazon or Microsoft you would buy a container platform that provides the above capabilities, allowing resources to focus on delivering unique business value and not solving IT problems that have already been solved. Do-it-yourself (DIY) container platforms as such, should not be considered for large IT enterprises focused on a wide variety of containerized applications and use cases. So what options exist for large enterprises?   Container platform from cloud providers (Google, Amazon, Microsoft) Container platform from vendor (Red Hat and others)  The leaders are clearly Google and Red Hat and this is also validated by upstream activities.  If you are willing to have lock-in at infrastructure level, don't want to leverage your existing IT and don't need on-premise capabilities, cloud provider such as Google (GKE) is the best choice. Otherwise Red Hat and OpenShift are easily the best choice if a multi-cloud, including possibly on-premise is a requirement.  Considerations for Integrating Container Platform into the Existing Ecosystem  As discussed, the goal should not be to build a completely new organization that adopts DevOps, Agile principals and containerization but rather, add the capability to the existing organization. This allows the entire organization to benefit and not just the new cool digitalization organization or whatever it is called. Of course it is easier said than done, most organizations shy away because it is in fact hard work and arguably carries more risk. There is a natural  resistance to change, existing ITIL process and other barriers need to be changed to clear the path for containerization. A great place to start would be completing a DevOps maturity assessment. Such an assessment could provide valuable information on where to focus and how to move forward with IT transformation. If done right, I am convinced this approach could deliver results just as fast as the Gartner Bi-modal (multiple organizational) method while providing huge benefits for the long-term.  You have decided to embrace microservices, DevOps and containers. You have completed a DevOps assessment, what now? Well the next step is to find a true DevOps champion, someone that won't be discouraged, can lead, has a vision, will breakdown the barriers and deliver a minimum viable platform for enabling mode 2 applications. The champion needs to identify groups or projects within organization that already have a DevOps mindset, are potentially already building applications in direction of microservices and would be willing to take part in a new shared platform as a pilot project. This can best be done in a series of discovery workshops where time can be spent learning about the applications, processes and most important, the people.  Once the pilot projects or early adopters have been identified it is time to turn attention toward the platform. The nice part about leveraging existing IT is you have a wealth of knowledge about security, networking storage and processes for managing IT services. You also have strong knowledge in IT service management, which is very advantageous. Only the processes and cultural mindsets need to be tweaked in order to fit into DevOps and eventually containers into existing organization. Now on to the technology!  Registry  The container platform registry is probably the best place to start. The registry contains all container images. Since containers, at least Docker containers are immutable, every change (new release, library) results in new container image that has to be updated in the registry. The registry plays a key role in ITIL as part of the definitive media library (DML). The good news is that you don't need to dramatically change what you are doing in regards to DML. Container images that are built can be for example, added to existing DML automatically through a CI/CD pipeline using Jenkins or similar tooling. Images can be scanned and an automatic approvals can be generated based on well-defined criteria. From here applications packaged into containers can be rolled out into further lifecylce stages across the platform, again automatically. Another benefit, many IT organizations already understand what content needs to be checked and have processes and tools in place to do so.  Container Images  Container images is one area where I see a lot of confusion. Most organizations try to make either development or operations responsible for what is inside a container. Unfortunately this does not work. The container consists of the application runtime plus Base OS (operations) and the application itself (development). Both groups share equal responsibility. This concept of shared responsibility didn't really exist in the past, we always had a clear boundary between development and operations. This is one reason why containers are referred to as a DevOps enabling technology, it sort of forces the issue. Since Docker images are layered, operations can provide base images (OS runtime) and developers can consume those images and layer their application on top. This allows both groups to work independently but share responsibility, the cornerstone of DevOps.  Container Runtime Environment  A container platform requires a container host and an Operating System capable of running containers. Containers do not contain, meaning they do not enforce any security that would prevent a compromised container from accessing another on a given host. Due to nature of microservices and containers, we have a lot of them running on a host, maybe 100s. If we think about security, say an HR application and a customer facing application. Considering both running on same host, we could have an issue from security perspective. This is where choosing a rock solid enterprise Operating System that leverages SELinux and other such technologies is absolutely critical. Most containers that are coming from Docker Hub run as root and are not well maintained, meaning they come with security vulnerabilities. This is a recipe for disaster in the enterprise. A secure container operating system while critical, can only addresses part of the problem however. A platform around it is needed to handle lifecycle, maintenance, verification and scanning of container images that can also automate needed ITIL process. While containers should never run as root, it may be required. In general, containers running as root should be isolated from non-root containers through the container platform, if required on separate physical servers. Again here we need much more than just technology.  Networking  Container orchestration requires networking. Not only do physical container hosts need to be connected, an overlay is needed to connect the container network segment of the physical container host to the container network segment of another physical container host. In addition there are plenty of security concerns that must be addressed. Containers must be isolated from one another (East/West). Kubernetes provides a namespace but without a proper SDN it will not be possible to isolate traffic. In addition within a namespace isolation between containers may also be required. Egress and Ingress (North/South) traffic also needs to be isolated in the enterprise. The service layer of container platforms acts as a proxy (handling service discovery and service load balancing) but this means that if an external service or user has access to one container, it has access to potentially all. Similarly, if a container can access an external service, so can all the other containers. Outbound traffic goes out the physical interface of a container host with the IP of that interface. Finally there are physical network barriers such as DMZ vs internal, where traffic must be isolated across physical network boundaries.  Storage  Some container platforms don't offer data persistency at all. One of the major advantages of Kubernetes as a technology is the integration with storage through the storage provisioner. Storage can be provisioned by the developer dynamically and storage is also provisioned or de-provisioned on the storage system. This allows storage to work in a DevOps model as waiting for tickets would clearly create an unwanted barrier. Most of storage today can be used in context of a container platform. I am not saying it should be used, there are also new approaches to storage that need to be considered. The biggest consideration here is traditional storage vs local storage vs container native storage.  Architectural Design  Designing a enterprise container platform requires a lot of upfront planning, design and discovery workshops. It is critical to understand requirements as well as limitations from security, identity management, networking, storage and compute standpoint. A single point of contact should be identified in all critical infrastructure groups. In addition pilot projects and their requirements around CPU/MEM as well as performance expectations should be well documented.  Next we want to consider critical container services such as ETCD, Scheduler, API, UI/CLI, Logging, Metrics, Docker Registry, Router and Container Storage.  ETCD - Stores state of cluster and persists changes dynamically Scheduler - Distribute and redistribute container workloads dynamically API - Inter-cluster communication and end-user actions (API/CLI/UI) Logging - Central logging for all applications running on platform Metrics - Performance data for all applications running on the platform Docker Registry - Store docker images and source-to-image streams Router - Proxy traffic from application endpoint (URL) to service Container Storage - Provides storage for container persistence Masters  Services like etcd, scheduler and API need to access all nodes where containers are running. These services are control in nature. As such it makes sense to group them into a role called master. Since these services are all active/active having three masters would be most beneficial from availability and scalability perspective. For a really large container cluster it may also be an option to deploy etcd service on it's own nodes. It is also important to ensure all masters have low and predictable latency for inter-master communications, nothing higher than 100ms, ideally lower.  Infrastructure Nodes   Common services that applications running on the platform share should be provided by infrastructure nodes. Since these services need to be available and scalable as well as have their own performance considerations it makes sense to separate them on infrastructure nodes. Infrastructure nodes are just like normal application nodes but dedicated to providing shared services useful for applications. Typical services include logging (Elasticsearch, Fluentd, Kibana), metrics (Prometheus or Hawkular), Docker Registry (docker) and Router (HA-proxy). For infrastructure nodes a three node cluster is desired for logging and metrics. These services typically use mogodb or a distributed no-sql datastore that would benefit greatly from three nodes.  Additional infrastructure nodes may also be needed for segregated network zones. A router (ha-proxy) is typically required to proxy traffic from an application endpoint or URL to the service running in Kubernetes. This solution at least allows external URLs to be created dynamically without having to update load balancers or DNS manually. As such where application nodes exist a router or infrastructure nodes must also exist. Finally, moving infrastructure nodes into physical network zones also allows for traffic isolation. Applications within a given zone are only reachable through the infrastructure nodes assigned to that zone. In case of zone specific infrastructure nodes, two is recommended since only the routers are typically running and only HA is needed.  Application Nodes  Applications should be dedicated to their own nodes and not be run on masters or infrastructure nodes. Application nodes can run on bare-metal nodes, Virtual Machines or a mix. Looking at security it may also be important to isolate applications from one another on separate physical nodes. As discussed this may be required for root vs non-root applications or application types HR vs customer facing. Application nodes can be shared or dedicated, depending on the consumption models being offered.  Storage Nodes  Ideally storage would be provided by the application nodes. This would bring compute and storage closer together.  Application nodes could provide local disks (SSDs) and using a software-defined storage, those disks could be abstracted providing a layer of management that integrates with the container platform. Of course it is also possible to run dedicated storage nodes. The downside here is that a shared storage platform adds more complexity and moved data further from compute. Today there many possibilities but storage is still in its early days. Pay attention to the capabilities, not all storage systems are well integrated and very few offer required enterprise capabilities like growing existing volumes.  Load Balancers  In order handle external application traffic, content related capabilities, dynamic discovery, load balancers are required. There are various solutions both software and hardware based. The most important decision is likely where to do TLS termination. It can be done at load balancer, at the routing (proxy) layer or application. Termination beyond the load balancer usually requires SNI. Without SNI options are rather limited. The load balancer may also be required to support websockets depending on the UI or other features of container platform. A lot of time should be spent understanding load balancer requirements up front.  Finally lets look at an example architecture. For starters, orientating design toward network boundaries is certainly a good starting point. With something as dynamic as containers, quite a lot of thought is required to avoid a slow change process that would severely limit the overall potential.  Below is an example of a high-level design based on network boundaries and some of the concepts already discussed.    The design takes into account four physical network zones. DMZ production (Zone W), DMZ Test (Zone X), Internal production (Zone Y), Internal Test (Zone Z). There are strict physical firewalls between all zones. A management zone is created (Zone Infra) so that the masters can communicate to all nodes (required) and so shared infrastructure services are also available to all nodes. These ports must be opened through the firewall to W, X, Y, Z zones from the infra zone. In this example storage is only required in three zones (Zone Infra, Zone Y and Zone X). In this case due to segregation at network layer and application locality within zone it makes most sense to run container storage in each zone, where required. It would however also be possible to design a shared storage environment across the zones via a separate, isolated storage network but due to complexity, security and locality this would not be my recommendation in most scenarios.  Isolating Egress  When accessing external services, outside platform (a database), containers will route the traffic out the nodes physical interface. Packets will be sent with physical IP of node. This means if you open an external database up to the node physical IP, all containers running on given node can now also access the database.  This is not ideal. The main solution is to deploy an egress router (proxy). This is a special router type where a gateway IP can be configured that has access to external service (database). Containers are given access to egress router through labeling of service. Containers will have a second interface configured using macvtap which can access the egress router. Traffic is then routed to external service (database) over egress router instead of the node physical IP.  Isolating Ingress  Since typically routers are shared, any user that can access one application behind a router can access another. This may not always be desired. Either an external IP can be used directly in the container or an application can have it's own ingress router (proxy) where specific IPs can be defined. The ingress router solution is much more flexible and dynamic. Similar to egress router, this allows for normal ACLs to be configured in order to limit scope of access.  Summary  In this article we discussed some of the impacts containers have on large IT organizations. Containers go far beyond a packaging format for applications. Culture, process and people must be addressed when implementing containers. Containers are only a technology, a platform is needed to enable DevOps and take full advantage of containerization. Some enterprises choose to build a completely new organizations to enable DevOps and containerization but although this may be the fastest way to get going, it remains rather shortsighted. Embracing DevOps culture and container technology into existing organization through a platform that integrates with traditional IT produces the best long-term results. Architecting an enterprise container platform requires many upfront discovery workshops and well defined pilot projects. Identifying contacts across all infrastructure teams as well as bringing them into container conversation is a critical early success factor. I wish you best of luck on your journey to containerization and hope this article provided some help in getting started!  Happy Containerizing!  (c) 2017 Keith Tenzer  ","categories": ["Containers"],
        "tags": ["Docker","Kubernetes","Linux","OpenShift","PaaS"],
        "url": "/containers/containers-in-large-it-enterprises/",
        "teaser": null
      },{
        "title": "OpenStack 12 (Pike) Lab Installation and Configuration Guide with Hetzner Root Servers",
        "excerpt":"  Overview   In this article we will focus on installing and configuring OpenStack Pike using RDO and the packstack installer. RDO is a community platform around Red Hat’s Enterprise OpenStack Distribution. It allows you to test the latest OpenStack capabilities on a stable platform such as Red Hat Enterprise Linux (RHEL) or CentOS. This guide will take you through setting up Hetzner root server, preparing environment for OpenStack, installing the OpenStack Pike release, adding a floating ip subnet through OVS, configuring networking, security groups, flavors, images and are other OpenStack related services. The outcome is a working OpenStack environment based on the Pike release that you can use as a baseline for testing your applications using OpenStack capabilities. The installation will create an all-in-one deployment however you can use this guide to create a multi-node deployment as well.    Root Server Specs  CPU: Intel(R) Core(TM) i7-3930K CPU @ 3.20GHz (12 Cores) Memory: 64GB RAM Disk: 2 x 3TB SATA Network: 1Gbit IPV4 Adresses: 1 x IPV4 + /29 Subnet (6 IPs) Hetzner is a hosting providing and auctions off used hardware for very competitive prices on a per/month basis (https://www.hetzner.de/sb). Hetzner provides one IP for each root server that is accessible via internet. It is a /32 so if you want additional IPs, like you would for OpenStack to use as floating ips you need to order additional subnet. You can order an additional subnet as I have done. Hetzner will route that subnet through the host IP of the /32. This requires creating an additional OVS bridge in OpenStack and this guide will go through that configuration.  Configure Root Server  As mentioned, Hetzner will give you access to your root server via IP and provide ability to manage the root server. Basic things like put root server in rescue mode or reboot. You can provide public ssh key or use password to access system. Please don't use password, this is the internet, bad things can and will happen.  Enter Rescue Mode  In order to install OS or repair it you need to get into rescue mode.        Configure Hetzner Firewall.  While server is rebootting you can modify your servers firewall rules for incoming traffic. By default firewall allows port 22 for SSH and nothing else. Firewall settings can be modified under your server's settings in Hetzner's web UI https://robot.your-server.de/server. For OpenStack I would recommend 80, 443, 22 and ICMP.    If you plan on running OpenShift on OpenStack you need to add some additional rules.    Connect to root server via ssh and private key.  # ssh -i .ssh/id_rsa.pub Create Root Server Configuration.  The root server in this case has two disks. They are being configured in non-redundant RAID 1 (stripping) configuration. OpenStack requires decent disk performance, I found when using mirroring it was not enough. If you want redundant configuration you need to pay extra for SSDs. Since we will setup LVM partitions, creating one for Cinder (cinder-volumes) is a good idea. You don't want to use loop back driver for Cinder. Finally you can provide your own image (if you want to use RHEL, you must do so). In this case we will just use provided CentOS image.  [Hetzner Setup] # vi config.txt  DRIVE1 /dev/sda DRIVE2 /dev/sdb SWRAID 1 SWRAIDLEVEL 0 BOOTLOADER grub HOSTNAME myrootserver.lab PART /boot ext3 512M PART lvm vg0 500G PART lvm cinder-volumes all  LV vg0 root / ext4 100G LV vg0 swap swap swap 5G LV vg0 var /var ext4 10G LV vg0 tmp /tmp ext4 10G LV vg0 home /home ext4 40G  IMAGE /root/.oldroot/nfs/install/../images/CentOS-74-64-minimal.tar.gz Install image and create partitions.  # installimage -a -c config.txt Reboot. # reboot now  &nbsp;   Disable password authentication. Remember this system is accessible from the internet. Unless you want people to constantly try and login, disable password authentication. # vi /etc/ssh/sshd_config PasswordAuthentication no  Restart sshd. # systemctl restart sshd  Verify LVM Volume Groups  Make sure a volume group cinder-volumes exists. This will be used for OpenStack storage as mentioned.  # vgs  VG #PV #LV #SN Attr VSize VFree  cinder-volumes 1 0 0 wz--n- &lt;4.97t &lt;4.97t  vg0 1 5 0 wz--n- &lt;499.75g &lt;334.75g Install OpenStack  Ensure local name resolution is working.  # vi /etc/hosts 144.76.52.111 myrootserver.lab myrootserverF Set hostname.  # hostnamectl set-hostname myrootserver.lab Enable RPMs.  [RHEL]  # subscription-manager register # subscription-manager list --available # subscription-manager attach --pool= # subscription-manager repos --disable=* # subscription-manager repos --enable=rhel-7-server-rpms # subscription-manager repos --enable=rhel-7-server-rh-common-rpms # subscription-manager repos --enable=rhel-7-server-extras-rpms # subscription-manager repos --enable=rhel-7-server-openstack-12-rpms # subscription-manager repos --enable=rhel-7-server-openstack-12-devtools-rpms [CentOS]  # yum install -y centos-release-openstack-pike Disable firewalld, OpenStack uses iptables.  # systemctl disable firewalld # systemctl stop firewalld Disable NetworkManager.  # systemctl stop NetworkManager # systemctl disable NetworkManager Install packstack packages.  # yum install -y openstack-packstack Install yum-utils and update the system.  # yum install -y yum-utils # yum update -y Configure Intel Virtualization for Directed I/O.  # vi /etc/default/grub --- GRUB_CMDLINE_LINUX=\"biosdevname=0 crashkernel=auto nomodeset rd.auto=1 consoleblank=0 intel_iommu=on\" --- # grub2-mkconfig -o /boot/grub2/grub.cfg Reboot.  # systemctl reboot Create packstack answers file for customizing the installer.  # packstack --gen-answer-file /root/answers.txt Update the packstack answers file and enable other OpenStack services.  # vi /root/answers.txt --- CONFIG_NTP_SERVERS=0.pool.ntp.org,1.pool.ntp.org,2.pool.ntp.org CONFIG_CONTROLLER_HOST=192.168.0.40  CONFIG_COMPUTE_HOSTS=192.168.0.41,192.168.0.42 CONFIG_NETWORK_HOSTS=192.168.0.40 CONFIG_STORAGE_HOST=192.168.0.40 CONFIG_KEYSTONE_ADMIN_PW=redhat CONFIG_PROVISION_DEMO=n CONFIG_HEAT_INSTALL=y CONFIG_HEAT_CFN_INSTALL=y CONFIG_HEAT_CLOUDWATCH_INSTALL=y CONFIG_CEILOMETER_INSTALL=y CONFIG_LBAAS_INSTALL=y CONFIG_CINDER_VOLUMES_CREATE=n CONFIG_NOVA_SCHED_RAM_ALLOC_RATIO=3.0 CONFIG_NOVA_LIBVIRT_VIRT_TYPE=kvm CONFIG_HORIZON_SSL=n --- Note: Regarding HORIZON_SSL. If you enable SSL it won't work with chrome unless you create a self-signed cert with subjectAltNames, chrome now blocks common self-signed certs.  Install OpenStack using packstack.  # packstack --answer-file /root/answers.txt --timeout=600  **** Installation completed successfully ****** Additional information:  * File /root/keystonerc_admin has been created on OpenStack client host 144.76.52.111. To use the command line tools you need to source the file.  * To access the OpenStack Dashboard browse to http://144.76.52.111/dashboard . Please, find your login credentials stored in the keystonerc_admin in your home directory.  * The installation log file is available at: /var/tmp/packstack/20180111-182033-bm7yZM/openstack-setup.log  * The generated manifests are available at: /var/tmp/packstack/20180111-182033-bm7yZM/manifests Configure Physical Network  Source the keystone admin profile.  # . /root/keystonerc_admin Backup the ifcfg-etho script.  # cp /etc/sysconfig/network-scripts/ifcfg-eno1 /root/ Configure external bridge br-ex.  # vi /etc/sysconfig/network-scripts/ifcfg-eno1 DEVICE=eno1 ONBOOT=yes TYPE=OVSPort DEVICETYPE=ovs OVS_BRIDGE=br-ex # vi /etc/sysconfig/network-scripts/ifcfg-br-ex DEVICE=br-ex BOOTPROTO=none ONBOOT=yes TYPE=OVSBridge DEVICETYPE=ovs USERCTL=yes PEERDNS=yes IPV6INIT=no IPADDR=144.76.52.111 NETMASK=255.255.255.255 SCOPE=\"peer 144.76.56.97\" Switch static route to br-ex.  Note: this is specific to Hetzner environment as the physical host will get a /32.  # mv /etc/sysconfig/network-scripts/route-eno1 /etc/sysconfig/network-scripts/route-br-ex  Add the eno1 physical interface to the br-ex bridge in openVswitch.Note: normally you use br-ex as your floating ip network. However at Heztner this is a /32 (one ipv4 IP). As such since additional IPs are needed for instances a second subnet is needed.  # ovs-vsctl add-port br-ex eno1; systemctl restart network.service   Configure Additional Floating IP Subnet  Since hetzner root server only has one ip (/32), another subnet is needed to add additional floating ips. In this case hetzner will route traffic from additional subnet through to /32 ip of root server. Here we need to create a new OVS bridge (br-ex2) for additional subnet and patch it to existing bridge (br-ex).  Create Openvswitch Bridge.   # ovs-vsctl add-br br-ex2 Patch bridge br-ex2 to br-ex.  # ovs-vsctl add-port br-ex2 patch2-0 Note: ignore error about missing interface, that is expected since we creating patch.  # ovs-vsctl set interface patch2-0 type=patch # ovs-vsctl set interface patch2-0 options:peer=\"patch0-2\" # ovs-vsctl add-port br-ex patch0-2 Note: ignore error about missing interface, that is expected since we creating patch.  # ovs-vsctl set interface patch0-2 type=patch # ovs-vsctl set interface patch0-2 options:peer=\"patch2-0\" Update l3_agent config.  vi /etc/neutron/l3_agent.ini --- gateway_external_network_id = '' external_network_bridge = '' --- Update neutron bridge mappings.  We are adding a second subnet, as such a mapping is required to physical interface.  # vi /etc/neutron/plugins/ml2/openvswitch_agent.ini  bridge_mappings=extnet:br-ex,extnet2:br-ex2 Configure ifcfg script for br-ex2.   # vi /etc/sysconfig/network-scripts/ifcfg-br-ex2 DEVICE=br-ex2 BOOTPROTO=none ONBOOT=yes TYPE=OVSBridge DEVICETYPE=ovs USERCTL=yes PEERDNS=yes IPADDR=144.76.132.225 NETMASK=255.255.255.248 SCOPE=\"peer 144.76.56.97\" IPV6INIT=no   Comment out default iptables REJECT rules.  By default iptables won't allow traffic from br-ex2 to br-ex.  # vi /etc/sysconfig/iptables --- #-A INPUT -j REJECT --reject-with icmp-host-prohibited #-A FORWARD -j REJECT --reject-with icmp-host-prohibited --- Restart iptables and networking.  # systemctl restart iptables; systemctl restart network  Configure OpenStack Environment  Nova Configuration  Nova uses filtering rules to find appropriate host when scheduling instances. In order for nova to recognize lvm storage the images type needs to be lvm and the volume group needs to be correct lvm volume. In addition, unless you want to wait really long when deleting nova volumes, set volume_clear to none.  # vi /etc/nova/nova.conf  [libvirt] --- images_type = lvm  volume_clear = none images_volume_group = cinder-volumes --- Restart Nova services.  # systemctl restart openstack-nova-compute # systemctl restart openstack-nova-api # systemctl restart openstack-nova-scheduler Cinder Configuration  By default the openstack install will configure an lvm volume group using loop. This is not ideal and why we created a volume group vg1. We will use the same setting volume_clear to none to ensure cinder volumes are deleted quickly.  Update cinder configuration.  # vi /etc/cinder/cinder.conf --- enabled_backends=lvm volume_clear = none  [lvm] volume_backend_name=lvm volume_driver=cinder.volume.drivers.lvm.LVMVolumeDriver iscsi_ip_address=144.76.52.111 iscsi_helper=lioadm volume_group=cinder-volumes volumes_dir=/var/lib/cinder/volumes --- Restart cinder services.  # systemctl restart openstack-cinder-volume # systemctl restart openstack-cinder-api Ceilometer Configuration  Aodh is the database for alarms that are triggered based on things such as autoscaling policies. The database needs to be initialized after installing OpenStack.  # aodh-dbsync Neutron Configuration  Create private network.  # openstack network create private # openstack subnet create --network private --allocation-pool \\ start=10.10.1.100,end=10.10.1.200 --dns-nameserver 213.133.98.98 \\ --subnet-range 10.10.1.0/24 private_subnet Create public network.   Note: these steps assume the physical network connected to br-ex2 is 144.76.132.224/29.  # openstack network create --provider-network-type flat \\ --provider-physical-network extnet2 --external public # openstack subnet create --network public --allocation-pool \\ start=144.76.132.226,end=144.76.132.230 --no-dhcp \\ --subnet-range 144.76.132.224/29 public_subnet Add a new router and configure router interfaces.  # openstack router create --no-ha router1 # openstack router set --external-gateway public router1 # openstack router add subnet router1 private_subnet Check to ensure network connectivity is working.  This is done by checking the network namespace of the qrouter (openstack router).  # ip netns show qrouter-88dde0ef-22a2-44b1-baa9-304273653bb1 qdhcp-f1582f71-b531-43af-99c1-299b603232fc # ip netns exec qrouter-88dde0ef-22a2-44b1-baa9-304273653bb1 ping www.redhat.com Glance Configuration  Upload a glance image.   In this case we will use a Cirros image because it is small and thus good for testing OpenStack.  # curl -O http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img # openstack image create --disk-format qcow2 \\ --container-format bare --public \\ --file /root/cirros-0.3.4-x86_64-disk.img \"Cirros 0.3.4\" Create a new m1.nano flavor for running Cirros image.  # openstack flavor create --ram 64 --disk 0 --ephemeral 0 --vcpus 1 --public m1.nano Configure Security Groups  Create Security Group for all access.  # openstack security group create all \\ --description \"Allow all ports\" # openstack security group rule create --protocol TCP \\ --dst-port 1:65535 --remote-ip 0.0.0.0/0 all # openstack security group rule create --protocol ICMP \\ --remote-ip 0.0.0.0/0 all Create Security Group for base access.  # openstack security group create base \\ --description \"Allow base ports\" # openstack security group rule create --protocol TCP \\ --dst-port 22 --remote-ip 0.0.0.0/0 base # openstack security group rule create --protocol TCP \\ --dst-port 80 --remote-ip 0.0.0.0/0 base   # openstack security group rule create --protocol TCP \\ --dst-port 443 --remote-ip 0.0.0.0/0 base # openstack security group rule create --protocol ICMP \\ --remote-ip 0.0.0.0/0 base   Create Private Key.  # openstack keypair create admin Save Private Key to file.  # vi /root/admin.pem -----BEGIN RSA PRIVATE KEY----- MIIEowIBAAKCAQEAwTrb+xdbpgY8hVOmftBIShqYUgXXDC/1gggakq8bkEdNnSku IaNGeJykzksjdksjd9383iejkjsu92wiwsajFLuE2lkh5dvk9s6hpfE/3UvSGk6m HWIMCf3nJUv8gGCM/XElgwNXS02c8pHWUywBiaQZpOsvjCqFLGW0cNZLAQ+yzrh1 dIWddx/E1Ppto394ejfksjdksjdksdhgu4t39393eodNlVQxWzmK4vrLWNrfioOK uRxjxY6jnE3q/956ie69BXbbvrZYcs75YeSY7GjwyC5yjWw9qkiBcV1+P1Uqs1jG 1yV0Zvl5xlI1M4b97qw0bgpjTETL5+iuidFPVwIDAQABAoIBAF7rC95m1fVTQO15 buMCa1BDiilYhw+Mi3wJgQwnClIwRHb8IJYTf22F/QptrrBd0LZk/UHJhekINXot z0jJ+WvtxVAA0038jskdjskdjksjksjkiH9Mh39tAtt2XR2uz/M7XmLiBEKQaJVb gD2w8zxqqNIz3438783787387s8s787s8sIAkP3ZMAra1k7+rY1HfCYsRDWxhqTx R5FFwYueMIldlfPdGxwd8hLrqJnDY7SO85iFWv5Kf1ykyi3PRA6r2Vr/0PMkVsKV XfxhYPkAOb2hNKRDhkvZPmmxXu5wy8WkGeq+uTWRY3DoyciuC4xMS0NMd6Y20pfp x50AhJkCgYEA8M2OfUan1ghV3V8WsQ94vguHqe8jzLcV+1PV2iTwWFBZDZEQPokY JkMCAtHFvUlcJ49yAjrRH6O+EGT+niW8xIhZBiu6whOd4H0xDoQvaAAZyIFoSmbX 2WpS74Ms5YSzVip70hbcXb4goDhdW9YxvTVqJlFrsGNCEa3L4kr2qFMCgYEAzWy0 5cSHkCWaygeYhFc79xoTnPxKZH+QI32dAeud7oyZtZeZDRyjnm2fEtDCEn6RtFTH NlI3W6xFkXcp1u0wbmYJJVZdn1u9aRsLzVmfGwEWGHYEfZ+ZtQH+H9XHXsi1nPpr Uy7Msd,sl,.swdko393j495u4efdjkfjdkjfhflCgYEA7VO6Xo/XdKPMdJx2EdXM y4kzkPFHGElN2eE7gskjdksjdksjkasnw33a23433434wk0P8VCksQlBlojjRVyu GgjDrMhGjWamEA1y3vq6eka3Ip0f+0w26mnXCYYAJslNstu2I04yrBVptF846/1E ElXlo5RVjYeWIzRmIEZ/qU8CgYB91kOSJKuuX3rMm46QMyfmnLC7D8k6evH+66nM 238493ijsfkjalsdjcws9fheoihg80eWDSAFDOASDF=OIA=FSLoiidsiisiisNDo ACh40FeKsHDby3LK8OeM9NXmeCjYeoZYNGimHForiCCT+rIniiu2vy0Z/q+/t3cM BgmAmQKBgCwCTX5kbLEUcX5IE6Nzh+1n/lkvIqlblOG7v0Y9sxKVxx4R9uXi3dNK 6pbclskdksdjdk22k2jkj2kalksx2koUeLzwHuRUpMavRhoTLP0YsdbQrjgHIA+p kDNrgFz+JYKF2K08oe72x1083RtiEr8n71kjSA+5Ua1eNwGI6AVl -----END RSA PRIVATE KEY----- # chmod 400 /root/admin.pem Start an Instance  Get Private Network Id.  # openstack network list +--------------------------------------+---------+--------------------------------------+ | ID | Name | Subnets | +--------------------------------------+---------+--------------------------------------+ | 72494c88-6d93-4eb7-929f-383cbedfa3e7 | public | a57856be-a22b-4a48-b3c3-75be46a4c477 | | 781e062d-5ab6-4ae8-a54e-8e72291df37e | private | b00316a1-812f-423a-8aca-bd6547692ad3 | +--------------------------------------+---------+--------------------------------------+ Create a mycirros Instance.  # openstack server create --flavor m1.nano --image \"Cirros 0.3.4\" \\ --nic net-id=781e062d-5ab6-4ae8-a54e-8e72291df37e --key-name admin \\ --security-group all mycirros # # openstack server list +--------------------------------------+----------+--------+---------------------+--------------+---------+ | ID | Name | Status | Networks | Image | Flavor | +--------------------------------------+----------+--------+---------------------+--------------+---------+ | 6f2719bb-f393-49ab-a409-1c4a0f992b2d | mycirros | ACTIVE | private=10.10.1.105 | Cirros 0.3.4 | m1.nano | +--------------------------------------+----------+--------+---------------------+--------------+---------+ Create Floating IP.  # openstack floating ip create public +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | created_at | 2018-01-12T09:52:40Z | | description | | | fixed_ip_address | None | | floating_ip_address | 144.76.132.230 | | floating_network_id | 72494c88-6d93-4eb7-929f-383cbedfa3e7 | | id | 7a7ac84f-57ee-4982-b2f5-35ebb9656b34 | | name | 144.76.132.230 | | port_id | None | | project_id | 92d3bc57ca504eaab4c29d3509064004 | | revision_number | 0 | | router_id | None | | status | DOWN | | updated_at | 2018-01-12T09:52:40Z | +---------------------+--------------------------------------+  Get Port Id of Instance.  # openstack port list +--------------------------------------+------+-------------------+-------------------------------------------------------------------------------+--------+ | ID | Name | MAC Address | Fixed IP Addresses | Status | +--------------------------------------+------+-------------------+-------------------------------------------------------------------------------+--------+ | 60407eee-c9b2-4e9c-81a6-5c38bb536c9b | | fa:16:3e:37:ac:2f | ip_address='144.76.132.227', subnet_id='a57856be-a22b-4a48-b3c3-75be46a4c477' | ACTIVE | | 717660b6-b9b0-4f76-8c63-f9fae5c9bd9b | | fa:16:3e:0c:14:30 | ip_address='10.10.1.105', subnet_id='b00316a1-812f-423a-8aca-bd6547692ad3' | ACTIVE | | 878ee68a-1b88-48d1-9bf7-057f59b833bb | | fa:16:3e:2a:1f:50 | ip_address='144.76.132.230', subnet_id='a57856be-a22b-4a48-b3c3-75be46a4c477' | N/A | | 8a7b6b21-1eb2-4750-b854-707462d8b38f | | fa:16:3e:53:c9:ef | ip_address='10.10.1.1', subnet_id='b00316a1-812f-423a-8aca-bd6547692ad3' | ACTIVE | | b40af80e-2794-4ca5-8141-9d8ad4e9c9f2 | | fa:16:3e:51:d9:cb | ip_address='10.10.1.100', subnet_id='b00316a1-812f-423a-8aca-bd6547692ad3' | ACTIVE | +--------------------------------------+------+-------------------+-------------------------------------------------------------------------------+--------+ Assign Floating IP to Instance Port.  # openstack floating ip set --port 717660b6-b9b0-4f76-8c63-f9fae5c9bd9b 144.76.132.230  Verify Floating IP in OpenStack Router.  # ip netns show qdhcp-dcfbabbd-c5d2-444c-ab60-546216550118 router-0f00050f-6590-42df-9136-32d22fea4ece # ip netns exec qrouter-0f00050f-6590-42df-9136-32d22fea4ece ip a --- 9: qg-60407eee-c9: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN qlen 1000  link/ether fa:16:3e:37:ac:2f brd ff:ff:ff:ff:ff:ff  inet 144.76.132.227/29 brd 144.76.132.231 scope global qg-60407eee-c9  valid_lft forever preferred_lft forever  inet 144.76.132.230/32 brd 144.76.132.230 scope global qg-60407eee-c9  valid_lft forever preferred_lft forever  inet6 fe80::f816:3eff:fe37:ac2f/64 scope link  valid_lft forever preferred_lft forever --- Test Instance.  Connect to mycirros instance using the private ssh key stored in the admin.pem file.  # ssh -i admin.pem cirros@144.76.132.230 $ ping www.redhat.com PING www.redhat.com (172.217.21.14): 56 data bytes 64 bytes from 172.217.21.14: seq=0 ttl=54 time=37.692 ms 64 bytes from 172.217.21.14: seq=1 ttl=54 time=27.758 ms 64 bytes from 172.217.21.14: seq=2 ttl=54 time=25.640 ms Install OpenStack Client  To administer OpenStack remotely the CLI tools are important. Below are steps to install OpenStack CLI tools for Pike.  [RHEL]  # subscription-manager repos --enable=rhel-7-server-openstack-12-tools-rpms # yum install -y python-openstackclient # yum install -y python-heatclient [CentOs]  # yum install -y https://rdoproject.org/repos/openstack-pike/rdo-release-pike-1.noarch.rpm # yum upgrade -y # yum install -y python-openstackclient # yum install -y python-heatclient Summary  This article was intended as a hands on guide for standing up an OpenStack Pike lab environment using RDO. In this guide we also tailored the environment to the Hetzner root server. Things can of course vary depending on your hardware platform and provider. Certainly the aim of this guide was to provide a more realistic deployment scenario for OpenStack. As mentioned RDO is a stable community platform built around Red Hat’s OpenStack Platform. It provides the ability to test the latest OpenStack features against either an enterprise platform (RHEL) or community platform (CentOS). Hopefully you found the information in this article useful. If you have anything to add or feedback, feel free to leave your comments.  Happy OpenStacking!  (c) 2018 Keith Tenzer    ","categories": ["OpenStack"],
        "tags": ["Cloud","KVM","Linux","OpenStack","Pike","RDO"],
        "url": "/openstack/openstack-12-pike-lab-installation-and-configuration-guide-with-hetzner-root-servers/",
        "teaser": null
      },{
        "title": "Security and Vulnerability Scanning of Container Images",
        "excerpt":"  Overview  In this article we will focus on security and vulnerability strategies for scanning container images. I know, in the past security was always viewed upon as an impedance to the speed of production but hopefully these days are behind us. Having a security breach, as you probably know, is one of the most costly things an organization can endure. It takes years to build up a reputation and only seconds to tear it down completely.  I still see today, many organizations ignoring container images completely because it is often misunderstood. Exactly what is inside a container image? Who should be responsible for it? How does it map to what we have done on servers? Security teams often don't understand containers or even know what questions to ask. We need to help them and it is our duty to do so. Unfortunately there are not very many tools that can help in broad sense. Containers are new and evolving at breakneck speed. That coupled with the fact that security can negatively impact the speed of a DevOps team (if not done right), it is no wonder we are at square one, in many cases.  Before we dive into more detail, let us review important security aspects of containers.   Containers can have various packaging formats, Docker is the most popular today Containers are immutable and as such are image based Container are never updated, any change always results in a new container Container images consist of layers (base, runtime, application) Container images require shared responsibility between dev and ops Containers don't contain, they are in fact, just processes  For more information I recommend reading about the 10 layers of container security.    Container images contain OS, runtime and application (everything but the Linux kernel basically). If a container is breached, there is not much in the way preventing further containers from being breached unless you have SELinux, limit kernel capabilities, run as non-priviledged user and follow many other best practices. OpenShift and using trusted, signed images by a vendor like Red Hat does all of this by default. Even still the attack surface can be large. You still think it is OK to not worry about container images and what is inside? Thankfully there are at least several solutions we will talk about from Red Hat for the container Platform OpenShift.   Container Catalog grades images provided by Red Hat and provides history of image as security patches are applied. Atomic CLI scans images and uses OpenSCAP to determine security vulnerabilities. CloudForms scans images using OpenSCAP (same as atomic) and also adds capabilities like taking action when container images are vulnerable, automatically scanning new images and even reporting.  All of these solutions are included with OpenShift.  Container Catalog  Red Hat offers a container catalog for all images that it provides. Images are maintained by Red Hat meaning they are frequently scanned and updated. The container catalog also provides the history of an image, each time it is updated a new tag is created. The image is graded according to a scale A to F.        Health Index Security Errata Conditions   Grade A  This image does not have any unapplied Critical or Important security errata   Grade B  This image is affected by Critical (no older than 7 days) or Important (no older than 30 days) security errata   Grade C  This image is affected by Critical (no older than 30 days) or Important (no older than 90 days) security errata   Grade D  This image is affected by Critical (no older than 90 days) or Important (no older than 12 months) security errata   Grade E  This image is affected by Critical or Important security errata no older than 12 months   Grade F  This image is affected by Critical or Important security errata older than 12 months   Unknown This image is missing metadata required to calculate a grade and cannot be scanned    Check image (example nodejs-6).  https://access.redhat.com/containers/?tab=overview&amp;platform=openshift#/registry.access.redhat.com/rhscl/nodejs-6-rhel7    We see the image has a health index of A and is also signed. We also see it runs as unprivileged user.  View Image History  Under tags, the history of the image can be viewed.    Image Details  By clicking on the tag name we are able to get more detail about a given image.    Any CVEs that affect the image are shown in addition to RPM advisory.  Atomic CLI Image Scanning  Atomic Cli provides image scanning using OpenSCAP. This tool can be used via Ansible or other automation in CI/CD pipeline to automatically scan images.  Install Atomic cli  If you aren't running RHEL Atomic you need to install the atomic cli. In this case we are doing so on the OpenShift master (master0) but it can be any node that has access to the container images. If image doesn't exist locally (in this case on master0) it must be pulled down.  [master0 ~]$ yum install -y atomic Get the image that should be scanned  [master0 ~]$ oc get image |grep nodejs sha256:947bf778fd9ee31a4cd06a702954aa0cd91e1eb81fef366a9080c0cfe3e35e43 172.30.187.230:5000/mynodejs/test@sha256:947bf778fd9ee31a4cd06a702954aa0cd91e1eb81fef366a9080c0cfe3e35e43 sha256:a9b89bb53fef405ea73f3eaff2dafa0c37c2cc988586b1a8a0e3bc19de07d4b8 registry.access.redhat.com/rhscl/nodejs-6-rhel7@sha256:a9b89bb53fef405ea73f3eaff2dafa0c37c2cc988586b1a8a0e3bc19de07d4b8 sha256:b6fee5146e6330e9890f8290746944ab1452f709f806588fbfb1ff5cadb5aaed registry.access.redhat.com/rhscl/nodejs-4-rhel7@sha256:b6fee5146e6330e9890f8290746944ab1452f709f806588fbfb1ff5cadb5aaed sha256:c17fa1ffa8e4acf5e66e10127aea2ae5ca3eda7c34aecc005aaf3b04da48814c 172.30.187.230:5000/mynodejs/nodejs-hello-world-v2@sha256:c17fa1ffa8e4acf5e66e10127aea2ae5ca3eda7c34aecc005aaf3b04da48814c   Scan image.  Ensure openscap image is installed.  [master0 ~]$ sudo atomic install registry.access.redhat.com/rhel7/openscap Using the image location we perform a scan.  [master0 ~]$ sudo atomic scan registry.access.redhat.com/rhscl/nodejs-6-rhel7@sha256:a9b89bb53fef405ea73f3eaff2dafa0c37c2cc988586b1a8a0e3bc19de07d4b8  registry.access.redhat.com/rhscl/nodejs-6-rhel7@sha256:a9b89bb53fef405ea73f3eaff2dafa0c37c2cc988586b1a8a0e3bc19de07d4b8 (ae9be2ffb565659)  The following issues were found:  RHSA-2018:0260: systemd security update (Moderate)  Severity: Moderate  RHSA URL: https://access.redhat.com/errata/RHSA-2018:0260  RHSA ID: RHSA-2018:0260-01  Associated CVEs:  CVE ID: CVE-2018-1049  CVE URL: https://access.redhat.com/security/cve/CVE-2018-1049  RHSA-2018:0180: kernel-alt security and bug fix update (Important)  Severity: Important  RHSA URL: https://access.redhat.com/errata/RHSA-2018:0180  RHSA ID: RHSA-2018:0180-01  Associated CVEs:  CVE ID: CVE-2017-1000405  CVE URL: https://access.redhat.com/security/cve/CVE-2017-1000405  RHSA-2017:0372: kernel-aarch64 security and bug fix update (Important)  Severity: Important  RHSA URL: https://access.redhat.com/errata/RHSA-2017:0372  RHSA ID: RHSA-2017:0372-01  Associated CVEs:  CVE ID: CVE-2016-5195  CVE URL: https://access.redhat.com/security/cve/CVE-2016-5195  CVE ID: CVE-2016-7039  CVE URL: https://access.redhat.com/security/cve/CVE-2016-7039  CVE ID: CVE-2016-8666  CVE URL: https://access.redhat.com/security/cve/CVE-2016-8666 Here we notice several important or high severity CVEs and a medium.  Using atomic cli may be enough if images going into registry are tightly controlled. This could work well if additional tooling such as Artifactory is used to persist container images. They can easily be scanned before being added to Artifactory as part of CI/CD process.  CloudForms Image Scanning  CloudForms provides additional capabilities for security and vulnerability scanning. You can configure policies to take action based on an vulnerable image. For example, not allowing vulnerable images to run or notifying security team. In addition, as soon as an image is created and pushed to OpenShift registry via CI/CD, it can be scanned. Reporting allows for understanding impact of vulnerable images across projects in OpenShift. Image users can be easily notified and it allows roles between development and operations to have clear delineation.  Configure CloudForms Provider for OpenShift  The first step is to configure CloudForms provider to access OpenShift environment.  Get management-admin token for management-infra project.  [master0 ~]$ oc get sa management-admin -o yaml -n management-infra apiVersion: v1 imagePullSecrets: - name: management-admin-dockercfg-4dfcr kind: ServiceAccount metadata:  creationTimestamp: 2018-02-09T21:01:32Z  name: management-admin  namespace: management-infra  resourceVersion: \"1092\"  selfLink: /api/v1/namespaces/management-infra/serviceaccounts/management-admin  uid: 6860deab-0ddc-11e8-ab6a-fa163e0b5deb secrets: - name: management-admin-token-q9clk - name: management-admin-dockercfg-4dfcr Get Secret for management-admin token.  [master0 ~]$ oc describe secret management-admin-token-q9clk -n management-infra Name: management-admin-token-q9clk Namespace: management-infra Labels:  Annotations: kubernetes.io/service-account.name=management-admin  kubernetes.io/service-account.uid=6860deab-0ddc-11e8-ab6a-fa163e0b5deb  Type: kubernetes.io/service-account-token  Data ==== token: eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJtYW5hZ2VtZW50LWluZnJhIiwia3ViZXJuZXRlcy32ay9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6Im1hbmFnZW1lbnQtYWRtaW4tdG9rZW4tcTljbGsiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYW55ddssaVudC5uYW1lIjoibWFuYWdlbWVudC1hZG1pbiIsImt1YmVybmV0ZXMuaW ca.crt: 1070 bytes namespace: 16 bytes service-ca.crt: 2186 bytes Add OpenShift Provider to CloudForms.  OpenShift by default creates a management-infra project with a management-admin SA and token. This project is used by CloudForms for access and image scanning.      Set Node Selector for management-infra project.  In this example we will set the region to infra. This means any nodes with region infra will run image scanning container. Infra nodes also generally run other shared platform services like router, registry, metrics and logging.  [master0 ~]$ oc edit namespace management-infra --- metadata:  annotations:    openshift.io/node-selector: region=infra --- Configure Security Scanning Policies  CloudForms ships with compliance and control policies that will automatically scan containers. If vulnerabilities are detected, containers with those vulnerabilities will be prevented from running. This is not enabled by default but I find it a bit aggressive and recommend scanning images and reporting them as non-compliant instead.  Login to CloudForms and navigate to control-&gt;explorer.  Copy OpenScap Compliance Policy.    Edit the copied OpenScap compliance policy.  Here we will remove the action to prevent container images that are non-compliant from running.    Add new container image condition for sti builder.    Under conditions accordion, create a new condition. We will want to ignore scanning the sti builder.    Add new container image condition for deployer.  Under conditions accordion, create a new condition. We will want to ignore scanning the deployer.    Copy control policy Analyse incoming container images and edit condition assignments.  Add the two newly created conditions.    Add new profile policy.  Under \"all profile policies\", add a new profile policy. Add the following policies:   Copy of OpenScap (compliance) Copy of Analyze incoming container images (control) Schedule compliance after smart state analysis (control)    Enable policy profile on OpenShift provider.  Navigate to Compute-&gt;Containers-&gt;Providers.    Our policy profile will ensure the following:   All images that change are scanned immediately The deployer, STI builder and image inspector are ignored Images with High vulnerability are marked as non-compliant  Perform Container Image Scanning  Each build creates a new image. As soon as build is pushed the image is automatically scanned. In this case a change was made which kicked off CI/CD, resulting in a build and a new image being pushed to registry.      Check pods under management-infra project.  Each image will trigger a scan. The image scanner container will mount the image and scan it using openscap.  [master0 ~]$ oc get pods -o wide -n management-infra NAME READY STATUS RESTARTS AGE IP NODE manageiq-img-scan-ea955 0/1 Running 0 2m 10.30.1.54 infra0 Check image scanner container logs.  [master0 ~]$ oc logs manageiq-img-scan-ea955 -n management-infra 2018/02/15 16:26:56 Pulling image 172.30.187.230:5000/mynodejs/nodejs-hello-world-v2@sha256:e5dba582855f9de07d9a00b2f2d0986b41d229112626593a4a6ba50ff53bbf49 2018/02/15 16:26:56 Authentication with Default Empty Authentication failed: unauthorized: authentication required 2018/02/15 16:27:07 Downloading Image (17Kb downloaded) 2018/02/15 16:27:09 Finished Downloading Image (17Kb downloaded) 2018/02/15 16:27:24 Extracting image 172.30.187.230:5000/mynodejs/nodejs-hello-world-v2@sha256:e5dba582855f9de07d9a00b2f2d0986b41d229112626593a4a6ba50ff53bbf49 to /var/tmp/image-inspector-545571508 2018/02/15 16:32:49 OpenSCAP scanning /var/tmp/image-inspector-545571508. Placing results in /var/tmp/image-inspector-scan-results-428093571 2018/02/15 16:33:43 Serving image content /var/tmp/image-inspector-545571508 on webdav://0.0.0.0:8080/api/v1/content/ Find image.  Once the image scanner runs we can navigate to the image in CloudForms under Compute-&gt;Containers-&gt;Container Images. We can use the search field to filter and find image by it's name. In this case it is of course the latest image.    View Image Details.  Here we can see that smart state analysis in CloudForms (container image scan) was run. Notice compliance is not-compliant. Two High severity and a medium severity rule failed. This is exactly what we also saw when running atomic image scan. Finally notice in addition to the OpenScap results, we also have inventory of all the packages and corresponding package versions, installed in the image.    Container Security and Vulnerability Reporting  Now that we are able to scan images and flag ones that have high security vulnerabilities, it is time to look into reporting. You could easily have 1000s of images so reporting becomes increasingly important to identify projects using high vulnerability images. This allows us to nicely segregate roles and responsibilities. The platform team can scan images and notify devops teams about vulnerabilities, who can in turn fix them. There are of course other models, just an idea.  Create Container Image Vulnerability Report.  Under CloudIntel-&gt;Reports-&gt;Custom add a new report.  Select Fields.    Configure Filters.  There are two types of filters: primary and secondary. Primary is used when doing select on database while secondary filters after records are returned from database. Primary filter we will set to last compliance failed. This will find only images that failed compliance check, in this case ones that have a high severity vulnerability. Secondary filter we will set to display only rules that have failed, are high severity and only in projects that have images.    Run Report.  Under Reports-&gt;Custom select the report \"OpenScap High Security Vulnerabilities\" and select queue from configuration dropdown. The report will show projects that have images with a high severity rule that failed.    Once projects and images are identified more detail may be obtained by looking at the OpenScap report. Under Compute-&gt;Containers-&gt;Container Images you can find the image and open it for more details. The OpenScap html report will contain details on specific rules and security violations.    Clicking the OpenScap HTML report will download it via browser. The report shows all rules and if they passed or failed.    You can dril into the rule and get more information. The summary shows the relevant CVEs.    Violations that caused the rule to fail are also shown.    Scanning Applications Inside Container Images  Until now we have been focused on mainly scanning the base OS image every container is built on. It is also possible and there are tools to allow scanning of layers above the base OS.  Red Hat provides pluggable API in Red Hat Enterprise Linux to support multiple scanners such as OpenSCAP, Aqua Security, Black Duck Hub, JFrog Xray and Twistlock. Red Hat OpenShift gives you the ability to use scanners with continuous integration and delivery (CI/CD) tools.  A best practice for application security is to integrate automated security testing directly into build or CI/CD tooling.   Integrate with Static Application Security Testing (SAST) and Dynamic Application Security Testing (DAST) tools like HP Fortify and IBM AppScan. Add Scanners for real-time checking against known vulnerabilities like Aqua Security, Black Duck, JFrog and Twistlock. Tools like these catalog open source packages in your container, notify you of any known vulnerabilities and update when new vulnerabilities are discovered in previously scanned packages. CI/CD process should include policies that automatically open issues when security vulnerabilities are discovered in the build process so the devops team can take immediate action to resolve problems.  Finally there is an effort underway by Red Hat, Google and others to standardize auditing and policy enforcement with Kubernetes.  Summary  In this article we looked into the topic of security and vulnerability scanning of container images. Unfortunately this is a topic that doesn't get nearly enough attention in my view. We discussed the importance of why you want to keep container images updated, signed and get them from only trusted sources. We looked into several solutions provided by Red Hat. The Container Catalog, Atomic Cli and CloudForms. A guide was provided to explore each of these solutions individually, not only to understand their value, but also how to use them. In the end I think all three tools provide valuable information to ensure security standards are upheld. Choosing one of them or all of them depends on the processes an organization has defined.  Happy Secure Containers!  (c) 2018 Keith Tenzer  ","categories": ["CloudForms"],
        "tags": ["Containers","OpenShift","Security"],
        "url": "/cloudforms/security-and-vulnerability-scanning-of-container-images/",
        "teaser": null
      },{
        "title": "OpenShift on OpenStack 1-2-3: Bringing IaaS and PaaS Together",
        "excerpt":"  Overview  In this article we will explore why you should consider tackling IaaS and PaaS together. Many organizations gave up on OpenStack during it's hype phase, but in my view it is time to reconsider the IaaS strategy. Two main factors are really pushing a re-emergence of interest in OpenStack and that is containers and cloud.  Containers require very flexible, software-defined infrastructure and are changing the application landscape fast. Remember when we had the discussions about pets vs cattle? The issue with OpenStack during it's hype phase was that the workloads simply didn't exist within most organizations, but now containers are changing that, from a platform perspective. Containers need to be orchestrated and the industry has settled in on Kubernetes for that purpose. In order to run Kubernetes you need quite a lot of flexibility at scale on the infrastructure level. You must be able to provide solid Software Defined Networking, Compute, Storage, Load Balancing, DNS, Authentication, Orchestration, basically everything and do so at a click of the button. Yeah we can all do that, right.  If we think about IT, there are two types of personas. Those that feel IT is generic, 80% is good enough and for them, it is a light switch: on or off. This persona has no reason whatsoever to deal with IaaS and should just go to the public cloud, if not already there. In other words, OpenStack makes no sense. The other persona feel IT adds compelling value to their business and going beyond 80% provides them with distinct business advantages. Anyone can go to public cloud but if you can turn IT into a competitive advantage then there may actually be a purpose for it. Unfortunately with the way many organizations go about IT today, it is not really viable, unless something dramatic happens. This brings me back to OpenStack. It is the only way an organization can provide the capabilities a public cloud offers while also matching price, performance and providing a competitive advantage. If we cannot achieve the flexibility of public cloud, the consumption model, the cost effectiveness and provide compelling business advantage then we ought to just give up right?  I also find it interesting that some organizations, even those that started in the public cloud are starting to see value in build-your-own. Dropbox for example, originally started using AWS and S3. Over last few years they built their own object storage solution, one that provided more value and saved 75 million over two years. They also did so with a fairly small team. I certainly am not advocating for doing everything yourself, I am just saying that we need to make a decision, does IT provide compelling business value? Can you do it for your business, better than the generic level playing field known as public cloud? If so, you really ought to be looking into OpenStack and using momentum behind containers to bring about real change.    OpenShift and the case for OpenStack  OpenShift of course is infrastructure independent. You can run it on public cloud, virtualization, baremetal or anything that can boot Red Hat Enterprise Linux. All organizations definitely want and will use the public cloud but likely will also want to maintain control, avoiding lock-in. OpenShift is the only way to truly get multi-cloud, enterprise Kubernetes. The idea here with OpenStack is to deliver the on-premise portion of multi-cloud, with the same capabilities as public cloud. Today organizations have an incredible investment in their on-premise IT. Even if you don't see IT as a value generator, it is clear you most likely won't want to divest all those resources at once. Growth will most likely be augmented by public cloud as opposed to a complete migration.  To the next point, what is the right infrastructure to actually run on? Certainly over the years a vast majority of applications have moved to virtualization platforms but not all. I expect this also remains. Why? Well beyond 16 vCPUs, VMs start getting into the law of diminishing returns. You end up getting less value out of hyperthreading and usually needing to limit vCPUs to number of cores. Baremetal may also have advantages in certain container use cases like large scale computing. With emergence of AI and also need for large data crunching, baremetal could actually be gaining steam as a future platform. Regardless the point here is you may want your containers to run in VMs (smaller OpenShift Nodes) or baremetal (larger OpenShift nodes) and this is highly dependent on application or workload. Finally, there are other factors that could make baremetal play important role that won't be covered, cost/performance or isolation/security.  If we stick to virtualization technology we have one and only one choice. This again is where OpenStack shines, at least Red Hat OpenStack. One of the components shipped is ironic (metal-as-a-service). Ironic allows us to manage baremetal just like a virtual machine, in fact in OpenStack there is no difference and why OpenStack refers to compute units as instances, because it could be either. OpenStack can provide OpenShift with VM or baremetal based nodes and much, much more.  OpenShift integration with OpenStack  OpenShift and OpenStack fit perfectly together. Below is a list of the major integration points.   Keystone provides identity and can be used to authenticate OpenShift or LDAP users. Ceilometer provides telemtry of IaaS allowing correlation using CloudForms between container, node and instance. Multi tenant could help if running many OpenShift clusters. Heat provides orchestration enabling dynamic scale-up or scale-down of OpenShift cluster. Nova provides OpenShift nodes as a VM or baremetal instance. Neutron provides SDN and through Kuryr (starting with Red Hat OpenStack 13) will allow neutron SDN to be consumed in OpenShift directly allowing single SDN to serve both container and non-container workloads. Cinder provides dynamic storage and provisioning for containers running in OpenShift. LBaaS provides load balancer for API across masters and for application traffic across infrastructure nodes running OpenShift router. Designate provides DNS and OpenShift needs either dynamic DNS or to use wildcard for application domains. Ironic plugs into Nova via ironic conductor and allows provisioning of baremetal systems.    OpenShift on OpenStack Architectures  Important to any underlying architecture discussion is how to group OpenShift masters, infrastructure and application nodes. OpenStack provides two different possibilities.  Resource vs AutoScaling Groups  Resource groups allow us to group instances together and apply affinity or anti-affinity policies via the OpenStack scheduler. AutoScaling groups allow us to group instances and based on alarms, scale-up or scale-down those instances automatically. At first glance, you would think for masters and infra nodes use resource groups and app nodes autoscaling groups. While autoscaling sounds great, especially for app nodes, there are a lot of possibilities that can lead to scaling either happening or not happening when desired. My experience is this can work well with simple wordpress type applications but not something more complex, like a container platform or OpenShift. Also another disadvantage with autoscaling groups is they don't support an index. Indexes within groups are used to increment the instance name: master0, master1 and so on. A final point is that you can easily scale resource groups, it just needs to be triggered by an update to the Heat stack. The nice thing is you can also control scaling and if it is to be automated, you have more flexibility than relying on alarms in Ceilometer. For all of these reasons I recommend creating three resource groups: masters, infras and nodes.  Two common OpenShift architectures for OpenStack are non-ha and ha within single tenant.  Non-HA  In this architecture we will have one master, one infra node and x application nodes. While certainly application availability can be achieved by deploying across multiple nodes, the master presents a single point of failure for the data plane. The infra node runs the OpenShift router and as such a failure here would mean incoming traffic to applications would be interrupted.    HA  The HA architecture typically has three masters, two infra nodes and x app nodes. There are variations where you could have 3 infra nodes if you are running metrics and logging services that require a third node. In addition you could also split etcd and run it independently, on three additional nodes. If east/west traffic is not allowed between network zones, then you would likely require two infra nodes in each zone, to handle incoming traffic for app nodes. There are many variations of course, but for now let us keep it simple.    Deploying OpenStack  In order to deploy OpenShift on OpenStack we obviously need OpenStack. Here are some guides to help.   OpenStack 12 (Pike) RDO Lab Installation and Configuration Guide on Hetzner Root Servers OpenStack 11 (Ocata) RDO Lab Installation and Configuration Guide Red Hat OpenStack Platform 10 (Newton) Installation and Configuration Guide  Once OpenStack is deployed you need to ensure a few things are in place.  Create Flavors  # openstack flavor create --ram 2048 --disk 30 --ephemeral 0 --vcpus 1 --public ocp.bastion # openstack flavor create --ram 8192 --disk 30 --ephemeral 0 --vcpus 2 --public ocp.master # openstack flavor create --ram 8192 --disk 30 --ephemeral 0 --vcpus 1 --public ocp.infra # openstack flavor create --ram 8192 --disk 30 --ephemeral 0 --vcpus 1 --public ocp.node Create RHEL Image  Download RHEL 7.4 Cloud (qcow2) Image  # openstack image create --disk-format qcow2 \\ --container-format bare --public \\ --file /root/rhel-server-7.4-x86_64-kvm.qcow2 \"rhel74\" Create Private Key  # openstack keypair create admin Save Private Key  # vi /root/admin.pem -----BEGIN RSA PRIVATE KEY----- MIIEowIBAAKCAQEAwTrb+xdbpgY8hVOmftBIShqYUgXXDC/1gggakq8bkEdNnSku IaNGeJykzksjdksjd9383iejkjsu92wiwsajFLuE2lkh5dvk9s6hpfE/3UvSGk6m HWIMCf3nJUv8gGCM/XElgwNXS02c8pHWUywBiaQZpOsvjCqFLGW0cNZLAQ+yzrh1 dIWddx/E1Ppto394ejfksjdksjdksdhgu4t39393eodNlVQxWzmK4vrLWNrfioOK uRxjxY6jnE3q/956ie69BXbbvrZYcs75YeSY7GjwyC5yjWw9qkiBcV1+P1Uqs1jG 1yV0Zvl5xlI1M4b97qw0bgpjTETL5+iuidFPVwIDAQABAoIBAF7rC95m1fVTQO15 buMCa1BDiilYhw+Mi3wJgQwnClIwRHb8IJYTf22F/QptrrBd0LZk/UHJhekINXot z0jJ+WvtxVAA0038jskdjskdjksjksjkiH9Mh39tAtt2XR2uz/M7XmLiBEKQaJVb gD2w8zxqqNIz3438783787387s8s787s8sIAkP3ZMAra1k7+rY1HfCYsRDWxhqTx R5FFwYueMIldlfPdGxwd8hLrqJnDY7SO85iFWv5Kf1ykyi3PRA6r2Vr/0PMkVsKV XfxhYPkAOb2hNKRDhkvZPmmxXu5wy8WkGeq+uTWRY3DoyciuC4xMS0NMd6Y20pfp x50AhJkCgYEA8M2OfUan1ghV3V8WsQ94vguHqe8jzLcV+1PV2iTwWFBZDZEQPokY JkMCAtHFvUlcJ49yAjrRH6O+EGT+niW8xIhZBiu6whOd4H0xDoQvaAAZyIFoSmbX 2WpS74Ms5YSzVip70hbcXb4goDhdW9YxvTVqJlFrsGNCEa3L4kr2qFMCgYEAzWy0 5cSHkCWaygeYhFc79xoTnPxKZH+QI32dAeud7oyZtZeZDRyjnm2fEtDCEn6RtFTH NlI3W6xFkXcp1u0wbmYJJVZdn1u9aRsLzVmfGwEWGHYEfZ+ZtQH+H9XHXsi1nPpr Uy7Msd,sl,.swdko393j495u4efdjkfjdkjfhflCgYEA7VO6Xo/XdKPMdJx2EdXM y4kzkPFHGElN2eE7gskjdksjdksjkasnw33a23433434wk0P8VCksQlBlojjRVyu GgjDrMhGjWamEA1y3vq6eka3Ip0f+0w26mnXCYYAJslNstu2I04yrBVptF846/1E ElXlo5RVjYeWIzRmIEZ/qU8CgYB91kOSJKuuX3rMm46QMyfmnLC7D8k6evH+66nM 238493ijsfkjalsdjcws9fheoihg80eWDSAFDOASDF=OIA=FSLoiidsiisiisNDo ACh40FeKsHDby3LK8OeM9NXmeCjYeoZYNGimHForiCCT+rIniiu2vy0Z/q+/t3cM BgmAmQKBgCwCTX5kbLEUcX5IE6Nzh+1n/lkvIqlblOG7v0Y9sxKVxx4R9uXi3dNK 6pbclskdksdjdk22k2jkj2kalksx2koUeLzwHuRUpMavRhoTLP0YsdbQrjgHIA+p kDNrgFz+JYKF2K08oe72x1083RtiEr8n71kjSA+5Ua1eNwGI6AVl -----END RSA PRIVATE KEY----- # chmod 400 /root/admin.pem Create Public Floating IP Network  # openstack network create --provider-network-type flat \\ --provider-physical-network extnet2 --external public Create Public Floating IP Subnet  # openstack subnet create --network public --allocation-pool \\ start=144.76.132.226,end=144.76.132.230 --no-dhcp \\ --subnet-range 144.76.132.224/29 public_subnet Create Router  # openstack router create --no-ha router1 Set Router Gateway  # openstack router set --external-gateway public router1 That is it! Everything else will be created automatically by the deployment of the OpenShift infrastructure. If you want to include more or less you can also easily update the Heat templates provided.  Deploying OpenShift on OpenStack 1-2-3  Once you have OpenStack environment configured, deploying OpenShift will be done using a simple three step phased approach.   Step 1 Deploy OpenShift Infrastructure using Heat and Ansible. Step 2 Install OpenShift using Ansible. Step 3 Configure OpenShift and additional services using Ansible.  The Heat templates, all playbooks and a README is provided in the following Github repository: https://github.com/ktenzer/openshift-on-openstack-123    This step is responsible for deploying OpenShift infrastructure. Ansible will be used to call Heat to deploy the infrastructure in OpenStack. The heat templates will create a private network, load balancers, cinder storage, connect to existing public network, boot all instance and prepare the bastion host. The bastion host is used to deploy and manage the OpenShift deployment.  [OpenStack Controller]  Clone Git Repository  # git clone https://github.com/ktenzer/openshift-on-openstack-123.git  Checkout release branch 1.0  # git checkout release-1.0  Change dir to repository  # cd openshift-on-openstack-123  Configure Parameters  # cp sample-vars.yml vars.yml  # vi vars.yml --- ### OpenStack Setting ### domain_name: ocp3.lab dns_forwarders: [213.133.98.98, 213.133.98.99] external_network: public service_subnet_cidr: 192.168.1.0/24 router_id:  image: rhel74 ssh_user: cloud-user ssh_key_name: admin stack_name: openshift openstack_version: 12 contact: admin@ocp3.lab heat_template_path: /root/openshift-on-openstack-123/heat/openshift.yaml  ### OpenShift Settings ### openshift_version: 3.7 docker_version: 1.12.6 openshift_ha: true registry_replicas: 2 openshift_user: admin openshift_passwd:   ### Red Hat Subscription ### rhn_username:  rhn_password:  rhn_pool:   ### OpenStack Instance Count ### master_count: 3 infra_count: 2 node_count: 2  ### OpenStack Instance Group Policies ### ### Set to 'affinity' if only one compute node ### master_server_group_policies: \"['anti-affinity']\" infra_server_group_policies: \"['anti-affinity']\" node_server_group_policies: \"['anti-affinity']\"  ### OpenStack Instance Flavors ### bastion_flavor: ocp.bastion master_flavor: ocp.master infra_flavor: ocp.infra node_flavor: ocp.node  Authenticate OpenStack Credentials  # source /root/keystonerc_admin  Disable host key checking  # export ANSIBLE_HOST_KEY_CHECKING=False  Deploy OpenStack Infrastructure for OpenShift  # ansible-playbook deploy-openstack-infra.yml \\n --private-key=/root/admin.pem -e @vars.yml   This step is responsible for preparing OpenShift environment. The hostnames will be set,  OpenShift inventory file dynamically generated, systems will be registered to rhn, required packages installed and docker, among other things properly configured.  Get IP address of the bastion Host  # openstack stack output show -f value -c output_value openshift ip_address  {   \"masters\": [     {       \"name\": \"master0\",       \"address\": \"192.168.1.19\"     },     {       \"name\": \"master1\",       \"address\": \"192.168.1.16\"     },     {       \"name\": \"master2\",       \"address\": \"192.168.1.15\"     }   ],   \"lb_master\": {     \"name\": \"lb_master\",     \"address\": \"144.76.134.230\"   },   \"infras\": [     {       \"name\": \"infra0\",       \"address\": \"192.168.1.10\"     },     {       \"name\": \"infra1\",       \"address\": \"192.168.1.11\"     }   ],   \"lb_infra\": {     \"name\": \"lb_infra\",     \"address\": \"144.76.134.229\"   },   \"bastion\": {     \"name\": \"bastion\",     \"address\": \"144.76.134.228\"   },   \"nodes\": [     {       \"name\": \"node0\",       \"address\": \"192.168.1.6\"     },     {       \"name\": \"node1\",       \"address\": \"192.168.1.13\"     }   ] }  SSH to the Bastion Host using cloud-user and Private Key  # ssh -i /root/admin.pem cloud-user@144.76.134.229  [Bastion Host]  Change Directory to Cloned Git Repository  # cd openshift-on-openstack-123  Authenticate OpenStack Credentials  [cloud-user@bastion ~]$ source /home/cloud-user/keystonerc_admin  Disable Host Key Checking  [cloud-user@bastion ~]$ export ANSIBLE_HOST_KEY_CHECKING=False  Prepare the Nodes for Deployment of OpenShift  [cloud-user@bastion ~]$ ansible-playbook prepare-openshift.yml \\n --private-key=/home/cloud-user/admin.pem -e @vars.yml  PLAY RECAP ***************************************************************************************** bastion                    : ok=15   changed=7    unreachable=0    failed=0 infra0                     : ok=18   changed=13   unreachable=0    failed=0 infra1                     : ok=18   changed=13   unreachable=0    failed=0 localhost                  : ok=7    changed=6    unreachable=0    failed=0 master0                    : ok=18   changed=13   unreachable=0    failed=0 master1                    : ok=18   changed=13   unreachable=0    failed=0 master2                    : ok=18   changed=13   unreachable=0    failed=0 node0                      : ok=18   changed=13   unreachable=0    failed=0 node1                      : ok=18   changed=13   unreachable=0    failed=0   This step is responsible for configuring a vanilla OpenShift environment. By default only the OpenShift router and registry will be configured. OpenShift will be deployed based on the dynamically generated inventory file in step 2. You can certainly edit the inventory file and make any changes. After deployment of OpenShift, there is a small post deployment playbook which will configure dynamic storage to use OpenStack Cinder. Optional steps are defined as well to configure metrics and logging if that is desired.  [Bastion Host]  Deploy OpenShift  [cloud-user@bastion ~]$ ansible-playbook -i /home/cloud-user/openshift-inventory --private-key=/home/cloud-user/admin.pem -vv /usr/share/ansible/openshift-ansible/playbooks/byo/config.yml PLAY RECAP ***************************************************************************************** infra0.ocp3.lab            : ok=183  changed=59   unreachable=0    failed=0 infra1.ocp3.lab            : ok=183  changed=59   unreachable=0    failed=0 localhost                  : ok=12   changed=0    unreachable=0    failed=0 master0.ocp3.lab           : ok=635  changed=265  unreachable=0    failed=0 master1.ocp3.lab           : ok=635  changed=265  unreachable=0    failed=0 master2.ocp3.lab           : ok=635  changed=265  unreachable=0    failed=0 node0.ocp3.lab             : ok=183  changed=59   unreachable=0    failed=0 node1.ocp3.lab             : ok=183  changed=59   unreachable=0    failed=0   INSTALLER STATUS *********************************************************************************** Initialization             : Complete Health Check               : Complete etcd Install               : Complete Master Install             : Complete Master Additional Install  : Complete Node Install               : Complete Hosted Install             : Complete Service Catalog Install    : Complete  Run Post Install Playbook  [cloud-user@bastion ~]$ ansible-playbook post-openshift.yml --private-key=/home/cloud-user/admin.pem -e @vars.yml  PLAY RECAP ************************************************************************************************************************** infra0                     : ok=4    changed=2    unreachable=0    failed=0 infra1                     : ok=4    changed=2    unreachable=0    failed=0 localhost                  : ok=7    changed=6    unreachable=0    failed=0 master0                    : ok=6    changed=4    unreachable=0    failed=0 master1                    : ok=6    changed=4    unreachable=0    failed=0 master2                    : ok=6    changed=4    unreachable=0    failed=0 node0                      : ok=4    changed=2    unreachable=0    failed=0 node1                      : ok=4    changed=2    unreachable=0    failed=0  Login in to UI  https://openshift.144.76.134.226.xip.io:8443  Optional  Configure Admin User  [cloud-user@bastion ~]$ ssh -i /home/cloud-user/admin.pem cloud-user@master0  Authenticate as system:admin User  [cloud-user@master0 ~]$ oc login -u system:admin -n default  Make User OpenShift Cluster Administrator  [cloud-user@master0 ~]$ oadm policy add-cluster-role-to-user cluster-admin admin  Install Metrics  Set Metrics to true in Inventory  [cloud-user@bastion ~]$ vi openshift_inventory ... openshift_hosted_metrics_deploy=true ...  Run Playbook for Metrics  [cloud-user@bastion ~]$ ansible-playbook -i /home/cloud-user/openshift-inventory --private-key=/home/cloud-user/admin.pem -vv /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-cluster/openshift-metrics.yml PLAY RECAP ************************************************************************************************************************** infra0.ocp3.lab            : ok=45   changed=4    unreachable=0    failed=0 infra1.ocp3.lab            : ok=45   changed=4    unreachable=0    failed=0 localhost                  : ok=11   changed=0    unreachable=0    failed=0 master0.ocp3.lab           : ok=48   changed=4    unreachable=0    failed=0 master1.ocp3.lab           : ok=48   changed=4    unreachable=0    failed=0 master2.ocp3.lab           : ok=205  changed=48   unreachable=0    failed=0 node0.ocp3.lab             : ok=45   changed=4    unreachable=0    failed=0 node1.ocp3.lab             : ok=45   changed=4    unreachable=0    failed=0   INSTALLER STATUS ******************************************************************************************************************** Initialization             : Complete Metrics Install            : Complete  Install Logging  Set logging to true in Inventory  [cloud-user@bastion ~]$ vi openshift_inventory ... openshift_hosted_logging_deploy=true ...  Run Playbook for Logging  [cloud-user@bastion ~]$ ansible-playbook -i /home/cloud-user/openshift-inventory --private-key=/home/cloud-user/admin.pem Summary  In this article we discussed the important role infrastructure plays when deploying a container platform such as OpenShift. We also discussed the basis for various infrastructure decisions, containers, public cloud vs on-premise. There are very compelling reasons why IaaS and PaaS fit so well together and why it is important to tackle both, not just one of them. OpenStack is a perfect fit for OpenShift from an infrastructure perspective and many of the integration points were discussed in detail. Finally a hands-on guide was provided to deploy OpenStack and OpenShift on OpenStack in an automated, easy three step process. Many try to avoid tackling infrastructure and just focus on OpenShift with their applications. I certainly cannot fault anyone for taking that approach, but I think you will get the most value, out of tackling both and if you use containers as an excuse to do so, then so be it.  Happy OpenShifting on OpenStack!  (c) 2018 Keith Tenzer  ","categories": ["OpenShift"],
        "tags": ["Cloud","Containers","IaaS","Kubernetes","OpenShift","OpenStack","PaaS"],
        "url": "/openshift/openshift-on-openstack-1-2-3-bringing-iaas-and-paas-together/",
        "teaser": null
      },{
        "title": "Disaster Recovery with Containers? You Bet!",
        "excerpt":"  Overview  In this article we will discuss the benefits containers bring to business continuance, reveal concepts for applying containers to disaster recovery and of course show disaster recovery of a live database between production and DR OpenShift environments. Business continuance of course is all about maintaining critical business functions, during and after a disaster has occurred.  Business continuance defines two main criteria: recovery point objective (RPO) and recovery time objective (RTO). RPO amounts to how much data loss is tolerable and RTO how quickly services can be restored when a disaster occurs. Disaster recovery outlines the processes as well as technology for how an organization responds to a disaster. Disaster recovery can be viewed as the implementation of RPO and RTO. Most organizations today have DR capabilities but there many challenges.   Cost - DR usually is at least doubles the price. Efficiency - DR requires regular testing and in the event of a disaster, resources must be available. This leads to idle resources for 99.9% of the time. Complexity - Updating applications is complex enough but DR requires a complete redeployment where the DR side almost never mirrors production due to cost. Outdated - Business continuance only deals with one aspect,  disaster recovery but as mentioned cloud-native applications are active/active so to be effective today, business continuance architectures must cover DR and multi-site. Slow - DR often is not 100% automated and recovery is often dependent on manual procedures that may not be up to date or even tested with the latest application deployment.  I would take these challenges even further and suggest that for many organizations business continuance and DR is nothing more than a false safety net. It costs a fortune and in the event of a true disaster probably won't be able to deliver RPO and RTO for all critical applications. How could it when DR is not part of the continuous deployment pipeline and being tested with each application update? How could it with the level of complexity and scale that exists today and not 100% automation?    A New Approach  It is well overdue that organizations refresh and re-think their disaster recovery strategies. The good news is business continuance forced most companies into a two-site architecture, even if it were just focused on DR, the infrastructure is already there. In addition with the emergence of public cloud we have a plethora of infrastructure options.  Re-thinking business continuance   DR &amp; Multi-site - Business continuance plans should focus on a true multi-site architecture where applications have choice, can go with active/passive traditional or active/active cloud-native approaches across regions or availability zones. Regions - Move away from limiting terminology such as production or DR. Each site should be a region such as Europe East or West. It should be just as easy to span applications across regions or configure a region for DR from the application perspective. Platform - It goes without saying you will want software-defined networking and storage. Most organizations will want to augment their private with public clouds. On the private cloud infrastructure side, if you aren't already doing OpenStack you really ought to be going down that path. Technology - All applications should be containerized. Containers allow for packaging the application with all dependencies and running it across any infrastructure. Containers are not just a DevOps technology, they are essential to multi-cloud. The application deployment configuration is implicit, allowing for 100% automated, fast, reproducible deployments. In addition containers allow DR to be treated as just another stage in the CI/CD pipeline.  Obviously containers is a no-brainer for cloud-native applications but anything can be containerized and there are many advantages of containerization for non cloud-native applications. Disaster recovery is just one example.  Using OpenStack allows us to move our already existing private cloud infrastructure to a multi-site, region and cloud approach. It allows organizations to mirror their public cloud environments so that end users don't even notice difference between organizationally defined regions, US West (OpenStack) and US East (AWS us-east-1) regions.  Containers also need a platform that provides orchestration, security, routing, storage and integration with underlying infrastructure platform. Kubernetes is obviously the industry standard for how to orchestrate containers but by itself isn't enough. OpenShift is the platform running on Kubernetes that can provide a true runtime for containerized applications across Google, Azure, AWS, OpenStack or whatever other infrastructure may exist.  Advantages of a new approach   Reduce Cost -  The DR site is no longer a single purpose site. It has been transformed into a region. Test or DR workloads could share infrastructure. This means 99.9% of the time, when we aren't in disaster mode, resource utilization remains high as test would be using those resources. Reduce RTO - Usually measured in hours, days or weeks. Again, RTO is how quickly it takes to get up and running after disaster. Using containers allows us to start applications in seconds and do so in 100% automated fashion. As such we can also greatly reduce RTO. The cost alone of restoring service several hours faster in event of disaster could alone pay for DR infrastructure for some organizations. Ensure DR Actually Works - The only way to ensure something will work is to test it. Using containers allows DR to be just another stage in CI/CD pipeline ensuring every application change is tested from DR perspective. Support Cloud Native - Expanding DR architecture from single to multi purpose enables the same infrastructure, already in place, to handle cloud native workloads as well. This of course, further increases the efficiency and drives down cost.  Getting Started  Since most organizations already have DR capabilities, the key question is how to get started and moving in the right direction with the least amount of cost as well as effort.  Here are some additional ideas to get started.   Don't rip and replace - Initially I would keep existing DR and design new approach that could either use existing infrastructure or new infrastructure (maybe public cloud). The idea is to introduce a new DR service that is better and at lower cost. If you achieve that, applications will be lining up to migrate and this is how you can expand container adoption at the same time. Start with multisite first architecture - Define your regions, workloads, production, dev, test and DR. Then decide where workloads should run. Ideally each region should be capable of running all workloads but that may not be possible. Understand resource usage - One of the main goals should be to increase utilization of existing infrastructure. Understand where current utilization lies and how to increase it. If your paying for resources all the time you better use them! Embrace public cloud - DR is an absolute perfect use case for a pay-per-use model. DR is only used when testing or in event of disaster. Not having to pay up front for required scale is a very hard offer to turn down. At a minimum ,I would provide capability to use public cloud as DR region. If business can run their DR in public cloud then it should be an option. Define Cost Models - Having your own DR infrastructure vs pay-per-use usually means paying up front vs deferred. Most organizations don't have the scale to really make pay-per-use work.  Understanding applications and their requirements are key. In most organizations you will have the following profiles.   Single site application (traditional) - runs on a single site has no disaster recovery. Multi site application (cloud-native) - spread evenly across multiple sites. Disaster recovery is built-in and is simply a matter of re-scaling. Usually three sites works better than two. DR application (traditional) - runs primarily on single site and can failover in the event of a disaster to a secondary site.  We should get to the point where a application owner simply selects one of these three profile and the application is setup accordingly, including CI/CD.  Making DR Effective and Efficient  Single site and multi site applications are pretty easy to deal with from infrastructure perspective but DR is not, let alone making it somehow efficient. As such we need to think out-of-the-box to really drive needed improvements. As discussed, one of the main issues with DR is the resources need to be available but otherwise are not used. Sure as mentioned above public cloud solves that easily but what if you have workloads that can't use public cloud or simply aren't there yet?  One idea here is to force the test and development environments to run in the region assigned for an applications DR. Under normal operations infrastructure at DR region is used for test, development and DR testing. In the event of a disaster development and test environments are scaled down and DR is scaled up. Using containers and a platform like OpenShift we can easily achieve this. In addition a global load balancer (or active/active ingress router) is needed to balance traffic between region running production and DR. A DR cut-over is simply an Blue/Green deployment, a matter of redirecting the URL. Such an architecture not only makes DR testing simple but ensures DR resources are in use, all the time. In the event that production is larger than all of development and test, additional resources may need to be available. Fortunately most test environments are larger than production.  The diagram below illustrates how a CI/CD concept using containers and OpenShift as a platform could work. OpenShift projects are split between Dev, Test, DR and production. The Dev, Test and DR environments run on one OpenShift cluster and Production another. Each application change goes through CI/CD pipeline where the change is tested in Dev, Test and DR before going to production. In the event of a disaster we simply scale Test and Dev to zero and scale-up DR.    Image Source: Red Hat  Storage and Replication  When considering DR for OpenShift/Kubernetes there are several layers that require replication. In OpenShift/Kubernetes there is the project/namespace level, the images and any persistent volumes.  Projects  Objects such as deployment configs, secrets or even build configs can easily be exported and imported. Everything in OpenShift/Kubernetes is an object and as such can easily be exported and imported via yaml or json. Some scripts to do exactly that are already being developed but can also be easily written. One of the key things to remember is that the SElinux groups, uids and ranges must be exactly the same between the production and DR project otherwise access to a persistent volume will be forbidden by SELinux.  Images  Since applications themselves are packaged nicely into containers, each application update results in a new image. The best way to distribute images to DR projects is to add DR to the CI/CD pipeline. Images would be pushed between OpenShift clusters using CI/CD pipeline. The above diagram illustrates this concept and how images can be promoted across the lifecycle stages.  Persistent Volumes  Containers themselves are immutable but can (in case of OpenShift/Kubernetes) mount Persistent Volumes (PVs) by making a Persistent Volume Claim (PVC). A PVC binds a PV to a particular pod where a single or multiple containers are running. A PV is a mapping to a physical disk or file system (in the case of file based storage such as NFS). If a pod is going to move nodes, storage must be unmapped, remapped and mounted on the new node before it can be made available to the pods and eventually containers. Thankfully this is all done automatically by OpenShift assuming the storage has a dynamic provisioner. Today OpenStack, Ceph, Gluster, Azure, Google and AWS all have dynamic provsioners that are shipped and supported in OpenShift. You will want to choose one of these which brings me back to OpenStack for on-premise. OpenStack in turn supports many other storage systems so the clear advantage here is the abstraction layer of the IaaS.  Replication itself of persistent volumes will be done either synchronous or asynchronously, depending on latency. Most DR scenarios today rely on asynchronous replication which is why recovery point objective (RPO) is typically measured in hours or days. In case of OpenStack if your production and DR sites are close (within 100km) you may choose to stretch OpenStack across sites. Three sites instead of two would be more ideal due to quorum but both could work. Another option at IaaS layer is separate OpenStack environments and then rely on underlying storage replication to ensure your persistent volumes are asynchronously replicated between sites or environments.  A Proof of Concept  As already alluded, multiple OpenShift clusters are needed. In the proof of concept I have created two OpenShift clusters Europe West and Europe East. Both are tenants in a single OpenStack environment.  The diagram below illustrates the architecture. In this case since we are talking about a single OpenStack environment, cinder storage is being provided across the cluster.    In this example I will show how to DR a live mariadb database running in container on OpenShift from Europe West to Europe East.  [Production OpenShift]   Export secrets persistent volume, persistent volume claim and deployment config. Export project itself. Scale production to 0 (this of course simulates a site failure).  [OpenStack Europe West]   Determine persistent volume mapping and identify cinder volume UUID in OpenStack. Using volume UUID start a transfer request of the volume. Moving storage between tenants in OpenStack is done via transfer, of course storage volumes must be visible to all nodes.  [OpenStack Europe East]   Using the transfer id and auth_key, issue a transfer accept to import the volume from europe west.  [DR OpenShift]   Create DR project (if not done so already) using the exact same SELinux Groups, UIDs and Ranges as production project. Import secrets from previous export into DR project. Import persistent volume (PV) from previous export. Import persistent volume claim (PVC) into DR project from previous export. Import deployment config into DR project from previous export. Start database. The OpenShift node will be scheduled, Cinder volume automatically mapped/mounted and the database will start.  Just like magic!  These of course are the different steps across both Paas and IaaS layers. Ideally you would create the DR project at time when production project is created and use CI/CD to keep things in sync. Then assuming storage is replicated you should be able to just scale replica in deployment and things will start. As mentioned for applications which are image and config dependent you would use CI/CD tooling.  CloudForms or CI/CD could be used to easily create the appropriate environments according to user inputs. BTW did I mention CloudForms is included in OpenShift? Here is an example of what you could build with CloudForms to provide an application environment as-a-service for the various application profiles.  Application Profile Choice    Project Details    Cost Center and Justification    Now we will go through a DR scenario.  Verify database on production site running in project drtest-prod.    Verify persistent volume on production site runngin in project drtest-prod.    Execute Script to perform DR failover.  The script is available here in Github.  The script takes six arguments.   Project name. Deployment config. Endpoint for OpenShift production cluster. Token for OpenShift production cluster. Endpoint for OpenShift DR cluster. Token for OpenShift production cluster.  [cloud-user@bastion ~]$ ./perform-dr-failover.sh drtest mariadb \\n https://openshift.144.76.134.230.xip.io:8443 PqIeLvATB4liHqLpd018yd2kppUt-4OiXX6BLMAjBSo \\n https://openshift.144.76.134.229.xip.io:8443 nQEbP4MD_6kY6k2YkRenljtKDWrvCT9Cy-uhb-YiNnY Production is scaled down.    Volume transfer request generated in Europe West OpenStack tenant.    Volume transfer request is accepted in Europe East OpenStack tenant.    Verify persistent volume on DR site running in project drtest-dr.    Deploy mariadb in europe west.    Notice error that the PVC already exists. This is of course expected.    Verify the mariadb pod has started successfully.    Verify the cinder volume was automatically mounted on the node where mariadb pod started.    Verify the mariadb contains database books and rows with authors as it did on production site.    Congrats, that was a DR cut-over between OpenShift clusters.  Summary  In this article we discussed the challenges that organizations have with business continuous. We discussed new ideas and concepts for disaster recovery using containers. Finally a proof-of-concept was provided, showing DR cut-over of a mariadb database across OpenShift clusters.  A new path forward is needed that not only is more cost effective, efficient but also handles disaster recovery and multi-cloud applications, together. Containers are the technology that package our applications, simplifying their deployment and enabling them to be deployed across multiple clouds and infrastructures. IT and DevOps teams have huge reasons to put everything into containers. It is the only way to deal with the level of scale and complexity that exists today. Most recognize this, but unfortunately not all. By providing a container driven disaster recovery solution that is not only better than what exists today, but also at a lower cost, with a lower RTO, could be a driving factor to get traditional non-container applications into containers. I think we can all agree that would be a good thing for everyone. I hope this article serves as a springboard for future architecture concepts relating to disaster recovery and multi-site.  Happy Containerization of Everything!  (c) 2018 Keith Tenzer  ","categories": ["OpenShift"],
        "tags": ["Business Continuance","Containers","Disaster Recovery","Docker","DR","IaaS","Kubernetes","OpenShift","OpenStack","PaaS"],
        "url": "/openshift/disaster-recovery-with-containers-you-bet/",
        "teaser": null
      },{
        "title": "OpenShift: Getting Started with the Service Broker",
        "excerpt":"  Overview  In this article we will look at the OpenShift service broker, understand how to integrate external services into OpenShift and even create a custom broker. First before we begin a big thanks to Marek Jelen and Paul Morie, Red Hatters who both helped me understand the service broker in greater detail.  Obviously if you are reading this article you already understand microservices, containers and why it is all so incredible awesome on OpenShift. Of course everything should be in a container but unfortunately it is going to take a while to get there. As we start dissecting and breaking down the monolithic architectures of the past, likely there will be a mix of lightweight services running in containers on OpenShift and other more heavy services (databases, ESBs, etc) running outside. In addition while the service catalog in OpenShift is vast, even allowing you to add your own custom services for anything that can run in OpenShift as-a-container using a template, there will be the need, especially with public cloud to connect to external services. Both of these use cases, on-premise external services and off-premise cloud services really made it obvious that a service broker and more robust service catalog was needed. Originally OpenShift did not have a service broker so you couldn't easily consume external services. All that existed was the service catalog and templates, so every service had to be a container running on OpenShift. Thankfully other companies also saw a need for an open service abstraction and the Open Service Broker API was born as an opensource project.    Service Catalog API, Controller and Broker  Red Hat quickly adopted the open service broker API as the basis for the service broker in OpenShift. A service broker is simply an endpoint that manages a set of services. Of course this wasn't enough, in addition the service catalog had to be re-worked in Kubernetes. The service catalog API and controller were added into the existing service catalog to provide the necessary abstraction that would allow all services to be represented by a broker or API endpoint.    image source: https://blog.openshift.com/whats-new-openshift-3-6-service-catalog-brokers-tech-preview/  Since OpenShift 3.7 this has been GA and all services are provided by a broker including the built-in OpenShift services. OpenShift comes with two built-in service brokers. The template service broker and the Ansible service broker.  List cluster service brokers.  [cloud-user@master0 ~]$ oc get clusterservicebroker NAME AGE ansible-service-broker 72d template-service-broker 72d The template service broker provides services to service catalog that are templates. Think of this was as the original service catalog with an API. If you can run the service as-a-container on OpenShift you would likely just create a template under the openshift project, no need for custom broker there.  View all templates part of service catalog.  [cloud-user@master0 ~]$ oc get templates -n openshift The Ansible service broker was also added to OpenShift and allows you to do something similar to template service broker, only using Ansible. The ASB though, unlike the template service broker enables you to connect services that are outside of OpenShift. You just need to create a playbook. The main idea or use case here is around infrastructure provisioning or setup. Using the ASB you can easily launch instances, applications, services on infrastructure platforms outside OpenShift. Maybe your developers need a load balancer, provision a database outside OpenShift, etc. Anything is possible. Likely if that is what you are trying to do you would use the ASB, no need for custom service broker.  Red Hat is also adding service brokers for the public cloud providers like AWS, Azure and GCE. So out-of-the-box you will be able to natively interface with public cloud services from directly inside OpenShift. You are probably thinking, well this is awesome, I don't need anything else. You are probably right, but just in case you might actually want to create your own service broker endpoint, for example gain fine granular control over a service, let's look at how to do it.  Building Customer Service Broker  We will now create a custom service broker using a project I cloned from Marek Jelen.  Login in as user with cluster-admin role.  [cloud-user@master0 ~]$ oc whoami admin Add cluster-admin role to user if missing.  [cloud-user@master0 ~]$ oc adm policy add-cluster-role-to-user \\ cluster-admin admin Create new project.  [cloud-user@master0 ~]$ oc new-project custom-broker Create service account with cluster-admin permission.  In order to provision services, certain privileges are required to do so on behalf of other users. To make things easy just make service account cluster-admin.  [cloud-user@master0 ~]$ oc adm policy add-cluster-role-to-user \\ cluster-admin system:serviceaccount:custom-broker:default Launch Service Broker.  The service broker in this case is written in ruby and is just really an example, not a proper implementation to follow. While you can use any language, go would be more ideal. I recommend checking out the open service broker starter pack Paul Morie provides https://github.com/pmorie/osb-starter-pack.  [cloud-user@master0 ~]$ oc new-app --name broker \\ ruby~https://github.com/ktenzer/service-broker-summit-demo.git Expose service broker service.  [cloud-user@master0 ~]$ oc expose svc broker Get broker service route.  Note the route, you need to provide that in the cluster service broker.  [cloud-user@master0 ~]$ oc get routes NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD broker broker-custom-broker.apps.144.76.134.230.xip.io broker 8080-tcp None Create cluster service broker.  You can also get example of broker.yaml from https://raw.githubusercontent.com/openshift-evangelists/service-broker-summit-demo/master/broker.yml.  [cloud-user@master0 ~]$ vi broker.yaml apiVersion: servicecatalog.k8s.io/v1beta1 kind: ClusterServiceBroker metadata:   name: summit-broker spec:   url:  http://broker-custom-broker.apps.144.76.134.230.xip.io  [cloud-user@master0 ~]$ oc create -f broker.yaml Create a workshop service using broker.  The service we will create are simply the directions on how to create the service broker called workshop. After creating the cluster service broker, the workshops service should now be visible in OpenShift service catalog (this may take several minutes to update, you can force update though). Once it is visible, simply click the service, create a new project for it, add a GUID (can be anything) and the service will be provisioned using the new service broker.  Force update of broker.  Simply increment the number of the relistRequest to force update.  [cloud-user@master0 ~]$ oc edit clusterservicebroker summit-broker  ...    relistRequests: 2  ... Find workshops service under 'other' in service catalog.    Select workshops service.    Provide a project, GUID and off you go.    Of course you can also use the CLI to provision a service using the broker.  Create service instance of workshops service.  [cloud-user@master0 ~]$ vi blablaServiceInstance.yaml apiVersion: servicecatalog.k8s.io/v1beta1 kind: ServiceInstance metadata:   name: workshops-q74t spec:   clusterServiceClassExternalName: workshops   clusterServicePlanExternalName: workshop   parameters:     GUID: blabla  &nbsp;  [cloud-user@master0 ~]$ of create -f blablaServiceInstance.yaml Expose workshop service.  Expose the provisioned workshop service to access it.  [cloud-user@master0 ~]$ oc project my-custom-service1 [cloud-user@master0 ~]$ oc expose svc workshop [cloud-user@master0 ~]$ oc get routes NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD broker broker-custom-broker.apps.144.76.134.230.xip.io broker 8080-tcp None workshop workshop-custom-broker.apps.144.76.134.230.xip.io workshop 8080 None Access workshop service via browser.  http://workshop-custom-broker.apps.144.76.134.230.xip.io  Summary  In this article we discussed the value of service abstraction and the need to integrate external services in OpenShift. We reviewed the OpenShift service catalog architecture and the open service broker API. Finally using a practical example, we built our own custom service broker and provisioned a service via the UI and CLI. Obviously this is just a starting point to understand the service broker and service catalog components in OpenShift. If this is interesting I encourage you to look into starter pack provided by Paul Morie. This will get you started building your own custom brokersservice broker  .  Happy Brokering of Services on OpenShift!  (c) 2018 Keith Tenzer  ","categories": ["OpenShift"],
        "tags": ["Containers","Kubernetes","Open Service Broker API","OpenShift","Service Broker","Service Catalog","Services"],
        "url": "/openshift/openshift-getting-started-with-the-service-broker/",
        "teaser": null
      },{
        "title": "Satellite on OpenStack 1-2-3: Systems Management in the Cloud",
        "excerpt":"   Overview  In this article we will explore an important part of day 2 operations in OpenStack or any IaaS, systems management. There are two ways to maintain applications: immutable or lifecyle. Satellite is a product from Red Hat that focuses on lifecycle management. Specifically the deployment, updating, patching and configuration of Red Hat Enterprise Linux (RHEL) as well as the applications running on top throughout entire lifecycle. We will discuss the value Satellite brings to OpenStack and why systems management is a key part of day 2 cloud operations. Investigate the Satellite architecture and how it applies to OpenStack. Finally we will go through hands-on deploy of Satellite on OpenStack, even deploying an instance and automatically connecting the instance to Satellite, all using Ansible.  The Value of Satellite in OpenStack  Satellite is the second product Red Hat created after RHEL. It has been around for over 10 years and recently gone through a major re-architecture from ground up to address cloud. Red Hat customers have used Satellite to create a standard operating environment (SOE) for RHEL and the applications that run on RHEL for 10+ years. Satellite provides the ability to create various content views and bring them together in a composite content view (a group of content views). This allows us to group content (RPMs, configuration management, Tar files, whatever else) and most importantly version it. Once we can group software and version it we can start thinking about release management across a lifecyle environment. A lifecycle environment is typically something similar the holy trinity: development, test and production. The versions of software for our OS and applications of course vary, you don't want to update software in production without testing in development or test right?    Below is an illustration of how content views relate to lifecycle environments.    As we mentioned a composite content view is created to group content views. This is the basis of an SOE. If we wanted to build an SOE for Java, we would have a content view for the OS (this is typically shared across many SOEs) and another one for Java. They would be combined in a composite content view and this is what would be presented to a host via a hostgroup. Hostgroups in Satellite are just groupings of similar hosts that inherit the same content view (likely a composite one), same configuration environment, etc.  In addition to building an SOE across lifecycle environment, Satellite also provides patching, security vulnerabilities, configuration management via either puppet or integration with Ansible Tower and what I am most excited about, integration with Insights that allows for predictive systems management through AI/ML.  Now if we look at what you get when you go to a public cloud platform AWS, Azure, GCE, it is very different. You can get a RHEL instance on demand but how is lifecycle management done? Well it isn't, they simply provide you the latest RPMs and you do a yum update. That is it. Great, no thanks! No SOE, no content view, no configuration management, nothing. OpenStack also does not provide anything here. Sure you can upload an image and deploy from that but what about lifecycle management? Your applications are ok with update to an image every time you make a change and complete redeploy? I would say 10% of your applications can handle that and for them, great that is the way to go but everyone else?  Why is lifecycle management forgotten in the cloud? Simple, the idea with cloud was cloud-native, everything is an image, you never update a running system, you throw it away and build a new one. That is cloud-native in a nutshell and of course is a concept we need for cloud-native or containerized applications but it doesn't help for the other 90% of applications in an enterprise. Combining OpenStack with Satellite, gets you to IaaS (something modern, nimble, agile, flexible) and allows you to run cloud-native + traditional workloads on the same platform. Imagine your enterprise, everything being under a single IaaS. What could you do? What value would that bring the business?  Satellite of course can be used in public cloud just like on OpenStack. It means however not using RHEL image from AWS, Azure, GCE, etc but bringing your own image and subscription from Red Hat.  Satellite 6 Architecture  As I mentioned, Satellite 5 existed almost 10 years but a few years ago Red Hat started over and built a new Satellite based on leading opensource projects. Satellite is a product that brings together the following opensource projects: foreman (provisioning), katello (content management), pulp (content repository), candelpin (subscription management),  Ansible Tower Integration (configuration management option 1) and puppet (configuration management option 2). Satellite consists of a server and one or more capsules. Capsules are used to scale or address network segmentation.  Below is illustration of the different Satellite components.    For configuration management you could use Ansible Tower, Puppet or even both. In the case of Ansible Tower only you would not run Puppet services on the capsule and Ansible Tower would leverage the Satellite 6 inventory as well as facts to communicate directly with instances.  The illustration below shows how we could apply the Satellite 6 architecture to OpenStack taking advantage of the multi-tenant capabilities and lifecycle environments.    In Satellite you can have many lifecycle stages, even for each application and in OpenStack there are various concepts on how to do multitenancy. This should just give an idea of how to apply the Satellite architecture and SOE lifecycle to OpenStack.  Value of solution:   IaaS to manage virtual and baremetal workloads. Enterprise grade and production proven with Red Hat OpenStack Platform. Manage cloud native and traditional workloads together. Provide Security / Vulnerability updates, patching, SOE, lifecycle management for traditional workloads and platforms such as PaaS that run on IaaS layer. Provide automation tool and framework, Ansible or Puppet to drive end-to-end automation once instances are deployed via Heat. Leverage entire Red Hat knowledge base, all support cases ever opened and Red Hat security / vulnerability database to provide insights, allowing problems to be seen before they are well, problems.  Add it all up and you got incredible business value. The only thing missing on top is OpenShift to provide a PaaS based on container technology and devops methodology. By the way that subject is covered in great detail here:  https://keithtenzer.com/2018/02/26/openshift-on-openstack-1-2-3-bringing-iaas-and-paas-together/    Deploy Satellite on OpenStack  Now that we have a solid foundation it is time to deploy Satellite on OpenStack. First, one thing we haven't talked about is provisioning. Of course both Satellite and OpenStack through Heat can provision virtual and baremetal instances. This is the only part of Satellite that actually overlaps with OpenStack. You will probably get varying opinions and certainly a requirements discussion is in order before making a decision regarding provisioning technology or process, nevertheless I will share my thoughts. My view is you should always use the infrastructure platform and it's native capabilities for provisioning while using an abstraction layer on top like Ansible. In this case that means using OpenStack Heat to create templates for deploying instances or groups of instances and Ansible to orchestrate Heat and also deploy software to those instances.  I have prepared several Ansible playbooks that do various things to not only automate the deployment of Satellite but also to automate connecting instances to Satellite (bootstrap) and even deploying an instance with Satellite bootstrap. Let us get started.  Launch a RHEL 7.5 Instance in OpenStack.    Alternatively you can use Heat and a template similar to one I have provider (https://github.com/ktenzer/satellite-on-openstack-123/blob/master/heat/instance.yaml). Make sure you add floating ip so the Satellite server can be accessed externally.  If you need more details on setting up OpenStack environment see the below posts.   OpenStack end-to-end deployment on Hetzner Server Red Hat OpenStack 10 Configuration and Setup  Or just search my blog for 'openstack' you will find a library of information, I promise ;)  Clone Git Repostory  Log onto instance and clone the git repo.  [root@sat6]# git clone https://github.com/ktenzer/satellite-on-openstack-123.git Checkout release-1.0 branch  [root@sat6]# git checkout release-1.0 Update Vars File  In Ansible vars are used to pass parameters into playbooks. I have created a single vars file with all the information needed to run all the playbooks. If you are only interested in deploying Satellite you do not need to configure OpenStack settings. You also do not need OpenStack settings for bootstrapping (configuring instances for Satellite).  [root@sat6]# cd satellite-openstack-123 [root@sat6]# cp sample_vars.yml vars.yml [root@sat6]# vi vars.yml --- ### General Settings ### ssh_user: cloud-user admin_user:  admin_passwd:   ### OpenStack Settings ### stack_name: myinstance heat_template_path: /root/satellite-on-openstack-123/heat/instance.yaml openstack_version: 12 openstack_user: admin openstack_passwd:  openstack_ip:   ### OpenStack Instance Settings ### hostname: rhel123 domain_name: novalocal external_network: public internal_network: internal internal_subnet: internal-subnet security_group: base flavor: m2.tiny image: rhel75 ssh_key_name: admin volume_size: 30 ssh_key_name: admin  ### Satellite Settings ### satellite_server: sat6.novalocal satellite_ip:  satellite_version: 6.3 activation_key: rhel7-base puppet_version: puppet4 puppet_environment: KT_RedHat_unstaged_rhel7_base_5 install_puppet: True puppet_logdir: /var/log/puppet puppet_ssldir: /var/lib/puppet/ssl org:  location:  manifest_file:  ### Red Hat Subscription ### rhn_username:  rhn_password:  rhn_pool: Configure Inventory  In Ansible an inventory is used to define and group hosts where we want to run playbooks. Dynamic inventories and everything imaginable are possible with Ansible Tower. In this case we have a static inventory.  [root@sat6]# cp sample.inventory inventory [root@sat6]# vi inventory [server] sat6.novalocal  [capsules]  [clients] rhel2.novalocal rhel1.novalocal We will only deploy a Satellite server with integrated capsule. I haven't yet tested deployment with multiple capsules but let me know if this is working or open git issue and I will look into it.  The clients group in inventory file is for Satellite 6 bootstrapping i.e. configuring instances automatically to use Satellite 6. You can leave this blank optionally.  Run Satellite Deployment Playbook  Since we are running on OpenStack you need a private key. You create a key in OpenStack and assign it to an instance when the instance is deployed. This key is needed. You can follow OpenStack guides listed above to understand this is more detail.  Run the playbook install-satellite.yml.  [root@sat6]# ansible-playbook install-satellite.yml \\ --private-key=/root/admin.pem -e @.vars.yml -i inventory  PLAY RECAP ***************************************************************************************** rhel1.novalocal : ok=10 changed=4 unreachable=0 failed=0 rhel2.novalocal : ok=10 changed=4 unreachable=0 failed=0 sat6.novalocal : ok=46 changed=18 unreachable=0 failed=0 After installation there are still some todos I haven't completed automated. You need to create a hostgroup, assign it a content view and puppet environment as well as assign repos (products) to activation key.  &nbsp;  Update activation key        Create hostgroup      Bootstrap Existing Instance to Satellite  If any instances already exist at time when Satellite 6 is installed the install-satellite.yml playbook will also configure instances listed in [clients] hostgroup of inventory file automatically for Satellite 6. This was also shown above. Mostly you will deploy instances and then either bootstrap them later or bootstrap them during deployment. I have provided playbooks to accomplish both.  Bootstrap Existing Instance  The bootstrap-clients.yml playbook will simply run the sat6-bootstrap role. The role is responsible for connecting an existing instance to Satellite. A good practice in Ansible is to put tasks into roles and make them reusable, I have followed that.  The Satellite bootstrap steps are as follows:   Install Satellite CA Certificate Register to Satellite with activation key Install Katello agent Start and enable goferd Install Red Hat Insights Install Puppet Configure Puppet  [root@sat6]# ansible-playbook bootstrap-clients.yml --private-key=/root/admin.pem -e @../vars.yml -i ../inventory  PLAY RECAP ***************************************************************************************** rhel1.novalocal : ok=8 changed=2 unreachable=0 failed=0 rhel123.novalocal : ok=8 changed=2 unreachable=0 failed=0 Deploy New Instance and Bootstrap using OpenStack Heat  You may of course want to deploy a new instance and as part of deployment automatically do the bootstrapping to Satellite. In this case there are two playbooks. The first is provided to configure the OpenStack client on host that run playbook so you can authenticate to OpenStack. The other is used to deploy a new instance using Heat and automatically bootstrap newly created instance to Satellite. IP address discovery is done dynamically by reading the output of the Heat stack, once provisioning is complete. Everything is of course driven through Ansible. In order to run these playbooks you must ensure the OpenStack settings in the vars file are configured correctly.  Configure OpenStack Client  As mentioned in order to communicate with OpenStack we need to authenticate to Keystone, the identity service. The playbook is setup-openstack-client.yml. In order to run it no inventory is needed since it will just configure the client on the localhost or host running playbook.  [root@sat6]# ansible-playbook setup-openstack-client.yml --private-key=/root/admin.pem -e @../vars.yml  PLAY RECAP ***************************************************************************************** localhost : ok=4 changed=1 unreachable=0 failed=0   Deploy New Instance with Heat and Bootstrap to Satellite  Once OpenStack client is setup we need to authenticate. This is done outside of the Ansible environment.  [root@sat6]# source /root/keystonerc_admin Authentication credentials are set in the environment. We are using OpenStack CLI through Ansible. Another option is to use OpenStack modules written for Ansible and then authentication is of course built-in. This is a much cleaner approach but also requires various python libraries and versions like shade.  Make sure strict ssh host key checking is off (StrictHostKeyChecking) in /etc/sshd/ssh_config or set option on cli for Ansible. If strict host key checking is on you are of course prompted when connecting to host via ssh for first time and automation requires no manual inputs.  [root@sat6(keystone_admin)]# export ANSIBLE_HOST_KEY_CHECKING=False Once authenticated run the provision-client.yml playbook. This will take a few minutes, as a new instance in OpenStack will be provisioned.  [root@sat6(keystone_admin)]# ansible-playbook provision-client.yml \\ --private-key=/root/admin.pem -e @../vars.yml PLAY RECAP ***************************************************************************************** localhost : ok=11 changed=6 unreachable=0 failed=0 rhel3 : ok=15 changed=12 unreachable=0 failed=0 If you are configuring puppet, a certificate needs to be signed in Satellite. You can of course setup auto-signed certificates but default is you need to sign. This means pupet agent run will fail. You need to go into Satellite, under Capsule and Certificates. There you can click sign to sign a certificate.    After signing certificate you can simply re-run playbook to do the first puppet run. This of course should happen every 30 minutes so you can wait for next automatic run.  Update and Manage Errata  Using Satellite, it is very easy to see what hosts have security vulnerabilities and where errata should be installed. You can of course also schedule updates using the scheduler built-in to Satellite. The Ansible playbook will do a yum update when enabled after host is provisioned from Heat and configured for Satellite. Sometimes however a critical issue comes about and systems should be patched immediately. Here we will walk through process.  Under Hosts-&gt;Content Hosts.    Select the newly provisioned instance rhel3.novalocal and choose manage errata.    Next select the erratas you would like to apply to rhel3.novalocal.    Click Install Selected to perform update.    Predicative System Management with Red Hat Insights  Obviously Satellite helps you once a security vulnerability is available or to control how new software updates are introduced into an applications lifecycle environment. However, what if we could identify problems or issues before they would arise? If you consider Red Hat as an organization has quite a lot of data that could help. Red Hat has all the information from support cases across all our customers. Red Hat has a deep and broad knowledge base with recommendations on not only OS but platform, such as OpenStack or OpenShift configurations. Given the support case and knowledge base, using AI/ML that data could be crunched against intelligent rule sets. We could check these rules against a configuration provided by customer to see if everything checks out immediately once a system is built or updated. Guess what? That is Red Hat Insights. Customers can opt-in and send information from their systems and it is run against rule sets build from knowledge in support cases as well as other sources like knowledge base. With every new case or knowledge base the rule set grows so Red Hat Insights is continually getting smarter. This of course is all integrated into Satellite.  Now that we have patched our newly deployed instance, rhel3.novalocal, we can view Insights to see if anything else needs to be done.  Actions  Under Satellite-&gt;Red Hat Insights select actions.    Inventory  We can view which actions apply to hosts by clicking inventories.    Here we see Insights found 5 actions that need attention.  Plans  In Insights plans allow us to resolve and track resolution of actions across systems.    Here we create a plan for our single instance rhel3.novalocal.  Resolutions  Red Hat Insights is pretty smart. There is often not just one resolution but multiple. For any actions that have more than one resolution, Insights will prompt user to make decision.    Once the plan is saved, Insights will also provide playbooks to actually address and resolve the issues according to resolutions provided by user.    Simply click download playbook to get an Ansible playbook that will resolve the issue without you having to do anything. Pretty cool right?  Below is the playbook to fix our performance issue downloaded downloaded in Satellite directly from Insights.  --- # Red Hat Insights has recommended one or more actions for you, a system administrator, to review and if you # deem appropriate, deploy on your systems running Red Hat software. Based on the analysis, we have automatically # generated an Ansible Playbook for you. Please review and test the recommended actions and the Playbook as # they may contain configuration changes, updates, reboots and/or other changes to your systems. Red Hat is not # responsible for any adverse outcomes related to these recommendations or Playbooks. # # Addresses maintenance plan 35821 (rhel3) # https://access.redhat.com/insights/planner/35821 # Generated by Red Hat Insights on Fri, 15 Jun 2018 12:04:33 GMT # Warning: Some of the rules in the plan do not have Ansible support and this playbook does not address them!  # Decreased performance when not using 'noop' or 'deadline' I/O scheduler on VM # Identifier: (vm_io_scheduler|VM_IO_SCHEDULER_V1,105,fix) # Version: a0e934f07d8167073546cbc5108c4345f92559a5 - name: Enable virtual-guest tuned profile   hosts: \"rhel3.novalocal\"   become: true   tasks:      - name: ensure tuned is installed       yum:         name: tuned         state: latest          - name: ensure tuned is started and enabled at boot       service:         name: tuned         state: started         enabled: true      - name: set tuned profile to virtual-guest       command: tuned-adm profile virtual-guest       check_mode: false   - name: run insights   hosts: rhel3.novalocal   become: True   gather_facts: False   tasks:     - name: run insights       command: redhat-access-insights       changed_when: false  Summary  In this article we discussed the importance of Satellite in the context of OpenStack. Providing a standard operating environment and lifecycle management for systems we don't want to build up / burn down every time a change is made. We looked at some architecture concepts and ideas as a starting point. Finally we went though deployment of Satellite, deployment of an instance, Satellite bootstrapping, security updates and of course predictive systems management with Red Hat Insights. All of this together means we can not only operate cloud-native applications but also much more challenging traditional applications that require a lifecycle in OpenStack. Most organizations today go about this the wrong way. They try to build a new greenfield that only addresses cloud-native and attempt to leave their legacy behind. While certainly the cloud-native greenfield approach at least gets you going with cloud-native applications and methodologies, you are doing so by leaving your legacy behind. Re-platforming legacy applications, especially ones already virtualized on OpenStack is a no-brainer and hopefully this article has shown the capabilities are all there. We should not simply forget or ignore the past, our legacy, but rather take it with us and use it to improve our future.  Happy OpenStacking!  (c) 2018 Keith Tenzer  ","categories": ["Satellite"],
        "tags": ["Cloud Computing","cloud management","Insights","OpenStack","Satellite 6","Security","system management"],
        "url": "/satellite/satellite-on-openstack-1-2-3-systems-management-in-the-cloud/",
        "teaser": null
      },{
        "title": "OpenStack 13 (Queens) Lab Installation and Configuration Guide for Hetzner Root Servers",
        "excerpt":"  Overview   In this article we will focus on installing and configuring OpenStack Queens using RDO and the packstack installer. RDO is a community platform around Red Hat’s Enterprise OpenStack Distribution. It allows you to test the latest OpenStack capabilities on a stable platform such as Red Hat Enterprise Linux (RHEL) or CentOS. This guide will take you through setting up Hetzner root server, preparing environment for OpenStack, installing the OpenStack Queens release, adding a floating ip subnet through OVS, configuring networking, security groups, flavors, images and are other OpenStack related services. The outcome is a working OpenStack environment based on the Queens release that you can use as a baseline for testing your applications using OpenStack capabilities. The installation will create an all-in-one deployment however you can use this guide to create a multi-node deployment as well.    Root Server Specs  CPU: Intel(R) Core(TM) i7-3930K CPU @ 3.20GHz (12 Cores) Memory: 64GB RAM Disk: 2 x 3TB SATA Network: 1Gbit IPV4 Adresses: 1 x IPV4 + /29 Subnet (6 IPs) Hetzner is a hosting providing and auctions off used hardware for very competitive prices on a per/month basis (https://www.hetzner.de/sb). Hetzner provides one IP for each root server that is accessible via internet. It is a /32 so if you want additional IPs, like you would for OpenStack to use as floating ips you need to order additional subnet. You can order an additional subnet as I have done. Hetzner will route that subnet through the host IP of the /32. This requires creating an additional OVS bridge in OpenStack and this guide will go through that configuration.  Configure Root Server  As mentioned, Hetzner will give you access to your root server via IP and provide ability to manage the root server. Basic things like put root server in rescue mode or reboot. You can provide public ssh key or use password to access system. Please don't use password, this is the internet, bad things can and will happen.  Enter Rescue Mode  In order to install OS or repair it you need to get into rescue mode.        Configure Hetzner Firewall.  While server is rebootting you can modify your servers firewall rules for incoming traffic. By default firewall allows port 22 for SSH and nothing else. Firewall settings can be modified under your server's settings in Hetzner's web UI https://robot.your-server.de/server. For OpenStack I would recommend 80, 443, 22 and ICMP.    If you plan on running OpenShift on OpenStack you need to add some additional rules.    Connect to root server via ssh and private key.  # ssh -i .ssh/id_rsa.pub Create Root Server Configuration.  The root server in this case has two disks. They are being configured in non-redundant RAID 1 (stripping) configuration. OpenStack requires decent disk performance, I found when using mirroring it was not enough. If you want redundant configuration you need to pay extra for SSDs. Since we will setup LVM partitions, creating one for Cinder (cinder-volumes) is a good idea. You don't want to use loop back driver for Cinder. Finally you can provide your own image (if you want to use RHEL, you must do so).  My colleague Andy Neeb also automated deployment using Hetzner API. If you are interested in automated deployment here is his Github repository: https://github.com/andyneeb/ansible-hetzner.  [CENTOS]  # vi config.txt  DRIVE1 /dev/sda DRIVE2 /dev/sdb SWRAID 1 SWRAIDLEVEL 0 BOOTLOADER grub HOSTNAME myrootserver.lab PART /boot ext3 512M PART lvm vg0 500G PART lvm cinder-volumes all  LV vg0 root / ext4 100G LV vg0 swap swap swap 5G LV vg0 var /var ext4 10G LV vg0 tmp /tmp ext4 10G LV vg0 home /home ext4 40G  IMAGE /root/.oldroot/nfs/install/../images/CentOS-74-64-minimal.tar.gz [RHEL]  Note: You need to provide your own RHEL image.  # vi config.txt  DRIVE1 /dev/sda DRIVE2 /dev/sdb SWRAID 1 SWRAIDLEVEL 0 BOOTLOADER grub HOSTNAME myrootserver.lab PART /boot ext3 512M PART lvm vg0 500G PART lvm cinder-volumes all  LV vg0 root / ext4 100G LV vg0 swap swap swap 5G LV vg0 var /var ext4 10G LV vg0 tmp /tmp ext4 10G LV vg0 home /home ext4 40G  IMAGE http://path/to/rhel/image Install image and create partitions.  # installimage -a -c config.txt    Note: you may see error with RHEL image if you renamed it to CentOS because Hetzner expects CentOS but it is safe to ignore.  Hetzner Online GmbH - installimage  Your server will be installed now, this will take some minutes You can abort at any time with CTRL+C ...  : Reading configuration done  : Loading image file variables done  : Loading centos specific functions done  1/18 : Deleting partitions done  2/18 : Test partition size done  3/18 : Creating partitions and /etc/fstab done  4/18 : Creating software RAID level 0 done  5/18 : Creating LVM volumes done  6/18 : Formatting partitions : formatting /dev/md/0 with ext3 done  : formatting /dev/vg0/root with ext4 done  : formatting /dev/vg0/swap with swap done  : formatting /dev/vg0/var with ext4 done  : formatting /dev/vg0/tmp with ext4 done  : formatting /dev/vg0/home with ext4 done  7/18 : Mounting partitions done  8/18 : Sync time via ntp done  9/18 : Downloading image (http) done  : Importing public key for image validation done  10/18 : Validating image before starting extraction warn  : No detached signature file found! 11/18 : Extracting image (http) done  12/18 : Setting up network config done  13/18 : Executing additional commands : Setting hostname done  : Generating new SSH keys done  : Generating mdadm config done  : Generating ramdisk done  : Generating ntp config done  14/18 : Setting up miscellaneous files done  15/18 : Configuring authentication : Fetching SSH keys done  : Disabling root password done  : Disabling SSH root login without password done  : Copying SSH keys done  16/18 : Installing bootloader grub done  17/18 : Running some centos specific functions failed  An error occured while installing the new system! See the debug file /root/debug.txt for details.   Reboot. # reboot now   Disable password authentication. Remember this system is accessible from the internet. Unless you want people to constantly try and login, disable password authentication. # vi /etc/ssh/sshd_config PasswordAuthentication no  Restart sshd. # systemctl restart sshd  Verify LVM Volume Groups  Make sure a volume group cinder-volumes exists. This will be used for OpenStack storage as mentioned.  # vgs  VG #PV #LV #SN Attr VSize VFree  cinder-volumes 1 0 0 wz--n- &lt;4.97t &lt;4.97t  vg0 1 5 0 wz--n- &lt;499.75g &lt;334.75g Install OpenStack  Ensure local name resolution is working.  # vi /etc/hosts 144.76.52.111 myrootserver.lab Set hostname.  # hostnamectl set-hostname myrootserver.lab Enable RPMs.  [RHEL]  # subscription-manager register # subscription-manager list --available # subscription-manager attach --pool= # subscription-manager repos --disable=* # subscription-manager repos --enable=rhel-7-server-rpms  # subscription-manager repos --enable=rhel-7-server-rh-common-rpms # subscription-manager repos --enable=rhel-7-server-extras-rpms # subscription-manager repos --enable=rhel-7-server-openstack-13-rpms # subscription-manager repos --enable=rhel-7-server-openstack-13-devtools-rpms Install packstack packages.  [CentOS]  # yum install -y centos-release-openstack-queens [RHEL]  # yum install -y openstack-packstack Install Python Setuptools  Python setuptools is required for packstack and missing for Queens so we need to add it manually.  # yum install -y python-setuptools Disable firewalld, OpenStack uses iptables.  # systemctl disable firewalld # systemctl stop firewalld Disable NetworkManager.  # systemctl stop NetworkManager # systemctl disable NetworkManager Install yum-utils and update the system.  # yum install -y yum-utils # yum update -y Configure Intel Virtualization for Directed I/O.  # vi /etc/default/grub --- GRUB_CMDLINE_LINUX=\"biosdevname=0 crashkernel=auto nomodeset rd.auto=1 consoleblank=0 intel_iommu=on\" --- # grub2-mkconfig -o /boot/grub2/grub.cfg Reboot.  # systemctl reboot Create packstack answers file for customizing the installer.  # packstack --gen-answer-file /root/answers.txt Update the packstack answers file and enable other OpenStack services.  # vi /root/answers.txt --- CONFIG_NTP_SERVERS=0.pool.ntp.org,1.pool.ntp.org,2.pool.ntp.org CONFIG_CONTROLLER_HOST=144.76.52.111 CONFIG_COMPUTE_HOSTS=144.76.52.111 CONFIG_NETWORK_HOSTS=144.76.52.111 CONFIG_STORAGE_HOST=144.76.52.111 CONFIG_KEYSTONE_ADMIN_PW=redhat CONFIG_PROVISION_DEMO=n CONFIG_HEAT_INSTALL=y CONFIG_HEAT_CFN_INSTALL=y CONFIG_CEILOMETER_INSTALL=y CONFIG_MAGNUM_INSTALL=y CONFIG_LBAAS_INSTALL=y CONFIG_CINDER_VOLUMES_CREATE=n CONFIG_NOVA_SCHED_RAM_ALLOC_RATIO=3.0 CONFIG_NOVA_LIBVIRT_VIRT_TYPE=kvm CONFIG_HORIZON_SSL=y --- Note: Regarding HORIZON_SSL. If you enable SSL it won't work with chrome unless you create a self-signed cert with subjectAltNames, chrome now blocks common self-signed certs.  Install OpenStack using packstack.  # packstack --answer-file /root/answers.txt --timeout=1500   **** Installation completed successfully ******  Additional information:  * NOTE : A selfsigned CA certificate was generated to be used for ssl, you should still change it do subordinate CA cert. In any case please save the contents of /root/packstackca/.  * File /root/keystonerc_admin has been created on OpenStack client host 144.76.56.119. To use the command line tools you need to source the file.  * NOTE : A certificate was generated to be used for ssl, You should change the ssl certificate configured in /etc/httpd/conf.d/ssl.conf on 144.76.56.119 to use a CA signed cert.  * To access the OpenStack Dashboard browse to https://144.76.56.119/dashboard . Please, find your login credentials stored in the keystonerc_admin in your home directory.  * The installation log file is available at: /var/tmp/packstack/20180714-072935-AsKJdI/openstack-setup.log  * The generated manifests are available at: /var/tmp/packstack/20180714-072935-AsKJdI/manifests Configure Physical Network  Source the keystone admin profile.  # . /root/keystonerc_admin Backup the ifcfg-etho script.  # cp /etc/sysconfig/network-scripts/ifcfg-eno1 /root/ Configure external bridge br-ex.  # vi /etc/sysconfig/network-scripts/ifcfg-eno1 DEVICE=eno1 ONBOOT=yes TYPE=OVSPort DEVICETYPE=ovs OVS_BRIDGE=br-ex # vi /etc/sysconfig/network-scripts/ifcfg-br-ex DEVICE=br-ex BOOTPROTO=none ONBOOT=yes TYPE=OVSBridge DEVICETYPE=ovs USERCTL=yes PEERDNS=yes IPV6INIT=no IPADDR=&lt;Hetner Root Server IP&gt; NETMASK=255.255.255.255 SCOPE=\"peer &lt;Hetzner Gateway IP&gt;\" Switch static route to br-ex.  Note: this is specific to Hetzner environment as the physical host will get a /32.  # mv /etc/sysconfig/network-scripts/route-eno1 /etc/sysconfig/network-scripts/route-br-ex  Add the eno1 physical interface to the br-ex bridge in openVswitch.  Note: this is point of no return!!! Double check your config, if there is typo you will need to got to rescue mode.  # ovs-vsctl add-port br-ex eno1; systemctl restart network.service   Configure Additional Floating IP Subnet  Since hetzner root server only has one ip (/32), another subnet is needed to add additional floating ips. In this case hetzner will route traffic from additional subnet through to /32 ip of root server. Here we need to create a new OVS bridge (br-ex2) for additional subnet and patch it to existing bridge (br-ex).  Create Openvswitch Bridge.   # ovs-vsctl add-br br-ex2 Patch bridge br-ex2 to br-ex.  # ovs-vsctl add-port br-ex2 patch2-0 Note: ignore error about missing interface, that is expected since we creating patch and haven't created interface yet.  # ovs-vsctl set interface patch2-0 type=patch # ovs-vsctl set interface patch2-0 options:peer=\"patch0-2\" # ovs-vsctl add-port br-ex patch0-2 Note: ignore error about missing interface, that is expected since we creating patch and haven't created interface yet.  # ovs-vsctl set interface patch0-2 type=patch # ovs-vsctl set interface patch0-2 options:peer=\"patch2-0\" Update neutron bridge mappings.  We are adding a second subnet, as such a mapping is required to physical interface.  # vi /etc/neutron/plugins/ml2/openvswitch_agent.ini  bridge_mappings=extnet:br-ex,extnet2:br-ex2 Configure ifcfg script for br-ex2.   # vi /etc/sysconfig/network-scripts/ifcfg-br-ex2 DEVICE=br-ex2 BOOTPROTO=none ONBOOT=yes TYPE=OVSBridge DEVICETYPE=ovs USERCTL=yes PEERDNS=yes IPADDR=&lt;Hetzner /29 first usable IP&gt; NETMASK=255.255.255.248 SCOPE=\"peer &lt;Hetzner Gateway IP from Root Server&gt;\" IPV6INIT=no   Comment out default iptables REJECT rules.  By default iptables won't allow traffic from br-ex2 to br-ex.  # vi /etc/sysconfig/iptables --- #-A INPUT -j REJECT --reject-with icmp-host-prohibited #-A FORWARD -j REJECT --reject-with icmp-host-prohibited --- Restart iptables and networking.  # systemctl restart iptables; systemctl restart network  Configure OpenStack Environment  Nova Configuration  Nova uses filtering rules to find appropriate host when scheduling instances. In order for nova to recognize lvm storage the images type needs to be lvm and the volume group needs to be correct lvm volume. In addition, unless you want to wait really long when deleting nova volumes, set volume_clear to none.  # vi /etc/nova/nova.conf  [libvirt] --- images_type = lvm  volume_clear = none images_volume_group = cinder-volumes --- Restart Nova services.  # systemctl restart openstack-nova-compute # systemctl restart openstack-nova-api # systemctl restart openstack-nova-scheduler Cinder Configuration  By default the openstack install will configure an lvm volume group using loop. This is not ideal and why we created a volume group vg1. We will use the same setting volume_clear to none to ensure cinder volumes are deleted quickly.  Update cinder configuration.  # vi /etc/cinder/cinder.conf --- enabled_backends=lvm volume_clear = none  [lvm] volume_backend_name=lvm volume_driver=cinder.volume.drivers.lvm.LVMVolumeDriver iscsi_ip_address=144.76.52.111 iscsi_helper=lioadm volume_group=cinder-volumes volumes_dir=/var/lib/cinder/volumes --- Restart cinder services.  # systemctl restart openstack-cinder-volume # systemctl restart openstack-cinder-api   Magnum Configuration  # vi /etc/magnum/magnum.conf --- [cinder] default_docker_volume_type = iscsi ---   # systemctl restart openstack-magnum-conductor # systemctl restart openstack-magnum-api Ceilometer Configuration  Aodh is the database for alarms that are triggered based on things such as autoscaling policies. The database needs to be initialized after installing OpenStack.  # aodh-dbsync Neutron Configuration  Create private network.  # openstack network create private # openstack subnet create --network private --allocation-pool \\ start=10.10.1.100,end=10.10.1.200 --dns-nameserver 213.133.98.98 \\ --subnet-range 10.10.1.0/24 private_subnet Create public network.   Note: these steps assume the physical network connected to br-ex2 is 144.76.132.224/29.  # openstack network create --provider-network-type flat \\ --provider-physical-network extnet2 --external public # openstack subnet create --network public --allocation-pool \\ start=144.76.132.226,end=144.76.132.230 --no-dhcp \\ --subnet-range 144.76.132.224/29 public_subnet Add a new router and configure router interfaces.  # openstack router create --no-ha router1 # openstack router set --external-gateway public router1 # openstack router add subnet router1 private_subnet Check to ensure network connectivity is working.  This is done by checking the network namespace of the qrouter (openstack router).  # ip netns show qrouter-88dde0ef-22a2-44b1-baa9-304273653bb1 qdhcp-f1582f71-b531-43af-99c1-299b603232fc # ip netns exec qrouter-88dde0ef-22a2-44b1-baa9-304273653bb1 ping www.redhat.com Glance Configuration  Upload a glance image.   In this case we will use a Cirros image because it is small and thus good for testing OpenStack.  # curl -O http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img # openstack image create --disk-format qcow2 \\ --container-format bare --public \\ --file /root/cirros-0.3.4-x86_64-disk.img \"Cirros 0.3.4\" Create a new m1.nano flavor for running Cirros image.  # openstack flavor create --ram 64 --disk 0 --ephemeral 0 --vcpus 1 --public m1.nano Configure Security Groups  Create Security Group for all access.  # openstack security group create all \\ --description \"Allow all ports\" # openstack security group rule create --protocol TCP \\ --dst-port 1:65535 --remote-ip 0.0.0.0/0 all # openstack security group rule create --protocol ICMP \\ --remote-ip 0.0.0.0/0 all Create Security Group for base access.  # openstack security group create base \\ --description \"Allow base ports\" # openstack security group rule create --protocol TCP \\ --dst-port 22 --remote-ip 0.0.0.0/0 base # openstack security group rule create --protocol TCP \\ --dst-port 80 --remote-ip 0.0.0.0/0 base   # openstack security group rule create --protocol TCP \\ --dst-port 443 --remote-ip 0.0.0.0/0 base # openstack security group rule create --protocol ICMP \\ --remote-ip 0.0.0.0/0 base   Create Private Key.  # openstack keypair create admin Save Private Key to file.  # vi /root/admin.pem -----BEGIN RSA PRIVATE KEY----- MIIEowIBAAKCAQEAwTrb+xdbpgY8hVOmftBIShqYUgXXDC/1gggakq8bkEdNnSku IaNGeJykzksjdksjd9383iejkjsu92wiwsajFLuE2lkh5dvk9s6hpfE/3UvSGk6m HWIMCf3nJUv8gGCM/XElgwNXS02c8pHWUywBiaQZpOsvjCqFLGW0cNZLAQ+yzrh1 dIWddx/E1Ppto394ejfksjdksjdksdhgu4t39393eodNlVQxWzmK4vrLWNrfioOK uRxjxY6jnE3q/956ie69BXbbvrZYcs75YeSY7GjwyC5yjWw9qkiBcV1+P1Uqs1jG 1yV0Zvl5xlI1M4b97qw0bgpjTETL5+iuidFPVwIDAQABAoIBAF7rC95m1fVTQO15 buMCa1BDiilYhw+Mi3wJgQwnClIwRHb8IJYTf22F/QptrrBd0LZk/UHJhekINXot z0jJ+WvtxVAA0038jskdjskdjksjksjkiH9Mh39tAtt2XR2uz/M7XmLiBEKQaJVb gD2w8zxqqNIz3438783787387s8s787s8sIAkP3ZMAra1k7+rY1HfCYsRDWxhqTx R5FFwYueMIldlfPdGxwd8hLrqJnDY7SO85iFWv5Kf1ykyi3PRA6r2Vr/0PMkVsKV XfxhYPkAOb2hNKRDhkvZPmmxXu5wy8WkGeq+uTWRY3DoyciuC4xMS0NMd6Y20pfp x50AhJkCgYEA8M2OfUan1ghV3V8WsQ94vguHqe8jzLcV+1PV2iTwWFBZDZEQPokY JkMCAtHFvUlcJ49yAjrRH6O+EGT+niW8xIhZBiu6whOd4H0xDoQvaAAZyIFoSmbX 2WpS74Ms5YSzVip70hbcXb4goDhdW9YxvTVqJlFrsGNCEa3L4kr2qFMCgYEAzWy0 5cSHkCWaygeYhFc79xoTnPxKZH+QI32dAeud7oyZtZeZDRyjnm2fEtDCEn6RtFTH NlI3W6xFkXcp1u0wbmYJJVZdn1u9aRsLzVmfGwEWGHYEfZ+ZtQH+H9XHXsi1nPpr Uy7Msd,sl,.swdko393j495u4efdjkfjdkjfhflCgYEA7VO6Xo/XdKPMdJx2EdXM y4kzkPFHGElN2eE7gskjdksjdksjkasnw33a23433434wk0P8VCksQlBlojjRVyu GgjDrMhGjWamEA1y3vq6eka3Ip0f+0w26mnXCYYAJslNstu2I04yrBVptF846/1E ElXlo5RVjYeWIzRmIEZ/qU8CgYB91kOSJKuuX3rMm46QMyfmnLC7D8k6evH+66nM 238493ijsfkjalsdjcws9fheoihg80eWDSAFDOASDF=OIA=FSLoiidsiisiisNDo ACh40FeKsHDby3LK8OeM9NXmeCjYeoZYNGimHForiCCT+rIniiu2vy0Z/q+/t3cM BgmAmQKBgCwCTX5kbLEUcX5IE6Nzh+1n/lkvIqlblOG7v0Y9sxKVxx4R9uXi3dNK 6pbclskdksdjdk22k2jkj2kalksx2koUeLzwHuRUpMavRhoTLP0YsdbQrjgHIA+p kDNrgFz+JYKF2K08oe72x1083RtiEr8n71kjSA+5Ua1eNwGI6AVl -----END RSA PRIVATE KEY----- # chmod 400 /root/admin.pem Start an Instance  Get Private Network Id.  # openstack network list +--------------------------------------+---------+--------------------------------------+ | ID | Name | Subnets | +--------------------------------------+---------+--------------------------------------+ | 72494c88-6d93-4eb7-929f-383cbedfa3e7 | public | a57856be-a22b-4a48-b3c3-75be46a4c477 | | 781e062d-5ab6-4ae8-a54e-8e72291df37e | private | b00316a1-812f-423a-8aca-bd6547692ad3 | +--------------------------------------+---------+--------------------------------------+ Create a mycirros Instance.  # openstack server create --flavor m1.nano --image \"Cirros 0.3.4\" \\ --nic net-id=781e062d-5ab6-4ae8-a54e-8e72291df37e --key-name admin \\ --security-group all mycirros # # openstack server list +--------------------------------------+----------+--------+---------------------+--------------+---------+ | ID | Name | Status | Networks | Image | Flavor | +--------------------------------------+----------+--------+---------------------+--------------+---------+ | 6f2719bb-f393-49ab-a409-1c4a0f992b2d | mycirros | ACTIVE | private=10.10.1.105 | Cirros 0.3.4 | m1.nano | +--------------------------------------+----------+--------+---------------------+--------------+---------+ Create Floating IP.  # openstack floating ip create public +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | created_at | 2018-01-12T09:52:40Z | | description | | | fixed_ip_address | None | | floating_ip_address | 144.76.132.230 | | floating_network_id | 72494c88-6d93-4eb7-929f-383cbedfa3e7 | | id | 7a7ac84f-57ee-4982-b2f5-35ebb9656b34 | | name | 144.76.132.230 | | port_id | None | | project_id | 92d3bc57ca504eaab4c29d3509064004 | | revision_number | 0 | | router_id | None | | status | DOWN | | updated_at | 2018-01-12T09:52:40Z | +---------------------+--------------------------------------+  Get Port Id of Instance.  # openstack port list +--------------------------------------+------+-------------------+-------------------------------------------------------------------------------+--------+ | ID | Name | MAC Address | Fixed IP Addresses | Status | +--------------------------------------+------+-------------------+-------------------------------------------------------------------------------+--------+ | 60407eee-c9b2-4e9c-81a6-5c38bb536c9b | | fa:16:3e:37:ac:2f | ip_address='144.76.132.227', subnet_id='a57856be-a22b-4a48-b3c3-75be46a4c477' | ACTIVE | | 717660b6-b9b0-4f76-8c63-f9fae5c9bd9b | | fa:16:3e:0c:14:30 | ip_address='10.10.1.105', subnet_id='b00316a1-812f-423a-8aca-bd6547692ad3' | ACTIVE | | 878ee68a-1b88-48d1-9bf7-057f59b833bb | | fa:16:3e:2a:1f:50 | ip_address='144.76.132.230', subnet_id='a57856be-a22b-4a48-b3c3-75be46a4c477' | N/A | | 8a7b6b21-1eb2-4750-b854-707462d8b38f | | fa:16:3e:53:c9:ef | ip_address='10.10.1.1', subnet_id='b00316a1-812f-423a-8aca-bd6547692ad3' | ACTIVE | | b40af80e-2794-4ca5-8141-9d8ad4e9c9f2 | | fa:16:3e:51:d9:cb | ip_address='10.10.1.100', subnet_id='b00316a1-812f-423a-8aca-bd6547692ad3' | ACTIVE | +--------------------------------------+------+-------------------+-------------------------------------------------------------------------------+--------+ Assign Floating IP to Instance Port.  # openstack floating ip set --port 717660b6-b9b0-4f76-8c63-f9fae5c9bd9b 144.76.132.230  Verify Floating IP in OpenStack Router.  # ip netns show qdhcp-dcfbabbd-c5d2-444c-ab60-546216550118 router-0f00050f-6590-42df-9136-32d22fea4ece # ip netns exec qrouter-0f00050f-6590-42df-9136-32d22fea4ece ip a --- 9: qg-60407eee-c9: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN qlen 1000  link/ether fa:16:3e:37:ac:2f brd ff:ff:ff:ff:ff:ff  inet 144.76.132.227/29 brd 144.76.132.231 scope global qg-60407eee-c9  valid_lft forever preferred_lft forever  inet 144.76.132.230/32 brd 144.76.132.230 scope global qg-60407eee-c9  valid_lft forever preferred_lft forever  inet6 fe80::f816:3eff:fe37:ac2f/64 scope link  valid_lft forever preferred_lft forever --- Test Instance.  Connect to mycirros instance using the private ssh key stored in the admin.pem file.  # ssh -i admin.pem cirros@144.76.132.230 $ ping www.redhat.com PING www.redhat.com (172.217.21.14): 56 data bytes 64 bytes from 172.217.21.14: seq=0 ttl=54 time=37.692 ms 64 bytes from 172.217.21.14: seq=1 ttl=54 time=27.758 ms 64 bytes from 172.217.21.14: seq=2 ttl=54 time=25.640 ms Install OpenStack Client  To administer OpenStack remotely the CLI tools are important. Below are steps to install OpenStack CLI tools for Queens.  [RHEL]  # subscription-manager repos --enable=rhel-7-server-openstack-12-tools-rpms # yum install -y python-openstackclient # yum install -y python-heatclient [CentOs]  # yum install -y https://rdoproject.org/repos/openstack-queens/rdo-release-queens-1.noarch.rpm # yum upgrade -y # yum install -y python-openstackclient # yum install -y python-heatclient Summary  This article was intended as a hands on guide for standing up an OpenStack Queens lab environment using RDO. In this guide we also tailored the environment to the Hetzner root server. Things can of course vary depending on your hardware platform and provider. Certainly the aim of this guide was to provide a more realistic deployment scenario for OpenStack. As mentioned RDO is a stable community platform built around Red Hat’s OpenStack Platform. It provides the ability to test the latest OpenStack features against either an enterprise platform (RHEL) or community platform (CentOS). Hopefully you found the information in this article useful. If you have anything to add or feedback, feel free to leave your comments.  Happy OpenStacking!  (c) 2018 Keith Tenzer    ","categories": ["OpenStack"],
        "tags": ["Cloud Computing","Hetzner","IaaS","KVM","Linux","OpenStack","Queens","RDO"],
        "url": "/openstack/openstack-13-queens-lab-installation-and-configuration-guide-for-hetzner-root-servers/",
        "teaser": null
      },{
        "title": "The Birth of the Niche Cloud?",
        "excerpt":"  source: https://en.wikipedia.org/wiki/Ecological_niche   Part I: Birth of the Niche Cloud? Part II: Building a Niche Cloud: A Pragmatic Approach  Overview  In this article we will take a step back from my typical technical discussions and how-to guides to think about the path that lies ahead of us, in our industry. Today we have a very polarizing environment, similar in fact to the US political system.  On one side you have extremely customized on-premise environments that serve specific purpose or business niches but on the whole are hard, if not impossible to maintain and very costly. On the other side you have generic public cloud, infrastructure that always works (well almost, nothing is perfect), scales and is available at click of a button with predictable pricing structure but doesn't fit specific purpose by default.  The industry has for many years recognized these worlds were growing further apart and defined the solution as hybrid cloud management to manage them or even bridge the worlds. But gluing two polarized worlds together was only ever destined to fail. Hybrid cloud, which often drags with it the management, is dead. I think more are interested in talking about multi-cloud and I see multi-cloud replacing what hybrid cloud once stood for. I see container platforms providing the glue, management platforms once promised.  The same battle purpose built vs generic repeats itself over and over in all facets of our lives. The truth is we need both. In this article we will explore a new thought or idea, called niche clouds which could provide a purpose built cloud of the future.    What is a niche cloud?  In short a cloud built for purpose composed of multiple generic clouds.  Those generic clouds are likely a combination of Google, Microsoft, Amazon and maybe on-premise clouds or those from third parties. A niche cloud would provide services to a market segment or business vertical such as retail, energy, travel, logistics, insurance, banking, manufacturing and so on. It would not only provide the business processes and methodology but also security, certification and regulatory needs those industries require.  I believe in the future organizations will get their IT from niche clouds not generic ones, probably most will be public. I believe the current public clouds will provide the basis for building such niche clouds.    Who will build niche clouds?  The market segment leaders. The knowledge to build the niche clouds is in the business verticals. They understand their business processes and requirements much better than one of the generic cloud providers. If you want infrastructure for a Bank for example, image a cloud that was not only certified but also met all regulatory compliance banks need while also providing capabilities or features tailored to banks.  The case for niche clouds  Again, today we have highly customized, hard to maintain, on-premise infrastructure environments and highly generic public clouds.  The choices and experiences couldn't be more extreme.  Looking into the future, I think the generic cloud is pretty much saturated and done very well by Amazon, Google and Microsoft. If you aren't them and you are doing a generic cloud, you are in the wrong business. Hence the move to purpose built or niche clouds.  The question is what happens to the highly customized on-premise environments? We all know they aren't sustainable, scalable or maintainable in their current form. But the purpose they provide is of value. Logically speaking we also know the public clouds can only ever be generic, serving the masses. They don't have the deep business understanding and knowledge to add market segment purpose to their clouds even if they wanted that. I am convinced business operations can be provided as-a-service and what history tells us, anything that can be provided as-a-service, will be.  If you follow my argument then the natural conclusion is someone will build purpose built services for specific market segments and those services will be public and consumed by others. The only question is how fast will this happen and who will do it?  Three Steps to start niche cloud journey?  Step One: Everything built with an API and external service consumption in mind.  The good news here, this is already happening, just not enough. Fortunately Jeff Bezos provided us a mandate on how to transition to an everything-as-a-service company. Let's face it, Amazon didn't get to where they are by accident. Many might question, well it is probably too late. No it isn't, very few companies in the world have been able to do this, likely because of their legacy and the inability to abstract it away. Then again Amazon also has legacy by now.  Jeff Bezos mandate from 2002.   All teams will henceforth expose their data and functionality through service interfaces. Teams must communicate with each other through these interfaces. There will be no other form of interprocess communication allowed: no direct linking, no direct reads of another team’s data store, no shared-memory model, no back-doors whatsoever. The only communication allowed is via service interface calls over the network. It doesn’t matter what technology they use. HTTP, Corba, Pubsub, custom protocols — doesn’t matter. All service interfaces, without exception, must be designed from the ground up to be externalizable. That is to say, the team must plan and design to be able to expose the interface to developers in the outside world. No exceptions. Anyone who doesn’t do this will be fired. Thank you; have a nice day!  Step two: Start small but determine your purpose, value, consumer, build it from what already exists in public cloud, start over on-premise and put everything in containers.  The easiest thing to do is start leveraging public clouds, since they already provide basic infrastructure at scale. It may also be desired to have an on-premise cloud. If so that effort should be done in parallel and should be done from scratch. OpenStack is the only technology that to me makes any sense on-premise. Maybe even a scaled down OpenStack, just consisting of Ironic, Nova and few other core services allowing you to provision metal. On top of all cloud platforms: public or on-premise, run a container platform and everything inside containers. Here you will definitely want a consistent K8s, that is available across all clouds. This will allow consistent deployment experience for all applications regardless of cloud and also abstract infrastructure away entirely.  Step three: Win the war for talent, or at least don't lose it!  Those bright young engineers you so badly want, are unfortunately smart enough to know, you don't change an oil tanker into a speed boat. You build a fleet of speed boats instead. This gives further credence to starting over. It took me a while to come to grips that existing IT environments, legacy, needs to just rot away in the corner somewhere until being decommissioned. If you try to make your ugly Datacenter beautiful, you will be doing it mostly with archaeologists, that see beauty, in what we all know is ugly. This means making waves, not all with follow the vision, many won't and many won't want to change. But what choice is there really? Keep on doing what we are doing all leads to same place, only now it is so much closer.  Summary  In this article we discussed where our industry may be headed. Public clouds are building blocks and should, provide consistent infrastructure layers where we need it, when we need it. But, in future most likely will consume purpose built clouds, an additional layer that differentiates clouds, along possibly market segments. Today most organizations are at a crossroads. They have what they have on-premise and are trying to adapt public cloud. Organizations need to address and start driving toward the future. Most of what exists today on-premise clearly needs to be maintained but a new platform must be built, there is no hope in some bridge or magic that will take what exists and make it future-ready. My advice is re-platform, build new applications or services and don't re-write the existing ones, instead abstract them away into obsoleteness, as is possible. Organizations should build everything as-a-service or not build anything at all. Application should all run in containers, regardless of big or small. Hopefully you enjoyed this article, please share your views and suggestions?  (c) 2018 Keith Tenzer  ","categories": ["Cloud"],
        "tags": ["Cloud","future","niche cloud","public cloud","technology"],
        "url": "/cloud/the-birth-of-the-niche-cloud/",
        "teaser": null
      },{
        "title": "OpenStack 14 (Rocky) Lab Installation and Configuration Guide for Hetzner Root Servers",
        "excerpt":"  Overview   In this article we will focus on installing and configuring OpenStack Rocky using RDO and the packstack installer. RDO is a community platform around Red Hat’s Enterprise OpenStack Distribution. It allows you to test the latest OpenStack capabilities on a stable platform such as Red Hat Enterprise Linux (RHEL) or CentOS. This guide will take you through setting up Hetzner root server, preparing environment for OpenStack, installing the OpenStack Rocky release, adding a floating ip subnet through OVS, configuring networking, security groups, flavors, images and are other OpenStack related services. The outcome is a working OpenStack environment based on the Rocky release that you can use as a baseline for testing your applications using OpenStack capabilities. The installation will create an all-in-one deployment however you can use this guide to create a multi-node deployment as well.    Root Server Specs  CPU: Intel(R) Xeon(R) CPU E5-1650 v3 @ 3.50GHz (6 cores) Memory: 128GB RAM Disk: 2 x 480GB SSD Network: 1Gbit IPV4 Adresses: 1 x IPV4 + /29 Subnet (6 IPs) Hetzner is a hosting providing and auctions off used hardware for very competitive prices on a per/month basis (https://www.hetzner.de/sb). Hetzner provides one IP for each root server that is accessible via internet. It is a /32 so if you want additional IPs, like you would for OpenStack to use as floating ips you need to order additional subnet. You can order an additional subnet as I have done. Hetzner will route that subnet through the host IP of the /32. This requires creating an additional OVS bridge in OpenStack and this guide will go through that configuration.  Configure Root Server  As mentioned, Hetzner will give you access to your root server via IP and provide ability to manage the root server. Basic things like put root server in rescue mode or reboot. You can provide public ssh key or use password to access system. Please don't use password, this is the internet, bad things can and will happen.  Enter Rescue Mode  In order to install OS or repair it you need to get into rescue mode.        Configure Hetzner Firewall.  While server is rebootting you can modify your servers firewall rules for incoming traffic. By default firewall allows port 22 for SSH and nothing else. Firewall settings can be modified under your server's settings in Hetzner's web UI https://robot.your-server.de/server. For OpenStack I would recommend 80, 443, 22 and ICMP.    If you plan on running OpenShift on OpenStack you need to add some additional rules.    Connect to root server via ssh and private key.  # ssh -i .ssh/id_rsa.pub Create Root Server Configuration.  The root server in this case has two disks. They are being configured in non-redundant RAID 1 (stripping) configuration. OpenStack requires decent disk performance, I found when using mirroring it was not enough. If you want redundant configuration or decent performance, you need to pay extra for SSDs. Since we will setup LVM partitions, creating one for Cinder (cinder-volumes) is a good idea. You don't want to use loop back driver for Cinder. Finally you can provide your own image (if you want to use RHEL, you must do so).  My colleague Andy Neeb also automated deployment using Hetzner API. If you are interested in automated deployment here is his Github repository: https://github.com/andyneeb/ansible-hetzner.  [CENTOS]  # vi config.txt  DRIVE1 /dev/sda DRIVE2 /dev/sdb SWRAID 1 SWRAIDLEVEL 0 BOOTLOADER grub HOSTNAME myrootserver.lab PART /boot ext3 512M PART lvm vg0 150G  LV vg0 root / ext4 50G LV vg0 swap swap swap 8G LV vg0 var /var ext4 10G LV vg0 tmp /tmp ext4 30G  IMAGE /root/.oldroot/nfs/install/../images/CentOS-75-64-minimal.tar.gz [RHEL]  Note: You need to provide your own RHEL image.  # vi config.txt  DRIVE1 /dev/sda  DRIVE2 /dev/sdb  SWRAID 1  SWRAIDLEVEL 0  BOOTLOADER grub  HOSTNAME myrootserver.lab  PART /boot ext3 512M  PART lvm vg0 150G   LV vg0 root / ext4 50G  LV vg0 swap swap swap 8G  LV vg0 var /var ext4 10G  LV vg0 tmp /tmp ext4 30G  IMAGE http://path/to/rhel/image Install image and create partitions.  # installimage -a -c config.txt    Note: you may see error with RHEL image if you renamed it to CentOS because Hetzner expects CentOS but it is safe to ignore.  Hetzner Online GmbH - installimage  Your server will be installed now, this will take some minutes You can abort at any time with CTRL+C ...  : Reading configuration done  : Loading image file variables done  : Loading centos specific functions done  1/18 : Deleting partitions done  2/18 : Test partition size done  3/18 : Creating partitions and /etc/fstab done  4/18 : Creating software RAID level 0 done  5/18 : Creating LVM volumes done  6/18 : Formatting partitions : formatting /dev/md/0 with ext3 done  : formatting /dev/vg0/root with ext4 done  : formatting /dev/vg0/swap with swap done  : formatting /dev/vg0/var with ext4 done  : formatting /dev/vg0/tmp with ext4 done  : formatting /dev/vg0/home with ext4 done  7/18 : Mounting partitions done  8/18 : Sync time via ntp done  9/18 : Downloading image (http) done  : Importing public key for image validation done  10/18 : Validating image before starting extraction warn  : No detached signature file found! 11/18 : Extracting image (http) done  12/18 : Setting up network config done  13/18 : Executing additional commands : Setting hostname done  : Generating new SSH keys done  : Generating mdadm config done  : Generating ramdisk done  : Generating ntp config done  14/18 : Setting up miscellaneous files done  15/18 : Configuring authentication : Fetching SSH keys done  : Disabling root password done  : Disabling SSH root login without password done  : Copying SSH keys done  16/18 : Installing bootloader grub done  17/18 : Running some centos specific functions failed  An error occured while installing the new system! See the debug file /root/debug.txt for details.   Reboot. # reboot now   Disable password authentication. Remember this system is accessible from the internet. Unless you want people to constantly try and login, disable password authentication. # vi /etc/ssh/sshd_config PasswordAuthentication no  Restart sshd. # systemctl restart sshd  Create Cinder Volumes LVM Pool  Create third partition on both disks using the remaing space available. Choose type FD (Linux Raid Autodetect). You can use fdisk or cfdisk.  # cfdisk /dev/sda # cfdisk /dev/sdb Detect new disks  # partprobe Create a raid 0 (stripe) using the newly created partitions  # mdadm --create --verbose /dev/md2 --level=0 --raid-devices=2 \\ /dev/sda3 /dev/sdb3 Create Physical Volume, Volume Group and Thin Pool  # pvcreate /dev/md2 # vgcreate cinder-volumes /dev/md2 # lvcreate -L 300G -T cinder-volumes/cinder-volumes-pool Verify LVM Volume Groups  Make sure a volume group cinder-volumes exists. This will be used for OpenStack storage as mentioned.  # vgs   VG             #PV #LV #SN Attr   VSize    VFree   cinder-volumes   1   1   0 wz--n- &lt;743.01g &lt;442.86g   vg0              1   4   0 wz--n-  149.87g   51.87g # lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert cinder-volumes-pool cinder-volumes twi-a-tz-- 300.00g 0.00 0.24 root vg0 -wi-ao---- 50.00g swap vg0 -wi-ao---- 8.00g tmp vg0 -wi-ao---- 30.00g var vg0 -wi-ao---- 10.00g Remove Linux Raid Device (Only if you want to re-create cinder-volumes)  In case something went wrong and you need to re-create the linux raid device. It can be tricky to remove since it writes raid information to the disk partition.  Remove LVM Volume  # vgremove cinder-volumes Stop raid device  # mdadm --stop /dev/md2 Zero raid disks # mdadm --zero-superblock /dev/sda3 # mdadm --zero-superblock /dev/sdb3 Install OpenStack  Ensure local name resolution is working.  # vi /etc/hosts 144.76.52.111 myrootserver.lab Set hostname.  # hostnamectl set-hostname myrootserver.lab Enable RPMs.  [RHEL]  # subscription-manager register # subscription-manager list --available # subscription-manager attach --pool= # subscription-manager repos --disable=* # subscription-manager repos --enable=rhel-7-server-rpms  # subscription-manager repos --enable=rhel-7-server-rh-common-rpms # subscription-manager repos --enable=rhel-7-server-extras-rpms # subscription-manager repos --enable=rhel-7-server-openstack-14-rpms # subscription-manager repos --enable=rhel-7-server-openstack-14-devtools-rpms Install packstack packages.  [CentOS]  # yum install -y centos-release-openstack-rocky [Both]  # yum install -y openstack-packstack [RHEL]  Disable firewalld, OpenStack uses iptables.  # systemctl disable firewalld # systemctl stop firewalld Disable NetworkManager.  # systemctl stop NetworkManager # systemctl disable NetworkManager [Both]  Install yum-utils and update the system.  # yum install -y yum-utils # yum update -y Configure Intel Virtualization for Directed I/O.  # vi /etc/default/grub --- GRUB_CMDLINE_LINUX=\"biosdevname=0 crashkernel=auto nomodeset rd.auto=1 consoleblank=0 intel_iommu=on\" --- # grub2-mkconfig -o /boot/grub2/grub.cfg Reboot.  # systemctl reboot Create packstack answers file for customizing the installer.  # packstack --gen-answer-file /root/answers.txt Update the packstack answers file and enable other OpenStack services.  # vi /root/answers.txt --- CONFIG_NTP_SERVERS=0.pool.ntp.org,1.pool.ntp.org,2.pool.ntp.org CONFIG_CONTROLLER_HOST=144.76.52.111 CONFIG_COMPUTE_HOSTS=144.76.52.111 CONFIG_NETWORK_HOSTS=144.76.52.111 CONFIG_STORAGE_HOST=144.76.52.111 CONFIG_KEYSTONE_ADMIN_PW=redhat CONFIG_PROVISION_DEMO=n CONFIG_HEAT_INSTALL=y CONFIG_HEAT_CFN_INSTALL=y CONFIG_CEILOMETER_INSTALL=y CONFIG_MAGNUM_INSTALL=y CONFIG_LBAAS_INSTALL=y CONFIG_CINDER_VOLUMES_CREATE=n CONFIG_NOVA_SCHED_RAM_ALLOC_RATIO=3.0 CONFIG_NOVA_LIBVIRT_VIRT_TYPE=kvm CONFIG_HORIZON_SSL=y --- Note: Regarding HORIZON_SSL. If you enable SSL it won't work with chrome unless you create a self-signed cert with subjectAltNames, chrome now blocks common self-signed certs.  Install OpenStack using packstack.  # packstack --answer-file /root/answers.txt --timeout=1500   **** Installation completed successfully ******  Additional information:  * NOTE : A selfsigned CA certificate was generated to be used for ssl, you should still change it do subordinate CA cert. In any case please save the contents of /root/packstackca/.  * File /root/keystonerc_admin has been created on OpenStack client host 144.76.56.119. To use the command line tools you need to source the file.  * NOTE : A certificate was generated to be used for ssl, You should change the ssl certificate configured in /etc/httpd/conf.d/ssl.conf on 144.76.56.119 to use a CA signed cert.  * To access the OpenStack Dashboard browse to https://144.76.56.119/dashboard . Please, find your login credentials stored in the keystonerc_admin in your home directory.  * The installation log file is available at: /var/tmp/packstack/20180714-072935-AsKJdI/openstack-setup.log  * The generated manifests are available at: /var/tmp/packstack/20180714-072935-AsKJdI/manifests Configure Physical Network  Source the keystone admin profile.  # . /root/keystonerc_admin Backup the ifcfg-etho script.  # cp /etc/sysconfig/network-scripts/ifcfg-eno1 /root/ Configure external bridge br-ex.  # vi /etc/sysconfig/network-scripts/ifcfg-eno1 DEVICE=eno1 ONBOOT=yes TYPE=OVSPort DEVICETYPE=ovs OVS_BRIDGE=br-ex # vi /etc/sysconfig/network-scripts/ifcfg-br-ex DEVICE=br-ex BOOTPROTO=none ONBOOT=yes TYPE=OVSBridge DEVICETYPE=ovs USERCTL=yes PEERDNS=yes IPV6INIT=no IPADDR=&lt;Hetner Root Server IP&gt; NETMASK=255.255.255.255 SCOPE=\"peer &lt;Hetzner Gateway IP&gt;\" Switch static route to br-ex.  Note: this is specific to Hetzner environment as the physical host will get a /32.  # mv /etc/sysconfig/network-scripts/route-eno1 /etc/sysconfig/network-scripts/route-br-ex  Add the eno1 physical interface to the br-ex bridge in openVswitch.  Note: this is point of no return!!! Double check your config, if there is typo you will need to got to rescue mode.  # ovs-vsctl add-port br-ex eno1; systemctl restart network.service   Configure Additional Floating IP Subnet  Since hetzner root server only has one ip (/32), another subnet is needed to add additional floating ips. In this case hetzner will route traffic from additional subnet through to /32 ip of root server. Here we need to create a new OVS bridge (br-ex2) for additional subnet and patch it to existing bridge (br-ex).  Create Openvswitch Bridge.   # ovs-vsctl add-br br-ex2 Patch bridge br-ex2 to br-ex.  # ovs-vsctl add-port br-ex2 patch2-0 Note: ignore error about missing interface, that is expected since we creating patch and haven't created interface yet.  # ovs-vsctl set interface patch2-0 type=patch # ovs-vsctl set interface patch2-0 options:peer=\"patch0-2\" # ovs-vsctl add-port br-ex patch0-2 Note: ignore error about missing interface, that is expected since we creating patch and haven't created interface yet.  # ovs-vsctl set interface patch0-2 type=patch # ovs-vsctl set interface patch0-2 options:peer=\"patch2-0\" Update neutron bridge mappings.  We are adding a second subnet, as such a mapping is required to physical interface.  # vi /etc/neutron/plugins/ml2/openvswitch_agent.ini  bridge_mappings=extnet:br-ex,extnet2:br-ex2 Configure ifcfg script for br-ex2.   # vi /etc/sysconfig/network-scripts/ifcfg-br-ex2 DEVICE=br-ex2 BOOTPROTO=none ONBOOT=yes TYPE=OVSBridge DEVICETYPE=ovs USERCTL=yes PEERDNS=yes IPADDR=&lt;Hetzner /29 first usable IP&gt; NETMASK=255.255.255.248 SCOPE=\"peer &lt;Hetzner Gateway IP from Root Server&gt;\" IPV6INIT=no   Comment out default iptables REJECT rules.  By default iptables won't allow traffic from br-ex2 to br-ex.  # vi /etc/sysconfig/iptables --- #-A INPUT -j REJECT --reject-with icmp-host-prohibited #-A FORWARD -j REJECT --reject-with icmp-host-prohibited --- Restart iptables and networking.  # systemctl restart iptables; systemctl restart network  Configure OpenStack Environment  Nova Configuration  Nova uses filtering rules to find appropriate host when scheduling instances. In order for nova to recognize lvm storage the images type needs to be lvm and the volume group needs to be correct lvm volume. In addition, unless you want to wait really long when deleting nova volumes, set volume_clear to none.  # vi /etc/nova/nova.conf  [libvirt] --- images_type = lvm  volume_clear = none images_volume_group = cinder-volumes --- Restart Nova services.  # systemctl restart openstack-nova-compute # systemctl restart openstack-nova-api # systemctl restart openstack-nova-scheduler Cinder Configuration  By default the openstack install will configure an lvm volume group using loop. This is not ideal and why we created a volume group vg1. We will use the same setting volume_clear to none to ensure cinder volumes are deleted quickly.  Update cinder configuration.  # vi /etc/cinder/cinder.conf --- enabled_backends=lvm volume_clear = none  [lvm] volume_backend_name=lvm volume_driver=cinder.volume.drivers.lvm.LVMVolumeDriver iscsi_ip_address=144.76.52.111 iscsi_helper=lioadm volume_group=cinder-volumes volumes_dir=/var/lib/cinder/volumes --- Restart cinder services.  # systemctl restart openstack-cinder-volume # systemctl restart openstack-cinder-api Magnum Configuration  # vi /etc/magnum/magnum.conf --- auth_uri=http://localhost:5000/v3 nodes_affinity_policy = soft-anti-affinity default_docker_volume_type = iscsi ---   # systemctl restart openstack-magnum-conductor # systemctl restart openstack-magnum-api   Ceilometer Configuration  Aodh is the database for alarms that are triggered based on things such as autoscaling policies. The database needs to be initialized after installing OpenStack.  # aodh-dbsync Neutron Configuration  Create private network.  # openstack network create private # openstack subnet create --network private --allocation-pool \\ start=10.10.1.100,end=10.10.1.200 --dns-nameserver 213.133.98.98 \\ --subnet-range 10.10.1.0/24 private_subnet Create public network.   Note: these steps assume the physical network connected to br-ex2 is 144.76.132.224/29.  # openstack network create --provider-network-type flat \\ --provider-physical-network extnet2 --external public # openstack subnet create --network public --allocation-pool \\ start=144.76.132.226,end=144.76.132.230 --no-dhcp \\ --subnet-range 144.76.132.224/29 public_subnet Add a new router and configure router interfaces.  # openstack router create --no-ha router1 # openstack router set --external-gateway public router1 # openstack router add subnet router1 private_subnet Check to ensure network connectivity is working.  This is done by checking the network namespace of the qrouter (openstack router).  # ip netns show qrouter-88dde0ef-22a2-44b1-baa9-304273653bb1 qdhcp-f1582f71-b531-43af-99c1-299b603232fc # ip netns exec qrouter-88dde0ef-22a2-44b1-baa9-304273653bb1 ping www.redhat.com Glance Configuration  Upload a glance image.   In this case we will use a Cirros image because it is small and thus good for testing OpenStack.  # curl -O http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img # openstack image create --disk-format qcow2 \\ --container-format bare --public \\ --file /root/cirros-0.3.4-x86_64-disk.img \"Cirros 0.3.4\" Create a new m1.nano flavor for running Cirros image.  # openstack flavor create --ram 64 --disk 0 --ephemeral 0 --vcpus 1 --public m1.nano Configure Security Groups  Create Security Group for all access.  # openstack security group create all \\ --description \"Allow all ports\" # openstack security group rule create --protocol TCP \\ --dst-port 1:65535 --remote-ip 0.0.0.0/0 all # openstack security group rule create --protocol ICMP \\ --remote-ip 0.0.0.0/0 all Create Security Group for base access.  # openstack security group create base \\ --description \"Allow base ports\" # openstack security group rule create --protocol TCP \\ --dst-port 22 --remote-ip 0.0.0.0/0 base # openstack security group rule create --protocol TCP \\ --dst-port 80 --remote-ip 0.0.0.0/0 base   # openstack security group rule create --protocol TCP \\ --dst-port 443 --remote-ip 0.0.0.0/0 base # openstack security group rule create --protocol ICMP \\ --remote-ip 0.0.0.0/0 base   Create Private Key.  # openstack keypair create admin Save Private Key to file.  # vi /root/admin.pem -----BEGIN RSA PRIVATE KEY----- MIIEowIBAAKCAQEAwTrb+xdbpgY8hVOmftBIShqYUgXXDC/1gggakq8bkEdNnSku IaNGeJykzksjdksjd9383iejkjsu92wiwsajFLuE2lkh5dvk9s6hpfE/3UvSGk6m HWIMCf3nJUv8gGCM/XElgwNXS02c8pHWUywBiaQZpOsvjCqFLGW0cNZLAQ+yzrh1 dIWddx/E1Ppto394ejfksjdksjdksdhgu4t39393eodNlVQxWzmK4vrLWNrfioOK uRxjxY6jnE3q/956ie69BXbbvrZYcs75YeSY7GjwyC5yjWw9qkiBcV1+P1Uqs1jG 1yV0Zvl5xlI1M4b97qw0bgpjTETL5+iuidFPVwIDAQABAoIBAF7rC95m1fVTQO15 buMCa1BDiilYhw+Mi3wJgQwnClIwRHb8IJYTf22F/QptrrBd0LZk/UHJhekINXot z0jJ+WvtxVAA0038jskdjskdjksjksjkiH9Mh39tAtt2XR2uz/M7XmLiBEKQaJVb gD2w8zxqqNIz3438783787387s8s787s8sIAkP3ZMAra1k7+rY1HfCYsRDWxhqTx R5FFwYueMIldlfPdGxwd8hLrqJnDY7SO85iFWv5Kf1ykyi3PRA6r2Vr/0PMkVsKV XfxhYPkAOb2hNKRDhkvZPmmxXu5wy8WkGeq+uTWRY3DoyciuC4xMS0NMd6Y20pfp x50AhJkCgYEA8M2OfUan1ghV3V8WsQ94vguHqe8jzLcV+1PV2iTwWFBZDZEQPokY JkMCAtHFvUlcJ49yAjrRH6O+EGT+niW8xIhZBiu6whOd4H0xDoQvaAAZyIFoSmbX 2WpS74Ms5YSzVip70hbcXb4goDhdW9YxvTVqJlFrsGNCEa3L4kr2qFMCgYEAzWy0 5cSHkCWaygeYhFc79xoTnPxKZH+QI32dAeud7oyZtZeZDRyjnm2fEtDCEn6RtFTH NlI3W6xFkXcp1u0wbmYJJVZdn1u9aRsLzVmfGwEWGHYEfZ+ZtQH+H9XHXsi1nPpr Uy7Msd,sl,.swdko393j495u4efdjkfjdkjfhflCgYEA7VO6Xo/XdKPMdJx2EdXM y4kzkPFHGElN2eE7gskjdksjdksjkasnw33a23433434wk0P8VCksQlBlojjRVyu GgjDrMhGjWamEA1y3vq6eka3Ip0f+0w26mnXCYYAJslNstu2I04yrBVptF846/1E ElXlo5RVjYeWIzRmIEZ/qU8CgYB91kOSJKuuX3rMm46QMyfmnLC7D8k6evH+66nM 238493ijsfkjalsdjcws9fheoihg80eWDSAFDOASDF=OIA=FSLoiidsiisiisNDo ACh40FeKsHDby3LK8OeM9NXmeCjYeoZYNGimHForiCCT+rIniiu2vy0Z/q+/t3cM BgmAmQKBgCwCTX5kbLEUcX5IE6Nzh+1n/lkvIqlblOG7v0Y9sxKVxx4R9uXi3dNK 6pbclskdksdjdk22k2jkj2kalksx2koUeLzwHuRUpMavRhoTLP0YsdbQrjgHIA+p kDNrgFz+JYKF2K08oe72x1083RtiEr8n71kjSA+5Ua1eNwGI6AVl -----END RSA PRIVATE KEY----- # chmod 400 /root/admin.pem Start an Instance  Get Private Network Id.  # openstack network list +--------------------------------------+---------+--------------------------------------+ | ID | Name | Subnets | +--------------------------------------+---------+--------------------------------------+ | 72494c88-6d93-4eb7-929f-383cbedfa3e7 | public | a57856be-a22b-4a48-b3c3-75be46a4c477 | | 781e062d-5ab6-4ae8-a54e-8e72291df37e | private | b00316a1-812f-423a-8aca-bd6547692ad3 | +--------------------------------------+---------+--------------------------------------+ Create a mycirros Instance.  # openstack server create --flavor m1.nano --image \"Cirros 0.3.4\" \\ --nic net-id=781e062d-5ab6-4ae8-a54e-8e72291df37e --key-name admin \\ --security-group all mycirros # # openstack server list +--------------------------------------+----------+--------+---------------------+--------------+---------+ | ID | Name | Status | Networks | Image | Flavor | +--------------------------------------+----------+--------+---------------------+--------------+---------+ | 6f2719bb-f393-49ab-a409-1c4a0f992b2d | mycirros | ACTIVE | private=10.10.1.105 | Cirros 0.3.4 | m1.nano | +--------------------------------------+----------+--------+---------------------+--------------+---------+ Create Floating IP.  # openstack floating ip create public +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | created_at | 2018-01-12T09:52:40Z | | description | | | fixed_ip_address | None | | floating_ip_address | 144.76.132.230 | | floating_network_id | 72494c88-6d93-4eb7-929f-383cbedfa3e7 | | id | 7a7ac84f-57ee-4982-b2f5-35ebb9656b34 | | name | 144.76.132.230 | | port_id | None | | project_id | 92d3bc57ca504eaab4c29d3509064004 | | revision_number | 0 | | router_id | None | | status | DOWN | | updated_at | 2018-01-12T09:52:40Z | +---------------------+--------------------------------------+  Get Port Id of Instance.  # openstack port list +--------------------------------------+------+-------------------+-------------------------------------------------------------------------------+--------+ | ID | Name | MAC Address | Fixed IP Addresses | Status | +--------------------------------------+------+-------------------+-------------------------------------------------------------------------------+--------+ | 60407eee-c9b2-4e9c-81a6-5c38bb536c9b | | fa:16:3e:37:ac:2f | ip_address='144.76.132.227', subnet_id='a57856be-a22b-4a48-b3c3-75be46a4c477' | ACTIVE | | 717660b6-b9b0-4f76-8c63-f9fae5c9bd9b | | fa:16:3e:0c:14:30 | ip_address='10.10.1.105', subnet_id='b00316a1-812f-423a-8aca-bd6547692ad3' | ACTIVE | | 878ee68a-1b88-48d1-9bf7-057f59b833bb | | fa:16:3e:2a:1f:50 | ip_address='144.76.132.230', subnet_id='a57856be-a22b-4a48-b3c3-75be46a4c477' | N/A | | 8a7b6b21-1eb2-4750-b854-707462d8b38f | | fa:16:3e:53:c9:ef | ip_address='10.10.1.1', subnet_id='b00316a1-812f-423a-8aca-bd6547692ad3' | ACTIVE | | b40af80e-2794-4ca5-8141-9d8ad4e9c9f2 | | fa:16:3e:51:d9:cb | ip_address='10.10.1.100', subnet_id='b00316a1-812f-423a-8aca-bd6547692ad3' | ACTIVE | +--------------------------------------+------+-------------------+-------------------------------------------------------------------------------+--------+ Assign Floating IP to Instance Port.  # openstack floating ip set --port 717660b6-b9b0-4f76-8c63-f9fae5c9bd9b 144.76.132.230  Verify Floating IP in OpenStack Router.  # ip netns show qdhcp-dcfbabbd-c5d2-444c-ab60-546216550118 router-0f00050f-6590-42df-9136-32d22fea4ece # ip netns exec qrouter-0f00050f-6590-42df-9136-32d22fea4ece ip a --- 9: qg-60407eee-c9: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN qlen 1000  link/ether fa:16:3e:37:ac:2f brd ff:ff:ff:ff:ff:ff  inet 144.76.132.227/29 brd 144.76.132.231 scope global qg-60407eee-c9  valid_lft forever preferred_lft forever  inet 144.76.132.230/32 brd 144.76.132.230 scope global qg-60407eee-c9  valid_lft forever preferred_lft forever  inet6 fe80::f816:3eff:fe37:ac2f/64 scope link  valid_lft forever preferred_lft forever --- Test Instance.  Connect to mycirros instance using the private ssh key stored in the admin.pem file.  # ssh -i admin.pem cirros@144.76.132.230 $ ping www.redhat.com PING www.redhat.com (172.217.21.14): 56 data bytes 64 bytes from 172.217.21.14: seq=0 ttl=54 time=37.692 ms 64 bytes from 172.217.21.14: seq=1 ttl=54 time=27.758 ms 64 bytes from 172.217.21.14: seq=2 ttl=54 time=25.640 ms Install OpenStack Client  To administer OpenStack remotely the CLI tools are important. Below are steps to install OpenStack CLI tools for Rocky.  [RHEL]  # subscription-manager repos --enable=rhel-7-server-openstack-14-tools-rpms # yum install -y python-openstackclient # yum install -y python-heatclient # yum install -y python-octavia [CentOs]  # yum install -y https://rdoproject.org/repos/openstack-rocky/rdo-release-rocky-1.noarch.rpm # yum upgrade -y # yum install -y python-openstackclient # yum install -y python-heatclient # yum install -y python-octavia Summary  This article was intended as a hands on guide for standing up an OpenStack Rocky lab environment using RDO. In this guide we also tailored the environment to the Hetzner root server. Things can of course vary depending on your hardware platform and provider. Certainly the aim of this guide was to provide a more realistic deployment scenario for OpenStack. As mentioned RDO is a stable community platform built around Red Hat’s OpenStack Platform. It provides the ability to test the latest OpenStack features against either an enterprise platform (RHEL) or community platform (CentOS). Hopefully you found the information in this article useful. If you have anything to add or feedback, feel free to leave your comments.  Happy OpenStacking!  (c) 2018 Keith Tenzer    ","categories": ["OpenStack"],
        "tags": ["Cloud","Hetzner","Linux","opensource","OpenStack","RDO","Rocky"],
        "url": "/openstack/openstack-14-rocky-lab-installation-and-configuration-guide-for-hetzner-root-servers/",
        "teaser": null
      },{
        "title": "Building A Niche Cloud: A Pragmatic Approach",
        "excerpt":"  source: https://innolectinc.com/how-smart-is-your-team/teamwork-ants-building-a-house/   Part I: Birth of the Niche Cloud? Part II: Building a Niche Cloud: A Pragmatic Approach  Overview  Before getting started you might want to read about the birth of the niche cloud in the first part of this two part series.  We have all heard the saying, you cannot teach an old dog new tricks. Yes that is true, but we aren't thankfully dogs. Learning to do something new of course, requires an open mind-set and a desire for change. Many organizations are getting left out of digital disruption these days because they keep falling back on old outdated ideas, behaviors and habits. Our minds are so full, so occupied and so tired we simply cannot grasp or don't have the energy for anything new. We spend our time applying what is new to what we know, which is old.  In this article lets reset our minds and look at an approach to build a niche cloud from the ground up. Instead of pealing back the onion we will apply layer after layer until we have the onion itself. Of course I realize there is a lot more and this article is just scratching at the surface, nevertheless it is an approach, a basic rule-set and guideline for getting started.    Define your Cloud Layers  Each layer must be independent of one another and all layers should be abstracted away behind our onion. When imaging a cloud, the first things is imagine the user and imagine him or her seeing the onion. Rule #1: The user doesn't see any layers, just the onion, but there are many, many layers.    Layers are necessary to abstract complexity. If you achieve a high enough abstraction level, you have essentially created a cloud, congratulations it really is that easy and yet also so complex thus I really think simplexity is a great way to understand the goal.  Every layer obviously requires automation. Ideally you should settle on an automation language and build a platform around your automation. Writing a script, playbook, recipe, package, whatever is not automation. It also isn't automation if individuals in different groups can perform their tasks automatically. Rule #2: Automation is about abstracting complexity, process, individual domain knowledge and making it standard or easily consumable across the cloud. Automation is important at every layer. It should be used to construct the layers, one by one but it also plays a key role at the business process layer and customer portal layer. This is the area where you will want to invest the most time and create the most value for your customers. As such all tasks or anything running on the cloud, must be implemented in the Automation layer.  Below is a diagram illustrating the key cloud layers. The entry point for customers is the portal. Customer are of course developers, users, anyone that will consume a service.    Automation Layer  Responsible for deployment of layers and tooling used within them. Each layer should be deployed individually with no coupling to other layers. Layers should be orchestrated into overall workflow, enabling end-to-end cloud deployment.  Infrastructure Layer  Provides an abstraction on baremetal. Abstracts physical compute, storage and network resources though it is possible to move those abstractions to another layer, for example the PaaS layer.  PaaS Layer  Provides abstraction for services, applications and their runtimes. Ensures applications are portable and decoupled from infrastructure. PaaS layer provides consistent application layer that is independent of individual clouds, can and may actually span clouds or cloud constructs.  Management Layer  Provides patching, security vulnerability detection, governance, chargeback, automation platform, analytics and more across cloud(s). Management tooling should be cloud agnostic and not coupled to underlying layers. Management layer should provide interfaces for business process layer.  Business Process Layer  Integration with backend systems, responsible for defining services, logic, billing and other such rule sets. Responsible for handling customer accounts and customer related activities.  Customer Portal Layer  Interface and API exposed to end customer. Abstraction of all complexity to provide a service which is simple to understand and consume. This is all about customer experience.  Products and Technologies  After a discussion around the various layer and their decoupling it is important to decide on technology and products. This is obviously where most people want to start but doing so without understanding the layers and having a strategy around automation, platform capabilities and a technology roadmap would be a major mistake. Rule #3: Don't lead with technology and don't just choose technology vendors solely because they exist or you have good experience with them. You should be building your Cloud from the ground-up. Choose your technology based on how it best compliments the required capabilities and of course the desired outcomes. Envision where you want to be several years from now.  I am going to provide examples using Red Hat products and technologies to build the layers and show a Red Hat approach. I think it provides a good example for how to create a technology map and map technologies to the various layers. Rule #4: Technology and products can and should be replaced down the road, don't get attached to them.  Automation Layer  This is the most important to get right. I think it is a no-brainer actually. Ansible provides a simple, easy language for automation. Ansible Tower provides a powerful platform around Ansible that among other things, gives Ansible an API. Using Ansible Tower would allow automation to be built, consumed and reused among various teams. Another thing I really like about Ansible is it is simple, everyone can take part, understand it and there are no heroics needed to operate. Rule #5: Anything that is dependent on individuals and heroes should be re-designed and replaced with something standard that is not.  Infrastructure Layer  This layer is responsible for the foundation of your Cloud. It runs on metal and needs to provide the same capabilities you would get from Amazon, Google or Microsoft. OpenStack is a no-brainer but I also think Red Hat OpenStack is a no-brainer. Red Hat OpenStack uses Director which builds an undercloud, manages metal and deploy, manages, upgrades the OpenStack overcloud. Director is built around Ironic and TripleO (OpenStack on OpenStack). Why require separate tools that need to be learned when you simply can use OpenStack to deploy OpenStack. Another critical piece is upgrades. Red Hat offers not only version to version but a fast-forward upgrade that lets you skip versions. Red Hat also provides support for 5 years on a single release. This layer needs to be rock solid and the Achilles heal of OpenStack is upgrade and lifecycle management. The reason you go with OpenStack is to abstract technology and allow you to change technologies without impacting the APIs. OpenStack is maybe the one thing you likely won't want to change, if done right and as such having a long support lifecycle is crucial.  PaaS Layer  While there are many, many options, I think the industry has settled on Kubernetes. Besides Google, who announced the project, Red Hat is the only company there since the beginning. Everyone is obviously jumping on the bandwagon and for good reason, every application should and will eventually run in containers. Portability of applications and a much, much faster, more streamlined release cycle alone is driving container adoption. OpenShift is by far the most advanced and enterprise ready Kubernetes. OpenShift is also a platform and this is why it makes a great choice. Kubernetes is just an orchestration layer, you need to build an ecosystem around it (container registry, sdn, security, governance, CI/CD, IaaS plugins and much more).  Management Layer  This layer is a little less straight forward and there is more room to come to other technology decision points. I think Ansible Tower is a given to provide Ansible-as-a-Platform. Other capabilities needed are patching, vulnerability detection, monitoring, governance and predictive systems management, going in the direction of AI. Clearly this is just a starting point and a lot more will fall into this layer.  Satellite would be used for patching of Red Hat Enterprise Linux and potentially other RPM based Linux distros. If you have windows you will likely need additional patching tooling there. I am not a fan of single tools that do everything in this area. I would go with the best tools for the Operating Systems you want to offer and abstract them away through Ansible primitives.  Monitoring and governance could be provided by CloudForms. It also supports other Clouds so if you are a multi-cloud organization, this can provide a layer to manage those various clouds. Governance is about policies. You need a rule-set and if those rules are violated then at a minimum people need to be notified. In addition taking action, shutting off non-compliant systems is also something to consider, depends on requirements really.  For monitoring you really need something predictive that is going in the direction of an AI. What you know won't kill you, it is what you don't. Insights looks at security, configuration, even at platform level against a rule-set that is generated using an AI approach. The rule-sets come from support cases, knowledge base articles and common best practice at Red Hat. The intelligence is these rules are constantly enhanced, created and updated. Therefore if another customer hits an issue, you wouldn't have to as a rule could be triggered that could warn ahead of time based on an issue someone else experienced. Pretty cool right?  How does this all come together at consumer level? Think about all the services and additional capabilities you could provide that are of value besides giving someone a dumb VM connected to the latest RPMs, like generic public cloud. You could provide lifecycle management, patch stages and so on. Governance could be used to enforce specific requirements in a standardized way. You could start to tailor your requirements to fit that of a niche cloud. Insights could be provided as a higher level service, maybe even a higher SLA level. There are many, many possibilities that go well beyond just being boring and doing what public cloud does.  Business Process Layer  This is your key value add. How you do billing, what processes you are able to expose, capabilities and features to enable. It is basically the logic for what gets exposed through the customer portal. Likely you will need something more sophisticated than Ansible. Business rules are also typically understood by business analysts not developers. Red Hat Decision Maker is based on drools and allows business analysts to implement rule-sets as example without needing to change code or be programmers. Think about how you offer services and what they cost? This is quite dynamic and you definitely need something that business people can change, tweak and understand.  Customer Portal Layer  Finally we are at the onion. This is what the customer sees. This layer should not have much logic, the intelligence should be in the business process layer. The portal is responsible for allowing customers to consume services that are offered. It is your store front and one that always has enough supply to satisfy every customer. It provides such a good abstraction that most think it is just magic and cannot begin to understand what they are seeing or experiencing. If only the knew what mess lies below? You may think these are lofty or impossible goals, but if you believe that, just don't start, it won't work is my advice. You can always be a consumer and have someone else do it for you, no shame in that.  Where to get started with customer portal? How about an innovation lab?    Building your customer portal is a great opportunity to work as well as learn from some of the best and brightest opensource developers in the world. The idea is similar to cooking. Sure you can get a recipe, even watch someone cook on YouTube. Following such recipes, probably will result in something decent or even good. But, what if you send yourself or team to a five star kitchen and cook with master chefs? What do you think would happen when your got back to your kitchen?  I will tell you. You will cook quite differently. You will change your tools, approach. You may even make changes to the kitchen itself. What would long term results produce? Which method would lead to them cooking better meals? I think it's obvious.  It is the same thing with software development and coding. Red Hat offers something unique here and I think it is perfect for building a cloud portal prototype while learning with Red Hat's top engineers and software developers at the core of opensource innovation. If this sounds interesting here is a lightning talk on the subject from Jeremy Brown (https://www.youtube.com/watch?v=9BVaTx_RJ9U).  The next diagram shows the key layers and technologies used in those layers.    The key to remember is each of these layers, just like the onion are independent and not coupled. Rule # 6: Building loosely coupled layers is key to future cloud longevity.  Building the Niche Cloud Layer by Layer  What I am going to provide is some basic guides to get going using Red Hat products and technologies. This should serve as a prototype or demo. By no means is this meant to be anything more than allowing you to get your feet wet and understand the concepts above.  Infrastructure Layer  For getting started purposes I recommend getting a Hetzner Root Server. These are physical servers in the cloud that are very inexpensive, Hetzner is also what I refer to as a niche cloud. They provide metal-as-a-service and a market place where you can auction or purchase used hardware on monthly basis, hence why it is so inexpensive. It is great for prototyping, demo or conceptual work.  To build OpenStack layer using RDO on RHEL or CentOS follow my blog here.  PaaS Layer  The PaaS can either be built using OpenShift Community (OKD) or OpenShift Enterprise. I have provided Ansible playbooks and documentation for deploying OpenShift on OpenStack in GitHub.  Management Layer  The management layer consists of several technologies and products. I have also provided playbooks and documentation to deploy Red Hat Satellite and Ansible Tower on OpenStack in GitHub. I have not documented Red Hat CloudForms because an image exists for OpenStack. This is pretty easy to get going using documentation provided by Red Hat.  Business Process Layer  The business process layer is of course highly specific. Nevertheless working with some of my colleagues, we have provided some use cases to help with business process layer. The use cases are implemented in Ansible. They can of course also be consumed via API from Ansible Tower. Below are some of the use cases.  Order Infrastructure Project with Quota    Order Instance    Order Application or Service    Order Database    Customer Portal Layer  Hopefully you like the idea of running an innovation lab to build prototype for your cloud portal. CloudForms could initially also be used to provided a generic portal. You definitely want to build your own but this could at least provide value in the beginning to get up and running fast while you take time to build a real portal. Below is example of the CloudForms customer portal.    Summary  In this article we discussed how to approach building your own niche cloud. The importance of decoupled layers. How the layers build upon one another. We discussed technology and products that could be used to build the layers from a Red Hat perspective. Finally guidelines and ideas were provided to help get you started in building your niche cloud. Below are the important rules worth repeating, that were mentioned throughout this article.   Rule #1: The user doesn't see any layers, just the onion, but there are many, many layers. Rule #2: Automation is about abstracting complexity, process, individual domain knowledge and making it standard or easily consumable across the cloud. Rule #3: Don't lead with technology and don't just choose technology vendors solely because they exist or you have good experience with them. Rule #4: Technology and products can and should be replaced down the road, don't get attached to them. Rule #5: Anything that is dependent on individuals and heroes should be re-designed and replaced with something standard that is not. Rule # 6: Building loosely coupled layers is key to future cloud longevity.  Happy Niche Clouding!  (c) 2018 Keith Tenzer  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  ","categories": ["Cloud"],
        "tags": ["Ansible","Cloud","CloudForms","innovationlabs","Insights","nichecloud","opensource","OpenStack","Satellite"],
        "url": "/cloud/building-a-niche-cloud-a-pragmatic-approach/",
        "teaser": null
      },{
        "title": "Getting Started with Machine Learning",
        "excerpt":"  source: https://news.sophos.com/en-us/2017/07/24/5-questions-to-ask-about-machine-learning/  Overview  In this article we will dive into machine learning. We will begin by understanding the concept, then look at some of the use cases, requirements and finally explore a real world scenario using a demo application.  Machine learning has the potential to dramatically change our lives, jobs and influence decision making on a scale that has never been seen before. It is one of the most exciting and also scary technological advancements to ever come around. It is the future but is also happening right now which is why there couldn't be a better time to get started than today.    Understanding Machine Learning  In order to properly understand machine learning we need to consider artificial intelligence. The goal of AI is to create machines, built from computer programs that have cognitive capabilities, are able to analyze their environment and make decisions like a human would. AI is a huge field and there is a lot of topics within it but ultimately it is about making decisions cognitively. Machine learning as such is the foundation or building block of AI. Machine learning primarily is concerned with making the right decision based on learning. Deep learning is a subset of machine learning and often confused with the latter. Deep learning applications are basically the same, where they differs is around how they learn. Deep learning is predicative, understands accuracy and when given metrics or guidelines is able to learn on it's own without supervision from humans. The below diagram shows how machine learning and deep learning fit into artificial intelligence.    The capability of deep learning being able to learn on it's own is made possible by neural networks. A neural network is composed of many layers and weighted connections, that when given an input attempts to find the best possible output. The algorithms are within the layers and responsible for determining the output. This is done by weighting, prediction and measuring accuracy against metrics.  A training set of data is then used to run a training cycle and build the neural network connections. The end result is a trained model with it's own learned rule-set, that would be used by an application to create a business value. I like to refer to the phase of using a trained model as the professional phase.  The below diagram shows a basic workflow for machine learning with both the training and professional phases.    The goal is always to improve the accuracy of the model and it's ability to match an input with the best possible output. As such, models will likely be continually improved over time and there will be a continuous improvement cycle between the training and professional phases. However a trained model is expected to handle future problems or even unknowns. It is thought that a model would not be updated multiple times a day or every few days but should last weeks or maybe even months. Nevertheless there is a need to continue to learn just like humans.  Machine Learning Use Cases  Today there are a lot of use cases for machine learning, in the future there will be even more, as new patterns or frameworks become available and understood. Some of the more common use cases I consider valid for today are shown below.    The important thing to remember is that machine learning will be replacing and greatly optimizing tasks not replacing humans themselves. The key as already stated about machine learning is to find the best output given an input, in order to make a decision. This probably also answers why search was one of the first use cases for machine learning. Regardless if we are talking about smart cars, analyzing medical imagery, stock market, providing personalized recommendations or even protecting security, the process or workflow is the same. The only thing that changes are the inputs/outputs, data labels or categorization, and sometimes the algorithms. As hinted creating new algorithms or defining new patters is really hard work and a really highly specialized field. However once the patterns or algorithms are known, they can be re-used and this is what everyone can leverage. For example if we take imagery, those patterns are well known and established. Depending on what you are doing you may only need to label your data and adapt what has already been done. If you are dealing with new types of imagery then it is still just a matter of adapting the frameworks that exist and tailoring it to your needs. An example of this is TenzorFlow (no affiliation to myself) which is a machine or deep learning framework that already has a lot of algorithms. TenzorFlow provides a abstraction and a workflow for handling the details of hitching input functions to output functions (neural connections). The whole point of machine learning is to analyze a very large data-set, train itself on all the patterns and connections to build a neural network and then based on a single input find the best possible output. Machine learning can do this better, faster and with more predictably than humans.  Machine Learning Requirements  Now that we have established a basic understanding for what machine learning is and where it can be used, let's get into some of the requirements. Below is a list of some of the more important things to take into consideration.   Requires GPUs, not CPUs for best performance High memory requirements as trained models are loaded in memory Large data-set for training Trained models are very large, many, many GBs Complex, diverse software stack (python, modules, compilers, hardware drivers, integration layers) Multi-cloud being able to leverage software stacks and data-sets across cloud platforms, including the existing enterprise environments  If you add up all the requirements you end up with needing a very dynamic and flexible environment that cannot only scale but meet the challenges around diversity and integration. Machine learning needs data and as such it likely won't be some one-off, but rather integrated into the various sources of data in your enterprise environments. Containers and specifically a kubernetes based container platform is the only solution that can effectively meet the demands of machine learning workloads.  Containers provide process isolation while also providing an easy mechanism to package and transport such a complex software stacks across multi-cloud environments. A container platform enables ability to do many releases and fast rollouts of new capabilities without downtime. Models can be compiled and stored on persistent volumes so they don't reside in the containers themselves. Machine learning workloads can easily be isolated to compute nodes with GPUs and large amounts of memory, typically running on baremetal from other workloads that may be better suited for VMs  or other hardware. Finally it is key that a container platform can run in the enterprise environment as well as public cloud. In addition must be open, allow necessary control and leverage the best capabilities in order to harness the true power machine learning offers.  Machine Learning Demo Application  As we have discussed there are two parts to machine learning: building a trained model and using it. Most people will be more interested with the latter as that is where you see business value. As such the demo focuses on using an already trained model. The demo application is a voice recognition system. It will take an audio file, decode audio into text and then translate that text to the language of our choice. The application is broken into three parts: frontend, backend and the trained model.  The front-end is written in nodejs while the back-end is python. Currently the container image has both but that could be broken apart with an additional API or middleware layer. The trained model is downloaded but stored on a persistent volume that is simply mounted in the container image. This allows us to make changes to front-end or back-end and quickly rebuild without having to download model again. It also allows for a separate CI/CD process for delivering the model. Trained models will also likely be continually improved in the goal to get better outputs and intelligence from the algorithms.  The back-end is using Mozilla Deepspeech to do the voice recognition. Mozilla DeepSpeech is a TenzorFlow implementation of Baidu's DeepSpeech architecture. We are using a basic trained English model (provided by DeepSpeech project) so accuracy is not nearly as good as it could if we trained the model to for example, with our voice, dialect or even other language characteristics. It is just intended for demo purposes not a real-world scenario. Once we have text we run the text through spell and accuracy correction and finally translate the text in to another language using Google Translate.  Deploy Machine Learning Demo Application  A Dockerfile is provided to build and deploy the application. Here we will deploy the application to OpenShift which is Red Hat's Enterprise Kubernetes platform. As I mentioned a container platform is critical and OpenShift provides an independent, open platform for machine learning workloads. If you don't have OpenShift I would recommend getting the community upstream version called OKD. If that isn't an option you can use plain-old boring Docker and do a docker build of the Dockerfile on your workstation.  Clone GitHub Repository  Clone the GitHub repository to your laptop or workstation.  $ git clone https://github.com/ktenzer/openshift-ml-demo.git Login to OpenShift and create new project  A project in OpenShift is a namespace or workspace for applications.    Add Template into OpenShift  Templates in OpenShift provide pre-determined automation for how to build and deploy an application. The template we have provided takes care of building and deploying the demo application as well as configuring a persistent volume for storage and a URL or route.    Next you can choose to process and save the template so it is available in the project or service catalog for later use. We will do both.    Update Template Parameters  Templates are parameterized by their creators. In this case we will leave the defaults.   Build Demo Application  After all the objects defined in the template are created, the build will start. In this step we will build a container image with the necessary software stack and python development environment to run the application.    Deploy Demo Application  The deployment is started after the build completes and the build artifact, which is a container image is saved in the OpenShift registry. The deployment schedules the application to run and the scheduler determines the most appropriate node. The images is then pulled from the OpenShift registry to the node. The pod will show a grey border while the image is being pulled to a node.    Once the image is pulled to the node it will be started on that node.    The first time the application starts it will download the trained model. You can follow this by looking in the pods and containers logs. Once the trained model is downloaded the nodejs server will be started and the application made available.    You can also access the pod and container and look at the downloaded models under the /deepspeech directory. This is of course a persistent model.    You can view the persistent volume used under storage in OpenShift.    Using Demo Application  To access application simply click the URL or route configured by the template.  Note: if it is still downloading the model (can take a while), application will not start or be available, so check the logs in the container and be patient the first time.    Click choose file to select an audio file. There is a demo.wav file in the GitHub repository provided for convenience. You can of course record your own audio just follow the requirements mentioned in the application.    Once you click submit the audio file is downloaded into the container image and run through the translate.py program which loads DeepSpeech and performs audio-to-text as well as translation to language chosen. Click the Results button to see the results.  Note: it may take a minute or two to see any results and the translation will be done step-by-step so you can see how DeepSpeech model is making decisions. You will notice it makes several mistakes that later are corrected. This really illustrates the power of machine learning and also the complexity of understanding voice in a compute program.    The last audio-to-text sentence and translation is displayed before end of translation message. As you can see it got everything almost perfect.    Once the model runs you will notice quite a bit of memory and some CPU usage. The model is loaded into memory so you can imagine the usage for a much larger model, this one is only around 2GB. CPU/GPU usage will vary depending on size of the audio file and data-set.    Video  Below is a video recording of the demo.  [wpvideo lLhyPenM]  Summary  In this article we gained a broader understanding for machine learning. By exploring use cases we saw just how wide reaching machine learning has become today and garnered new appreciation for it's potential in the future. Looking into the requirements of machine learning workloads we were able to see the value of containers and a container platform. I am convinced given the diversity of the software stack, the integration required into multiple clouds as well as data-sets and the flexibility needed that a container platform, is the only option we should be considering for machine learning workloads. Finally a demo was provided to put more context around machine learning, show that it is actually simple to utilize and allow us to understand better what running machine learning workloads actually means.  Happy Machine Learning!  (c) 2018 Keith Tenzer  &nbsp;  ","categories": ["OpenShift"],
        "tags": ["MachineLearning","AI","Deepspeech","Containers","Kubernetes"],
        "url": "/openshift/getting-started-with-machine-learning/",
        "teaser": null
      },{
        "title": "It's a PaaS: Vanilla Kubernetes vs OpenShift on OpenStack Lab Setup Guide",
        "excerpt":"    Overview  Often a lot of people seem to confuse Kubernetes with OpenShift or a platform-as-a-service (PaaS). Kubernetes is of course on it's own, not. It is an orchestration layer or technology for containers but a lot is missing to really call it a platform. OpenShift is Red Hat enterprise Kubernetes platform. It contains Kubernetes but also a whole lot more which make it a true platform. So which is right for you? It depends a lot on your requirements and what you are trying to achieve. The purpose of this article is to setup an environment for running a workshop that compares the Kubernetes experience with OpenShift in order to gain more insight and understanding in what you may actually need. Many people sit down with slides or at a whiteboard, but I really find that is not adequate and you really need to experience it, first hand.    This article will guide you through building an OpenStack IaaS environment and then create a vanilla Kubernetes environment using Magnum (a OpenStack service for providing container technologies) and one using OpenShift. Both environments will run on OpenStack. You will need a physical system. My recommendation is 64GB RAM, 1-2 Socket CPU and SSDs. You can get such a server on a monthly basis at the Hetzner cloud for cheap, including 15 additional IPV4 floating ips. The guide explains how to set everything up on Hetzner as well. Hetzner provides an auction market for used hardware which is why it is so cheap. Below is a link to get going on ordering a Hetzner server.  https://www.hetzner.com/sb  Deploy OpenStack Environment  You can choose whatever OpenStack distribution you want as long as it has Magnum and LBaaS or Octavia. Currently none of the major distributions ship Magnum. This is because Magnum is not yet very stable and is considered experimental. Nevertheless it is a good way to get a vanilla Kubernetes cluster going.  We will be using the RDO community distribution of OpenStack which running on CentOS, this allows for automatically configuring Magnum. The following blog is a step-by-step guide of deploying OpenStack on Hetzner.  https://keithtenzer.com/2018/07/17/openstack-13-queens-lab-installation-and-configuration-guide-for-hetzner-root-servers/  If you are interested in an automated approach a colleague of mine has written an Ansible playbook to deploy OpenStack on hetzner for this use case.  https://github.com/ktenzer/openshift-on-openstack-123  Configure Magnum Kubernetes Cluster  Magnum provides templates to deploy kubernetes, mesos or docker swarn. The idea is to create a template and then deploy a cluster from the template. Heat is used to deploy the cluster. The concept of Magnum is similar to that of Kubernetes in the cloud (Amazon, Microsoft and Google). There is no multi-tenancy or isolation between namespaces. You typically end up running lots and lots of clusters, one per application maybe even. Below is a diagram to help visualize the deployment.    Download Fedora Atomic Image  $ wget https://download.fedoraproject.org/pub/alt/atomic/stable/Fedora-Atomic-27-20180419.0/CloudImages/x86_64/images/Fedora-Atomic-27-20180419.0.x86_64.qcow2 Add Image to OpenStack Glance  $ openstack image create --disk-format=qcow2 --container-format=bare \\ --file=Fedora-Atomic-27-20180419.0.x86_64.qcow2 \\ --property os_distro='fedora-atomic' --public fedora-atomic-latest Create Template  Under \"Container Infra\" -&gt; \"Cluster Templates\" create a new template. Select Kubernetes for the orchestration engine.    Choose flavor and configure storage settings.    Configure container network settings.    Deploy Template  Once the template is created you simply click deploy cluster from the template. Choose name for cluster and select proper template.    Choose number of masters and nodes and size of docker volume.    Choose flavors and admin key for accessing cluster.    Follow deployment from Heat.  # openstack stack event list k8s-demo-4ogiuqtpozdr 2018-12-15 10:16:42Z [k8s-demo-4ogiuqtpozdr]: CREATE_IN_PROGRESS Stack CREATE started 2018-12-15 10:16:43Z [k8s-demo-4ogiuqtpozdr.nodes_server_group]: CREATE_IN_PROGRESS state changed 2018-12-15 10:16:43Z [k8s-demo-4ogiuqtpozdr.nodes_server_group]: CREATE_COMPLETE state changed 2018-12-15 10:16:44Z [k8s-demo-4ogiuqtpozdr.network]: CREATE_IN_PROGRESS state changed 2018-12-15 10:16:45Z [k8s-demo-4ogiuqtpozdr.secgroup_kube_minion]: CREATE_IN_PROGRESS state changed 2018-12-15 10:16:46Z [k8s-demo-4ogiuqtpozdr.secgroup_kube_master]: CREATE_IN_PROGRESS state changed 2018-12-15 10:16:46Z [k8s-demo-4ogiuqtpozdr.secgroup_kube_minion]: CREATE_COMPLETE state changed 2018-12-15 10:16:47Z [k8s-demo-4ogiuqtpozdr.network]: CREATE_COMPLETE state changed 2018-12-15 10:16:47Z [k8s-demo-4ogiuqtpozdr.api_lb]: CREATE_IN_PROGRESS state changed 2018-12-15 10:16:47Z [k8s-demo-4ogiuqtpozdr.etcd_lb]: CREATE_IN_PROGRESS state changed 2018-12-15 10:16:48Z [k8s-demo-4ogiuqtpozdr.secgroup_kube_master]: CREATE_COMPLETE state changed 2018-12-15 10:17:01Z [k8s-demo-4ogiuqtpozdr.etcd_lb]: CREATE_COMPLETE state changed 2018-12-15 10:17:03Z [k8s-demo-4ogiuqtpozdr.api_lb]: CREATE_COMPLETE state changed 2018-12-15 10:17:03Z [k8s-demo-4ogiuqtpozdr.kube_masters]: CREATE_IN_PROGRESS state changed 2018-12-15 10:24:23Z [k8s-demo-4ogiuqtpozdr.kube_masters]: CREATE_COMPLETE state changed 2018-12-15 10:24:23Z [k8s-demo-4ogiuqtpozdr.api_address_lb_switch]: CREATE_IN_PROGRESS state changed 2018-12-15 10:24:24Z [k8s-demo-4ogiuqtpozdr.etcd_address_lb_switch]: CREATE_IN_PROGRESS state changed 2018-12-15 10:24:25Z [k8s-demo-4ogiuqtpozdr.api_address_lb_switch]: CREATE_COMPLETE state changed 2018-12-15 10:24:25Z [k8s-demo-4ogiuqtpozdr.etcd_address_lb_switch]: CREATE_COMPLETE state changed 2018-12-15 10:24:25Z [k8s-demo-4ogiuqtpozdr.api_address_floating_switch]: CREATE_IN_PROGRESS state changed 2018-12-15 10:24:25Z [k8s-demo-4ogiuqtpozdr.kube_minions]: CREATE_IN_PROGRESS state changed 2018-12-15 10:24:26Z [k8s-demo-4ogiuqtpozdr.api_address_floating_switch]: CREATE_COMPLETE state changed 2018-12-15 10:30:32Z [k8s-demo-4ogiuqtpozdr.kube_minions]: CREATE_COMPLETE state changed 2018-12-15 10:30:32Z [k8s-demo-4ogiuqtpozdr]: CREATE_COMPLETE Stack CREATE completed successfully [Kubernetes Master]  Log into the Kubernetes master and get the admin service account token.  # kubectl get secrets -n kube-system NAME TYPE DATA AGE admin-token-gkfx2 kubernetes.io/service-account-token 3 1h coredns-token-wg568 kubernetes.io/service-account-token 3 1h default-token-9h58d kubernetes.io/service-account-token 3 1h heapster-token-2kr9d kubernetes.io/service-account-token 3 1h kube-dns-autoscaler-token-8h4lg kubernetes.io/service-account-token 3 1h kubernetes-dashboard-certs Opaque 0 1h kubernetes-dashboard-key-holder Opaque 2 55m kubernetes-dashboard-token-tfxjj kubernetes.io/service-account-token 3 1h # kubectl describe secret admin-token-gkfx2 -n kube-system Name: admin-token-gkfx2 Namespace: kube-system Labels: &lt;none&gt; Annotations: kubernetes.io/service-account.name=admin kubernetes.io/service-account.uid=8796d78d-0053-11e9-9468-fa163ed886d7  Type: kubernetes.io/service-account-token  Data ==== token: eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi10b2tlbi1na2Z4MiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJhZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6Ijg3OTZkNzhkLTAwNTMtMTFlOS05NDY4LWZhMTYzZWQ4ODZkNyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTphZG1pbiJ9.KFGUddyzu6Z_fGJAC0EszF2UNe6zIBZ-kxI0tDbBlCsR8Slh2QNKuQ5PuFeSo7V1cb3SLcqoFHB_MjWsQmNfX_gIgqd1uuMe82dSW0xaK17zNvZG2qVNZdzWgGgqRErJ2j2hLL2BnH8K5i--O6nmpJn8eom-OJXq7UhdGu8gAq_FFYt86R5PI7u3e7tOjaY5BHKTi_tkBYEQdpJizu0fTHbJ-s-vJac2yx4puHkAcE_OHCpXL2nDdp92vvj3iqI8v11iR1IJ5OrVU5drITbIiNRk_RHe7Fcwfyu47oRXzs1XpRZCoZBPSRG_KCZYX1g8k0KDP9nNHkiDGTWoDEk-ICUZP5V1rHtNdB1kKNUAQ_vw_fSK3erfP0Aim2g3roLSl_P6K3uvoZV825U9biaVtxGFBwCFE49uzH_PM9HA4xtghIajqTZpW4vXn_vbBOSlt26RHex08AVWaPp0UYHyyEj8b6leryiPnd-tOoVSV7iZa5nlECoffGXoVq7ISWXPldu2Qt7GH2olE-nmOyOoKmU2ETUGHds_x2EpN9Fp6kJkFhJ0FfPblJAi0JEEJ5cSjUGKuq5dKRwZgoiS00bW2_UznDXAql6yHAcDzf6-PqYY4oSEfk4UxpBz6RafE-Y_ba6VWT5cu8_S8HDsBAdmuefjL7qXLN2xgsCFwy26FxI ca.crt: 1046 bytes namespace: 11 bytes Deploy Bastion  In order to access the environment it strongly recommended to setup a bastion. You will want to use same OS image as is used for Kubernetes. In this case that is fedora atomic. The reason is you will want kubectl client. You could however do this on any system and then setup the kubectl client.  From OpenStack UI in the project deploy a new instance using the fedora atomic image.  [Bastion]  Log into the bastion and install the following rpms. Since this is fedora atomic we need to use rpm-ostree.  # ssh -i &lt;openstack keypair&gt; fedora@&lt;bastion floating ip&gt; # rpm-ostree install kubernetes # rpm-ostree install git # rpm-ostree install ansible # rpm-ostree install python-passlib Reboot bastion  # systemctl reboot Log back into bastion and setup kubectl credentials with admin service account.  # kubectl config set-credentials cluster-admin \\ --server=https://176.9.171.115:6443 --insecure-skip-tls-verify \\ --token=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi10b2tlbi1na2Z4MiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJhZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6Ijg3OTZkNzhkLTAwNTMtMTFlOS05NDY4LWZhMTYzZWQ4ODZkNyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTphZG1pbiJ9.KFGUddyzu6Z_fGJAC0EszF2UNe6zIBZ-kxI0tDbBlCsR8Slh2QNKuQ5PuFeSo7V1cb3SLcqoFHB_MjWsQmNfX_gIgqd1uuMe82dSW0xaK17zNvZG2qVNZdzWgGgqRErJ2j2hLL2BnH8K5i--O6nmpJn8eom-OJXq7UhdGu8gAq_FFYt86R5PI7u3e7tOjaY5BHKTi_tkBYEQdpJizu0fTHbJ-s-vJac2yx4puHkAcE_OHCpXL2nDdp92vvj3iqI8v11iR1IJ5OrVU5drITbIiNRk_RHe7Fcwfyu47oRXzs1XpRZCoZBPSRG_KCZYX1g8k0KDP9nNHkiDGTWoDEk-ICUZP5V1rHtNdB1kKNUAQ_vw_fSK3erfP0Aim2g3roLSl_P6K3uvoZV825U9biaVtxGFBwCFE49uzH_PM9HA4xtghIajqTZpW4vXn_vbBOSlt26RHex08AVWaPp0UYHyyEj8b6leryiPnd-tOoVSV7iZa5nlECoffGXoVq7ISWXPldu2Qt7GH2olE-nmOyOoKmU2ETUGHds_x2EpN9Fp6kJkFhJ0FfPblJAi0JEEJ5cSjUGKuq5dKRwZgoiS00bW2_UznDXAql6yHAcDzf6-PqYY4oSEfk4UxpBz6RafE-Y_ba6VWT5cu8_S8HDsBAdmuefjL7qXLN2xgsCFwy26FxI Setup a context and save cluster settings.  # kubectl config set-context cluster-admin # kubectl config use-context cluster-admin # kubectl config set-cluster k8s-student \\ --server=https://&lt;ip of master api load balancer&gt;:6443 \\ --insecure-skip-tls-verify # kubectl config set-context cluster-admin --user=cluster-admin \\ --cluster=k8s-student Check to ensure it working. You should see the pods running under kube-system.  # kubectl get pods -n kube-system Create student accounts on Kubernetes  Clone GitHub repository.  $ git clone https://github.com/ktenzer/its-a-paas.git $ cd its-a-paas/k8s/playbooks Run playbook to create system users.  $ ansible-playbook create_users.yml Run playbook to create student service accounts.  $ ansible-playbook create_sa.yml Run playbook to create role binding.  $ ansible-playbook create_rolebinding.yml Save generated tokens under student home directories.  $ ./save_token.sh Copy token to credentials directory where user account passwords are stored.  $ sudo ./copy_token.sh Configure OpenShift Cluster  OpenShift is Red Hat's enterprise Kubernetes platform. There is also a community version of OpenShift called OKD. OpenShift offers quite a lot more than vanilla Kubernetes, as mentioned, which is what Magnum of course deploys. Below is a list of additional capabilities OpenShift brings with it, in addition to Kubernetes.   Service Catalog with many ready-to-go application runtimes, databases, frameworks and middleware. Source-to-Image (S2I) allows source code to act as layer in container and re-use other layers so build runs many times faster. Day 2 Operations: Aggregated Logging (EFK), Monitoring (Prometheus/Grafana), Metrics (Hawkular), Alerting (Pro,metheus) and Events. UI/API that encompasses entire platform (Orchestration, Container Images, Builds, Deployments, Service Catalog and more). Network and Security Isolation between projects, containers and namespaces. SDN out-of-box based on openvswitch but supporting many SDN plugins (Nuage, Contrail, NSX-T, Cisco ACI and more). Out-of-box solution for routing Ingress and Egress Traffic. Container Native Storage (based on GlusterFS, runs in containers using local disks and integrates with platform). Concept for Infrastructure and Storage Nodes, Kubernetes just has masters and nodes. CRI-O OCI based implementation of Kubernetes Container Runtime Interface (supports any OCI compliant container image format). And much much more...  Deploy OpenShift on OpenStack  We have automated the deployment of OpenShift on OpenStack in Ansible. The playbooks can deploy OpenShift Enterprise, OKD or even a disconnected install. Instructions and the playbooks themselves can be found in the following GitHub repository.  https://github.com/ktenzer/openshift-on-openstack-123  You will of course need to prepare a few things like a OpenStack project, image and tenant network in OpenStack but those things are documented in the GitHub readme. A typical OpenShift environment will be 3 x Masters, 2-3 x Infras and 2 or more Nodes. Below is a diagram to help visualize the deployment.    [Bastion]  Log into the openshift bastion.  $ ssh -i &lt;openstack keypair&gt; cloud-user@&lt;bastion floating ip&gt; Create student accounts on OpenShift  Clone GitHub repository.  $ git clone https://github.com/ktenzer/its-a-paas.git $ cd its-a-paas/openshift/playbooks Run playbook to create system users.  $ ansible-playbook create_users.yml Run playbook to create openstack users.  $ source /home/cloud-user/keystonerc_admin Note: You may need to copy the keystonerc file to the bastion host.  $ ansible-playbook create_openstack_users.yml Note: If you are running both Magnum and OpenShift labs and want the same usernames and passwords, simply copy the its-a-paas/k8s/playbooks/credentials directory before running the playbooks. If the credentials are already existing they will be used instead of new ones generated.  Running the Magnum and OpenShift Labs  By default there are 40 student accounts created student0-student40. The passwords are located in the credentials/&lt;#&gt;/password.txt file. Provide students with their username and password as well as the IPs of the Magnum and OpenShift Bastions.  For Magnum students also need the floating ips of all of the Kubernetes nodes. This is because Magnum provides no way to route traffic into platform as such we are exposing services out the node physical floating IPs.  For OpenShift students need the URL to the master API/UI. By default we use xip.io for DNS so it is just floating ip of the master in OpenStack (https://openshift.&lt;master lb floating ip&gt;.xip.io:8443). In the future we plan on using Designate from OpenStack which provides DNSaaS.  The lab can be found at the below GitHub repository.  https://github.com/ktenzer/its-a-paas  The exercises for Kubernetes are located under the k8s directory. The OpenShift exercises are located under the openshift directory. The first three exercises are the same for both but OpenShift has additional exercises that focus on day 2 operations.  Summary  In this article we went through the steps in order to build a software-defined cloud with distinct, decoupled infrastructure (IaaS) and platform (PaaS) layers. The deployment of the IaaS and the two different PaaS layers Magnum as well as OpenShift are opinionated deployments. We went through the steps to prepare for a lab or workshop environment with the goal to give students the ability to try out some of the differences between vanilla Kubernetes and OpenShift.  Happy Learning!  (c) 2018 Keith Tenzer  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  ","categories": ["OpenShift"],
        "tags": ["Containers","IaaS","Kubernetes","Linux","OpenShift","OpenStack","PaaS"],
        "url": "/openshift/its-a-paas-vanilla-kubernetes-vs-openshift-on-openstack-lab-setup-guide/",
        "teaser": null
      },{
        "title": "Container Native Virtualization (Kubevirt): The Future for Virtual Machines is Here!",
        "excerpt":"  Overview  Immediately after Solomon Hykes first showed Docker to the public at PyCon in 2013, in his now famous \"docker run demo\", IT folk started asking, what does this mean for virtualization? We only spent the previous 10-15 years virtualizing, seemingly everything, so understandably people were slightly apprehensive. Industries had been built and careers established, clearly virtualization would be an important part of the future and not simply replaced, right?  In this article we will aim to understand the value of virtualization in a container-driven world, explore the current virtualization capabilities in Kubernetes and get started with Container Native Virtualization (Kubevirt) using Red Hat's Kubernetes enterprise distribution, OpenShift.    The Value of Virtualization in a Container-Driven World  If we think about the real value of virtualization as a game changer, it is the ability to abstract hardware from the operating system. Everything else such as isolation, manageability, efficiency, etc are all features. Basically a virtualization platform is a hardware agnostic scheduler for operating systems. Looking at the value containers provide, it is as an abstraction between applications and operating system. Everything else such as multi-cloud, portability, speed, agility, deployment strategies, etc are all features.  In the end both Virtualization and Containerization are doing basically one thing and that is providing an abstraction layer. The only difference is where the abstraction occurs. For Virtualization it is the operating system, while for containers it is the application. Clearly without a doubt we need both. That is not the question. The question is do we need both in separate platforms, or can it all be collapsed and if so what are the advantages?  I think ultimately a single platform will emerge that provides both virtualization as well as containers and that platform already exists today, Kubernetes. There is a huge amount of overlap with what a virtualization platform is doings vs that of a container platform like Kubernetes. Scheduling, monitoring, quotas, RBAC, integration with compute, network, storage, HA and so on. In addition you need separate teams and operations processes to run various platforms so there is a lot of running costs that can be avoided. It makes zero sense to continue down the path of multiple platforms.  Bringing Virtualization to Kubernetes  The upstream project working on bringing virtualization to Kubernetes is called Kubevirt. Kubernetes is a perfect platform for virtual machines. Kubernetes already has a lot more intelligence that any virtualization platform ever had or will. It has built-in a framework called the Operator Framework. An operator is essentially a way to package, manage and most importantly operate Kubernetes native applications. The operate part is what allows us to build intelligence into how processes or even events should be automatically handled for a specific application. Taking a step back, what is an application? Basically a program that runs on the operating system. Virtualization is by definition an application. The operator framework is used to provide an operator for running virtualization on Kubernetes because in fact, a virtual machine is just an application.  Today you are already able to run virtual machines on Kubernetes and it is even technology preview in Red Hat's enterprise Kubernetes platform, OpenShift. Kubevirt works essentially by running libvirt (KVM) in a container. Libvirt is the process running virtual machines and supports the qcow2 image format. The only real difference is now, Libvirt runs in a container and can be controlled, scheduled as well as managed via Kubernetes. This truly brings both worlds together.    Kubevirt has come a long way in a relatively short period of time. Still some gaps do exists and there is more work to be done in area of HA, live migration and dynamic resource sharing. Kubernetes and specifically cloud-native, where Kubernetes started has a different view on HA. Getting the virtualization primitives complete will take some time but it is happening and the operator framework is a key piece.  Getting Started with Kubevirt  In this example I am using OpenShift but this will work the same on Kubernetes. OpenShift is enterprise-grade Kubernetes plus a lot of additional things. Instead of \"oc\" cli command you would substitute that with the \"kubectl\" command. Keep in mind that for OpenShift Kubevirt is still technology preview.  Setup permissions  The Virtualization control plane needs additional permissions in it's namespace. By default OpenShift is very restrictive and SELinux prevents such access rights.  $ oc adm policy add-scc-to-user privileged -n kube-system \\ -z kubevirt-privileged $ oc adm policy add-scc-to-user privileged -n kube-system \\ -z kubevirt-controller $ oc adm policy add-scc-to-user privileged -n kube-system \\ -z kubevirt-apiserver Deploy Kubevirt components  These components are part of the control plane and facilitate communications to virtual machines as well as enable their primitives.  $ RELEASE=v0.11.0 $ oc apply -f https://github.com/kubevirt/kubevirt/releases/download/${RELEASE}/kubevirt.yam $ oc apply -f https://github.com/kubevirt/kubevirt/releases/download/${RELEASE}/kubevirt.yam $ oc apply -f https://github.com/kubevirt/kubevirt/releases/download/${RELEASE}/kubevirt.yaml $ curl -O -L https://github.com/kubevirt/kubevirt/releases/download/${RELEASE}/virtctl-${RELEASE}-linux-amd64 Install Virtctl Client  Virtctl provides a CLI utility to interact with virtualk machines.  $ sudo mv virtctl-${RELEASE}-linux-amd64 /usr/local/bin/virtctl $ sudo chmod +x /usr/local/bin/virtctl Configure Container Data Importer  The cdi services allows for importing virtual machines. This is also optional.  $ oc adm policy add-scc-to-user privileged -z cdi-sa $ VERSION=v1.4.1 $ oc create -f https://github.com/kubevirt/containerized-data-importer/releases/download/$VERSION/cdi-controller.yaml $ oc apply create -f https://github.com/kubevirt/containerized-data-importer/releases/download/$VERSION/cdi-controller.yaml $ oc create -f https://github.com/kubevirt/containerized-data-importer/releases/download/$VERSION/cdi-controller.yaml Verify Kubevirt Deployment  The Kubevirt pods are marked in bold.  $ oc get pods -n kube-system NAME READY STATUS RESTARTS AGE cdi-api-5c69fdb6db-6bknl 1/1 Running 0 6d cdi-api-5c69fdb6db-dl95k 0/1 Evicted 0 6d cdi-api-5c69fdb6db-gp5mv 0/1 Evicted 0 16d cdi-api-5c69fdb6db-mwwxs 0/1 Evicted 0 6d cdi-deployment-5fddc58c4d-mxpsn 1/1 Running 9 28d cdi-uploadproxy-85b8ff4884-x4kp2 1/1 Running 2 28d master-api-master0 1/1 Running 241 36d master-api-master1 1/1 Running 237 36d master-api-master2 1/1 Running 243 36d master-controllers-master0 1/1 Running 233 36d master-controllers-master1 1/1 Running 231 36d master-controllers-master2 1/1 Running 221 36d master-etcd-master0 1/1 Running 2495 36d master-etcd-master1 1/1 Running 2482 36d master-etcd-master2 1/1 Running 2489 36d virt-api-5cf4576cc8-br65d 1/1 Running 0 2h virt-api-5cf4576cc8-mpzrl 1/1 Unknown 1 28d virt-api-5cf4576cc8-qq5k6 1/1 Running 0 15d virt-api-5cf4576cc8-w7wk6 0/1 Evicted 0 26d virt-controller-5cdd99564c-9x2mr 1/1 Running 25 28d virt-controller-5cdd99564c-hsp6k 1/1 Running 0 26d virt-handler-67xjk 1/1 NodeLost 1 28d virt-handler-hhd2s 1/1 Running 1 28d virt-handler-rhvpp 1/1 Running 2 28d Check Services, Kubevirt services are marked in bold.  $ oc get service -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cdi-api ClusterIP 172.30.219.181 &lt;none&gt; 443/TCP 28d cdi-uploadproxy ClusterIP 172.30.136.133 &lt;none&gt; 443/TCP 28d kube-controllers ClusterIP None &lt;none&gt; 8444/TCP 36d kubelet ClusterIP None &lt;none&gt; 10250/TCP 36d kubevirt-prometheus-metrics ClusterIP 172.30.69.165 &lt;none&gt; 443/TCP 28d virt-api ClusterIP 172.30.76.176 &lt;none&gt; 443/TCP 28d Configure Kubevirt UI  This is optional but I personally find the UI very nice from user experience point-of-view.  $ oc new-project kubevirt-web-ui $ git clone https://github.com/kubevirt/web-ui-operator.git $ cd web-ui-operator/ $ oc apply -f deploy/service_account.yaml $ oc adm policy add-scc-to-user anyuid -z kubevirt-web-ui-operator $ oc apply -f deploy/service_account.yaml -n kubevirt-web-ui $ oc adm policy add-scc-to-user anyuid -z kubevirt-web-ui-operator $ oc apply -f deploy/role.yaml $ oc apply -f deploy/role_extra_for_console.yaml $ oc apply -f deploy/role_binding.yaml $ oc apply -f deploy/role_binding_extra_for_console.yaml $ oc apply -f deploy/crds/kubevirt_v1alpha1_kwebui_crd.yaml $ oc apply -f deploy/operator.yaml $ oc apply -f deploy/crds/kubevirt_v1alpha1_kwebui_cr.yaml Verify Kubevirt UI Deployment  Verify pods.  $ oc get pods -n kubevirt-web-ui NAME READY STATUS RESTARTS AGE console-6b66b66dd7-9gdvn 1/1 Unknown 1 28d console-6b66b66dd7-wwxm8 1/1 Running 0 15d kubevirt-web-ui-operator-6dd9547864-f5t49 0/1 Running 0 15d kubevirt-web-ui-operator-6dd9547864-n8fxd 1/1 Unknown 1 26d Verify services.  $ oc get services -n kubevirt-web-ui NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE console ClusterIP 172.30.124.253 &lt;none&gt; 443/TCP 28d Verify routes.  $ oc get routes -n kubevirt-web-ui NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD console kubevirt-web-ui.apps.46.4.207.246.xip.io console https reencrypt/Redirect None Connect to Kubevirt UI  Using web browser enter the route, in this case https://kubevirt-web-ui.apps.46.4.207.246.xip.io. You will be prompted to login using OpenShift credentials.  Deploying a Virtual Machine  In this example we will be using a template to deploy a virtual machine.  Create Project (namespace) for Virtual Machines    Using CLI to create new project  $ oc create project vms Deploy Virtual Machine Template  First login to the Kubevirt UI. You can either list routes as we did above for the project or look in OpenShift UI as is shown below.    The Kubevirt UI also shows overall status of the cluster. This is to provide an admin single-pane-of-glass regardless of containers or virtual machines.    Select virtual machines and create virtual machine from template.    Enter the yaml which is available in github.    Select create to deploy virtual machine template.  Using CLI to deploy virtual machine template.  $ oc create -f https://raw.githubusercontent.com/kubevirt/demo/master/manifests/vm.yaml Note: this template will deploy virtual machine in default project. You can change that by setting \"namespace\" to something else in the metadata section of template.  Start Virtual Machine  Once template is deployed you should be able to start the virtual machine.    That is it, pretty easy and amazing how well virtual machines are already integrated into Kubernetes and OpenShift. The best is yet to come!  Summary  In this article we discussed the value of virtualization and containers, together is better and clearly the way forward. We examined the value of collapsing platforms to have a single platform to drive both container and virtual machine workloads. We looked at where Container Native Virtualization (Kubevirt) is at today and explained some of the gaps versus traditional virtualization. Finally we walked through a hands-on guide to get Container Native Virtualization (Kubevirt) up and running on Kubernetes or OpenShift.  Happy Containers and Virtual Machines Existing Together!  (c) 2018 Keith Tenzer  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  ","categories": ["OpenShift"],
        "tags": ["cnv","Container Native Virtualization","Kubernetes","Kubevirt","Linux","OpenShift","opensource","Virtual Machines","Vitualization"],
        "url": "/openshift/container-native-virtualization-kubevirt-the-future-for-virtual-machines-is-here/",
        "teaser": null
      },{
        "title": "The Coffee is Getting Cold, It's Time to GO - Getting Started Building Microservices",
        "excerpt":"  Overview  Java has been around a really, really long time. Certainly it continues to evolve and has evolved. Java has always been a \"can do anything\" programming language. It has more frameworks and middleware than there are stars in the sky. It is portable anywhere and of course probably 8 out of 10 developers today know Java to some degree. Given all of this though is Java the path forward?  Looking forward I think the clear trend is microservices and beyond. Therefore the question is a lot simpler, is Java the best path forward for microservices?    How Java Became #1  Java was released in 1996. At that time the biggest issue was compiling code and porting. At this time there were many Unix, Linux, Windows flavors and many other variants. Open systems was the key initiative. Java came along and essentially offered instant portability. It created the JVM and runtime environment that would guarantee execution of code, regardless of platform, as long as there was a JRE. In one swoop, Java took away the problem of portability from developers, yay!  Another thing Java did was create a language that was much more readable, easier to learn and way more modular than anything that existed previously. It was not an accident Java became the #1 and has been for so long.  However Java Got Fat...  Just like with anything, if you are #1 you might get complacent and sit on the couch. As Java grew over the years, it got bigger and bigger and bigger. More framework, framework to manage framework, tools to manage tools, don't get me started with Gradle and Maven. It grew and grew into the can do anything language for everything. Often frameworks were used as a crutch, code as a result became sloppy, technical debt grew, scalability was not by design and then came cloud. Things started breaking, surprise.  Java has since, been trying to lose some weight, springboot and other so-called lightweight frameworks came along. By lightweight we still mean overweight and slow from microservice perspective, just maybe not fat. Microservices did indeed change the game and possibly, could be the trigger that ends Java's reign.  Microservices Change the Rules  With the introduction of not only microservices, but also container platforms to run microservices, the rules have changed, just like in the 90s when Java was born, under the open systems initiative.   Containers means compiling code and portability is no longer an issue. The value of a JVM is much less than it once was. Startup times and time to load code is critical in microservices. If your code is slow to load it really hampers your options and presents a lot of challenges. Microservices are mostly REST APIs that exchange data, asynchronous operations are key. Microservices require scalability and scalability generally requires concurrency. Languages that are compact, script-like, easy to write and read are preferred. Languages with simple dependency management are ideal.  At the time when Java was created none of these rules applied, it was developed to meet a totally different set of goals and challenges. Yet many Java developers go along on their merry way, trying to port their monoliths to microservices, using the same mentality that got them there? This just isn't logical. If the rules changed then clearly something else would be a wiser choice that adapts to the new game better?  Thankfully there are some interesting choices. One is Golang and the other could be Quarkus. Both attempt to adhere to the new rules. Quarkus is native statically compiled Java and uses an ultra-fast JVM called Graal. It also has built-in frameworks for building microservices that help scale your code such as vertx. In this aspect it is similar to Golang. It is very lightweight and fast. I haven't seen comparison to Golang but it should be similar in this regard. Where things start to look different though is when we get into concurrency.  Doing concurrency and scalability in Java is really hard and Java allows you to write bad code, there is a lot of it. Quarkus has a much tighter opinion and narrow set of tooling so that should help steer developers better than its predecessor. Asynchronous programming is also not that easy in Java, again depends on frameworks but it isn't baked-in by any means. In Golang concurrency is a feature, it is built-in and that alone should get your attention. In addition Golang is a super modern language, easy to write, easy to read, no fluff and dependency management is not stabbing your eyes out with an ice pick.  Good old Java of course can still provide value and does in microservices, I am just saying it is likely not the best choice if you are starting from scratch. If I could sum it up I would say, look into Quarkus if the goal is porting Java app to microservice or building Java microservices and otherwise Golang. Of course there are many other languages as well, but none of them in my opinion apply to the challenges of today as well as Golang or Quarkus.  Getting Started with Golang  All the code examples are available in the following Github repository: https://github.com/ktenzer/go-hello-world.  Setup your environment  $ vi ~/.bashrc  # User specific aliases and functions export GOPATH=/home/ktenzer/go export GOBIN=/home/ktenzer PATH=$PATH:$GOBIN Create go directory  $ mkdir -p /home/ktenzer/go/src Install Go  $ sudo dnfs install -y go Optionally you can install manually as well https://golang.org/doc/install  Clone Git Repo  $ git clone &lt;repo url&gt; /home/ktenzer/go/src Create src and vendor directories in repo  $ mkdir -p /home/ktenzer/go/src/&lt;repo&gt;/src $ mkdir -p /home/ktenzer/go/src/&lt;repo&gt;/vendor Install Dependency Management Tool  You will want to use other frameworks and tools. The dependency management tool I would recommend is called dep.  $ curl https://raw.githubusercontent.com/golang/dep/master/install.sh | sh Initialize Dep  $ cd /home/ktenzer/go/src/&lt;repo&gt; $ dep init Add dependencies  $ dep ensure --add github.com/gorilla/mux This will add the dependency and version to Gopkg.toml file automatically.  [[constraint]] name = \"github.com/gorilla/mux\" version = \"1.7.2\" Creating First Microservice  As mentioned concurrency is built-in, one of the great advantages of Golang for microservices. As such the net/http module already has concurrency, no extra code needed. The only thing that needs to be deciding upon is what to you for URL routing? There are several Frameworks: Gorilla, Gin, Chi, etc. There are of course lots of pros and cons. Some route URLs faster while others have more capabilities. Personally I like Gorilla, it isn't the fastest but I like it's utility.  In this microservice we will run a concurrent http service that responds to a status API route with a message and a version. We will output JSON.  Main.go  In the main.go we will simply intantiate a new URL router and configure http service to listen on port 8000 and implement our router.   package main   import (      \"log\"      \"net/http\" )  func main() {      router := NewRouter()      log.Fatal(http.ListenAndServe(\":8000\", router)) }  Routes.go  In the routes we obviously configure API endpoints and routing. Here we have a single endpoint \"/status\". Calls sent to the /status endpoint will be handled or routed to our GetStatusEndpoint function.  In addition we create a new router object with our configuration. Each route incoming route is processed, checked if it exists and if so the route is logged and a handler function is called.  package main  import (     \"github.com/gorilla/mux\"     \"net/http\" )  type Route struct {     Name string     Method string     Pattern string     HandlerFunc http.HandlerFunc  }  type Routes []Route func NewRouter() *mux.Router {     router := mux.NewRouter().StrictSlash(true)     for _, route := range routes {         var handler http.Handler         handler = route.HandlerFunc         handler = LogApi(handler, route.Name)         router.             Methods(route.Method).             Path(route.Pattern).             Name(route.Name).             Handler(handler)     }      return router  }  var routes = Routes{     Route{         \"GetStatusEndpoint\",         \"GET\",         \"/status\",         GetStatusEndpoint,     }, } Handlers.go  The handers are where we want to implement our API. In this case we just set our status struct and return it as json using the json encoder.   package main  import (     \"encoding/json\"     \"net/http\" )  func GetStatusEndpoint(w http.ResponseWriter, r *http.Request) {     var status Status     status.Msg = \"Hello World\"     status.Version = \"1.0.0\"     json.NewEncoder(w).Encode(status) }  Logger.go  Each incoming URL is logged, this is called in the routes.go, this is just the implementation for that function.  package main import (     \"log\"     \"net/http\"     \"time\" )  func LogApi(handler http.Handler, apiRoute string) http.Handler {     return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {         startTime := time.Now()         handler.ServeHTTP(w, r)         log.Printf(             \"%s\\t%s\\t%s\\t%s\",             r.Method,             r.RequestURI,             apiRoute,             time.Since(startTime),         )     }) } Status.go  A struct to store status of service, nothing more to say.   package main  type Status struct {     Msg string `json:\"msg\"`     Version string `json:\"version\"` }  Compiling the code  Compiling code in Golang is really easy, just run go.  $ go install go-hello-world/src/hello The binary will be called hello and exist in the $GOBIN directory, in this case /home/ktenzer.  Running the code  Since Golang produces a binary you simply execute it.  $ /home/ktenzer/hello Using curl we can hit our /status API endpoint.  $ curl http://localhost:8080/status {\"msg\":\"Hello World\",\"version\":\"1.0.0\"} Check hello service  Since we log API requests you should see a message printed to stdout.  $ /home/ktenzer/hello 2019/06/13 13:13:11 GET /status GetStatusEndpoint 58.842µs Also very interesting is the time the API took to process, in this case 58.842 micro-seconds. As mentioned Gorilla isn't one of the faster URL routers either.  Summary  In this article we have discussed some of the history of Java. We talked about how the rules are changing for microservices, why Golang or Quarkus may be a better choice than standard Java for concurrent, asynchronous microservices and both could very likely become languages of choice in the microservice space. Finally we went through a hello-world implementation of a microservice written in Golang. This should give you a good idea of how to get started using Golang. Feedback is always welcome.  (c) 2019 Keith Tenzer  &nbsp;  ","categories": ["cloud-native"],
        "tags": ["code","concurrency","Containers","Golang","Kubernetes","microservice","OpenShift","Quarkus","scalability"],
        "url": "/cloud-native/the-coffee-is-getting-cold-its-time-to-go-getting-started-building-microservice/",
        "teaser": null
      },{
        "title": "Powerful but Simple CI/CD with Azure DevOps and Go",
        "excerpt":"  Overview  Microsoft has wasted little time getting value out of their GitHub acquisition. They have now fully integrated GitHub and authentication into an already powerful DevOps platform called \"Azure DevOps\". I have until this moment had zero enjoyment, setting up and maintaining CI/CD tooling usually involving some form of our dear butler, Jenkins. Nothing wrong with our old Jenkins but let's face it, he is just overhead at this point, better to just put him to rest, he has earned it.  Azure DevOps has the following value:   It's in the cloud, consumed as-a-service Completely Integrated with GitHub It is free Authentication using GitHub user Don't need to use it with Azure Supports basically every language, I am doing CI/CD with Go Simple yaml to configure no Groovy/DML Jenkins horror Yaml pipeline files auto-generated for your language (just needs minor tweaks) Your code is built, unit tests are run, you can do acceptance tests and it is setup in a few minutes    Getting Started with Go  Sign-up  I simply used my GitHub account: https://azure.microsoft.com/en-in/services/devops/  Create Project  You can create a private or public project.    Select Source Control Management  I am using GitHub.    Select Repository  In this case I am using ktenzer/go-hello-world    Configure Pipeline  Azure DevOps generates a base pipeline so it is really easy to tweak things. In the case of Go, hopefully you are using dep for dependency management. Due to your source tree structure and using dep or other tools you may need to tweak a few things. You can view my source tree at https://github.com/ktenzer/go-hello-world.  # Go # Build your Go project. # Add steps that test, save build artifacts, deploy, and more: # https://docs.microsoft.com/azure/devops/pipelines/languages/go  trigger: - master - feature/*  pr: - master  pool:   vmImage: 'ubuntu-latest'  variables:     GOPATH: '$(system.defaultWorkingDirectory)/gopath' # Go workspace path   GOBIN:  '$(GOPATH)/bin' # Go binaries path   PLUGIN_DIR: '$(GOBIN)'   modulePath: '$(GOPATH)/src/hello' # Path to the module's code  steps: - script: |     echo \"GOBIN $(GOBIN)\"     echo \"GOPATH $(GOPATH)\"     echo \"REPO PATH $(GOPATH)/src/github.com/$(build.repository.name)\"      mkdir -p '$(GOPATH)'     mkdir -p '$(GOBIN)'     mkdir -p '$(GOPATH)/pkg'     mkdir -p '$(modulePath)'     echo \"Moving $(system.defaultWorkingDirectory)/* To $(modulePath)\"     mv $(system.defaultWorkingDirectory)/* $(modulePath)     shopt -s extglob     shopt -s dotglob     echo '##vso[task.prependpath]$(GOBIN)'     echo '##vso[task.prependpath]$(GOROOT)/bin'   displayName: 'Set up the Go workspace'  - script: |     go version     if [ -f Gopkg.toml ]; then         curl https://raw.githubusercontent.com/golang/dep/master/install.sh | sh         dep ensure     else       echo \"ERROR no Gopkg.toml found!\"       exit 1     fi     go install hello/src/hello   workingDirectory: '$(GOPATH)/src/hello'   displayName: 'Get dependencies, then build'  - script: |     $(GOBIN)/hello &amp;     PS=`ps -ef |grep hello |grep -v grep |awk '{print $2}'`     echo \"hello $PS\"     curl http://localhost:8000/status     kill -9 $PS  The trigger will always run pipeline for every new commit. The pr will force all pull requests (in this case in feature branch) to go through pipeline and validation. This allows someone else to review the changes before merging into master.  In addition to building the code using dep I am providing and example of a simple acceptance test for this service using curl to send API response.  Since you essentially have a bash interface through the \"script\" module you can really do anything and do so very easily.  Save Pipeline  The pipeline yaml file is stored directly in the Github repository.    Once pipeline is committed it is started. From now on any code change or update to the Repository will cause the pipeline to run.    Each pipeline step is shown and you can drill in to see details. Makes it really easy to troubleshoot especially since you can use bash commands like echo to see variables or paths.  Drilling into the Cmdline step we see our acceptance test execute.    CI/CD Demo  Now that we can successfully build our code using Azure DevOps it is time to see it in action. I recommend doing all work in a feature branch and then a pull request from that. In this case we will commit bad code that breaks the build and fix it. A common CI/CD lifecycle without of course jeopardizing the master branch.  First we commit broken code via a pull request. We can see the pipeline is running.    The pipeline failed and updated the commit request in the feature branch.    More details on the failed commit.    Next we will fix the broken code. Again pipeline runs but this time it is successful.    Finally the code can be merged to the master branch.    Looking at Azure DevOps we can also see both of the pipeline runs and notice the icon for a branch.    That is how easy CI/CD can be!  Summary  In this article we discussed Azure DevOps and it's value for offloading CI/CD to the cloud. We also saw a hands-on tutorial to get Azure Devops up and running with Go.  Many organizations already use GitHub private and public, in my view, this is just the next logical step. Why maintain CI/CD infrastructure and deal with the complexities on-premise? I must say I was very surprised at the quality and ease of Azure DevOps. It took a friend of mine quite some time to convince me to try it but I finally did and am not looking back.  Happy CI/CD'ing!  (c) 2019 Keith Tenzer  ","categories": ["code"],
        "tags": ["Azure","Cloud","continuous development","continuous improvement","DevOps","Golang"],
        "url": "/code/powerful-but-simple-ci-cd-with-azure-devops-and-go/",
        "teaser": null
      },{
        "title": "CI/CD with Ansible Tower and GitHub",
        "excerpt":"  Overview  Over the last few years CI/CD (Continuous Integration/Continuous Deployment) thanks to new technologies has become a lot easier. It should no longer be a major thorn in the side of developers. Many are moving to cloud platforms which has CI/CD built-in (Azure DevOps for example), others are using Kubernetes which clearly reduces a lot of the complexity around CI/CD. Still at many organizations I see Jenkins or other complex and often homegrown tooling. I certainly recognize this tooling was needed but in 2019 there are better, more streamlined options. Now I get it, our butler Jenkins has served us well, for many years, he has become part of our family. But just like the famous Butler, Alfred from Batman, he has gotten old and likely it is time to look into retirement.  In this article we will discuss and demonstrate how to use Ansible Tower and GitHub for CI/CD.  A video presentation and demonstration is available at following URL: https://youtu.be/lyk-CRVXs8I    Why Ansible Tower for CI/CD?  Most organizations are adopting an infrastructure-as-code approach, that is to say everyone, even operations are becoming developers or at least working and thinking like developers. The automation tool of choice or even standard if you will is clearly Ansible. However Ansible is just an automation runtime, it has no API, RBAC, Credentials and does not enable collaboration between teams. This is where Ansible Tower (or AWX the opensource project) shines. Here are some advantages Ansible Tower bring to CI/CD:   Reuse infrastructure and operations playbooks in CI/CD Better collaboration between devops and infrastructure teams Simple CI/CD, just Ansible, no DML, no Groovy, no god knows what Ansible Tower is a enterprise product not a project like Jenkins Streamlined CI/CD instead of every devops team on their own Self organizing and governing because that is what Ansible does best  Now clearly Ansible Tower cannot do everything. It does not have for example a built-in CI/CD approval process and does not specifically focus on CI/CD. Instead Ansible Tower is an automation platform. It views the world through job templates (a playbook) and workflows (group of playbooks). In this case a workflow is a CI/CD pipeline with an API in front of it of course. Once you have your workflows or pipelines and an API in front of them you are 90% done. The rest is just how the process should flow and you might not even need any extra tooling to do that, certainly not complex tooling like Jenkins.  Ansible Tower CI/CD Architecture  The point of this article is to demonstrate that CI/CD can work very simply and with just Ansible Tower and GitHub. The diagram below illustrates what we will be building. This focuses only really on CI but CD would just be additional workflows.    The main purpose of CI is of course to protect the master branch so it always builds. The only way you do that of course is to check code into another branch (like a feature branch), test that code, review code and only merge to master when you are sure all tests have passed. The above architecture accomplished exactly that and does so with a very simplified approach that leverages Ansible Tower as our CI engine. We won't be going into the CD details in this article, but that would just be additional workflows to deploy artifacts generated by the CI process into dev-&gt;test-&gt;production. Using this architecture one could use GitHub releases to store artifacts. GitHub has ability to then trigger a webhook when latest release is updated which in turn could trigger an Ansible Tower CD workflow.  Ansible Tower Configuration  First we will want to configure a workflow in Ansible Tower that will be a CI/CD pipeline. I think it is good practice to have one workflow per pipeline. In this example we have a single workflow which handles CI.  Configure Credentials  We need machine credentials for Fedora since we will be accessing a fedora host to execute our build. In Ansible Tower credentials allow you to access systems without needing to connect to them directly, like you would with normal Ansible. Users don't even need to know credentials.    Configure Inventory  Inventories in Ansible Tower are generated dynamically. This is important because when we provision a VM we don't at the time know what it's IP will be. Of course after we provision we want to access the VM. The solution is to perform an inventory update after provisioning and then parameterize provisioning and execution playbooks so they create/use same hostnames.  Here we are using OpenStack but this could be any public or private cloud. For doing CI we created an inventory called development and this is what we will be later using in our job templates.    Configure Projects  Projects in Ansible Tower are simply SCM repositories where playbooks reside. In this case of course a GitHub repository. Here notice the developer playbooks to do the build are located with the source code where the provisioning playbooks are in a different repository. This is where we get collaboration between infrastructure operations teams and devops. Something that rarely existed in CI/CD.  Links to go-hello-world and paas-and-iaas repositories.    Configure Job Templates  For our CI pipeline we will need to run three playbooks. Each playbook is a job template. In Ansible Tower a job template is a playbook, it's inventory, survey, credentials and various other things that go way beyond plain old Ansible. The three job templates are one to provision a fedora instance (from infrastructure / operations team), a playbook to perform the go build / tests (from devops team) and a playbook to cleanup (from infrastructure / operations team).    Configure Ansible Tower Workflow  As mentioned a workflow will bring all the playbooks/job templates together. Here we configure a workflow called GO CI/CD Workflow. Notice the below workflow. We are using our three job templates provision fedora, run go build and remove instance. In addition the second step performs an inventory update for the development inventory. A workflow allows you to perform granular inventory updates wherever needed.    We aren't quite done, the last step is to add a survey to the Workflow. Each of the playbooks of course accept parameters. These need to be added in form of survey. A survey provides a user friendly interface for parameterizing workflows or job templates in Ansible Tower.    Ansible Tower CI/CD Setup  The first thing to do is setup tower-cli. This is a much better tool than curl to trigger Ansible Tower. From the webhook we will use tower-cli to update projects in Ansible Tower and launch workflows.  Install tower-cli  My fellow Colleague, Andreas Neeb created a role to install tower-cli.  https://github.com/andyneeb/ansible-demo-infra/tree/master/roles/tower-cli-setup  Simply create playbook that uses role on localhost or host in your inventory where you want to install tower-cli.  Fork go-hello-world  &nbsp;  Configure GitHub Webhook Token  In order to update commit status in GitHub we need permissions and a webhook token.  In GitHub under your user-&gt;settings go to developer settings.    Select personal access tokens and create a new one.    Add a note and select repo:status. This is the minimum permission in GitHub needed to update a commit status. For CI/CD you need to be able in GitHub update commit status.  Configure GitHub Webhook  In GitHub go to the go-hello-world project. Under settings create a new webhook.    Make sure content-type is application/json and set a secret, this is a password that will be used to authenticate to the webhook listener we will deploy next.  Install Webhook Listener on Ansible Tower  The folks at Arctiq created a role and playbook for setting up a webhook listener on Ansible Tower.  https://github.com/ArctiqTeam/tower-webhook  Simply clone the repository, update the vars.yml  and hosts file according.  # vi hosts [webhook] webhookserver ansible_host=46.4.207.248 ansible_ssh_port=22 # vi vars.yml --- projects: - id: 7 name: go-hello-world # export GH_SECRET= # export TOWER_PASSWORD= # ansible-playbook install.yml The playbook will create a systemctl service called webhook so it can easily be started and stopped. The webhook will run on port 9000.   Configure Webhook Listener  In order to use webhook for CI/CD some changes need to be made. Update the webhook.conf as follows.  # vi /etc/webhook/webhook.conf [   {     \"id\": \"go-hello-world\",     \"execute-command\": \"/usr/local/bin/go-hello-world.sh\",     \"pass-environment-to-command\":     [       {         \"source\": \"payload\",         \"name\": \"head_commit.id\",         \"envname\": \"COMMIT_ID\"       },       {         \"source\": \"payload\",         \"name\": \"pusher.name\",         \"envname\": \"PUSHER_NAME\"       },       {         \"source\": \"payload\",         \"name\": \"pusher.email\",         \"envname\": \"PUSHER_EMAIL\"       },       {         \"source\": \"payload\",         \"name\": \"head_commit.id\",         \"envname\": \"COMMIT_ID\"       },       {         \"source\": \"payload\",         \"name\": \"pusher.name\",         \"envname\": \"PUSHER_NAME\"       },       {         \"source\": \"payload\",         \"name\": \"ref\",         \"envname\": \"REF\"       }     ],     \"trigger-rule\":     {       \"match\":       {         \"type\": \"payload-hash-sha1\",         \"secret\": \"&lt;password&gt;\",         \"parameter\":         {           \"source\": \"header\",           \"name\": \"X-Hub-Signature\"         }       }     }   } ]  Restart webhook  # systemctl restart webhook Next update the shell script go-hello-world.sh. This will be triggered each time a push request is sent to GitHub. It will also receive environment parameters needed for CI from GitHub.  # vi /usr/local/bin/go-hello-world.sh #!/bin/sh printenv &gt;/tmp/env.out BRANCH=`echo $REF |sed -e 's/refs\\/heads\\///g'`  ### Set Github commit to pending CI/CD curl \"https://api.GitHub.com/repos/ktenzer/go-hello-world/statuses/$COMMIT_ID?access_token=79dc2a78c4c009c3c08e6cbc8233559c535fc8dc\" -H \"Content-Type: application/json\" -X POST -d \"{\\\"state\\\": \\\"pending\\\",\\\"context\\\": \\\"continuous-integration/go-hello-world\\\", \\\"description\\\": \\\"Ansible_Tower\\\", \\\"target_url\\\": \\\"https://46.4.207.248\\\"}\"  ### Update Project in Tower tower-cli project update 7 -v -h 46.4.207.248 --wait if [ $? != 0 ]; then         curl \"https://api.GitHub.com/repos/ktenzer/go-hello-world/statuses/$COMMIT_ID?access_token=&lt;token&gt;\" -H \"Content-Type: application/json\" -X POST -d \"{\\\"state\\\": \\\"error\\\",\\\"context\\\": \\\"continuous-integration/go-hello-world\\\", \\\"description\\\": \\\"Ansible_Tower\\\", \\\"target_url\\\": \\\"https://46.4.207.248\\\"}\"         exit 1 fi  ### Execute CI/CD Workflow in Tower tower-cli workflow_job launch --workflow-job-template=18 -v -h 46.4.207.248 -e openstack_user=admin -e openstack_passwd=&lt;password&gt; -e openstack_auth_url=http://85.10.236.4:5000/v3 -e openstack_project=development -e instance_name=gobuilder -e instance_security_group=dev_base -e instance_image=fedora_28 -e instance_ssh_key_name=admin -e ssh_user=fedora -e instance_network=development -e instance_flavor=m1.small -e repo_url=https://github.com/ktenzer/go-hello-world.git -e repo_branch=$BRANCH -e app_path=hello/src -e app_name=hello --wait  ### Update Github commit based on success or failure of CI/CD workflow if [ $? != 0 ]; then         curl \"https://api.GitHub.com/repos/ktenzer/go-hello-world/statuses/$COMMIT_ID?access_token=&lt;token&gt;\" -H \"Content-Type: application/json\" -X POST -d \"{\\\"state\\\": \\\"error\\\",\\\"context\\\": \\\"continuous-integration/go-hello-world\\\", \\\"description\\\": \\\"Ansible_Tower\\\", \\\"target_url\\\": \\\"https://46.4.207.248\\\"}\"         exit 1  else         curl \"https://api.GitHub.com/repos/ktenzer/go-hello-world/statuses/$COMMIT_ID?access_token=&lt;token&gt;\" -H \"Content-Type: application/json\" -X POST -d \"{\\\"state\\\": \\\"success\\\",\\\"context\\\": \\\"continuous-integration/go-hello-world\\\", \\\"description\\\": \\\"Ansible_Tower\\\", \\\"target_url\\\": \\\"https://46.4.207.248\\\"}\"         exit 0 fi  Run CI/CD Pipeline using Ansible Tower  Clone go-hello-world repository  # git clone https://github.com/ktenzer/go-hello-world.git Create feature branch  # git branch patch-1 # git checkout patch-1 Add broken code  # vi src/hello/main.go ... func main() {         &lt;broken&gt; \trouter := NewRouter() ...  Commit broken code  # git commit -a -m \"broken code\"  # git push origin patch-1 Create pull request in GitHub  Under branches you will now see the patch-1 branch.    You can now create a pull request in GitHub.    Notice that CI/CD is in progress and the source is Ansible Tower. Of course the CI will fail since we broke the code. In Tower you can see looking at the go build job.    Looking at our pull request in GitHub it also now shows error.    Notice the CI/CD process spawned a VM to perform build and run tests.    In this case the instance is not removed since the test failed. The idea is likely the developer would want to fix it and since we are using Ansible, once a fix is committed we will start where we left off, saving potentially a lot of time.  Fix broken code  # vi src/hello/main.go ... func main() { \trouter := NewRouter() ...  Commit broken code  # git commit -a -m \"fixed code\"  # git push origin patch-1 This will start the CI/CD process again except this time of course our code builds and CI is successful.  Looking at GitHub we see the pull request has gone through CI and all tests are successful.    In Ansible Tower we also see the jobs completed.    At this point a developer could review the code and merge it. In GitHub we will now merge the pull request.    The CI process is started again this time for the master branch. Once master builds we have a release and can start the CD process. We would as mentioned above, push the release to GitHub releases and that would in turn trigger another workflow or several workflows to deploy the release in various dev, test and production environments. We will not be doing this but I think you get the picture.  Summary  In this article we discussed a simpler solution for CI/CD by using just Ansible Tower and GitHub. We saw the advantages of making Ansible reusability the cornerstone of CI/CD, not only in simplicity but bringing infrastructure/operations teams together with development. We went through the configuration and saw a demonstration of how the solution works. If you are interested further or have any questions/feedback please feel free to reach out.  Happy CI/CD’ing!  (c) 2019 Keith Tenzer  &nbsp;  &nbsp;  &nbsp;  &nbsp;  ","categories": ["code"],
        "tags": ["ci/cd","Ansible","Ansible Tower","development","DevOps","Golang","opensource"],
        "url": "/code/ci-cd-with-ansible-tower-and-github/",
        "teaser": null
      },{
        "title": "How to Create a RHEL 8 Image for Hetzner Root Servers",
        "excerpt":"  Overview  In this article I will explain how to create your own custom RHEL 8 image for hetzner root servers. Hetzner offers new and used physical servers on a per/month basis (https://www.hetzner.de/sb). They offer extremely competitive pricing. These servers are perfect for demo or lab environments where you are operating your own platform and want it running 24/7!  Hetzner unfortunately does not offer RHEL images however, does provide a means for bringing your own image. A colleague of mine, Ingo Boernig, already shared a blog on how to do this for RHEL 7.5. This article with cover RHEL 8.0.    Install RHEL 8 in a VM  Download RHEL 8 ISO DVD from https://access.redhat.com/downloads/content/479/ver=/rhel---8/8.0/x86_64/product-software. Using KVM virt-manager (or another hypervisor) install RHEL 8 in a VM.    Once the VM boots follow the installation and use defaults. Select minimal server from the software selection. Also configure network to boot using DHCP.    Prepare RHEL 8 VM for Hetzner  Log into the VM using ssh or console.  Hetzner only supports one boot kernel. As such we will remove the rescue kernel.  # rm /boot/vmlinuz-0-rescue-adcc72dfe3ed4c049ffff0ec950a90d9 # rm /boot/initramfs-0-rescue-adcc72dfe3ed4c049ffff0ec950a90d9.img Hetzner creates a ram disk and uses the dracut tool. It expects dracut to be installed under /sbin. This is not the case with RHEL 8 so we will add a symlink.  # ln -s /usr/bin/dracut /sbin/dracut Finally create a tar zip of the RHEL 8 OS. The image needs to be called CentOS-75-el-x86_64-minimal.tar.xz because Hetzner is expecting certain image names but I assure you this will be RHEL 8.  # tar cJvf CentOS-75-el-x86_64-minimal.tar.xz --exclude=/dev \\ --exclude=/proc --exclude=/sys --exclude=/CentOS-75-el-x86_64-minimal.tar.xz / Deploy Image on Hetzner Server  In order to install OS or repair it you need to get into Hetner server rescue mode.        Using secure copy, upload the image to the Hetzner server /root directory.  # scp CentOS-75-el-x86_64-minimal.tar.xz root@hetzner_ip:/root Log into your Hetzner server using your ssh key. You should be placed into rescue mode.  Create a Hetzner configuration and use your custom image  root@rescue# vi config.txt  DRIVE1 /dev/sda  DRIVE2 /dev/sdb  SWRAID 1  SWRAIDLEVEL 0  BOOTLOADER grub  HOSTNAME keithcloud.lab  PART /boot ext3 512M  PART lvm vg0 150G   LV vg0 root / ext4 50G  LV vg0 swap swap swap 8G  LV vg0 var /var ext4 10G  LV vg0 tmp /tmp ext4 30G  IMAGE /root/CentOS-75-el-x86_64-minimal.tar.xz Install Image on root server.   root@rescue# installimage -a -c config.txt  Hetzner Online GmbH - installimage  Your server will be installed now, this will take some minutes You can abort at any time with CTRL+C ...  : Reading configuration done  : Loading image file variables done  : Loading centos specific functions done  1/18 : Deleting partitions done  2/18 : Test partition size done  3/18 : Creating partitions and /etc/fstab done  4/18 : Creating software RAID level 0 done  5/18 : Creating LVM volumes done  6/18 : Formatting partitions : formatting /dev/md/0 with ext3 done  : formatting /dev/vg0/root with ext4 done  : formatting /dev/vg0/swap with swap done  : formatting /dev/vg0/var with ext4 done  : formatting /dev/vg0/tmp with ext4 done  : formatting /dev/vg0/home with ext4 done  7/18 : Mounting partitions done  8/18 : Sync time via ntp done  9/18 : Downloading image (http) done  : Importing public key for image validation done  10/18 : Validating image before starting extraction warn  : No detached signature file found! 11/18 : Extracting image (http) done  12/18 : Setting up network config done  13/18 : Executing additional commands : Setting hostname done  : Generating new SSH keys done  : Generating mdadm config done  : Generating ramdisk done  : Generating ntp config done  14/18 : Setting up miscellaneous files done  15/18 : Configuring authentication : Fetching SSH keys done  : Disabling root password done  : Disabling SSH root login without password done  : Copying SSH keys done  16/18 : Installing bootloader grub done  17/18 : Running some centos specific functions failed  An error occured while installing the new system! See the debug file /root/debug.txt for details. Note: You will see an error because Hetzner will want perform a dnf update as last step. On a RHEL image this won't work as you haven't setup a subscription. This error is safe to ignore. As long as you got to step 17 you are fine.  Reboot root server.  root@rescue# reboot now Log back into Hetzner root server and verify RHEL version.   [root@keithcloud ~]# cat /etc/redhat-release Red Hat Enterprise Linux release 8.0 (Ootpa) Summary  In this article we showed how to build a custom RHEL 8 image for Hetzner servers. Feedback is always welcome so please don't be shy.  Happy Clouding!  (c) 2019 Keith Tenzer  ","categories": ["Cloud"],
        "tags": ["Hetzner"],
        "url": "/cloud/how-to-create-a-rhel-8-image-for-hetzner-root-servers/",
        "teaser": null
      },{
        "title": "OpenStack 15 (Stein) Lab Installation and Configuration Guide for Hetzner Root Servers",
        "excerpt":"  Overview   In this article we will focus on installing and configuring OpenStack Stein using RDO and the packstack installer. RDO is a community platform around Red Hat’s Enterprise OpenStack Distribution. It allows you to test the latest OpenStack capabilities on a stable platform such as Red Hat Enterprise Linux (RHEL) or CentOS. This guide will take you through setting up Hetzner root server, preparing environment for OpenStack, installing the OpenStack Stein release, adding a floating ip subnet through OVS, configuring networking, security groups, flavors, images and are other OpenStack related services. The outcome is a working OpenStack environment based on the Stein release that you can use as a baseline for testing your applications using OpenStack capabilities. The installation will create an all-in-one deployment however you can use this guide to create a multi-node deployment as well.    Root Server Specs  CPU: Intel(R) Xeon(R) CPU E5-1650 v3 @ 3.50GHz (12 Cores) Memory: 128GB RAM Disk: 2 x 480GB SSD Network: 1Gbit IPV4 Adresses: 1 x IPV4 + /29 Subnet (6 IPs) Hetzner is a hosting providing and auctions off used hardware for very competitive prices on a per/month basis (https://www.hetzner.de/sb). Hetzner provides one IP for each root server that is accessible via internet. It is a /32 so if you want additional IPs, like you would for OpenStack to use as floating ips you need to order additional subnet. You can order an additional subnet as I have done. Hetzner will route that subnet through the host IP of the /32. This requires creating an additional OVS bridge in OpenStack and this guide will go through that configuration.  Configure Root Server  As mentioned, Hetzner will give you access to your root server via IP and provide ability to manage the root server. Basic things like put root server in rescue mode or reboot. You can provide public ssh key or use password to access system. Please don’t use password, this is the internet, bad things can and will happen.  Enter Rescue Mode  In order to install OS or repair it you need to get into rescue mode.        Configure Hetzner Firewall.  While server is rebootting you can modify your servers firewall rules for incoming traffic. By default firewall allows port 22 for SSH and nothing else. Firewall settings can be modified under your server’s settings in Hetzner’s web UI https://robot.your-server.de/server. For OpenStack I would recommend 80, 443, 22 and ICMP.    If you plan on running OpenShift on OpenStack you need to add some additional rules.    Connect to root server via ssh and private key.  # ssh -i .ssh/id_rsa.pub Create Root Server Configuration.  The root server in this case has two disks. They are being configured in non-redundant RAID 1 (stripping) configuration. OpenStack requires decent disk performance, I found when using mirroring it was not enough. If you want redundant configuration or decent performance, you need to pay extra for SSDs. Since we will setup LVM partitions, creating one for Cinder (cinder-volumes) is a good idea. You don’t want to use loop back driver for Cinder. Finally you can provide your own image (if you want to use RHEL, you must do so).  My colleague Andy Neeb also automated deployment using Hetzner API. If you are interested in automated deployment here is his Github repository: https://github.com/andyneeb/ansible-hetzner.  [CENTOS]  # vi config.txt  DRIVE1 /dev/sda DRIVE2 /dev/sdb SWRAID 1 SWRAIDLEVEL 0 BOOTLOADER grub HOSTNAME myrootserver.lab PART /boot ext3 512M PART lvm vg0 150G  LV vg0 root / ext4 50G LV vg0 swap swap swap 8G LV vg0 var /var ext4 10G LV vg0 tmp /tmp ext4 30G  IMAGE /root/.oldroot/nfs/install/../images/CentOS-75-64-minimal.tar.gz [RHEL]  Note: You need to provide your own RHEL image. I have documented in a separate post how to do that. Once you have image you can upload it to the root server /root directory. Since Hetzner does not understand RHEL you will need to ensure the image name is CentOS-75-64-minimal.tar.gz.  # vi config.txt  DRIVE1 /dev/sda  DRIVE2 /dev/sdb  SWRAID 1  SWRAIDLEVEL 0  BOOTLOADER grub  HOSTNAME myrootserver.lab  PART /boot ext3 512M  PART lvm vg0 150G   LV vg0 root / ext4 50G  LV vg0 swap swap swap 8G  LV vg0 var /var ext4 10G  LV vg0 tmp /tmp ext4 30G  IMAGE /root/CentOS-75-64-minimal.tar.gz Install image and create partitions.  # installimage -a -c config.txt               Note: you may see error with RHEL image if you renamed it to CentOS because Hetzner expects CentOS but it is safe to ignore.  Hetzner Online GmbH - installimage  Your server will be installed now, this will take some minutes You can abort at any time with CTRL+C ...  : Reading configuration done  : Loading image file variables done  : Loading centos specific functions done  1/18 : Deleting partitions done  2/18 : Test partition size done  3/18 : Creating partitions and /etc/fstab done  4/18 : Creating software RAID level 0 done  5/18 : Creating LVM volumes done  6/18 : Formatting partitions : formatting /dev/md/0 with ext3 done  : formatting /dev/vg0/root with ext4 done  : formatting /dev/vg0/swap with swap done  : formatting /dev/vg0/var with ext4 done  : formatting /dev/vg0/tmp with ext4 done  : formatting /dev/vg0/home with ext4 done  7/18 : Mounting partitions done  8/18 : Sync time via ntp done  9/18 : Downloading image (http) done  : Importing public key for image validation done  10/18 : Validating image before starting extraction warn  : No detached signature file found! 11/18 : Extracting image (http) done  12/18 : Setting up network config done  13/18 : Executing additional commands : Setting hostname done  : Generating new SSH keys done  : Generating mdadm config done  : Generating ramdisk done  : Generating ntp config done  14/18 : Setting up miscellaneous files done  15/18 : Configuring authentication : Fetching SSH keys done  : Disabling root password done  : Disabling SSH root login without password done  : Copying SSH keys done  16/18 : Installing bootloader grub done  17/18 : Running some centos specific functions failed  An error occured while installing the new system! See the debug file /root/debug.txt for details.   Reboot. # reboot now   Disable password authentication. Remember this system is accessible from the internet. Unless you want people to constantly try and login, disable password authentication. # vi /etc/ssh/sshd_config PasswordAuthentication no  Restart sshd. # systemctl restart sshd  Create Cinder Volumes LVM Pool  Create third partition on both disks using the remaing space available. Choose type FD (Linux Raid Autodetect). You can use fdisk or cfdisk.  # cfdisk /dev/sda # cfdisk /dev/sdb Detect new disks  # partprobe Create a raid 0 (stripe) using the newly created partitions  # mdadm --create --verbose /dev/md2 --level=0 --raid-devices=2 \\ /dev/sda3 /dev/sdb3 Create Physical Volume, Volume Group and Thin Pool  # pvcreate /dev/md2 # vgcreate cinder-volumes /dev/md2 # lvcreate -L 300G -T cinder-volumes/cinder-volumes-pool Verify LVM Volume Groups  Make sure a volume group cinder-volumes exists. This will be used for OpenStack storage as mentioned.  # vgs   VG             #PV #LV #SN Attr   VSize    VFree   cinder-volumes   1   1   0 wz--n- &lt;743.01g &lt;442.86g   vg0              1   4   0 wz--n-  149.87g   51.87g # lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert cinder-volumes-pool cinder-volumes twi-a-tz-- 300.00g 0.00 0.24 root vg0 -wi-ao---- 50.00g swap vg0 -wi-ao---- 8.00g tmp vg0 -wi-ao---- 30.00g var vg0 -wi-ao---- 10.00g Remove Linux Raid Device (Only if you want to re-create cinder-volumes)  In case something went wrong and you need to re-create the linux raid device. It can be tricky to remove since it writes raid information to the disk partition.  Remove LVM Volume  # vgremove cinder-volumes Stop raid device  # mdadm --stop /dev/md2 Zero raid disks # mdadm --zero-superblock /dev/sda3 # mdadm --zero-superblock /dev/sdb3 Install OpenStack  Ensure local name resolution is working.  # vi /etc/hosts 144.76.52.111 myrootserver.lab Set hostname.  # hostnamectl set-hostname myrootserver.lab Enable RPMs.  [RHEL]  # subscription-manager register # subscription-manager list --available # subscription-manager attach --pool= # subscription-manager repos --disable=* # subscription-manager repos --enable=rhel-8-for-x86_64-baseos-rpms \\ --enable=rhel-8-for-x86_64-appstream-rpms \\ --enable=rhel-8-for-x86_64-highavailability-rpms \\ --enable=fast-datapath-for-rhel-8-x86_64-rpms \\ --enable=advanced-virt-for-rhel-8-x86_64-rpms \\ --enable=openstack-15-for-rhel-8-x86_64-rpms \\ --enable=openstack-15-tools-for-rhel-8-x86_64-rpms \\ --enable=openstack-15-devtools-for-rhel-8-x86_64-rpms Install packstack packages.  [CentOS]  # dnf install -y centos-release-openstack-stein [Both]  # dnf install -y openstack-packstack [RHEL]  Disable firewalld, OpenStack uses iptables.  # systemctl disable firewalld # systemctl stop firewalld Disable Network Manager  [Both]  In RHEL 8 NetworkManager is default and this causes issues with packstack. As such we will disable NetworkManager and enable the network-scripts stack.  # dnf install -y network-scripts # readlink $(readlink $(which ifup)) /etc/sysconfig/network-scripts/ifup # touch /etc/sysconfig/disable-deprecation-warnings # systemctl stop NetworkManager # systemctl disable NetworkManager #systemctl enable network #systemctl start network Switch to Advanced Virt Modules  # dnf -y module disable virt:rhel # dnf -y module enable virt:8.0.0 Install yum-utils and update the system.  # dnf install -y dnf-utils # dnf update -y Configure Intel Virtualization for Directed I/O.  # vi /etc/default/grub --- GRUB_CMDLINE_LINUX=\"biosdevname=0 crashkernel=auto nomodeset rd.auto=1 consoleblank=0 intel_iommu=on\" --- # grub2-mkconfig -o /boot/grub2/grub.cfg Set SELinux to Permissive  Unfortunately SELinux is not working with this latest packstack.  # setenforce 0 # vi /etc/selinux/config --- SELINUX=permissive --- Reboot.  # reboot now Create packstack answers file for customizing the installer.  # packstack --os-neutron-ml2-tenant-network-types=vxlan \\ --os-neutron-ml2-mechanism-drivers=openvswitch \\ --os-neutron-ml2-type-drivers=vxlan,flat \\ --os-neutron-l2-agent=openvswitch \\ --keystone-admin-passwd=&lt;admin password&gt; \\ --nova-libvirt-virt-type=kvm \\ --cinder-volumes-create=n \\ --provision-demo=n \\ --os-heat-install=y \\ --os-neutron-lbaas-install=y \\ --os-swift-storage-size=75G \\ --gen-answer-file /root/answers.txt Install OpenStack using packstack.  # packstack --answer-file /root/answers.txt --timeout=1500  **** Installation completed successfully ******  Additional information: * Parameter CONFIG_NEUTRON_L2_AGENT: You have choosen OVN neutron backend. Note that this backend does not support LBaaS, VPNaaS or FWaaS services. Geneve will be used as encapsulation method for tenant networks * Time synchronization installation was skipped. Please note that unsynchronized time on server instances might be problem for some OpenStack components. * File /root/keystonerc_admin has been created on OpenStack client host 136.243.43.239. To use the command line tools you need to source the file. * To access the OpenStack Dashboard browse to http://136.243.43.239/dashboard . Please, find your login credentials stored in the keystonerc_admin in your home directory. * The installation log file is available at: /var/tmp/packstack/20191025-135509-j6dbbfha/openstack-setup.log * The generated manifests are available at: /var/tmp/packstack/20191025-135509-j6dbbfha/manifests Fix issue with Horizon dashboard  # chmod 644 /usr/share/openstack-dashboard/openstack_dashboard/local/enabled/_1481_project_ng_loadbalancersv2_panel.py Configure Physical Network  Source the keystone admin profile.  # . /root/keystonerc_admin Backup the ifcfg-etho script.  # cp /etc/sysconfig/network-scripts/ifcfg-eno1 /root/ Configure external bridge br-ex.  # vi /etc/sysconfig/network-scripts/ifcfg-eno1 DEVICE=eno1 ONBOOT=yes TYPE=OVSPort DEVICETYPE=ovs OVS_BRIDGE=br-ex # vi /etc/sysconfig/network-scripts/ifcfg-br-ex DEVICE=br-ex BOOTPROTO=none ONBOOT=yes TYPE=OVSBridge DEVICETYPE=ovs USERCTL=yes PEERDNS=yes IPV6INIT=no IPADDR= NETMASK=255.255.255.255 SCOPE=\"peer &lt;gateway address&gt;\" Switch static route to br-ex.  Note: this is specific to Hetzner environment as the physical host will get a /32.  # mv /etc/sysconfig/network-scripts/route-eno1 /etc/sysconfig/network-scripts/route-br-ex  Add the eno1 physical interface to the br-ex bridge in openVswitch.  Note: this is point of no return!!! Double check your config, ensure network interface is correct, if there is typo you will need to got to rescue mode.  # ovs-vsctl add-port br-ex eno1; systemctl restart network.service   Configure Additional Floating IP Subnet  Since hetzner root server only has one ip (/32), another subnet is needed to add additional floating ips. In this case hetzner will route traffic from additional subnet through to /32 ip of root server. Here we need to create a new OVS bridge (br-ex2) for additional subnet and patch it to existing bridge (br-ex).  Create Openvswitch Bridge.   # ovs-vsctl add-br br-ex2 Patch bridge br-ex2 to br-ex.  # ovs-vsctl add-port br-ex2 patch2-0 Note: ignore error about missing interface, that is expected since we creating patch and haven’t created interface yet.  # ovs-vsctl set interface patch2-0 type=patch # ovs-vsctl set interface patch2-0 options:peer=\"patch0-2\" # ovs-vsctl add-port br-ex patch0-2 Note: ignore error about missing interface, that is expected since we creating patch and haven’t created interface yet.  # ovs-vsctl set interface patch0-2 type=patch # ovs-vsctl set interface patch0-2 options:peer=\"patch2-0\" Update neutron bridge mappings.  We are adding a second subnet, as such a mapping is required to physical interface.  # vi /etc/neutron/plugins/ml2/openvswitch_agent.ini  bridge_mappings=extnet:br-ex,extnet2:br-ex2 Configure ifcfg script for br-ex2.   # vi /etc/sysconfig/network-scripts/ifcfg-br-ex2 DEVICE=br-ex2 BOOTPROTO=none ONBOOT=yes TYPE=OVSBridge DEVICETYPE=ovs USERCTL=yes PEERDNS=yes IPADDR= NETMASK=255.255.255.248 SCOPE=\"peer &lt;subnet gateway address&gt;\" IPV6INIT=no   Comment out default iptables REJECT rules.  By default iptables won’t allow traffic from br-ex2 to br-ex.  # vi /etc/sysconfig/iptables --- #-A INPUT -j REJECT --reject-with icmp-host-prohibited #-A FORWARD -j REJECT --reject-with icmp-host-prohibited --- Restart iptables and networking.  # systemctl restart iptables; systemctl restart network  Configure OpenStack Environment  Neutron Configuration  Set the default dns servers to be used when creating a subnet.  vi /etc/neutron/dhcp_agent.ini --- debug=False dnsmasq_dns_servers=213.133.98.98,213.133.99.99 --- Restart Neutron DHCP Agent  # systemctl restart neutron-dhcp-agent Nova Configuration  Nova uses filtering rules to find appropriate host when scheduling instances. In order for nova to recognize lvm storage the images type needs to be lvm and the volume group needs to be correct lvm volume. In addition, unless you want to wait really long when deleting nova volumes, set volume_clear to none.  # vi /etc/nova/nova.conf  [libvirt] --- images_type = lvm  volume_clear = none images_volume_group = cinder-volumes --- Restart Nova services.  # systemctl restart openstack-nova-compute # systemctl restart openstack-nova-api # systemctl restart openstack-nova-scheduler Cinder Configuration  By default the openstack install will configure an lvm volume group using loop. This is not ideal and why we created a volume group vg1. We will use the same setting volume_clear to none to ensure cinder volumes are deleted quickly.  Update cinder configuration.  # vi /etc/cinder/cinder.conf --- enabled_backends=lvm volume_clear = none  [lvm] volume_backend_name=lvm volume_driver=cinder.volume.drivers.lvm.LVMVolumeDriver iscsi_ip_address=144.76.52.111 iscsi_helper=lioadm volume_group=cinder-volumes volumes_dir=/var/lib/cinder/volumes --- Restart cinder services.  # systemctl restart openstack-cinder-volume # systemctl restart openstack-cinder-api     Ceilometer Configuration  Aodh is the database for alarms that are triggered based on things such as autoscaling policies. The database needs to be initialized after installing OpenStack.  # aodh-dbsync Neutron Configuration  Create private network.  # openstack network create admin # openstack subnet create --network admin --allocation-pool \\ start=10.10.1.100,end=10.10.1.200 --dns-nameserver 213.133.98.98 \\ --subnet-range 10.10.1.0/24 admin_subnet Create public network.   Note: these steps assume the physical network connected to br-ex2 is 144.76.132.224/29.  # openstack network create --provider-network-type flat \\ --provider-physical-network extnet2 --external public # openstack subnet create --network public --allocation-pool \\ start=144.76.132.226,end=144.76.132.230 --no-dhcp \\ --subnet-range 144.76.132.224/29 public_subnet Add a new router and configure router interfaces.  # openstack router create admin_router # openstack router set --external-gateway public admin_router # openstack router add subnet admin_router admin_subnet Check to ensure network connectivity is working.  This is done by checking the network namespace of the qrouter (openstack router).  # ip netns show qrouter-88dde0ef-22a2-44b1-baa9-304273653bb1 qdhcp-f1582f71-b531-43af-99c1-299b603232fc # ip netns exec qrouter-88dde0ef-22a2-44b1-baa9-304273653bb1 ping www.redhat.com Glance Configuration  Upload a glance image.  In this case we will use a Cirros image because it is small and thus good for testing OpenStack.  # curl -O http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img # openstack image create --disk-format qcow2 \\ --container-format bare --public \\ --file /root/cirros-0.3.4-x86_64-disk.img \"Cirros 0.3.4\" Create a new m1.nano flavor for running Cirros image.  # openstack flavor create --ram 64 --disk 0 --ephemeral 0 --vcpus 1 --public m1.nano Configure Security Groups  Create Security Group for all access.  # openstack security group create all \\ --description \"Allow all ports\" # openstack security group rule create --protocol TCP \\ --dst-port 1:65535 --remote-ip 0.0.0.0/0 all # openstack security group rule create --protocol ICMP \\ --remote-ip 0.0.0.0/0 all Create Security Group for base access.  # openstack security group create base \\ --description \"Allow base ports\" # openstack security group rule create --protocol TCP \\ --dst-port 22 --remote-ip 0.0.0.0/0 base # openstack security group rule create --protocol TCP \\ --dst-port 80 --remote-ip 0.0.0.0/0 base   # openstack security group rule create --protocol TCP \\ --dst-port 443 --remote-ip 0.0.0.0/0 base # openstack security group rule create --protocol ICMP \\ --remote-ip 0.0.0.0/0 base   Create Private Key.  # openstack keypair create admin Save Private Key to file.  # vi /root/admin.pem -----BEGIN RSA PRIVATE KEY----- MIIEowIBAAKCAQEAwTrb+xdbpgY8hVOmftBIShqYUgXXDC/1gggakq8bkEdNnSku IaNGeJykzksjdksjd9383iejkjsu92wiwsajFLuE2lkh5dvk9s6hpfE/3UvSGk6m HWIMCf3nJUv8gGCM/XElgwNXS02c8pHWUywBiaQZpOsvjCqFLGW0cNZLAQ+yzrh1 dIWddx/E1Ppto394ejfksjdksjdksdhgu4t39393eodNlVQxWzmK4vrLWNrfioOK uRxjxY6jnE3q/956ie69BXbbvrZYcs75YeSY7GjwyC5yjWw9qkiBcV1+P1Uqs1jG 1yV0Zvl5xlI1M4b97qw0bgpjTETL5+iuidFPVwIDAQABAoIBAF7rC95m1fVTQO15 buMCa1BDiilYhw+Mi3wJgQwnClIwRHb8IJYTf22F/QptrrBd0LZk/UHJhekINXot z0jJ+WvtxVAA0038jskdjskdjksjksjkiH9Mh39tAtt2XR2uz/M7XmLiBEKQaJVb gD2w8zxqqNIz3438783787387s8s787s8sIAkP3ZMAra1k7+rY1HfCYsRDWxhqTx R5FFwYueMIldlfPdGxwd8hLrqJnDY7SO85iFWv5Kf1ykyi3PRA6r2Vr/0PMkVsKV XfxhYPkAOb2hNKRDhkvZPmmxXu5wy8WkGeq+uTWRY3DoyciuC4xMS0NMd6Y20pfp x50AhJkCgYEA8M2OfUan1ghV3V8WsQ94vguHqe8jzLcV+1PV2iTwWFBZDZEQPokY JkMCAtHFvUlcJ49yAjrRH6O+EGT+niW8xIhZBiu6whOd4H0xDoQvaAAZyIFoSmbX 2WpS74Ms5YSzVip70hbcXb4goDhdW9YxvTVqJlFrsGNCEa3L4kr2qFMCgYEAzWy0 5cSHkCWaygeYhFc79xoTnPxKZH+QI32dAeud7oyZtZeZDRyjnm2fEtDCEn6RtFTH NlI3W6xFkXcp1u0wbmYJJVZdn1u9aRsLzVmfGwEWGHYEfZ+ZtQH+H9XHXsi1nPpr Uy7Msd,sl,.swdko393j495u4efdjkfjdkjfhflCgYEA7VO6Xo/XdKPMdJx2EdXM y4kzkPFHGElN2eE7gskjdksjdksjkasnw33a23433434wk0P8VCksQlBlojjRVyu GgjDrMhGjWamEA1y3vq6eka3Ip0f+0w26mnXCYYAJslNstu2I04yrBVptF846/1E ElXlo5RVjYeWIzRmIEZ/qU8CgYB91kOSJKuuX3rMm46QMyfmnLC7D8k6evH+66nM 238493ijsfkjalsdjcws9fheoihg80eWDSAFDOASDF=OIA=FSLoiidsiisiisNDo ACh40FeKsHDby3LK8OeM9NXmeCjYeoZYNGimHForiCCT+rIniiu2vy0Z/q+/t3cM BgmAmQKBgCwCTX5kbLEUcX5IE6Nzh+1n/lkvIqlblOG7v0Y9sxKVxx4R9uXi3dNK 6pbclskdksdjdk22k2jkj2kalksx2koUeLzwHuRUpMavRhoTLP0YsdbQrjgHIA+p kDNrgFz+JYKF2K08oe72x1083RtiEr8n71kjSA+5Ua1eNwGI6AVl -----END RSA PRIVATE KEY----- # chmod 400 /root/admin.pem Start an Instance  Get Private Network Id.  # openstack network list +--------------------------------------+---------+--------------------------------------+ | ID | Name | Subnets | +--------------------------------------+---------+--------------------------------------+ | 72494c88-6d93-4eb7-929f-383cbedfa3e7 | public | a57856be-a22b-4a48-b3c3-75be46a4c477 | | 781e062d-5ab6-4ae8-a54e-8e72291df37e | private | b00316a1-812f-423a-8aca-bd6547692ad3 | +--------------------------------------+---------+--------------------------------------+ Create a mycirros Instance.  # openstack server create --flavor m1.nano --image \"Cirros 0.3.4\" \\ --nic net-id=781e062d-5ab6-4ae8-a54e-8e72291df37e --key-name admin \\ --security-group all mycirros # # openstack server list +--------------------------------------+----------+--------+---------------------+--------------+---------+ | ID | Name | Status | Networks | Image | Flavor | +--------------------------------------+----------+--------+---------------------+--------------+---------+ | 6f2719bb-f393-49ab-a409-1c4a0f992b2d | mycirros | ACTIVE | private=10.10.1.105 | Cirros 0.3.4 | m1.nano | +--------------------------------------+----------+--------+---------------------+--------------+---------+ Create Floating IP.  # openstack floating ip create public +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | created_at | 2018-01-12T09:52:40Z | | description | | | fixed_ip_address | None | | floating_ip_address | 144.76.132.230 | | floating_network_id | 72494c88-6d93-4eb7-929f-383cbedfa3e7 | | id | 7a7ac84f-57ee-4982-b2f5-35ebb9656b34 | | name | 144.76.132.230 | | port_id | None | | project_id | 92d3bc57ca504eaab4c29d3509064004 | | revision_number | 0 | | router_id | None | | status | DOWN | | updated_at | 2018-01-12T09:52:40Z | +---------------------+--------------------------------------+  Get Port Id of Instance.  # openstack port list +--------------------------------------+------+-------------------+-------------------------------------------------------------------------------+--------+ | ID | Name | MAC Address | Fixed IP Addresses | Status | +--------------------------------------+------+-------------------+-------------------------------------------------------------------------------+--------+ | 60407eee-c9b2-4e9c-81a6-5c38bb536c9b | | fa:16:3e:37:ac:2f | ip_address='144.76.132.227', subnet_id='a57856be-a22b-4a48-b3c3-75be46a4c477' | ACTIVE | | 717660b6-b9b0-4f76-8c63-f9fae5c9bd9b | | fa:16:3e:0c:14:30 | ip_address='10.10.1.105', subnet_id='b00316a1-812f-423a-8aca-bd6547692ad3' | ACTIVE | | 878ee68a-1b88-48d1-9bf7-057f59b833bb | | fa:16:3e:2a:1f:50 | ip_address='144.76.132.230', subnet_id='a57856be-a22b-4a48-b3c3-75be46a4c477' | N/A | | 8a7b6b21-1eb2-4750-b854-707462d8b38f | | fa:16:3e:53:c9:ef | ip_address='10.10.1.1', subnet_id='b00316a1-812f-423a-8aca-bd6547692ad3' | ACTIVE | | b40af80e-2794-4ca5-8141-9d8ad4e9c9f2 | | fa:16:3e:51:d9:cb | ip_address='10.10.1.100', subnet_id='b00316a1-812f-423a-8aca-bd6547692ad3' | ACTIVE | +--------------------------------------+------+-------------------+-------------------------------------------------------------------------------+--------+ Assign Floating IP to Instance Port.  # openstack floating ip set --port 717660b6-b9b0-4f76-8c63-f9fae5c9bd9b 144.76.132.230  Verify Floating IP in OpenStack Router.  # ip netns show qdhcp-dcfbabbd-c5d2-444c-ab60-546216550118 router-0f00050f-6590-42df-9136-32d22fea4ece # ip netns exec qrouter-0f00050f-6590-42df-9136-32d22fea4ece ip a --- 9: qg-60407eee-c9: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN qlen 1000  link/ether fa:16:3e:37:ac:2f brd ff:ff:ff:ff:ff:ff  inet 144.76.132.227/29 brd 144.76.132.231 scope global qg-60407eee-c9  valid_lft forever preferred_lft forever  inet 144.76.132.230/32 brd 144.76.132.230 scope global qg-60407eee-c9  valid_lft forever preferred_lft forever  inet6 fe80::f816:3eff:fe37:ac2f/64 scope link  valid_lft forever preferred_lft forever --- Test Instance.  Connect to mycirros instance using the private ssh key stored in the admin.pem file.  # ssh -i admin.pem cirros@144.76.132.230 $ ping www.redhat.com PING www.redhat.com (172.217.21.14): 56 data bytes 64 bytes from 172.217.21.14: seq=0 ttl=54 time=37.692 ms 64 bytes from 172.217.21.14: seq=1 ttl=54 time=27.758 ms 64 bytes from 172.217.21.14: seq=2 ttl=54 time=25.640 ms Install OpenStack Client  To administer OpenStack remotely the CLI tools are important. Below are steps to install OpenStack CLI tools for Stein.  [RHEL]  # subscription-manager repos --enable=rhel-7-server-openstack-14-tools-rpms # yum install -y python-openstackclient # yum install -y python-heatclient # yum install -y python-octavia [CentOs]  # yum install -y https://rdoproject.org/repos/openstack-stein/rdo-release-stein-1.noarch.rpm # yum upgrade -y # yum install -y python-openstackclient # yum install -y python-heatclient # yum install -y python-octavia Summary  This article was intended as a hands on guide for standing up an OpenStack Rocky lab environment using RDO. In this guide we also tailored the environment to the Hetzner root server. Things can of course vary depending on your hardware platform and provider. Certainly the aim of this guide was to provide a more realistic deployment scenario for OpenStack. As mentioned RDO is a stable community platform built around Red Hat’s OpenStack Platform. It provides the ability to test the latest OpenStack features against either an enterprise platform (RHEL) or community platform (CentOS). Hopefully you found the information in this article useful. If you have anything to add or feedback, feel free to leave your comments.  Happy OpenStacking!  (c) 2018 Keith Tenzer    ","categories": ["OpenStack"],
        "tags": ["Cloud","Linux","opensource","OpenStack","RDO","RHEL","Stein"],
        "url": "/openstack/openstack-15-stein-lab-installation-and-configuration-guide-for-hetzner-root-servers/",
        "teaser": null
      },{
        "title": "Automated Infrastructure in the On-Premise Datacenter - OpenShift 4.2 on OpenStack 15 (Stein)",
        "excerpt":"   Overview  In this article we start a new journey, automated infrastructure in the on-premise datacenter. We will deploy OpenShift 4.2 on OpenStack. As I am sure you are aware, OpenShift is Red Hat's enterprise kubernetes platform. Kubernetes is of course the brains but by itself is not a platform. OpenShift brings with kubernetes, monitoring, aggregate logging, container registry, security, automated deployment/upgrade, developer experience, huge middleware tooling built around JBoss, serverless frameworks, ISTIO (service mesh), CI/CD integration and the key word \"ENTERPRISE\".    OpenShift 4 brings with it, one thing that I feel is a breakthrough in our industry. It is the single feature I am most excited about and one of the reasons OpenShift 4 was re-designed, Installer Provision Infrastructure (IPI). Using IPI, OpenShift is capable of not only installing itself but deploying as well as managing the underlying infrastructure needed, from compute, to storage and network. Further using IPI OpenShift can also update, even upgrade itself over the air. This is a huge step forward in platform intelligence and where we need to be to deal with today's complexity.  Users can also still opt for User Provisioned Infrastructure (UPI), if they want to bring their own infrastructure, deal with micro-managing everything and waste a lot of time but why? Yeah I thought so too.  Up until now IPI has only supported public clouds: AWS, Azure and Google. Now with OpenShift 4.2 it is supporting OpenStack. For the first time we can bring IPI into the on-premise Datacenter where it is IMHO most needed. This single feature has the potential to revolutionize on-premise environments and bring them into the cloud-age with a single click and that promise is truly something to get excited about!  Pre-requisites  Before we start the installation it is important to understand some pre-requisites and how to properly configure them.  Deploy OpenStack  I have provided a guide for deploying OpenStack Stein (RHOSP 15) using packstack on Hetzner root servers here. Even if you aren't using a Hetzner root server this guide can help you spin up a demo OpenStack environment using RDO packstack. If you have Red Hat OpenStack Platform (RHOSP) 15 even better!  Configure External DNS  If you have problems later, this will likely be the cause, trust me from experience, get your DNS sorted from the beginning.  You will need a DNS domain, mine is keithtenzer.com. Once you have a domain you need to manage it somewhere. I can recommend cloudflare, they even have a free tier. Once you have that you need to create two A records under the domain (keithtenzer.com).  api.ocp4 144.76.130.237 The OpenShift installer will ask for a floating ip for API traffic. This is required to install OpenShift. The OpenShift installer will configure an ingres port in OpenStack and use keepalived across the masters.  *.apps.ocp4 144.76.130.228 A DNS wildcard is used to send all application traffic to another IP. An additional IP is needed because the OpenShift router operator (ha-proxy) is responsible for handling application traffic not the API operator. The installer will also not automatically configure this floating ip on the ingress port like it does for API. That needs to be done after the installation.  The diagram below shows my DNS configuration.    OpenStack Configuration  To deploy OpenShift we will need an external (floating ip) network, a compute flavor and setup a project properly.  In order to create an floating ip network you need a subnet of physical IPs that can communicate with the outside world. They don't have to be public but need to be routable to whatever is the \"outside world\", meaning outside of OpenStack.  In this example we have a flat internet routable network 144.76.130.224/28.  Authenticate to OpenStack  # source /root/keystonerc_admin Create public network  (keystone_admin)]# openstack network create --provider-network-type flat \\ --provider-physical-network extnet2 --external public Create public subnet  (keystone_admin)]# openstack subnet create --network public \\ --allocation-pool start=144.76.130.226,end=144.76.130.238 --no-dhcp \\ --subnet-range 144.76.130.224/28 public_subnet Create OpenStack flavor for OpenShift nodes  (keystone_admin)]# openstack flavor create --ram 16384 --disk 25 \\ --vcpu 4 --public ocp.controller Setup Project  (keystone_admin)]# openstack project create openshift_4 (keystone_admin)]# openstack quota set --ram -1 --cores -1 openshift_4 (keystone_admin)]# openstack quota set --secgroups 3 --secgroup-rules 60 \\ openshift_4 (keystone_admin)]# openstack user create --password '&lt;password&gt;' \\ openshift_admin (keystone_admin)]# openstack role add --project openshift_4 --user \\ openshift_admin admin (keystone_admin)]# openstack role add --user openshift_admin --project \\ openshift_4 swiftoperator Create project RC file  # vi keystonerc_openshift_admin unset OS_SERVICE_TOKEN export OS_USERNAME=openshift_admin export OS_PASSWORD='&lt;password&gt;' export OS_REGION_NAME=RegionOne export OS_AUTH_URL=http://136.243.43.239:5000/v3 export PS1='[\\u@\\h \\W(keystone_openshift_admin)]\\$ '  export OS_PROJECT_NAME=openshift_4 export OS_USER_DOMAIN_NAME=Default export OS_PROJECT_DOMAIN_NAME=Default export OS_IDENTITY_API_VERSION=3 Authenticate to OpenShift user  (keystone_admin)]# source /root/keystonerc_openshift_admin Set temporary key for Swift  The Container OS ignite configuration file used for bootstrapping is stored in Swift (OpenStack Object Store). This allows for URL access using a temporary key.  (keystone_openshift_admin)]# openstack object store account set --property \\ Temp-URL-Key=redhat Download Red Hat OpenStack Container OS image  You can download the Red Hat Container OS (rhcos) qcow2 image from access.redhat.com or this link for the 4.2 release. Once you have the image copy it over to your environment.  Create RHOS Image  First make sure you get the latest RHOS image. Check if the image is a compressed file once downloaded.  (keystone_openshift_admin)]# file rhcos-4.2.0-x86_64-openstack.qcow2 rhcos-4.2.0-x86_64-openstack.qcow2: gzip compressed data (keystone_openshift_admin)]# mv rhcos-4.2.0-x86_64-openstack.qcow2 \\ rhcos-4.2.0-x86_64-openstack.qcow2.gz (keystone_openshift_admin)]# gunzip rhcos-4.2.0-x86_64-openstack.qcow2.gz (keystone_openshift_admin)]# file rhcos-4.2.0-x86_64-openstack.qcow2 rhcos-4.2.0-x86_64-openstack.qcow2: QEMU QCOW Image (v3) (keystone_openshift_admin)]# openstack image create rhcos \\ --container-format bare --disk-format qcow2 --public --file \\ /root/rhcos-4.2.0-x86_64-openstack.qcow2 Download OpenShift Installer, Command Line Tools and Pull Secret  You can download the installer, command line tools and pull secret at following URL: https://cloud.redhat.com/openshift/install. You will need the pull secret later.  Select your provider, in this case OpenStack.    Download the installer, CLI and your pull secret.    Once you have the installer and CLI tools downloaded, extract them into /usr/bin.  (keystone_openshift_admin)]# tar xvfz openshift-install-linux-4.2.0.tar.gz \\ -C /usr/bin/ (keystone_openshift_admin)]# tar xvfz openshift-client-linux-4.2.0.tar.gz \\ -C /usr/bin/ Create floating-ip for API  (keystone_openshift_admin)]# openstack floating ip create \\ --floating-ip-address 144.76.130.237 public (keystone_openshift_admin)]# openstack floating ip list +--------------------------------------+---------------------+------------------+--------------------------------------+--------------------------------------+----------------------------------+ | ID | Floating IP Address | Fixed IP Address | Port | Floating Network | Project | +--------------------------------------+---------------------+------------------+--------------------------------------+--------------------------------------+----------------------------------+ | fd98ca80-613f-4d36-819f-c5be66522439 | 144.76.130.234 | 10.10.1.186 | e17b2845-a4a2-44f1-8388-549a352a9fff | 0b8817e8-ffcc-4403-956c-eaadfb785e57 | 4e4c6f30c2c843258239efc5e699bec6 | +--------------------------------------+---------------------+------------------+--------------------------------------+--------------------------------------+----------------------------------+ Configure OpenShift 4.2  Once OpenStack environment is setup properly and we have our external DNS sorted, the installation configuration for OpenShift can begin.  Get Project ID  The OpenStack project id is needed for the next section.  (keystone_openshift_admin)]# openstack project show openshift_4 -c id \\ -f value c36f0b1258fe447fbbbdcf6e2d2e4e0a Configure clouds.yaml  OpenShift IPI can configure and manage multiple OpenShift on OpenStack clouds. In this section we will just configure one. All the information needed except the project id is available in the keystonerc file.  (keystone_openshift_admin)]# vi clouds.yaml clouds:   openstack:     auth:       auth_url: http://136.243.43.239:5000/v3       project_name: openshift_4       username: openshift_admin       password: &lt;password&gt;       user_domain_name: Default       project_domain_name: Default       project_id: c36f0b1258fe447fbbbdcf6e2d2e4e0a Create Install Configuration  OpenShift IPI will create a directory with the installation configuration. The manifests and ignition configs can be optionally created, you should not have to change them but you could nevertheless.  (keystone_openshift_admin)]# openshift-install create install-config \\ --dir=ocp_install ? SSH Public Key /root/.ssh/id_rsa.pub ? Platform openstack ? Cloud openstack ? ExternalNetwork public ? APIFloatingIPAddress 144.76.130.237 ? FlavorName ocp.controller ? Base Domain keithtenzer.com ? Cluster Name ocp4 ? Pull Secret [? for help] ************************************************ Update the installation  We will reduce the number of workers from three to two, set the region, disable octaviaSupport and trunkSupport.  (keystone_openshift_admin)]# vi ocp_install/install-config.yaml --- - hyperthreading: Enabled name: worker platform: {} replicas: 2 --- octaviaSupport: \"0\" region: \"RegionOne\" trunkSupport: \"0\" --- Create Ignition files (optional)  In case you want to change something in the ignite configuration.  (keystone_openshift_admin)]# openshift-install create ignition-configs --dir Create Installation Manifests (optional)  In case you want to change something in one of the manifests.  (keystone_openshift_admin)]# openshift-install create manifests --dir ./ocp_install/ Run the Installer  (keystone_openshift_admin)]# time openshift-install --log-level=debug \\ create cluster --dir ocp_install --- INFO Consuming \"Install Config\" from target directory INFO Waiting up to 30m0s for the Kubernetes API at https://api.ocp4.keithtenzer.com:6443... INFO API v1.14.6+2e5ed54 up INFO Waiting up to 30m0s for bootstrapping to complete... DEBUG Still waiting for the cluster to initialize: Working towards 4.2.0: 97% complete DEBUG Still waiting for the cluster to initialize: Working towards 4.2.0: 98% complete DEBUG Still waiting for the cluster to initialize: Working towards 4.2.0: 98% complete DEBUG Still waiting for the cluster to initialize: Working towards 4.2.0: 99% complete, waiting on authentication, console, image-registry, ingress, monitoring, operator-lifecycle-manager-packageserver DEBUG Still waiting for the cluster to initialize: Working towards 4.2.0: 99% complete DEBUG Still waiting for the cluster to initialize: Working towards 4.2.0: 100% complete, waiting on authentication DEBUG Cluster is initialized INFO Waiting up to 10m0s for the openshift-console route to be created... DEBUG Route found in openshift-console namespace: console DEBUG Route found in openshift-console namespace: downloads DEBUG OpenShift console route is created INFO Install complete! INFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/root/ocp_install/auth/kubeconfig' INFO Access the OpenShift web-console here: https://console-openshift-console.apps.ocp4.keithtenzer.com Once the installation completes we still need to bind the floating-ip for application traffic to the ingres port in OpenStack. In OpenShift 4, everything is an operator, including the console. Accessing any applications will go through the OpenShift router using the wildcard domain. In our case we created *.apps.ocp4.keithtenzer.com with IP 144.76.130.228.  Create Floating-ip for Application Traffic  (keystone_openshift_admin)]# openstack floating ip create \\ --floating-ip-address 144.76.130.228 public Bind Floating-ip to Ingres Port  The port will be called &lt;cluster name&gt;-&lt;clusterID&gt;-ingress-port or in our case ocp4-lq2ww-ingress-port.  (keystone_openshift_admin)]# openstack floating ip set --port \\ ocp4-lq2ww-ingress-port 144.76.130.228 At this point you should be able to connect to the console using the kubeadmin account and the password generated by the installer.    Destroy the cluster  Destroying the cluster is just as easy as creating one.  (keystone_openshift_admin)]# time openshift-install --log-level=debug \\ destroy cluster --dir ocp_install (keystone_openshift_admin)]# rm -rf ocp_install/ Validate the Environment  Using the CLI tools we will now authenticate and validate the environment.  Authenticate  To authenticate we simply need to export the kubeconfig created by the installer.  (keystone_openshift_admin)]# export KUBECONFIG=/root/ocp_install/auth/kubeconfig List all nodes  We can list all controllers and workers and view their status.  (keystone_openshift_admin)]# oc get nodes NAME                      STATUS   ROLES    AGE    VERSION ocp4-lq2ww-master-0       Ready    master   166m   v1.14.6+c07e432da ocp4-lq2ww-master-1       Ready    master   166m   v1.14.6+c07e432da ocp4-lq2ww-master-2       Ready    master   166m   v1.14.6+c07e432da ocp4-lq2ww-worker-kxnjh   Ready    worker   155m   v1.14.6+c07e432da ocp4-lq2ww-worker-zhmfl   Ready    worker   155m   v1.14.6+c07e432da Get Cluster Version  Shows not only the version but the cluster state.  (keystone_openshift_admin)]#  oc get clusterversion NAME      VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS version   4.2.0     True        False         149m    Cluster version is 4.2.0 Get Cluster Operators  Remember in OpenShift 4, everything is an operator. Here we can view their individual status.  (keystone_openshift_admin)]# oc get clusteroperator authentication                             4.2.0     True        False         False      150m cloud-credential                           4.2.0     True        False         False      166m cluster-autoscaler                         4.2.0     True        False         False      158m console                                    4.2.0     True        False         False      153m dns                                        4.2.0     True        False         False      165m image-registry                             4.2.0     True        False         False      155m ingress                                    4.2.0     True        False         False      156m insights                                   4.2.0     True        False         False      166m kube-apiserver                             4.2.0     True        False         False      164m kube-controller-manager                    4.2.0     True        False         False      163m kube-scheduler                             4.2.0     True        False         False      163m machine-api                                4.2.0     True        False         False      166m machine-config                             4.2.0     True        False         False      165m marketplace                                4.2.0     True        False         False      158m monitoring                                 4.2.0     True        False         False      154m network                                    4.2.0     True        False         False      163m node-tuning                                4.2.0     True        False         False      162m openshift-apiserver                        4.2.0     True        False         False      161m openshift-controller-manager               4.2.0     True        False         False      164m openshift-samples                          4.2.0     True        False         False      158m operator-lifecycle-manager                 4.2.0     True        False         False      165m operator-lifecycle-manager-catalog         4.2.0     True        False         False      165m operator-lifecycle-manager-packageserver   4.2.0     True        False         False      156m service-ca                                 4.2.0     True        False         False      166m service-catalog-apiserver                  4.2.0     True        False         False      162m service-catalog-controller-manager         4.2.0     True        False         False      162m storage                                    4.2.0     True        False         False      159m List All Pods  Operators of course in the end, deploy pods which run containers. Using this command you can get the status of all pods in the cluster.  (keystone_openshift_admin)]# oc get pods -A Subscribe Cluster  New in OpenShift 4 is a portal management for OpenShift clusters. Here you can see the overall status, manage subscriptions and even access the console.  Simply login to https://cloud.redhat.com/openshift/ using your credentials.    Here we can see that our newly provisioned cluster is not subscribe. To subscribe the cluster login to https://access.redhat.com/management/systems. Here you will see the compute nodes and can simply attach the OpenShift subscription.    Configure Htpasswd Identity Provider  OpenShift integrates with many identity providers. In the case of OpenStack, normally you would likely want to use Keystone. For test environments using htpasswd is also an option. To configure htpasswd we will create a user/password, store it in a secret and then configure the identity provider by creating a custom resource. In OpenShift 4, custom resource definitions are how you integrate other technologies with the platform in an API consistent way.  Create htpasswd file  # htpasswd -c -B -b /root/ocp.htpasswd admin &lt;password&gt; Create Secret  # oc create secret generic htpass-secret \\ --from-file=htpasswd=/root/ocp.htpasswd -n openshift-config   Create Custom Resource  # vi htpasswd-cr.yaml apiVersion: config.openshift.io/v1 kind: OAuth metadata:   name: cluster spec:   identityProviders:   - name: my_htpasswd_provider     mappingMethod: claim     type: HTPasswd     htpasswd:       fileData:         name: htpass-secret # oc apply -f htpasswd-cr.yaml Add role cluster-admin to user  The admin user exists in our htpasswd identity provider but needs to be given permission in OpenShift.  # oc adm policy add-cluster-role-to-user cluster-admin admin Login using admin user  # oc login -u admin OpenShift 4 support multiple identity providers. Once we have added our htpasswd provider we need to select it when logging into the web console.    Adding Nodes and Changing Flavor  The beauty of the OpenStack Installer Provisioned Infrastructure (IPI) is you can easily scale nodes and even change their flavors.  First we can see what machine sets exist in OpenShift.  $ oc get machinesets -n openshift-machine-api NAME                DESIRED   CURRENT   READY   AVAILABLE   AGE ocp4-qchw8-worker   2         2         1       1           8d Next we can see details of our machine set.   oc get machinesets ocp4-qchw8-worker -n openshift-machine-api -o yaml apiVersion: machine.openshift.io/v1beta1 kind: MachineSet metadata:   creationTimestamp: \"2020-01-20T12:52:43Z\"   generation: 1   labels:     machine.openshift.io/cluster-api-cluster: ocp4-qchw8     machine.openshift.io/cluster-api-machine-role: worker     machine.openshift.io/cluster-api-machine-type: worker   name: ocp4-qchw8-worker   namespace: openshift-machine-api   resourceVersion: \"3797711\"   selfLink: /apis/machine.openshift.io/v1beta1/namespaces/openshift-machine-api/machinesets/ocp4-qchw8-worker   uid: c0183898-3b83-11ea-9ccb-fa163e99d432 spec:   replicas: 2   selector:     matchLabels:       machine.openshift.io/cluster-api-cluster: ocp4-qchw8       machine.openshift.io/cluster-api-machineset: ocp4-qchw8-worker   template:     metadata:       creationTimestamp: null       labels:         machine.openshift.io/cluster-api-cluster: ocp4-qchw8         machine.openshift.io/cluster-api-machine-role: worker         machine.openshift.io/cluster-api-machine-type: worker         machine.openshift.io/cluster-api-machineset: ocp4-qchw8-worker     spec:       metadata:         creationTimestamp: null       providerSpec:         value:           apiVersion: openstackproviderconfig.openshift.io/v1alpha1           cloudName: openstack           cloudsSecret:             name: openstack-cloud-credentials             namespace: openshift-machine-api           flavor: ocp.controller           image: rhcos           kind: OpenstackProviderSpec           metadata:             creationTimestamp: null           networks:           - filter: {}             subnets:             - filter:                 name: ocp4-qchw8-nodes                 tags: openshiftClusterID=ocp4-qchw8           securityGroups:           - filter: {}             name: ocp4-qchw8-worker           serverMetadata:             Name: ocp4-qchw8-worker             openshiftClusterID: ocp4-qchw8           tags:           - openshiftClusterID=ocp4-qchw8           trunk: true           userDataSecret:             name: worker-user-data status:   availableReplicas: 1   fullyLabeledReplicas: 2   observedGeneration: 1   readyReplicas: 1   replicas: 2 Notice our machine set has two replicas, meaning we have two worker nodes in our OpenShift cluster. If we want to scale our cluster to three workers we just run a simple command. OpenShift takes care of everything, including provisioning the infrastructure.  $ oc scale --replicas=3 machineset ocp4-qchw8-worker -n openshift-machine-api If we want to change our flavor we can just update the machine set configuration.  $ oc edit machineset ocp4-qchw8-worker -n openshift-machine-api --- cloudsSecret:   name: openstack-cloud-credentials   namespace: openshift-machine-api flavor: ocp.controller2 image: rhcos kind: OpenstackProviderSpec --- In order to change the flavor you need to scale the worker machine set to 0. In order to do that you must relocate the router pods in the openshift-ingress project. You can also kill the router pods after scaling to 0 as simple workaround. OpenShift will not delete the last node until the router pod is killed.  Configuring Retention for Prometheus  In OpenShift 4 prometheus is used to store metrics used in monitoring and alerting. By default prometheus will store data locally using 'empty dir'. It's default retention is 15 days but you can change it to your desire. You can also add a persistent volume so the data will survive node restarts.  First create a new config map.  $ oc -n openshift-monitoring create configmap \\ cluster-monitoring-config Edit config map and add retention.  $ oc -n openshift-monitoring edit configmap cluster-monitoring-config --- data:   config.yaml: |     prometheusK8s:       retention: 24h --- Summary  In this article we discussed OpenShift 4 and the value of installer provisioned infrastructure. It goes well beyond installation, that is just the starting point. OpenShift 4 is capable of operating itself to a certain degree, including automated upgrades. This will not only increase availability but also give operations teams a lot of time back to do work on more innovative, productive projects. We showed how to properly prepare for an OpenShift 4.2 installation on OpenStack using IPI. Finally, using IPI we successfully deployed and validated an OpenShift 4.2 cluster running on OpenStack. I look forward to hearing about your feedback as we explore this brave new world together.  Happy OpenShifting!  (c) 2019 Keith Tenzer  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  ","categories": ["OpenShift"],
        "tags": ["Automation","Cloud","Containers","IPI","Kubernetes","Linux","OpenShift","opensource","OpenStack","PaaS","Stein"],
        "url": "/openshift/automated-infrastructure-in-the-on-premise-datacenter-openshift-4-2-on-openstack-15-stein/",
        "teaser": null
      },{
        "title": "Getting Started with Volume Snapshots in OpenShift 4 and OpenShift Container Storage",
        "excerpt":"  Overview  Volume snapshots are the ability to create snapshots of persistent volumes in kubernetes using the container storage interface (csi) driver. The csi driver allows storage solutions to integrate into kubernetes and expose their technologies. Snapshots of course, have been and are a key technology when discussing data workloads because they enable backup/restore seamlessly, on-demand and in a split second. Even though volume snapshots are in the alpha stage, several storage providers already have integrations, including one that is very interesting, Ceph RDB.    Ceph is a software-defined storage platform that is rock solid, very mature and provides file, block as well as object storage. In this case we will using block storage. In Ceph block is provided through rbd (Rados Block Device). A container-native storage solution called Rook, based completely on kubernetes operators provides a container-native deployment of Ceph on Kubernetes or OpenShift. In this article we are using OpenShift 4.2 GA and a internal beta release of OpenShift Container Storage (based on Rook).  How it Works  The CSI driver allows for a snapshotter to be implemented. The snapshotter runs as a side-car container and watches the kubernetes API for snapshot related events from the CSI driver. In order to create a volume snapshot a volume snapshot class must exist. This is similar to the storage class but defines the snapshotter and access to the appropriate CSI driver. Once a volume snapshot class exists a volume snapshot can be created for a given persistent volume claim (pvc). The volume snapshot will then trigger the snapshot operation on the storage device, in this case Ceph RBD. The volume snapshot allows the snapshotter to provide metadata about the snapshot contents to the end-user.    Volume Snapshot Backup  Now that we understand the basics we will see how to actually create a backup. We will deploy a mariadb database on persistent storage using Ceph RBD, create a table in the database with some records, setup a volume snapshot class and finally perform a volume snapshot of the database.  Deploy Mariadb database  We deployed a persistent mariadb database using ceph rbd provisioner.    Add table and records to database  Connect to database.  $ oc rsh -n databases mariadb-6-rswv6 sh-4.2$ mysql -u root Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MariaDB connection id is 20 Server version: 10.2.22-MariaDB MariaDB Server  Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.  Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. MariaDB [(none)]&gt; use sampledb; Create table authors and add some records.  MariaDB [(none)]&gt; CREATE TABLE authors (id INT, name VARCHAR(20), email VARCHAR(20));  MariaDB [(none)]&gt; INSERT INTO authors (id,name,email) VALUES(1,\"Keith\",\"keith@domain.com\");  MariaDB [(none)]&gt; INSERT INTO authors (id,name,email) VALUES(2,\"John\",\"john@domain.com\");  MariaDB [(none)]&gt; INSERT INTO authors (id,name,email) VALUES(3,\"Bob\",\"bob@domain.com\"); Show records for table authors.  MariaDB [books]&gt; select * from authors; +------+-------+-----------------+ | id   | name  | email           | +------+-------+-----------------+ |    1 | Keith | keith@domain.com| |    2 | John  | john@domain.com | |    3 | Bob   | bob@domain.com  | +------+-------+-----------------+ 3 rows in set (0.00 sec) Get the Ceph RBD Image  First get the persistent volume name for our mariadb.  $ oc get pv -o 'custom-columns=NAME:.spec.claimRef.name,PVNAME:.metadata.name,STORAGECLASS:.spec.storageClassName,VOLUMEHANDLE:.spec.csi.volumeHandle' NAME                      PVNAME                                     STORAGECLASS                  VOLUMEHANDLE ocs-deviceset-0-0-cc8nr   pvc-08b360c9-0214-11ea-a05b-005056814c6f   thin                          &lt;none&gt; ocs-deviceset-1-0-bntqm   pvc-08b53c69-0214-11ea-a05b-005056814c6f   thin                          &lt;none&gt; ocs-deviceset-2-0-g65tl   pvc-08b70a3e-0214-11ea-a05b-005056814c6f   thin                          &lt;none&gt; db-noobaa-core-0          pvc-27708237-0214-11ea-a05b-005056814c6f   ocs-storagecluster-ceph-rbd   0001-0011-openshift-storage-0000000000000001-2f81ea18-0214-11ea-aa51-0a580a81000d rook-ceph-mon-a           pvc-9dd37deb-0213-11ea-a05b-005056814c6f   thin                          &lt;none&gt; rook-ceph-mon-b           pvc-a12c254a-0213-11ea-a05b-005056814c6f   thin                          &lt;none&gt; mariadb                   pvc-a370d1ed-021b-11ea-bca7-005056818b87   ocs-storagecluster-ceph-rbd   0001-0011-openshift-storage-0000000000000001-a380eb7b-021b-11ea-aa51-0a580a81000d rook-ceph-mon-c           pvc-a42a79c9-0213-11ea-a05b-005056814c6f   thin Next get the Ceph rbd image mapped to our persistent volume.  [ktenzer@keith-laptop ~]$ oc get pv pvc-a370d1ed-021b-11ea-bca7-005056818b87 -o jsonpath='{.spec.csi.volumeHandle}' | cut -d '-' -f 6- | awk '{print \"csi-vol-\"$1}' csi-vol-a380eb7b-021b-11ea-aa51-0a580a81000d Create Volume Snapshot Class  In order to create the volume snapshot class some information is needed. We need to know the snapshotter, the provisioner secret and the secret namespace.  $ oc get sc ocs-storagecluster-ceph-rbd -o yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata:   creationTimestamp: \"2019-11-08T10:35:52Z\"   name: ocs-storagecluster-ceph-rbd   ownerReferences:   - apiVersion: ocs.openshift.io/v1     blockOwnerDeletion: true     controller: true     kind: StorageCluster     name: ocs-storagecluster     uid: 893852b9-0213-11ea-bca7-005056818b87   resourceVersion: \"30740\"   selfLink: /apis/storage.k8s.io/v1/storageclasses/ocs-storagecluster-ceph-rbd   uid: 89666c5f-0213-11ea-bab7-00505681830f parameters:   clusterID: openshift-storage   csi.storage.k8s.io/fstype: ext4   csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node   csi.storage.k8s.io/node-stage-secret-namespace: openshift-storage   csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner   csi.storage.k8s.io/provisioner-secret-namespace: openshift-storage   imageFeatures: layering   imageFormat: \"2\"   pool: ocs-storagecluster-cephblockpool provisioner: openshift-storage.rbd.csi.ceph.com reclaimPolicy: Delete volumeBindingMode: Immediate Create volume snapshot class.  $ vi snapshot-class.yaml  apiVersion: snapshot.storage.k8s.io/v1alpha1 kind: VolumeSnapshotClass metadata:   name: ocs-storagecluster-ceph-rbd-snapshot snapshotter: openshift-storage.rbd.csi.ceph.com parameters:   clusterID: openshift-storage   csi.storage.k8s.io/snapshotter-secret-name: rook-csi-rbd-provisioner   csi.storage.k8s.io/snapshotter-secret-namespace: openshift-storage $ oc apply -f snapshot-class.yaml Create Volume Snapshot  Now that we have our volume snapshot class we can snapshot a persistent volume claim.  $ vi create-snapshot.yaml  apiVersion: snapshot.storage.k8s.io/v1alpha1 kind: VolumeSnapshot metadata:   name: mariadb-1 spec:   snapshotClassName: ocs-storagecluster-ceph-rbd-snapshot   source:     name: mariadb     kind: PersistentVolumeClaim $ oc apply -f create-snapshot.yaml -n databases Show volume snapshot.  [ktenzer@keith-laptop ~]$ oc get volumesnapshot  NAME AGE  mariadb-1 35s Show the volume snapshot content. The snapshotter provides metadata about snapshots to kubernetes and stores that information in the snapshot content.  $ oc get volumesnapshotcontent  NAME                                               AGE snapcontent-fdf13bf1-0221-11ea-bab7-00505681830f   3m2s $ oc get volumesnapshotcontent snapcontent-fdf13bf1-0221-11ea-bab7-00505681830f -o yaml apiVersion: snapshot.storage.k8s.io/v1alpha1 kind: VolumeSnapshotContent metadata:   creationTimestamp: \"2019-11-08T12:19:22Z\"   finalizers:   - snapshot.storage.kubernetes.io/volumesnapshotcontent-protection   generation: 1   name: snapcontent-fdf13bf1-0221-11ea-bab7-00505681830f   resourceVersion: \"74329\"   selfLink: /apis/snapshot.storage.k8s.io/v1alpha1/volumesnapshotcontents/snapcontent-fdf13bf1-0221-11ea-bab7-00505681830f   uid: fef777ae-0221-11ea-a05b-005056814c6f spec:   csiVolumeSnapshotSource:     creationTime: 1573215562000000000     driver: openshift-storage.rbd.csi.ceph.com     restoreSize: 1073741824     snapshotHandle: 0001-0011-openshift-storage-0000000000000001-fe517800-0221-11ea-aa51-0a580a81000d   deletionPolicy: Delete   persistentVolumeRef:     apiVersion: v1     kind: PersistentVolume     name: pvc-a370d1ed-021b-11ea-bca7-005056818b87     resourceVersion: \"55833\"     uid: a3ef7f08-021b-11ea-bab7-00505681830f   snapshotClassName: ocs-storagecluster-ceph-rbd-snapshot   volumeSnapshotRef:     apiVersion: snapshot.storage.k8s.io/v1alpha1     kind: VolumeSnapshot     name: mariadb-1     namespace: databases     resourceVersion: \"74316\"     uid: fdf13bf1-0221-11ea-bab7-00505681830f Verify Snapshot on Ceph RBD  OpenShift Container Storage provides a tools pods for easily accessing the Ceph cluster. Save the pod name to a vairable.  TOOLS_POD=$(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name) Using tools pod show information about mariadb Ceph rbd image.  $ oc rsh -n openshift-storage $TOOLS_POD  rbd -p ocs-storagecluster-cephblockpool info csi-vol-a380eb7b-021b-11ea-aa51-0a580a81000d                                        rbd image 'csi-vol-a380eb7b-021b-11ea-aa51-0a580a81000d': \tsize 1 GiB in 256 objects \torder 22 (4 MiB objects) \tsnapshot_count: 0 \tid: 5617782da596 \tblock_name_prefix: rbd_data.5617782da596 \tformat: 2 \tfeatures: layering \top_features:  \tflags:  \tcreate_timestamp: Fri Nov  8 11:33:52 2019 \taccess_timestamp: Fri Nov  8 11:33:52 2019 \tmodify_timestamp: Fri Nov  8 11:33:52 2019 List all snapshots for mariadb Ceph rbd image.  $ oc rsh -n openshift-storage $TOOLS_POD rbd snap ls ocs-storagecluster-cephblockpool/csi-vol-a380eb7b-021b-11ea-aa51-0a580a81000d SNAPID NAME                                          SIZE  PROTECTED TIMESTAMP                      6 csi-snap-fe517800-0221-11ea-aa51-0a580a81000d 1 GiB yes       Fri Nov  8 12:19:22 2019 Volume Snapshot Restore  Now that we have performed and validated a backup lets do a restore. We will drop the table to simulate a loss of data and then perform a restore from our snapshot and hopefully get access to our data again.  Connect to database  $ oc rsh -n databases mariadb-6-rswv6 sh-4.2$ mysql -u root Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MariaDB connection id is 20 Server version: 10.2.22-MariaDB MariaDB Server  Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.  Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. MariaDB [(none)]&gt; use sampledb; Drop table  MariaDB [books]&gt; drop table authors; Query OK, 0 rows affected (0.03 sec) Verify table is dropped  MariaDB [books]&gt; select * from authors; ERROR 1146 (42S02): Table 'books.authors' doesn't exist At this point we have simulated a restore scenario, our data is gone!  Scale Deployment Config to 0  Before we can do a snapshot restore we need to shutdown the database by stopping the pod.  [ktenzer@keith-laptop ~]$ oc scale dc/mariadb --replicas=0 deploymentconfig.apps.openshift.io/mariadb scaled Perform Snapshot Restore  To restore the snapshot we can use CSI provider or directly using the Ceph tools. Using the CSI provider will create a new PVC that is backed by the snapshot. In this case the snapshot will be cloned, thus creating a new read/write volume. Using the Ceph tools however we can rollback the snapshot without changing the underlying PV/PVC.  Snapshot Restore with CSI  $ vi restore-snapshot.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata:   name: mariadb-restore spec:   storageClassName: ocs-storagecluster-ceph-rbd   dataSource:     name: mariadb-1     kind: VolumeSnapshot     apiGroup: snapshot.storage.k8s.io   accessModes:     - ReadWriteOnce   resources:     requests:       storage: 1Gi $ oc apply -f restore-snapshot.yaml Once new PVC is created, the deployment config needs to be updated to use the new volume.  $ oc edit cd/mariadb --- volumes: - name: mariadb   persistentVolumeClaim:     claimName: mariadb-restore --- Snapshot Restore with Ceph Tools  Using the tools pod we can simply rollback the snapshot without creating a new PV/PVC.  $ oc rsh -n openshift-storage $TOOLS_POD rbd snap rollback ocs-storagecluster-cephblockpool/csi-vol-a380eb7b-021b-11ea-aa51-0a580a81000d@csi-snap-76040419-0228-11ea-aa51-0a580a81000d Rolling back to snapshot: 100% complete...done. Scale Deployment Config to 1  After snapshot restore start database.  oc scale dc/mariadb --replicas=1 deploymentconfig.apps.openshift.io/mariadb scaled Connect to database  [ktenzer@keith-laptop ~]$ oc rsh -n databases mariadb-6-rswv6 sh-4.2$ mysql -u root Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MariaDB connection id is 20 Server version: 10.2.22-MariaDB MariaDB Server  Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.  Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. MariaDB [(none)]&gt; use sampledb; Show records for table authors.  MariaDB [books]&gt; select * from authors; +------+-------+-----------------+ | id   | name  | email           | +------+-------+-----------------+ |    1 | Keith | keith@domain.com| |    2 | John  | john@domain.com | |    3 | Bob   | bob@domain.com  | +------+-------+-----------------+ 3 rows in set (0.00 sec) The restore using our snapshot succeeded and as you can see our data is back...magic!  Summary  In this article we discussed how volume snapshots work and are implemented in kubernetes. Using OpenShift 4.2 and OpenShift Container Storage we demonstrated a backup and recovery of a mariadb database using volume snapshots in kubernetes. While it is early, this is a game changer for data workloads. Using volume snapshot we finally have a mechanism to integrate storage snapshots into kubernetes and as such provide proper backup and recovery solutions. I have no doubt this will lead to a large adoption of data workloads on kubernetes so get ready!  Happy Snapshotting!  (c) 2019 Keith Tenzer  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  ","categories": ["OpenShift"],
        "tags": ["Ceph","Kubernetes","OCS","OpenShift","RBD","Rook","Snapshot","Storage","volume"],
        "url": "/openshift/getting-started-with-volume-snapshots-in-openshift-4-and-kubernetes-1-12/",
        "teaser": null
      },{
        "title": "My Mentor and Friend Andi Neeb",
        "excerpt":"  Andi Presenting at Ansible Roadshow in June 2019    Andi and I in the lounge on our way back from a sales kickoff in 2016  It has been a few weeks since I learned of the tragic passing of my friend, colleague and mentor Andreas Neeb. I am just now able to gather my thoughts, in an attempt to pass on the lessons I learned from one of the truly great, Red Hatters. First I would like to remember how Andi and I became colleagues and friends.    It was a cold and frosty February 1st, 2015 and my first day at Red Hat. I was greeted at the reception by my new manager Juergen \"Buddy\" Hoffman and new colleague Andi Neeb. After a quick introduction Andi explained it was also his first day, somehow I thought he was a long-time Red Hatter. He seemed so confident, comfortable and at peace with the situation. I was quite nervous, even terrified. We both were brought to our new desks where our phones and laptops were already awaiting. We both ironically, ordered iPhones and Thinkpad's. We quickly opened our iPhones and then kinda looked at our Thinkpad. Andi said \"so you didn't have the courage to order a Macbook either?\". I laughed and said \"yeah we are so screwed, eating your own dog food is a novel idea until you have to do it\". Years later and laptops later Andi and I would continue to use Linux, we just moved from RHEL to Fedora.  Andi and I would go through sales bootcamp together in the following weeks. We competed fiercely yet supported and helped one another at the same time. It was here that our trust and friendship really came to be. From sales bootcamp we decided to go for RHCA (Red Hat Certified Architect). Now this isn't just a test or class, it is seven, highly technical certifications in Red Hat products, where you actually have to know how to do things as the tests are practical. It had never been done in less than a year (in Germany), so we decided to go for 6 months. We studied together and test for test worked at our goal while pushing each other forward. Only together were we able to accomplish such a goal.  Looking back, Andi was really a mentor for me from day 1. When I struggled, didn't have an answer or was contemplating difficult scenarios at work or life, it was Andi Neeb I trusted and confided with. He always had a suggestion, insight or idea that helped me move forward and make sense of the world around me. The greatest mentors I think, are the ones who don't even seem to be mentors. If we stop and think about how we got somewhere, it is quickly realized without them it wouldn't happen. Andi was such a person and mentor for me.  Here are some of the lessons I learned from Andi that I would like to share.   Don't be selfish, helping others is more rewarding for you and them If everyone agrees on something, hit the panic button Don't be afraid to challenge anything We win when all points are considered and the best one is chosen Support your family, friends and colleagues, these are life's treasures Be realistic, focus on the next steps while keeping the overall goal in perspective There is a lot of passion in compassion, be humble If it feels right, it is right, follow your heart Value is in the eye of the beholder, finding value is like treasure hunting, it is fun and rewarding Do what you love, follow your dreams, don't burden yourself and other's with work  Red Hat is a place where not only actions matter but also how they are carried out. It is a place where ideas, sharing and teams matter more than the individuals. It's a place where trust, compassion and openness form the bed rock.  Growing up at Red Hat you constantly hear stories of great Red Hatters, not because they were the smartest, the most gifted, had the best idea, executed...no but because they were successful at inspiring others to be better people and leave the world a slightly better place than they found it. It's easy to immortalize bravery of people jumping into burning buildings to save others and I am not going to downplay that kind of sacrifice and bravery. There are people however, that do the little things every day to help their families, friends and colleagues. They aren't as concerned about the result but how they got there. Andi was such an everyday hero. He was the best of us, a true Red Hatter and for me, for the rest of my days, someone I will strive to follow and be like.  Life seems so final, it has a beginning and an end. The question for me is really, is it an end or just the start of something else. I know this is where faith comes and what you believe is exactly that, a belief, nobody can deny or confirm it. One thing though I believe in is that we live in each others hearts and memories. Our impact on this world as such goes well beyond our lifespan. Is that immortality? I don't know but it is something more than just an end. Hold your loved ones in your hearts and memories, never forget what they stood for, carry their lessons forward. May Andi's legacy continue on in our hearts, forever.  Rest In Peace Andi  &nbsp;  ","categories": ["General"],
        "tags": ["andi neeb","andreas neeb","friend","life","mentor","Red Hat","red hatter"],
        "url": "/general/my-mentor-and-friend-andi-neeb/",
        "teaser": null
      },{
        "title": "Automation on the Edge - Ansible Tower with Isolated Nodes",
        "excerpt":"  Overview  In this article we will look at providing an automation platform for edge computing. Generally I would say edge computing is about moving data-handling away from centralized systems, towards local sources of data capture. An edge can be a remote office, server, laptop, tablet, smartphone or IoT anything endpoint. Edge computing is really just taking distributed computing one step further and removing the requirement to have always-connected network segments.    There are many use cases for edge computing, from classical retail, travel, logistics, banking, virtually every market segment has a need for edge computing. The trend is that the edges is becoming increasingly more intelligent, requiring a much more complicated software-stack, similar to what we observe in a centralized IT environment. In order to manage, maintain and update edge services an automation platform is obviously critical. Ansible as an automation language and Ansible Tower offer the perfect platform for edge computing.  Retail Edge Use Case  In order to take a closer look, lets now focus on an edge use case in retail. If we look at a classical retail environment, we have centralized IT, remote stores and distribution warehouses. You might think brick and mortar is dead but not so fast. Customers still want to try things out or see them in person (especially with food/clothes) before buying. They also desire advice and service with higher-end, more technically advanced items. Classical retailers want their customers to frequent their stores, as such they need to offer them a more personal experience and a reason to visit the store. Offering their customers exactly what they want, when they want it, for the right price is a good start.  In order to do this however, retailers need to understand their customers much better than even Google. Once they understand a customer they need a just in time offering which requires perfect logistics execution from warehouses. They need to provide their customers excellent experiences in the store, for example knowing how many customers are shopping in a given store or waiting in a checkout line to dynamically react to changing demands.  Ultimately it all requires a lot of intelligence and software in the store because that is where the customers are going to be (hopefully). The challenge though is that retailers still need to manage the stores centrally, while providing each store a certain degree of autonomy. A store must also be fully operational, even if it's connection to central IT is severed.  Not all stores will want to roll out services or updates on a fixed schedule and being able to rollout completely new stores, in an expected time frame, requires a complete automation strategy.  Ansible Automation, specifically Ansible Tower provides the perfect automation platform to enable central management and through a feature called isolated nodes, provides the store it's autonomy. In addition Ansible Tower also provides a powerful role-based access model and job scheduler to further enable autonomy. Automation playbooks can be provided as-a-service, using Ansible Tower Survey's, allowing for easy consumption. Update, rollout of new services or even a new store is just a click away and it needs to be as time-to-market is of the essence. The below illustration shows a typical retail environment with two centralized datacenters and X stores.    In this example we see Ansible Tower (the platform for automation) is deployed centrally in both datacenters. Each store has its own instance group with its own nodes. This allows stores to scale independently of one another. In addition Ansible Tower allows for isolated nodes which are nodes that have a special piece of software allowing them to execute jobs locally. This means Ansible jobs are no longer affected by network latencies or connectivity issues between the central datacenter and the store.  Deploy Ansible Tower with Isolated Nodes  Before starting you will need two RHEL 8 systems and of course they need to be entitled with subscriptions.  Register with subscription manager and enable repos.  # subscription-manager register # subscription-manager list --available # subscription-manager attach --pool=&lt;pool id&gt; # subscription-manager repos --disable=* # subscription-manager repos --enable=rhel-8-for-x86_64-baseos-rpms \\ --enable=ansible-2.8-for-rhel-8-x86_64-rpms \\ --enable=rhel-8-for-x86_64-appstream-rpms There are a few ways you can deploy Ansible Tower, my recommendation is to use the setup bundle. Download latest setup bundle on the system you want to run Tower.  # curl -O https://releases.ansible.com/ansible-tower/setup-bundle/ansible-tower-setup-bundle-latest.el8.tar.gz Extract the setup bundle.  # gunzip ansible-tower-setup-bundle-latest.el8.tar.gz  # tar xvf ansible-tower-setup-bundle-latest.el8.tar Ansible Tower uses (who would've thought) Ansible to install itself. As such the installation configuration is an Ansible inventory file.  Setup Ansible Tower inventory file.  # cd ansible-tower-setup-bundle-3.6.2-1 # vi inventory  [tower] tower ansible_connection=local  [isolated_group_storeA] node1  [isolated_group_storeA:vars] controller=tower  [database] tower  [all:vars] admin_password='&lt;password&gt;'  pg_host='tower' pg_port='5432'  pg_database='awx' pg_username='awx' pg_password='&lt;password&gt;' pg_sslmode='prefer' # set to 'verify-full' for client-side enforced SSL  rabbitmq_username=tower rabbitmq_password='&lt;password&gt;' rabbitmq_cookie=cookiemonster  # Isolated Tower nodes automatically generate an RSA key for authentication; # To disable this behavior, set this value to false # isolated_key_generation=true  # SSL-related variables  # If set, this will install a custom CA certificate to the system trust store. # custom_ca_cert=/path/to/ca.crt  # Certificate and key to install in nginx for the web UI and API # web_server_ssl_cert=/path/to/tower.cert # web_server_ssl_key=/path/to/tower.key  # Use SSL for RabbitMQ inter-node communication. Because RabbitMQ never # communicates outside the cluster, a private CA and certificates will be # created, and do not need to be supplied. # rabbitmq_use_ssl=False  # Server-side SSL settings for PostgreSQL (when we are installing it). # postgres_use_ssl=False # postgres_ssl_cert=/path/to/pgsql.crt In our case the Ansible Tower system is known as tower and reachable via hostsfile. The isolated node is known as node1 and reachable via hostsfile. When defining isolated nodes we need to define an isolated group. Each isolated group needs to designate who is it's controller group. The default group is tower which is where tower nodes exist but it could also be another instance group where tower nodes exist. Aside from setting up isolated group we are installing all tower services and database on node called tower.  Before we run install we probably want to run it through tmux so in case our ssh session is interrupted, we can attach back to the terminal. For those new to RHEL 8, tmux has replaced screen.  # dnf install -y tmux # tmux Of course since security is important all of us should be using ssh keys and disabling password authentication. To get the Ansible Tower installation working with ssh keys we need to add the keys to the local ssh configuration dynamically.  # exec ssh-agent bash # ssh-add /path/to/private_key Finally we can run the installer and grab some coffee or if you are me a cappuccino.  # ./setup.sh Configuring Isolated Nodes  In Ansible Tower we configure job templates to run against an instance group. A job template is a playbook, its inventory, environment, vars, credentials and some other things. An instance group is just a group of nodes. An instance group can contain normal nodes or isolated nodes. The difference being that isolated nodes have a piece of software running that allows them to execute the job template and it's environment locally. Ansible Tower will as such push the job template (playbook) and environment to isolated node where it will be executed locally on isolated node. Ansible Tower will poll the isolated node. In the case of a normal node, Ansible Tower will not push the job template or environment, it will instead run the playbook centrally and execute only the playbook tasks through the node.  In Ansible Tower under instance groups we can see all groups. In our configuration we have two groups. Tower which itself is a node and storeA which has an isolated node called node1.    Instance groups can be configured in job templates under the instance groups field.    In our environment we can select tower or storeA as the instance group. StoreA is a group that contains our isolated node.    A job template will execute against its instance group. Again if that instance group contains isolated nodes, the playbook and environment will be pushed for local execution.  Summary  In this article we discussed the advantages of edge computing and the need for an automation platform that can fulfill it's network requirements. We explained the broad uses of edge computing architectures, it is no longer just limited to IoT but fast becoming a standard capability in any modern platform. In retail we saw how a very traditional market segment can take advantage of automation and edge computing. Finally a step-by-step guide on how to deploy Ansible Tower with isolated nodes for solving edge computing automation was provided.  Happy Edge Automating!  (c) 2020 Keith Tenzer  ","categories": ["Ansible"],
        "tags": ["Ansible Tower","Automation","Edge","Edge Computing","Isolated Nodes"],
        "url": "/ansible/automation-on-the-edge-ansible-tower-with-isolated-nodes/",
        "teaser": null
      },{
        "title": "OpenShift Operator SDK: Go Getting Started Guide Part II",
        "excerpt":"  Overview  In this article we will provide a hands-on guide to building your very first operator in Go. Using the Operator SDK we will learn how to create boilerplate code, build and deploy an operator.  This article is part of a series that will walk you through understanding and building operators in Go or Ansible end-to-end.   OpenShift Operator Getting Started Part I OpenShift Operator SDK: Go Getting Started Guide Part II OpenShift Operator SDK: Ansible Getting Started Guide Part III OpenShift Operator Lifecycle Management Guide: Integrating Operators in OLM Part IV    Setup Development Environment  There are some prerequisites needed to build the operator-sdk and eventually your own operators. In this case I am using Fedora Linux.  Install Docker 17.03+  Add the docker ce repositories. Note: you can also use podman and buildah instead of Docker for those that want a complete and clean divorce.  $ sudo dnf -y install dnf-plugins-core  $ sudo dnf config-manager \\     --add-repo \\     https://download.docker.com/linux/fedora/docker-ce.repo Install docker-ce  $ sudo dnf -y install docker-ce docker-ce-cli $ sudo systemctl start docker $ sudo systemctl enable docker Install Go 1.13+  Download Go and extract it to /usr/local  $ curl -O https://dl.google.com/go/go1.13.6.linux-amd64.tar.gz $ sudo tar -C /usr/local -xzf go1.13.6.linux-amd64.tar.gz Update bash profile and set GOPATH/GOBIN  $ vi ~/.bash_profile  --- # User specific environment and startup programs  PATH=$PATH:$HOME/.local/bin:$HOME/bin:/usr/local/go/bin export GOPATH=/home/ktenzer/go export GOBIN=/home/ktenzer export PATH --- $ source ~/.bash_profile Check Go version  $ go version go version go1.13.6 linux/amd64 Install Mercurial 3.9+  $ sudo dnf -y install mercurial Install Bazaar 2.7.0+  $ sudo dnf -y install -y bzr Build Operator SDK  Now that the prerequisites are installed we will build the operator sdk from source.  $ go get -d github.com/operator-framework/operator-sdk $ cd $GOPATH/src/github.com/operator-framework/operator-sdk $ git checkout master $ make tidy $ make install The Makefile will deploy the operator-sdk CLI into the home directory. Move it to /usr/local/bin.  $ sudo mv ~/operator-sdk /usr/local/bin Check the operator-sdk version  $ operator-sdk version operator-sdk version: \"v0.15.0-5-g6b4e1370\", commit: \"6b4e1370f18e5d028b283375b740026d2efb0a44\", go version: \"go1.13.6 linux/amd64\" Building Your First Operator  The SDK is able to generate not only boilerplate code but also the CRDs, controller and API endpoints. In this example we will create a cars operator. It will simply provide an endpoint allowing us to do CRUD on car objects. Each car object will spawn a pod with a base image.  Change directory to the github.com source directory.  $ cd $GOPATH/src/github.com Note: create directory if it doesn't exist  Using operatore-sdk cli create boilerplate code.  $ operator-sdk new cars-operator --repo github.com/cars-operator cd cars-operator Add new API CRD for our car operator  $ operator-sdk add api --api-version=cars.example.com/v1alpha1 \\ --kind=Car Add controller API that watches our cars  $ operator-sdk add controller --api-version=cars.example.com/v1alpha1 \\  --kind=Car Create User on Quay.io  We will be using quay to store operator images. Authenticate using Github or Gmail to https://quay.io. Once authenticated go to account settings and set a password.  Test Quay.io Credentials  $ sudo docker login quay.io Build Operator  Using operator-sdk cli build the operator which will push the image to your local Docker registry.  $ sudo PATH=$PATH:/usr/local/go/bin operator-sdk build \\ quay.io/ktenzer/cars-operator Note: Substitute ktenzer for your username.  Push Operator to your Quay.io Account  $ sudo docker push quay.io/ktenzer/cars-operator:latest Make Quay Repository Public  By default your Quay repository will be private. If we want to access it from OpenShift we need to make it public. Login to quay.io with your username/password  Select the cars-operator repository    Under Repository Settings enable visibility to be public    Update Image in operator.yaml  We need to point the image to the location in our Quay repository.  vi deploy/operator.yaml --- # Replace this with the built image name image: quay.io/ktenzer/cars-operator command: --- Deploy Cars Operator on OpenShift  Now that the operator is built and pushed to our Quay repository we can deploy it on an OpenShift cluster.  Authenticate to OpenShift Cluster  $oc login https://api.ocp4.keithtenzer.com:6443 Create new project for our operator  $ oc new-project cars-operator Setup service accounts and role bindings  $ oc create -f deploy/service_account.yaml $ oc create -f deploy/role.yaml $ oc create -f deploy/role_binding.yaml Create the CRD for our operator  $ oc create -f deploy/crds/cars.example.com_cars_crd.yaml Deploy our operator  $ oc create -f deploy/operator.yaml Create the CR for our operator  $ oc create -f deploy/crds/cars.example.com_v1alpha1_car_cr.yaml Using Cars Operator  The cars operator will automatically deploy an example-car. Whe can query our car object just like any other Kubernetes object. This is the beauty of CR/CRDs and operators. We can easily extend the Kubernetes API without needing to understand it's complexity.  $ oc get car NAME AGE example-car 31m Next we can get information about our example-car.  $ oc get car example-car -o yaml apiVersion: cars.example.com/v1alpha1 kind: Car metadata:   creationTimestamp: \"2020-01-25T12:15:45Z\"   generation: 1   name: example-car   namespace: cars-operator   resourceVersion: \"2635723\"   selfLink: /apis/cars.example.com/v1alpha1/namespaces/cars-operator/cars/example-car   uid: 6a424ef9-3f6c-11ea-a391-fa163e9f184b spec:   size: 3   Looking at the running pods in our cars-operator project we see the operator and our example-car.  $ oc get pods -n cars-operator NAME                            READY   STATUS    RESTARTS   AGE cars-operator-b98bff54d-t2465   1/1     Running   0          5m example-car-pod                 1/1     Running   0          5m Create a new Car  Lets now create a BMW car.  $ vi bmw.yaml kind: Car metadata:   name: bmw spec:   size: 1 $ oc create -f bmw.yaml Here we can see we now have a BMW car.  $ oc get car NAME          AGE bmw           11m example-car   31m Of course we can get information about our BMW car.  $ oc get car bmw -o yaml apiVersion: cars.example.com/v1alpha1 kind: Car metadata:   creationTimestamp: \"2020-01-25T12:35:25Z\"   generation: 1   name: bmw   namespace: cars-operator   resourceVersion: \"2644044\"   selfLink: /apis/cars.example.com/v1alpha1/namespaces/cars-operator/cars/bmw   uid: 294bc47f-3f6f-11ea-b32c-fa163e3e8e24 spec:   size: 1 Finally as with the example-car, the operator will start a new pod when the BMW car is created.  $ oc get pods -n cars-operator NAME                            READY   STATUS    RESTARTS   AGE bmw-pod                         1/1     Running   0          10m cars-operator-b98bff54d-t2465   1/1     Running   0          14m example-car-pod                 1/1     Running   0          14m Cleanup  Follow these steps to remove the operator cleanly.  $ oc delete -f deploy/crds/cars.example.com_v1alpha1_car_cr.yaml $ oc delete -f deploy/operator.yaml $ oc delete -f deploy/role.yaml $ oc delete -f deploy/role_binding.yaml $ oc delete -f deploy/service_account.yaml $ oc delete -f deploy/crds/cars.example.com_cars_crd.yaml $ oc delete project cars-operator Summary  In this article a step-by-step guide was provided to setup a development environment, generate boilerplate code and deploy our custom cars operator using Go on OpenShift using the Operator Framework.  Happy Operatoring!  (c) 2020 Keith Tenzer  &nbsp;  &nbsp;  ","categories": ["OpenShift"],
        "tags": ["Containers","Go","Kubernetes","Linux","OpenShift","Operator","Operator Framework"],
        "url": "/openshift/openshift-operator-sdk-go-getting-started-guide-part-ii/",
        "teaser": null
      },{
        "title": "OpenStack 16 (Train) Lab Installation and Configuration Guide for Hetzner Root Servers",
        "excerpt":"  Overview   In this article we will focus on installing and configuring OpenStack Train using RDO and the packstack installer. RDO is a community platform around Red Hat’s Enterprise OpenStack Distribution. It allows you to test the latest OpenStack capabilities on a stable platform such as Red Hat Enterprise Linux (RHEL) or CentOS. This guide will take you through setting up Hetzner root server, preparing environment for OpenStack, installing the OpenStack Train release, adding a floating ip subnet through OVS, configuring networking, security groups, flavors, images and are other OpenStack related services. The outcome is a working OpenStack environment based on the Train release that you can use as a baseline for testing your applications using OpenStack capabilities. The installation will create an all-in-one deployment however you can use this guide to create a multi-node deployment as well.    Root Server Specs  CPU: Intel(R) Xeon(R) CPU E5-1650 v3 @ 3.50GHz (12 Cores) Memory: 128GB RAM Disk: 2 x 480GB SSD Network: 1Gbit IPV4 Adresses: 1 x IPV4 + /29 Subnet (6 IPs) Hetzner is a hosting providing and auctions off used hardware for very competitive prices on a per/month basis (https://www.hetzner.de/sb). Hetzner provides one IP for each root server that is accessible via internet. It is a /32 so if you want additional IPs, like you would for OpenStack to use as floating ips you need to order additional subnet. You can order an additional subnet as I have done. Hetzner will route that subnet through the host IP of the /32. This requires creating an additional OVS bridge in OpenStack and this guide will go through that configuration.  Configure Root Server  As mentioned, Hetzner will give you access to your root server via IP and provide ability to manage the root server. Basic things like put root server in rescue mode or reboot. You can provide public ssh key or use password to access system. Please don’t use password, this is the internet, bad things can and will happen.  Enter Rescue Mode  In order to install OS or repair it you need to get into rescue mode.        Configure Hetzner Firewall.  While server is rebootting you can modify your servers firewall rules for incoming traffic. By default firewall allows port 22 for SSH and nothing else. Firewall settings can be modified under your server’s settings in Hetzner’s web UI https://robot.your-server.de/server. For OpenStack I would recommend 80, 443, 22 and ICMP.    If you plan on running OpenShift on OpenStack you need to add some additional rules.    Connect to root server via ssh and private key.  # ssh -i .ssh/id_rsa.pub Create Root Server Configuration.  The root server in this case has two disks. They are being configured in non-redundant RAID 1 (stripping) configuration. OpenStack requires decent disk performance, I found when using mirroring it was not enough. If you want redundant configuration or decent performance, you need to pay extra for SSDs. Since we will setup LVM partitions, creating one for Cinder (cinder-volumes) is a good idea. You don’t want to use loop back driver for Cinder. Finally you can provide your own image (if you want to use RHEL, you must do so).  My colleague Andy Neeb also automated deployment using Hetzner API. If you are interested in automated deployment here is his Github repository: https://github.com/andyneeb/ansible-hetzner.  [CENTOS]  # vi config.txt  DRIVE1 /dev/sda DRIVE2 /dev/sdb SWRAID 1 SWRAIDLEVEL 0 BOOTLOADER grub HOSTNAME myrootserver.lab PART /boot ext3 512M PART lvm vg0 150G  LV vg0 root / ext4 50G LV vg0 swap swap swap 8G LV vg0 var /var ext4 10G LV vg0 tmp /tmp ext4 30G  IMAGE /root/.oldroot/nfs/install/../images/CentOS-75-64-minimal.tar.gz [RHEL]  Note: You need to provide your own RHEL image. I have documented in a separate post how to do that. Once you have image you can upload it to the root server /root directory. Since Hetzner does not understand RHEL you will need to ensure the image name is CentOS-75-64-minimal.tar.gz.  # vi config.txt  DRIVE1 /dev/sda  DRIVE2 /dev/sdb  SWRAID 1  SWRAIDLEVEL 0  BOOTLOADER grub  HOSTNAME myrootserver.lab  PART /boot ext3 512M  PART lvm vg0 150G   LV vg0 root / ext4 50G  LV vg0 swap swap swap 8G  LV vg0 var /var ext4 10G  LV vg0 tmp /tmp ext4 30G  IMAGE /root/CentOS-75-64-minimal.tar.gz Install image and create partitions.  # installimage -a -c config.txt   Note: you may see error with RHEL image if you renamed it to CentOS because Hetzner expects CentOS but it is safe to ignore.  Hetzner Online GmbH - installimage  Your server will be installed now, this will take some minutes You can abort at any time with CTRL+C ...  : Reading configuration done  : Loading image file variables done  : Loading centos specific functions done  1/18 : Deleting partitions done  2/18 : Test partition size done  3/18 : Creating partitions and /etc/fstab done  4/18 : Creating software RAID level 0 done  5/18 : Creating LVM volumes done  6/18 : Formatting partitions : formatting /dev/md/0 with ext3 done  : formatting /dev/vg0/root with ext4 done  : formatting /dev/vg0/swap with swap done  : formatting /dev/vg0/var with ext4 done  : formatting /dev/vg0/tmp with ext4 done  : formatting /dev/vg0/home with ext4 done  7/18 : Mounting partitions done  8/18 : Sync time via ntp done  9/18 : Downloading image (http) done  : Importing public key for image validation done  10/18 : Validating image before starting extraction warn  : No detached signature file found! 11/18 : Extracting image (http) done  12/18 : Setting up network config done  13/18 : Executing additional commands : Setting hostname done  : Generating new SSH keys done  : Generating mdadm config done  : Generating ramdisk done  : Generating ntp config done  14/18 : Setting up miscellaneous files done  15/18 : Configuring authentication : Fetching SSH keys done  : Disabling root password done  : Disabling SSH root login without password done  : Copying SSH keys done  16/18 : Installing bootloader grub done  17/18 : Running some centos specific functions failed  An error occured while installing the new system! See the debug file /root/debug.txt for details.   Reboot. # reboot now   Disable password authentication. Remember this system is accessible from the internet. Unless you want people to constantly try and login, disable password authentication. # vi /etc/ssh/sshd_config PasswordAuthentication no  Restart sshd. # systemctl restart sshd  Create Cinder Volumes LVM Pool  Create third partition on both disks using the remaing space available. Choose type FD (Linux Raid Autodetect). You can use fdisk or cfdisk.  # cfdisk /dev/sda # cfdisk /dev/sdb Detect new disks  # partprobe Create a raid 0 (stripe) using the newly created partitions  # mdadm --create --verbose /dev/md2 --level=0 --raid-devices=2 \\ /dev/sda3 /dev/sdb3 Create Physical Volume, Volume Group and Thin Pool  # pvcreate /dev/md2 # vgcreate cinder-volumes /dev/md2 # lvcreate -L 300G -T cinder-volumes/cinder-volumes-pool Verify LVM Volume Groups  Make sure a volume group cinder-volumes exists. This will be used for OpenStack storage as mentioned.  # vgs   VG             #PV #LV #SN Attr   VSize    VFree   cinder-volumes   1   1   0 wz--n- &lt;743.01g &lt;442.86g   vg0              1   4   0 wz--n-  149.87g   51.87g # lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert cinder-volumes-pool cinder-volumes twi-a-tz-- 300.00g 0.00 0.24 root vg0 -wi-ao---- 50.00g swap vg0 -wi-ao---- 8.00g tmp vg0 -wi-ao---- 30.00g var vg0 -wi-ao---- 10.00g Remove Linux Raid Device (Only if you want to re-create cinder-volumes)  In case something went wrong and you need to re-create the linux raid device. It can be tricky to remove since it writes raid information to the disk partition.  Remove LVM Volume  # vgremove cinder-volumes Stop raid device  # mdadm --stop /dev/md2 Zero raid disks # mdadm --zero-superblock /dev/sda3 # mdadm --zero-superblock /dev/sdb3 Install OpenStack  Ensure local name resolution is working.  # vi /etc/hosts 144.76.52.111 myrootserver.lab Set hostname.  # hostnamectl set-hostname myrootserver.lab Enable RPMs.  [RHEL]  # subscription-manager register # subscription-manager list --available # subscription-manager attach --pool= # subscription-manager repos --disable=* # subscription-manager repos --enable=rhel-8-for-x86_64-baseos-rpms \\ --enable=rhel-8-for-x86_64-appstream-rpms \\ --enable=rhel-8-for-x86_64-highavailability-rpms \\ --enable=fast-datapath-for-rhel-8-x86_64-rpms \\ --enable=advanced-virt-for-rhel-8-x86_64-rpms \\ --enable=openstack-16-for-rhel-8-x86_64-rpms \\ --enable=openstack-16-tools-for-rhel-8-x86_64-rpms \\ --enable=openstack-16-devtools-for-rhel-8-x86_64-rpms Install packstack packages.  [CentOS]  # dnf install -y centos-release-openstack-train [Both]  # dnf install -y openstack-packstack [RHEL]  Disable firewalld, OpenStack uses iptables.  # systemctl disable firewalld # systemctl stop firewalld Disable Network Manager  [Both]  In RHEL 8 NetworkManager is default and this causes issues with packstack. As such we will disable NetworkManager and enable the network-scripts stack.  # dnf install -y network-scripts # readlink $(readlink $(which ifup)) # touch /etc/sysconfig/disable-deprecation-warnings # systemctl stop NetworkManager # systemctl disable NetworkManager #systemctl enable network #systemctl start network Switch to Advanced Virt Modules  # dnf -y module disable virt:rhel # dnf -y module enable virt:8.0.0 Install yum-utils and update the system.  # dnf install -y dnf-utils # dnf update -y Configure Intel Virtualization for Directed I/O.  # vi /etc/default/grub --- GRUB_CMDLINE_LINUX=\"biosdevname=0 crashkernel=auto nomodeset rd.auto=1 consoleblank=0 intel_iommu=on\" --- # grub2-mkconfig -o /boot/grub2/grub.cfg Set SELinux to Permissive  Unfortunately SELinux is not working with this latest packstack.  # setenforce 0 # vi /etc/selinux/config --- SELINUX=permissive --- Reboot.  # reboot now Create packstack answers file for customizing the installer.  # packstack --os-neutron-ml2-tenant-network-types=vxlan \\ --os-neutron-ml2-mechanism-drivers=openvswitch \\ --os-neutron-ml2-type-drivers=vxlan,flat \\ --os-neutron-l2-agent=openvswitch \\ --keystone-admin-passwd=&lt;admin password&gt; \\ --nova-libvirt-virt-type=kvm \\ --cinder-volumes-create=n \\ --provision-demo=n \\ --os-heat-install=y \\ --os-swift-storage-size=75G \\ --gen-answer-file /root/answers.txt Install OpenStack using packstack.  # packstack --answer-file /root/answers.txt --timeout=1500  **** Installation completed successfully ******  Additional information: * Parameter CONFIG_NEUTRON_L2_AGENT: You have choosen OVN neutron backend. Note that this backend does not support LBaaS, VPNaaS or FWaaS services. Geneve will be used as encapsulation method for tenant networks * Time synchronization installation was skipped. Please note that unsynchronized time on server instances might be problem for some OpenStack components. * File /root/keystonerc_admin has been created on OpenStack client host 136.243.43.239. To use the command line tools you need to source the file. * To access the OpenStack Dashboard browse to http://136.243.43.239/dashboard . Please, find your login credentials stored in the keystonerc_admin in your home directory. * The installation log file is available at: /var/tmp/packstack/20191025-135509-j6dbbfha/openstack-setup.log * The generated manifests are available at: /var/tmp/packstack/20191025-135509-j6dbbfha/manifests Configure Physical Network  Source the keystone admin profile.  # . /root/keystonerc_admin Backup the ifcfg-etho script.  # cp /etc/sysconfig/network-scripts/ifcfg-eno1 /root/ Configure external bridge br-ex.  # vi /etc/sysconfig/network-scripts/ifcfg-eno1 DEVICE=eno1 ONBOOT=yes TYPE=OVSPort DEVICETYPE=ovs OVS_BRIDGE=br-ex # vi /etc/sysconfig/network-scripts/ifcfg-br-ex DEVICE=br-ex BOOTPROTO=none ONBOOT=yes TYPE=OVSBridge DEVICETYPE=ovs USERCTL=yes PEERDNS=yes IPV6INIT=no IPADDR= NETMASK=255.255.255.255 SCOPE=\"peer &lt;gateway address&gt;\" Switch static route to br-ex.  Note: this is specific to Hetzner environment as the physical host will get a /32.  # mv /etc/sysconfig/network-scripts/route-eno1 /etc/sysconfig/network-scripts/route-br-ex  Add the eno1 physical interface to the br-ex bridge in openVswitch.  Note: this is point of no return!!! Double check your config, ensure network interface is correct, if there is typo you will need to got to rescue mode.  # ovs-vsctl add-port br-ex eno1; systemctl restart network.service   Configure Additional Floating IP Subnet  Since hetzner root server only has one ip (/32), another subnet is needed to add additional floating ips. In this case hetzner will route traffic from additional subnet through to /32 ip of root server. Here we need to create a new OVS bridge (br-ex2) for additional subnet and patch it to existing bridge (br-ex).  Create Openvswitch Bridge.   # ovs-vsctl add-br br-ex2 Patch bridge br-ex2 to br-ex.  # ovs-vsctl add-port br-ex2 patch2-0 Note: ignore error about missing interface, that is expected since we creating patch and haven’t created interface yet.  # ovs-vsctl set interface patch2-0 type=patch # ovs-vsctl set interface patch2-0 options:peer=\"patch0-2\" # ovs-vsctl add-port br-ex patch0-2 Note: ignore error about missing interface, that is expected since we creating patch and haven’t created interface yet.  # ovs-vsctl set interface patch0-2 type=patch # ovs-vsctl set interface patch0-2 options:peer=\"patch2-0\" Update neutron bridge mappings.  We are adding a second subnet, as such a mapping is required to physical interface.  # vi /etc/neutron/plugins/ml2/openvswitch_agent.ini  bridge_mappings=extnet:br-ex,extnet2:br-ex2 Configure ifcfg script for br-ex2.   # vi /etc/sysconfig/network-scripts/ifcfg-br-ex2 DEVICE=br-ex2 BOOTPROTO=none ONBOOT=yes TYPE=OVSBridge DEVICETYPE=ovs USERCTL=yes PEERDNS=yes IPADDR= NETMASK=255.255.255.248 SCOPE=\"peer &lt;subnet gateway address&gt;\" IPV6INIT=no   Comment out default iptables REJECT rules.  By default iptables won’t allow traffic from br-ex2 to br-ex.  # vi /etc/sysconfig/iptables --- #-A INPUT -j REJECT --reject-with icmp-host-prohibited #-A FORWARD -j REJECT --reject-with icmp-host-prohibited --- Restart iptables and networking.  # systemctl restart iptables; systemctl restart network  Configure OpenStack Environment  Neutron Configuration  Set the default dns servers to be used when creating a subnet.  # vi /etc/neutron/dhcp_agent.ini --- debug=False dnsmasq_dns_servers=213.133.98.98,213.133.99.99 --- Restart Neutron DHCP Agent  # systemctl restart neutron-dhcp-agent Nova Configuration  Nova uses filtering rules to find appropriate host when scheduling instances. In order for nova to recognize lvm storage the images type needs to be lvm and the volume group needs to be correct lvm volume. In addition, unless you want to wait really long when deleting nova volumes, set volume_clear to none.  # vi /etc/nova/nova.conf  [libvirt] --- images_type = lvm  volume_clear = none images_volume_group = cinder-volumes --- Restart Nova services.  # systemctl restart openstack-nova-compute # systemctl restart openstack-nova-scheduler Cinder Configuration  By default the openstack install will configure an lvm volume group using loop. This is not ideal and why we created a volume group vg1. We will use the same setting volume_clear to none to ensure cinder volumes are deleted quickly.  Update cinder configuration.  # vi /etc/cinder/cinder.conf --- enabled_backends=lvm volume_clear = none  [lvm] volume_backend_name=lvm volume_driver=cinder.volume.drivers.lvm.LVMVolumeDriver iscsi_ip_address=144.76.52.111 iscsi_helper=lioadm volume_group=cinder-volumes volumes_dir=/var/lib/cinder/volumes --- Restart cinder services.  # systemctl restart openstack-cinder-volume # systemctl restart openstack-cinder-api     Ceilometer Configuration  Aodh is the database for alarms that are triggered based on things such as autoscaling policies. The database needs to be initialized after installing OpenStack.  # aodh-dbsync Neutron Configuration  Create private network.  # openstack network create admin # openstack subnet create --network admin --allocation-pool \\ start=10.10.1.100,end=10.10.1.200 --dns-nameserver 213.133.98.98 \\ --subnet-range 10.10.1.0/24 admin_subnet Create public network.   Note: these steps assume the physical network connected to br-ex2 is 144.76.132.224/29.  # openstack network create --provider-network-type flat \\ --provider-physical-network extnet2 --external public # openstack subnet create --network public --allocation-pool \\ start=144.76.132.226,end=144.76.132.230 --no-dhcp \\ --subnet-range 144.76.132.224/29 public_subnet Add a new router and configure router interfaces.  # openstack router create admin_router # openstack router set --external-gateway public admin_router # openstack router add subnet admin_router admin_subnet Check to ensure network connectivity is working.  This is done by checking the network namespace of the qrouter (openstack router).  # ip netns show qrouter-88dde0ef-22a2-44b1-baa9-304273653bb1 qdhcp-f1582f71-b531-43af-99c1-299b603232fc # ip netns exec qrouter-88dde0ef-22a2-44b1-baa9-304273653bb1 ping www.redhat.com Glance Configuration  Upload a glance image.  In this case we will use a Cirros image because it is small and thus good for testing OpenStack.  # wget http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img # openstack image create --disk-format qcow2 \\ --container-format bare --public \\ --file /root/cirros-0.3.4-x86_64-disk.img \"Cirros 0.3.4\" Create a new m1.nano flavor for running Cirros image.  # openstack flavor create --ram 64 --disk 0 --ephemeral 0 --vcpus 1 --public m1.nano Configure Security Groups  Create Security Group for all access.  # openstack security group create all \\ --description \"Allow all ports\" # openstack security group rule create --protocol TCP \\ --dst-port 1:65535 --remote-ip 0.0.0.0/0 all # openstack security group rule create --protocol ICMP \\ --remote-ip 0.0.0.0/0 all Create Security Group for base access.  # openstack security group create base \\ --description \"Allow base ports\" # openstack security group rule create --protocol TCP \\ --dst-port 22 --remote-ip 0.0.0.0/0 base # openstack security group rule create --protocol TCP \\ --dst-port 80 --remote-ip 0.0.0.0/0 base   # openstack security group rule create --protocol TCP \\ --dst-port 443 --remote-ip 0.0.0.0/0 base # openstack security group rule create --protocol ICMP \\ --remote-ip 0.0.0.0/0 base   Create Private Key.  # openstack keypair create admin Save Private Key to file.  # vi /root/admin.pem -----BEGIN RSA PRIVATE KEY----- MIIEowIBAAKCAQEAwTrb+xdbpgY8hVOmftBIShqYUgXXDC/1gggakq8bkEdNnSku IaNGeJykzksjdksjd9383iejkjsu92wiwsajFLuE2lkh5dvk9s6hpfE/3UvSGk6m HWIMCf3nJUv8gGCM/XElgwNXS02c8pHWUywBiaQZpOsvjCqFLGW0cNZLAQ+yzrh1 dIWddx/E1Ppto394ejfksjdksjdksdhgu4t39393eodNlVQxWzmK4vrLWNrfioOK uRxjxY6jnE3q/956ie69BXbbvrZYcs75YeSY7GjwyC5yjWw9qkiBcV1+P1Uqs1jG 1yV0Zvl5xlI1M4b97qw0bgpjTETL5+iuidFPVwIDAQABAoIBAF7rC95m1fVTQO15 buMCa1BDiilYhw+Mi3wJgQwnClIwRHb8IJYTf22F/QptrrBd0LZk/UHJhekINXot z0jJ+WvtxVAA0038jskdjskdjksjksjkiH9Mh39tAtt2XR2uz/M7XmLiBEKQaJVb gD2w8zxqqNIz3438783787387s8s787s8sIAkP3ZMAra1k7+rY1HfCYsRDWxhqTx R5FFwYueMIldlfPdGxwd8hLrqJnDY7SO85iFWv5Kf1ykyi3PRA6r2Vr/0PMkVsKV XfxhYPkAOb2hNKRDhkvZPmmxXu5wy8WkGeq+uTWRY3DoyciuC4xMS0NMd6Y20pfp x50AhJkCgYEA8M2OfUan1ghV3V8WsQ94vguHqe8jzLcV+1PV2iTwWFBZDZEQPokY JkMCAtHFvUlcJ49yAjrRH6O+EGT+niW8xIhZBiu6whOd4H0xDoQvaAAZyIFoSmbX 2WpS74Ms5YSzVip70hbcXb4goDhdW9YxvTVqJlFrsGNCEa3L4kr2qFMCgYEAzWy0 5cSHkCWaygeYhFc79xoTnPxKZH+QI32dAeud7oyZtZeZDRyjnm2fEtDCEn6RtFTH NlI3W6xFkXcp1u0wbmYJJVZdn1u9aRsLzVmfGwEWGHYEfZ+ZtQH+H9XHXsi1nPpr Uy7Msd,sl,.swdko393j495u4efdjkfjdkjfhflCgYEA7VO6Xo/XdKPMdJx2EdXM y4kzkPFHGElN2eE7gskjdksjdksjkasnw33a23433434wk0P8VCksQlBlojjRVyu GgjDrMhGjWamEA1y3vq6eka3Ip0f+0w26mnXCYYAJslNstu2I04yrBVptF846/1E ElXlo5RVjYeWIzRmIEZ/qU8CgYB91kOSJKuuX3rMm46QMyfmnLC7D8k6evH+66nM 238493ijsfkjalsdjcws9fheoihg80eWDSAFDOASDF=OIA=FSLoiidsiisiisNDo ACh40FeKsHDby3LK8OeM9NXmeCjYeoZYNGimHForiCCT+rIniiu2vy0Z/q+/t3cM BgmAmQKBgCwCTX5kbLEUcX5IE6Nzh+1n/lkvIqlblOG7v0Y9sxKVxx4R9uXi3dNK 6pbclskdksdjdk22k2jkj2kalksx2koUeLzwHuRUpMavRhoTLP0YsdbQrjgHIA+p kDNrgFz+JYKF2K08oe72x1083RtiEr8n71kjSA+5Ua1eNwGI6AVl -----END RSA PRIVATE KEY----- # chmod 400 /root/admin.pem Start an Instance  Get Private Network Id.  # openstack network list +--------------------------------------+---------+--------------------------------------+ | ID | Name | Subnets | +--------------------------------------+---------+--------------------------------------+ | 72494c88-6d93-4eb7-929f-383cbedfa3e7 | public | a57856be-a22b-4a48-b3c3-75be46a4c477 | | 781e062d-5ab6-4ae8-a54e-8e72291df37e | private | b00316a1-812f-423a-8aca-bd6547692ad3 | +--------------------------------------+---------+--------------------------------------+ Create a mycirros Instance.  # openstack server create --flavor m1.nano --image \"Cirros 0.3.4\" \\ --nic net-id=781e062d-5ab6-4ae8-a54e-8e72291df37e --key-name admin \\ --security-group all mycirros # # openstack server list +--------------------------------------+----------+--------+---------------------+--------------+---------+ | ID | Name | Status | Networks | Image | Flavor | +--------------------------------------+----------+--------+---------------------+--------------+---------+ | 6f2719bb-f393-49ab-a409-1c4a0f992b2d | mycirros | ACTIVE | private=10.10.1.105 | Cirros 0.3.4 | m1.nano | +--------------------------------------+----------+--------+---------------------+--------------+---------+ Create Floating IP.  # openstack floating ip create public +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | created_at | 2018-01-12T09:52:40Z | | description | | | fixed_ip_address | None | | floating_ip_address | 144.76.132.230 | | floating_network_id | 72494c88-6d93-4eb7-929f-383cbedfa3e7 | | id | 7a7ac84f-57ee-4982-b2f5-35ebb9656b34 | | name | 144.76.132.230 | | port_id | None | | project_id | 92d3bc57ca504eaab4c29d3509064004 | | revision_number | 0 | | router_id | None | | status | DOWN | | updated_at | 2018-01-12T09:52:40Z | +---------------------+--------------------------------------+  Get Port Id of Instance.  # openstack port list +--------------------------------------+------+-------------------+-------------------------------------------------------------------------------+--------+ | ID | Name | MAC Address | Fixed IP Addresses | Status | +--------------------------------------+------+-------------------+-------------------------------------------------------------------------------+--------+ | 60407eee-c9b2-4e9c-81a6-5c38bb536c9b | | fa:16:3e:37:ac:2f | ip_address='144.76.132.227', subnet_id='a57856be-a22b-4a48-b3c3-75be46a4c477' | ACTIVE | | 717660b6-b9b0-4f76-8c63-f9fae5c9bd9b | | fa:16:3e:0c:14:30 | ip_address='10.10.1.105', subnet_id='b00316a1-812f-423a-8aca-bd6547692ad3' | ACTIVE | | 878ee68a-1b88-48d1-9bf7-057f59b833bb | | fa:16:3e:2a:1f:50 | ip_address='144.76.132.230', subnet_id='a57856be-a22b-4a48-b3c3-75be46a4c477' | N/A | | 8a7b6b21-1eb2-4750-b854-707462d8b38f | | fa:16:3e:53:c9:ef | ip_address='10.10.1.1', subnet_id='b00316a1-812f-423a-8aca-bd6547692ad3' | ACTIVE | | b40af80e-2794-4ca5-8141-9d8ad4e9c9f2 | | fa:16:3e:51:d9:cb | ip_address='10.10.1.100', subnet_id='b00316a1-812f-423a-8aca-bd6547692ad3' | ACTIVE | +--------------------------------------+------+-------------------+-------------------------------------------------------------------------------+--------+ Assign Floating IP to Instance Port.  # openstack floating ip set --port 717660b6-b9b0-4f76-8c63-f9fae5c9bd9b 144.76.132.230  Verify Floating IP in OpenStack Router.  # ip netns show qdhcp-dcfbabbd-c5d2-444c-ab60-546216550118 router-0f00050f-6590-42df-9136-32d22fea4ece # ip netns exec qrouter-0f00050f-6590-42df-9136-32d22fea4ece ip a --- 9: qg-60407eee-c9: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN qlen 1000  link/ether fa:16:3e:37:ac:2f brd ff:ff:ff:ff:ff:ff  inet 144.76.132.227/29 brd 144.76.132.231 scope global qg-60407eee-c9  valid_lft forever preferred_lft forever  inet 144.76.132.230/32 brd 144.76.132.230 scope global qg-60407eee-c9  valid_lft forever preferred_lft forever  inet6 fe80::f816:3eff:fe37:ac2f/64 scope link  valid_lft forever preferred_lft forever --- Test Instance.  Connect to mycirros instance using the private ssh key stored in the admin.pem file.  # ssh -i admin.pem cirros@144.76.132.230 $ ping www.redhat.com PING www.redhat.com (172.217.21.14): 56 data bytes 64 bytes from 172.217.21.14: seq=0 ttl=54 time=37.692 ms 64 bytes from 172.217.21.14: seq=1 ttl=54 time=27.758 ms 64 bytes from 172.217.21.14: seq=2 ttl=54 time=25.640 ms Install OpenStack Client  To administer OpenStack remotely the CLI tools are important. Below are steps to install OpenStack CLI tools for Train.  [RHEL]  # subscription-manager repos --enable=openstack-16-tools-for-rhel-8-x86_64-rpms # yum install -y python-openstackclient # yum install -y python-heatclient # yum install -y python-octavia [CentOs]  # yum install -y https://rdoproject.org/repos/openstack-train/rdo-release-train-1.noarch.rpm # yum upgrade -y # yum install -y python-openstackclient # yum install -y python-heatclient # yum install -y python-octavia Summary  This article was intended as a hands on guide for standing up an OpenStack Train lab environment using RDO. In this guide we also tailored the environment to the Hetzner root server. Things can of course vary depending on your hardware platform and provider. Certainly the aim of this guide was to provide a more realistic deployment scenario for OpenStack. As mentioned RDO is a stable community platform built around Red Hat’s OpenStack Platform. It provides the ability to test the latest OpenStack features against either an enterprise platform (RHEL) or community platform (CentOS). Hopefully you found the information in this article useful. If you have anything to add or feedback, feel free to leave your comments.  Happy OpenStacking!  (c) 2020 Keith Tenzer    ","categories": ["OpenShift"],
        "tags": ["Cloud","Cloud Computing","Linux","OpenStack","Virtualization"],
        "url": "/openshift/openstack-16-train-lab-installation-and-configuration-guide-for-hetzner-root-servers/",
        "teaser": null
      },{
        "title": "The Great Human Experiment: Global Disconnection from Coronavirus",
        "excerpt":"  source: https://starwars.fandom.com/wiki/Shaak_Ti  Cornavirus has arrived at the global level, it is likely only a matter of time before it is declared a world pandemic. Most are wondering how long it will stay and when will things go back to normal? Personally I think we reached the point, there will be no going back and that is not necessarily a bad thing. In this article we will discuss why the cornavirus is just as much an opportunity for the human race as it is a threat. We will discuss how technology can help and how a global world can function, disconnected.    The Human Interaction  It is of course in our nature to have personal face-to-face contact with one another. We require a personal relationship in order to build trust. Without trust, a relationship cannot exist and nothing meaningful will develop. Coronavirus has managed to cripple the world economies in a matter of weeks because of our human nature. Our world, globalization as it exists today does not function without face-to-face human interactions. At this point there are only two solutions   Ignore coronavirus and just keep on going and hope for the best with good hygiene Change the way we work, live and travel  I think we can consider ourselves lucky as a whole actually, this virus could have had a much higher fatality rate and yet it showed us our greatest strength, the human connection, is also our greatest weakness.  The Cost of Human Interaction  As our world has become more global, human interaction means a lot more travel. While this has provided an economic boom around the globe, it is also I am afraid at a very, very high cost. The below diagram shows the explosion of world travel.    source: https://ourworldindata.org/tourism  Below is a graph showing CO2 increases and it's effect on global temperatures. It is obvious to see the likely correlation between global travel and climate change. Of course, there are many other impacts but global travel is a big one and it has been increasing dramatically.    source: https://www.globalchange.gov/browse/multimedia/global-temperature-and-carbon-dioxide  Now I am not advocating that we stay at home and stop all travel but rather that the balance is simply out-of-sync. Ironically climate change was supposed to bring us back to equilibrium but it may indirectly be the coronavirus that moves the needle first, at least slightly by reducing travel and forcing alternatives.  A New Normal and a lot of Disruption  I think it is clear that we will not be ignoring the coronavirus and that it likely won't just go away in the near-term. Based on that assumption, I think we will be changing the way we work and live, not just temporarily but maybe, permanently. There will be an adjustment time but likely once we adjust, we won't go back. For many it simply means instead of going to the office, you work at home or have a remote meeting. Those that are involved in producing goods, work in factories and such will have to adapt further. Regular screening, testing and more isolation will likely be needed in the place of work which require significant investment, depending on how long the coronavirus is a threat. All of this will create a lot of new opportunities, areas of investment and of course bring traditional ones (like the travel industry) to the brink. The longer the coronavirus lasts the more disruptive change will occur. It is impossible to say where this all leads other than at least a temporary new normal involving a lot less travel and consumption.  Technology, Life, Work and Culture  In order to truly experience something, you have to feel it, you have to smell it and you certainly have to taste it. Our senses and the ability to forge experiences from them is the human experience. However, do you really need to be there in person at ground zero? I don't think so at all. With technology, virtual reality and how things are evolving, maybe we will have less of a need to physically go places in the future to get those experiences.  At work we also need experiences and interactions. Most companies are under invested compared to technology capabilities. Organizations just never evolved from the days of where you had to go to the office or the client to get anything done. Now though companies will have to adapt, cultures will change. I am convinced with the right technology, the experiences will be just as good.  Events around the world are cancelled but do we really need to visit an exhibition hall, in some city, half way across the world, to have an experience? Companies are already adapting, Red Hat for example just announced, the largest opensource event in the world, Red Hat Summit, will be virtual (https://www.redhat.com/en/blog/moving-red-hat-summit-2020-virtual-experience).  I am convinced it will not only succeed but excel, other companies will follow. Who would have thought of a virtual conference at that scale just a few weeks ago?  I work at Red Hat, a company I am so proud to be a part of because of the culture. At Red Hat most people work from wherever, they are all over the world. We look for the best people and worry about logistics second. Red Hat has the strongest company culture I have ever experienced, yet there is limited direct connection points. How is it possible to have a culture you can breath, taste and feel without a central point of reference or focus? It's because culture is not about a place, it is about the people that take part in it. Culture can exist and thrive through any medium, physical or virtual.  The opensource community has long adapted to this new world because people form all over the world, different organizations, countries came together to work on real problems, effectively. They did it mostly, without any face-to-face meetings. A way of working evolved, a culture evolved and it is successful not only in a limited travel world but even a zero travel world. The rest of the world will figure this out and follow because now, unfortunately there is no choice.  There will be many business casualties and sadly also lost life. However, there is opportunity to learn and make the world a better, more robust, sustainable place. Let us not miss this opportunity!  (c) 2020 Keith Tenzer  &nbsp;  &nbsp;  ","categories": ["General"],
        "tags": ["coronavirus","culture","globalization","opensource","remote"],
        "url": "/general/the-great-human-experiment-global-disconnection-from-coronavirus/",
        "teaser": null
      },{
        "title": "OpenShift Application Certificate Management with Let's Encrypt",
        "excerpt":"  Overview  In this short article we will look at a solution for application certificates in OpenShift. Let's Encrypt is a non-profit certificate authority and provides easy on-demand TLS certificates. Each application you create that you want to expose to users will of course have it's own URL and require a TLS certificate. It can be quite tedious to manage these certificates and deploy them manually. Kubernetes platforms also require an innovative, built-in native approach to properly mitigate complexity.  Thankfully a fellow RHatter (Tomáš Nožička) has created a k8s admission controller that integrates with let's encrypt. A k8s admission controller is a pattern for extending kubernetes platform capabilities by reacting to API events in real-time.  In this case the admission controller watches the route APIs. If a new route is added, plus has the right annotation, the admission controller will automatically register the route with Let's Encrypt, wait for the certificate and finally configure the certificate automatically in the route.  Tomáš has provided the code and yaml for an easy deployment in the following Github repository: https://github.com/tnozicka/openshift-acme. While hee does provide documentation there are a few additional steps that need explanation when creating a route. I decided to as such put it all together in a simple concise post.    Configure Let's Encrypt OpenShift Admission Controller  Here we will enable the k8s admission controller on the entire cluster but it is possible to limit the scope to individual namespaces. See Github repository for more information.  Create new project  $ oc new-project acme Deploy Controller  $ oc apply -fhttps://raw.githubusercontent.com/tnozicka/openshift-acme/master/deploy/cluster-wide/{clusterrole,serviceaccount,issuer-letsencrypt-live,deployment}.yaml Create Service Account and Role Binding  $ oc create clusterrolebinding openshift-acme --clusterrole=openshift-acme \\ --serviceaccount=\"$( oc project -q ):openshift-acme\" --dry-run -o yaml | \\ oc apply -f - Add Certificate to Application Route  Now that the k8s admission controller is deployed we will create a route. In OpenShift a route is how we expose a service to the outside world. Since we will provide a custom TLS certificate, route termination should be on the edge. As such we can use http from the router edge to the service.  Create Route  $ vi letsencrypt_route.yaml kind: Route apiVersion: route.openshift.io/v1 metadata:   name: meet   labels:     app: jitsi   annotations:     openshift.io/host.generated: 'true'     kubernetes.io/tls-acme: \"true\" spec:   host: meet-jitsi.apps.ocp4.keithtenzer.com   to:     kind: Service     name: web     weight: 100   port:     targetPort: http   tls:     termination: edge   wildcardPolicy: None  Validate Route  As soon as the ACME admission controller sees a route with the annotation kubernetes.io/tls-acme: \"true\" it will register URL with Let's Encrypt. A temporary route will be created so once the certificate is accepted it can be pulled down and added to route.    Once process completes the temporary exposer route is removed and the certificate and private key are added to the original route.    It's that easy and you now have a valid certificate for your application in OpenShift.  Summary  In this article we showed how to setup application certificates in an OpenShift enviornment using a k8s admission controller. This solution also shows the value of a k8s admission controller being able to customize and adapt the environment to an organizations governance model.  (c) 2020 Keith Tenzer  ","categories": ["OpenShift"],
        "tags": ["LetsEncypt","Certificates"],
        "url": "/openshift/openshift-application-certificate-management-with-lets-encrypt/",
        "teaser": null
      },{
        "title": "OpenShift Operator Getting Started Part I",
        "excerpt":"  Overview  In this article we will introduce the concept of Operators, the Operator Framework and Operator Lifecycle Management. This article is part of a series that will walk you through understanding and building operators in Go or Ansible end-to-end.   OpenShift Operator Getting Started Part I OpenShift Operator SDK: Go Getting Started Guide Part II OpenShift Operator SDK: Ansible Getting Started Guide Part III OpenShift Operator Lifecycle Management Guide: Integrating Operators in OLM Part IV  First, what exactly is an Operator?  An Operator is a method of packaging, deploying and managing a Kubernetes application.  source: https://coreos.com/operators/  Or simply, it is the application that can deploy itself, manage itself and update itself. Welcome to the brave new world, where we don’t spend time doing repetitive manually tasks, but rather put our knowledge into software so it can do it for us, better.    The Kubernetes Operator pattern was introduced by CoreOS engineers in 2016 as again, a way to implement and inject application specific domain knowledge into the running of containerized applications. The idea was to couple Service Reliability Engineering (SRE) output with the application and provide a standard programmable path to do so. SRE teams as you likely know, operate applications by creating software. Most organizations unfortunately don’t have the luxury of having highly paid software engineers to run their applications, nor the time to build such software. Now, with Operators and the Operator Framework, there is a way for vendors or customers to provide that domain knowledge using a standardized reusable approach. The result is applications that run themselves or come with their own pre-programmed built-in SRE team. This is obviously a huge game-changer and differentiator for operating applications. In my opinion it is the only way to deal with the increased complexity we see today. This is the reason eventually, that every application will likely be deployed in containers on Kubernetes. It is simply no longer, with today’s complexity, possible for application domain knowledge to exist in a few people’s heads, that in turn need to operate application lifecycle manually. Think of an operator as the new Linux “Package Manager” and Kubernetes as the new “Linux”.  Introducing the Operator Framework  The Operator Framework is a toolkit to make it easy to build, scale and run operators. It includes the SDK, Operator Lifecycle Manager and Metering.  Operator SDK – Provides tooling to build and package operators. It abstracts Kubernetes API.    Operator Lifecycle Manager – Management control plane for operators. It governs who can access/deploy a given operator, namespaces where operators can run and lifecycle management, such as updates to an operator.    Metering – Provides ability to record historical usage which in turn can be used for operator reporting.  Understanding How It All Works  Using the Operator SDK we create operators. An operator can be written in Go or Ansible. An operator provides one or more custom resource definitions (CRDs) to allow users to interact with the application using the standard kubernetes API. A CRD is simply an extention to the kubernetes API. It also provides a custom resource (CR) for how a user interacts with the CRD. In addition the operator provides methods for how to install, delete and update itself. The operator has the ability to watch CRDs. If a user creates, deletes or updates a CR, the operator will see that and call the necessary functions. Those functions can be implemented in Go or Ansible. Yep all you need to build an operator end-to-end is Ansible knowledge!  Once you have an operator that can be deployed, it is time to look into Operator Lifecycle Management. OpenShift comes with OLM already built-in but it can also be added to any standard kubernetes cluster. OLM has the concept of a catalog. This is essentially an operator that provides one or more application bundles. OpenShift has several built-in catalogs.  $ oc get catalogsource -n openshift-marketplace NAME                      DISPLAY                   TYPE   PUBLISHER          AGE certified-operators       Certified Operators       grpc   Red Hat            18d community-operators       Community Operators       grpc   Red Hat            18d  redhat-operators          Red Hat Operators         grpc   Red Hat            18d Red Hat, ISVs, Partners, anyone can bundle their application operators into a catalog. Operator Hub simply lists the applications exposed by a catalog via OLM. Each application bundle is composed of the operator, it's own k8s objects and a manifest. The manifest is exposed via the cluster server version (CSV) CRD and consumed in OLM. This allows for flexibility to version operators and their manifests separately.  Summary  In this article we discussed the Operator Framework for Kubernetes and OpenShift. A powerful framework for building, packaging and deploying kubernetes-native applications. The Operator Framework consists of the Operator SDK, Operator Lifecycle Management and Metering. Through Operators and the Operator Lifecycle Management the full power of kubernetes can be utilized, allowing application operators to have a built-in SRE approach which in turn increases application availability, programability and efficiency.  Happy Operatoring!  (c) 2020 Keith Tenzer  &nbsp;  &nbsp;  &nbsp;  &nbsp;  ","categories": ["OpenShift"],
        "tags": ["cloud-native","Containers","Kubernetes","olm","OpenShift","Operator","Operator Framework","Operator Lifecycle Management","operator-sdk"],
        "url": "/openshift/openshift-operator-getting-started-part-i/",
        "teaser": null
      },{
        "title": "OpenShift Operator Lifecycle Management Guide: Integrating Operators in OLM Part IV",
        "excerpt":"  Overview  In this article we will provide a hands-on guide to integrating your already built Operator with the Operator Lifecycle Manager (OLM). Using the Operator SDK and OPM tool we will create the application manifests and images so your application Operator can be managed through OLM.  This article is part of a series that will walk you through understanding and building operators in Go or Ansible end-to-end.   OpenShift Operator Getting Started Part I OpenShift Operator SDK: Go Getting Started Guide Part II OpenShift Operator SDK: Ansible Getting Started Guide Part III OpenShift Operator Lifecycle Management Guide: Integrating Operators in OLM Part IV    In order to add your Operator to OLM you will need two tools: the operator-sdk and opm. Download the binaries for operator-sdk and opm. Install them under /usr/local/bin.  Create Cluster Service Version  The Cluster Service Version (CSV) is the manifest that allows your application operator to be bundled and exposed to the OLM API. The CSV is itself a CRD in kubernetes. Before creating a the CSV manifest you need the deployment scafolding. These are the objects and CRDs that your Operator require. These are created automatically when using the operator-sdk to create a application under the deploy directory.  $ operator-sdk new cars-operator --repo github.com/cars-operator When generating an CSV you will provide the path to the deploy directory where those objects can be found.  $ operator-sdk generate csv --csv-version 1.0.0 --deploy-dir deploy The generate csv command will create an olm_catalog/cars-operator/manifests directory. There you will find the 'clusterserviceversion' yaml. You will however want to edit and add a lot. As such I have provided an example csv for an Operator I built.  Note: If you want to add an icon, so it shows up nicely in Operator Hub, the image needs to be a base64 bit stream inside the csv.  $ cat myimage.png | base64 Create Application Operator Catalog Bundle  Now that we have generated and updated our CSV we can create an application bundle.  $ sudo operator-sdk bundle create quay.io/ktenzer/podium-operator-catalog:latest \\ --channels beta --package podium-operator-catalog --directory \\ deploy/olm-catalog/podium-operator/manifests This will package the application manifest into a container image. Push the image to a public or private repository.  $ sudo docker push quay.io/ktenzer/podium-operator-catalog:latest Create Application Operator Catalog Index  Once we have bundled an application operator we can add it to a catalog index. The catalog index provides one or more application bundles to OLM. This is also what the Operator Hub directly interfaces with.  $ sudo opm index add -c docker --bundles \\ quay.io/ktenzer/podium-operator-catalog:latest \\ --tag quay.io/ktenzer/podium-operator-index:latest Again an image is created that needs to be pushed to our private or public repository.  $ sudo docker push quay.io/ktenzer/podium-operator-index:latest Deploy Catalog Source  A catalog source is a CRD that defines an OLM catalog and points to a catalog index image, for example the one we just created.  Create a catalog source yaml file that points to the catalog index image.  $ vi catalog_source.yaml apiVersion: operators.coreos.com/v1alpha1 kind: CatalogSource metadata:   name: podium-operator-catalog spec:   sourceType: grpc   image: quay.io/ktenzer/podium-operator-index:latest   displayName: Podium Operator Catalog   publisher: Podium Community Deploy the catalog source under the openshift-marketplace namespace or where you have deployed OLM.  $ oc create -f catalog_source.yaml -n openshift-marketplace Below we can see a list of catalogs. Again each catalog can contain one or more applications. All are default in OpenShift, except the one in bold which I added.  $ oc get catalogsource -n openshift-marketplace NAME                     DISPLAY                 TYPE PUBLISHER        AGE certified-operators      Certified Operators     grpc Red Hat          18d community-operators      Community Operators     grpc Red Hat          18d podium-operator-catalog  Podium Operator Catalog grpc Podium Community 31m redhat-operators         Red Hat Operators       grpc Red Hat          18d Each catalog has it's own operator / pod.  $ oc get pods -n openshift-marketplace NAME                                    READY   STATUS    RESTARTS   AGE certified-operators-76d9d8b886-bnsz7    1/1     Running   0          14h community-operators-74d675f545-g7d2t    1/1     Running   0          18h podium-operator-catalog-67gsk           1/1     Running   0          31m marketplace-operator-75f49679d7-n7v2r   1/1     Running   0          17d redhat-operators-87d549bf4-mxf7w        1/1     Running   0          13h Once a catalog operator exists, the applications it offers will show up in Operator Hub and can be installed.    Operators can be cluster-wide or namespace scoped. The Operator Lifecycle Management also provides a subscription, allowing updates to be pulled from various channels. This is similar to the relationship between an RPM and Repository.    After you deploy application via Operator Hub it will launch the operator. Cluster-wide operators will run in the openshift-operators namespace while namespaced operators will only run in a user defined namespace.  $ oc get pods -n openshift-operators NAME                               READY   STATUS    RESTARTS   AGE podium-operator-6855dc9478-q65bt   1/1     Running   0          38m Summary  In this article we stepped through the process of integrating an already existing operator with the Operator Lifecycle Manager. We saw how to generate an application manifest, build an application catalog bundle, add the bundle to an application index and finally deploy the application catalog, seamlessly integrating with Operator Hub. Using OLM provides not only a way to install applications but manage their lifecycle, releases and updates.  Happy Operatoring!  (c) 2020 Keith Tenzer  ","categories": ["OpenShift"],
        "tags": ["Containers","Kubernetes","olm","OpenShift","Operator","Operator Lifecycle Management","operator-sdk"],
        "url": "/openshift/openshift-operator-lifecycle-management-guide-integrating-operators-in-olm-part-iv/",
        "teaser": null
      },{
        "title": "OpenShift Operator SDK: Ansible Getting Started Guide Part III",
        "excerpt":"  Overview  In this article we will provide a hands-on guide to building your very first operator in Ansible. Using the Operator SDK we will learn how to create boilerplate code, build and deploy an operator.  This article is part of a series that will walk you through understanding and building operators in Go or Ansible end-to-end.   OpenShift Operator Getting Started Part I OpenShift Operator SDK: Go Getting Started Guide Part II OpenShift Operator SDK: Ansible Getting Started Guide Part III OpenShift Operator Lifecycle Management Guide: Integrating Operators in OLM Part IV    Setup Development Environment  There are some prerequisites needed to develop and build an operator using Ansible. Also this guide and the operator-sdk assume you know Ansible roles. If you aren not yet up to speed please read about Ansible roles before proceeding.  Install Docker 17.03+  Add the docker ce repositories. Note: you can also use podman and buildah instead of Docker for those that want a complete and clean divorce.  $ sudo dnf -y install dnf-plugins-core  $ sudo dnf config-manager \\     --add-repo \\     https://download.docker.com/linux/fedora/docker-ce.repo Install docker-ce  $ sudo dnf -y install docker-ce docker-ce-cli $ sudo systemctl start docker $ sudo systemctl enable docker Install Ansible and Module Dependencies  Install ansible  $ dnf install ansible The Ansible runner and http runner is used to run a local version of the operator. This is very useful for development and testing.  $ pip3 install --user ansible-runner $ pip3 install --user ansible-runner-http Install required python modules  $ pip3 install --user requests $ pip3 install --user openshift Install Operator Framework SDK  You can simply downloada pre-built release and install it under /usr/local/bin.  Building Your First Operator  The SDK is able to generate not only boilerplate code but also the CRDs, controller and API endpoints. In this example we will create a cars operator. It will simply provide an endpoint allowing us to do CRUD on car objects. Each car object will spawn a pod with a base image.  Using operatore-sdk cli create boilerplate code.  $ operator-sdk new cars-operator --api-version=cars.example.com/v1alpha1 --kind=Car --type=ansible The operator sdk creates boilerplate roles.  $ ls cars-operator build  deploy  molecule  requirements.yml  roles  watches.yaml The build directory containers Dockerfile, deploy is where the yaml files are for creating the k8s operator objects (including CRD/CR), the roles directory contains a role called car and finally watches.yaml is the mapping from a k8s object / CRD to a roll.  By default when a car instance is created the operator will execute the role car. You can of course change the behavior, even configure finalizers for dealing with deletion of components not under the control of k8s (see the sdk guide)  Run Operator locally  The Ansible operator can be run locally. Simply deploy the CRD, service account and role.  Create new project for our operator  $ oc new-project cars-operator Setup service accounts and role bindings  $ oc create -f deploy/service_account.yaml $ oc create -f deploy/role.yaml $ oc create -f deploy/role_binding.yaml Create the CRD for our operator  $ oc create -f deploy/crds/cars.example.com_cars_crd.yaml Run Operator locally  $ operator-sdk run --local Create a car  $ vi car.yaml apiVersion: cars.example.com/v1alpha1 kind: Car metadata:   name: example-car spec:   size: 3   bmw:     model: m3   audi:     model: rs4 $ oc create -f car.yaml Anything under 'spec' will be passed to Ansible as a variable. If you want to reference the size in the ansible role you simply use ''. You can also access nested variables such as the car model by using ''.  Build Ansible Operator  Create User on Quay.io  We will be using quay to store operator images. Authenticate using Github or Gmail to https://quay.io. Once authenticated go to account settings and set a password.  Test Quay.io Credentials  $ sudo docker login quay.io Build Operator  Using operator-sdk cli build the operator which will push the image to your local Docker registry. Make sure you are in the cars-operator directory.  $ sudo operator-sdk build quay.io/ktenzer/cars-operator Note: Substitute ktenzer for your username.  Push Operator to your Quay.io Account  $ sudo docker push quay.io/ktenzer/cars-operator:latest Make Quay Repository Public  By default your Quay repository will be private. If we want to access it from OpenShift we need to make it public. Login to quay.io with your username/password  Select the cars-operator repository    Under Repository Settings enable visibility to be public    Update Image in operator.yaml  We need to point the image to the location in our Quay repository.  vi deploy/operator.yaml --- # Replace this with the built image name image: quay.io/ktenzer/cars-operator command: --- Deploy Cars Operator on OpenShift  Now that the operator is built and pushed to our Quay repository we can deploy it on an OpenShift cluster.  Authenticate to OpenShift Cluster  $oc login https://api.ocp4.keithtenzer.com:6443 Create new project for our operator  $ oc new-project cars-operator Setup service accounts and role bindings  $ oc create -f deploy/service_account.yaml $ oc create -f deploy/role.yaml $ oc create -f deploy/role_binding.yaml Create the CRD for our operator  $ oc create -f deploy/crds/cars.example.com_cars_crd.yaml Deploy our operator  $ oc create -f deploy/operator.yaml Create the CR for our operator  $ oc create -f deploy/crds/cars.example.com_v1alpha1_car_cr.yaml Using Cars Operator  The cars operator will automatically deploy an example-car. Whe can query our car object just like any other Kubernetes object. This is the beauty of CR/CRDs and operators. We can easily extend the Kubernetes API without needing to understand it’s complexity.  $ oc get car NAME AGE example-car 31m Next we can get information about our example-car.  $ oc get car example-car -o yaml apiVersion: cars.example.com/v1alpha1 kind: Car metadata:   creationTimestamp: \"2020-01-25T12:15:45Z\"   generation: 1   name: example-car   namespace: cars-operator   resourceVersion: \"2635723\"   selfLink: /apis/cars.example.com/v1alpha1/namespaces/cars-operator/cars/example-car   uid: 6a424ef9-3f6c-11ea-a391-fa163e9f184b spec:   size: 3   Looking at the running pods in our cars-operator project we see the operator and our example-car.  $ oc get pods -n cars-operator NAME                            READY   STATUS    RESTARTS   AGE cars-operator-b98bff54d-t2465   1/1     Running   0          5m example-car-pod                 1/1     Running   0          5m Create a new Car  Lets now create a BMW car.  $ vi bmw.yaml kind: Car metadata:   name: bmw spec:   size: 3   bmw:     model: m3 $ oc create -f bmw.yaml Here we can see we now have a BMW car.  $ oc get car NAME          AGE bmw           11m example-car   31m Of course we can get information about our BMW car.  $ oc get car bmw -o yaml apiVersion: cars.example.com/v1alpha1 kind: Car metadata:   creationTimestamp: \"2020-01-25T12:35:25Z\"   generation: 1   name: bmw   namespace: cars-operator   resourceVersion: \"2644044\"   selfLink: /apis/cars.example.com/v1alpha1/namespaces/cars-operator/cars/bmw   uid: 294bc47f-3f6f-11ea-b32c-fa163e3e8e24 spec:   size: 1 Finally as with the example-car, the operator will start a new pod when the BMW car is created.  $ oc get pods -n cars-operator NAME                            READY   STATUS    RESTARTS   AGE bmw-pod                         1/1     Running   0          10m cars-operator-b98bff54d-t2465   1/1     Running   0          14m example-car-pod                 1/1     Running   0          14m Cleanup  Follow these steps to remove the operator cleanly.  $ oc delete -f deploy/crds/cars.example.com_v1alpha1_car_cr.yaml $ oc delete -f deploy/operator.yaml $ oc delete -f deploy/role.yaml $ oc delete -f deploy/role_binding.yaml $ oc delete -f deploy/service_account.yaml $ oc delete -f deploy/crds/cars.example.com_cars_crd.yaml $ oc delete project cars-operator Summary  In this article a step-by-step guide was provided to setup a development environment, generate boilerplate code and deploy our custom cars operator using Ansible on OpenShift with the Operator Framework.  Happy Operatoring!  (c) 2020 Keith Tenzer  ","categories": ["OpenShift"],
        "tags": ["Ansible","Containers","Kubernetes","OpenShift","Operator","Operator Framework","operator-sdk"],
        "url": "/openshift/openshift-operator-sdk-ansible-getting-started-guide-part-iii/",
        "teaser": null
      },{
        "title": "Red Hat Subscription Reporting Guide",
        "excerpt":"  source: https://www.cio.com/article/3199910/zuora-blazes-a-new-trail-to-the-subscription-economy-and-a-post-erp-world.html  Overview  This article will look at the various options available, to do subscription reporting for Red Hat products. Many large organizations sometimes struggle to keep track of what subscriptions are being used, often maintaining their own spreadsheets. This can be very error prone and time consuming. Systems can even be subscribed to the wrong subscriptions, for example a virtual machine running RHEL using a physical subscription. Many Red Hat customers have several different products, not just RHEL and being able to actively inventory the entire subscription landscape is critical.    Options for Subscription Reporting  There are various options for subscription reporting depending on if you have Satellite (Red Hat Systems Management) or not.  Subscription Entitlement Report   Starting with Satellite 6.7 there is now a canned report that is very customizable. This is the recommended way of reporting subscriptions in Satellite 6.7+. Provides granular host level subscription reporting.  Example of the out-of-the-box entitlement report is here.  Red Hat Discovery Tool  The discovery tool can do reporting on subscriptions in Satellite and non-Satellite environments. Provides granular host level subscription reporting. This is the recommendation if you don't have Satellite 6.7+ or have systems not subscribing through Satellite. The discovery tool can be installed in a connected or disconnected environment.  The discovery tool also has an upstream project and is available via the community: https://github.com/quipucords/quipucords  Red Hat Subscription Watch  This is a cloud service offering that can aggregate subscription reporting across various infrastructure or even organizations. Data is sent either from Red Hat Insights or Satellite to a customers portal on cloud.redhat.com. This is a recommended addition to provide additional visibility but does not provide granular information at the host level.  Subscription Inventory Script  Provides granular host level subscription reporting for Satellite environments. This is a script not a product so use at your own risk. However this is recommended if you have Satellite 6.6 or below and cannot for whatever reason leverage the Red Hat Discovery tool.  https://github.com/RedHatSatellite/sat6Inventory/tree/product-cert  Using the Satellite Subscription Inventory Script  Since the other options are products and well documented, I will focus on showing how to do subscription reporting using the inventory script on RHEL 7. First you will need to deploy a RHEL 7 system.  Enable Correct RHEL Repos  First ensure all repositories are disabled and then just enable the rhel-7-server-rpms.  $ sudo subscription-manager repos --disable=* $ sudo subscription-manager repos --enable=rhel-7-server-rpms Install python 2.6+  $ sudo yum install -y python Install Git (Only for connected setup)  $ sudo yum install -y git Clone git repository (Only for connected setup)  $ git clone https://github.com/RedHatSatellite/sat6Inventory.git If the system isn't connected to the internet then you need to download from the git repository. You can download the script from https://github.com/RedHatSatellite/sat6Inventory/tree/product-cert by selecting download. You can then copy the tarball to your system and extract it.    Change directory  cd sat6Inventory Run the inventory script  You need to pass the admin user, password and the Satellite organization.  ./sat6Inventory.py -s sat-bc87.rhpds.opentlc.com -l admin -p  \\ -o 'EXAMPLE.COM' The script will output a CSV.  $ ls EXAMPLE.COM_inventory_report.csv LICENSE README.md Sample Report sat6Inventory.py You can now import the script output CSV into a spreadsheet program and manipulate as desired.  A sample report is also provided here.  Reconciling mismatched entitlements  If you have a Satellite environment with RHEL physical and VDC (virtual subscriptions) it is possible that a virtual RHEL host is possibly using a physical instead of virtual subscription. To identify such hosts you can run the below command from your Satellite server.  $ sudo foreman-rake katello:virt_who_report Summary  In this article we covered the various options for doing subscription reporting of Red Hat products. We also showed a short hands-on guide for using the inventory script. Hopefully this provided guidance to choose the most appropriate approach to subscription reporting.  (c) 2020 Keith Tenzer  ","categories": ["General"],
        "tags": ["Red Hat","Reporting","RHEL","Satellite","Subscription"],
        "url": "/general/red-hat-subscription-reporting-guide/",
        "teaser": null
      },{
        "title": "Windows Automation with Ansible: Getting Started Guide",
        "excerpt":"  Overview  In this article we will focus on how to get started with automation of windows using Ansible. Specifically we will look at installing 3rd party software and OS updates. Automation is the basis for cloud-computing or cloud-native patterns and breeds a culture of innovation. Let's face it, we cannot innovate, if we are stuck doing mundane tasks and manual labor. Ansible has revolutionized automation because until Ansible, automating was rather complicated and required lots of domain knowledge. Ansible provides an automation language that the entire organization can use because it is so easy and so flexible. The best thing about Ansible though it it brings teams together and fosters a devops culture. It brings the Linux and Windows world together!    How Ansible Works  Ansible provides a runtime for executing playbooks. A playbook is simply a group of plays which are essentially steps executed in order. A play is one or more tasks. A task is simply the execution of an Ansible module. An Ansible module is python code that does something, like install a software package, update the system, change a configuration file, check if something is set correctly, etc. There are 1000s of Ansible modules and a huge community around it.  In order to run a playbook against hosts you need an inventory which is simply a grouping of hosts and host vars (parameters). That is about it for the basics. Below is a diagram showing the Ansible automation engine architecture.    This guide assumes you know some Ansible or are familar with the basics. If not I created an Ansible getting started guide here. I highly recommend giving it a look before proceeding.  Prerequisites  In order for a windows host to be managed by Ansible there are a few prerequisites   Powershell version 3.0 or higher (Should be fine with Windows 2012 or higher) .NET Framework 4.0 or higher (should be fine with Windows 2012 or higher) Windows Remote Management Listener or SSH (cygwin) Windows 7, 8.1, and 10, and server OSs including Windows Server 2008, 2008 R2, 2012, 2012 R2, 2016, and 2019 Chocolatey for installing 3rd party software WSUS for updating OS packages and patching  Install Chocalatey and WSUS  There are various tools that can be used. I recommend using Chocolatey for installing packages and WSUS for OS updates/patching.  Install Chocalately  Using powershell we can install chocalately.  PS C:\\windows\\system32&gt; Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1')) Install WSUS  Open server manager. In the top right under manage you can add or change roles.    Under server roles select Windows Service Update Services. This will install WSUS. Once installed there will be a WSUS option in server manager. You can select it and configure what packages should be updated, etc.    Open WSUS and check that the computer is showing up under 'All Computers'. If not and you are running outside of domain you may need the following fix.  c:\\&gt; net stop wuauserv c:\\&gt; regsvr32 /s wuapi.dll c:\\&gt; regsvr32 /s wups.dll c:\\&gt; regsvr32 /s wuaueng.dll c:\\&gt; regsvr32 /s wucltui.dll c:\\&gt; regsvr32 /s msxml3.dll c:\\&gt; cd %windir%\\SoftwareDistribution c:\\&gt; rd /s/q DataStore c:\\&gt; mkdir DataStore c:\\&gt; rd /s/q Download c:\\&gt; mkdir Download c:\\&gt; net start wuauserv c:\\&gt; reg delete HKLM\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\WindowsUpdate /v AccountDomainSid /f c:\\&gt; reg delete HKLM\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\WindowsUpdate /v PingID /f c:\\&gt; reg delete HKLM\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\WindowsUpdate /v SusClientId /f c:\\&gt; wuauclt /resetauthorization c:\\&gt; wuauclt /detectnow c:\\&gt; wuauclt /reportnow Configuring Windows Remote Management for Ansible  Ansible requires no agents. It uses what the OS provides for communication. In Windows you can use SSH or Windows Remote Management (WinRM). Since SSH is more or less bolted on, WinRM is usually the preferred choice.  Test to ensure WinRM is working  First install WinRM if it isn't installed. Execute the hostname command through WinRM.  PS C:\\windows\\system32&gt; winrs -r:http://:5985/wsman -u: -p: hostname win2012 Optionally you can also test WinRM by making a remote desktop connection from another windows host.  Note: you may run into an issue where you get an authentication error and a CredSSL issue if you have an older windows version. This can also happen if you don't have a valid SSL certificate. If you run into this issue, update your registry.  c:\\&gt;reg add \"HKLM\\Software\\Microsoft\\Windows\\CurrentVersion\\Policies\\System\\CredSSP\\Parameters\" /f /v AllowEncryptionOracle /t REG_DWORD /d 2 Enable basic auth  There are several authentication methods. In this example we will use basic authentication which is username/password. This is of course the least secure method.  PS C:\\&gt; Set-Item -Path WSMan:\\localhost\\Service\\Auth\\Basic -Value $true Update WinRM   A script is provided by Ansible community to check WinRM and make necessary changes to allow Ansible to connect. In this case we are using basic authentication but likely you will want to use something more secure. The script can be found here and other authentication options are documented in the script header.  Open a powershell command prompt.  Store URL path to script.  PS C:\\&gt; $url = \"https://raw.githubusercontent.com/ansible/ansible/devel/examples/scripts/ConfigureRemotingForAnsible.ps1\" Store location for the script  PS C:\\&gt; $file = \"$env:temp\\ConfigureRemotingForAnsible.ps1\" Download script and output to file locally.  PS C:\\&gt; (New-Object -TypeName System.Net.WebClient).DownloadFile($url, $file) Execute the script.  PS C:\\&gt; powershell.exe -ExecutionPolicy ByPass -File $file Ok. Check WinRM connection  PS C:\\windows\\system32&gt; winrm enumerate winrm/config/Listener Listener Address = * Transport = HTTP Port = 5985 Hostname Enabled = true URLPrefix = wsman CertificateThumbprint ListeningOn = 10.10.1.139, 127.0.0.1, ::1, fe80::5efe:10.10.1.139%22, fe80::8155:327a:aeaf:365a%12  Listener Address = * Transport = HTTPS Port = 5986 Hostname Enabled = true URLPrefix = wsman CertificateThumbprint = 7a38de2c212764a54de106dc756f7cbc275156a3 ListeningOn = 10.10.1.139, 127.0.0.1, ::1, fe80::5efe:10.10.1.139%22, fe80::8155:327a:aeaf:365a%12 Ensure the HTTP/HTTPS ports are open.  More details about WinRM setup and how to setup a listener manually are documented here. In this case I used the default listener configured by WinRM.  Create Inventory File  We will create an inventory file with just a single host in a group called windows. From here on out we will be working on a Linux server where we have Ansible installed.  Create Ansible inventory file  $ vi inventory [windows] 138.204.12.111 Test ansible connection  Using the inventory file we can test if Ansible can communicate with our windows server.  $ ansible -i ../inventory windows -m win_ping -e ansible_connection=winrm \\ -e ansible_user=Admin -e ansible_password=&lt;password&gt; \\ -e ansible_winrm_transport=basic \\ -e ansible_winrm_server_cert_validation=ignore  138.201.147.202 | SUCCESS =&gt; { \"changed\": false, \"ping\": \"pong\" } Windows Patch Management  Now that Ansible is working with WinRM we can automate. In this case we will automate software package installation and updates.  Playbook and roles are available here.  Create Playbook  We haven't talked about roles. In Ansible roles are how we make playbooks reusable. It is always good practice to create roles. A role essentially allows you to organize Ansible plays and their dependencies together allowing them to be consumed easily. In order to use roles you need to create a certain directory structure and hierarchy. Create a playbook that imports our roles.  $ vi windows_baseline.yaml --- - name: Windows Baseline   hosts: \"\"   connection: winrm   gather_facts: true   vars:     ansible_user: \"\"     ansible_password: \"\"     ansible_connection: winrm      ansible_winrm_transport: basic      ansible_winrm_server_cert_validation: ignore    tasks:     - name: Install Baseline Packages       include_role:         name: install      - name: Perform Updates       include_role:         name: updates  Here we are setting hosts, ansible user and password as variables. These inputs must be provided when executing the playbook. Hosts should be the name of our host group from our inventory file. This playbook has two tasks, a role to install packages and a role to do an OS update.  Create Ansible install role  At a minimum your role will need a tasks directory  $ mkdir -p roles/install/tasks Create a task to install git using the chocalatey module.  $ vi roles/install/tasks/main.yaml --- - name: Install Git   win_chocolatey:     name: git     state: present  Create Ansible patch update role  At a minimum your role will need a tasks directory  $ mkdir -p roles/updates/tasks Create a task to perform an OS update.  vi roles/updates/tasks/main.yaml --- - name: Update windows packages   win_updates:     category_names:       - CriticalUpdates       - SecurityUpdates     reboot: yes     reboot_timeout: 500  Run playbook using inventory  The '-vvvvv' allows the playbook to run in debug mode for maximum verbosity.  $ ansible-playbook -i ./inventory -e target=windows -e user=Admin -e password= windows_baseline.yaml PLAY [Windows Baseline] ****************************************************************************  TASK [Gathering Facts] ***************************************************************************** ok: [138.201.147.202]  TASK [Install Baseline Packages] *******************************************************************  TASK [install : Install Git] *********************************************************************** ok: [138.201.147.202]  TASK [Perform Updates] *****************************************************************************  TASK [updates : Update windows packages] *********************************************************** changed: [138.201.147.202]  PLAY RECAP ***************************************************************************************** 138.201.147.202            : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=  Here are some additional examples of windows playbooks that may be of interest on your journey.  Summary  In this article we discussed the value of automation and why it is just a game changer. We provided a step-by-step on preparing a windows host for Ansible. Finally using the Ansible automation language, showed how to use native windows tooling to install and update OS patches. Hopefully this will provide a good starting point for a journey into windows automation with Ansible.  (c) 2020 Keith Tenzer  ","categories": ["Ansible"],
        "tags": ["Automation","Windows"],
        "url": "/ansible/windows-automation-with-ansible-getting-started-guide/",
        "teaser": null
      },{
        "title": "OpenShift 4 AWS IPI Installation Getting Started Guide",
        "excerpt":"      Happy new year as this will be the first post of 2021! 2020 was obviously a challenging year, my hope is I will have more time to devote to blogging in 2021. Please reach out and let me know what topics would be most helpful.    Overview      In this Article we will walk through an OpenShift deployment using the IPI (Installer Provisioned Infrastructure) method on AWS. OpenShift offers two possible deployment methods: IPI (as mentioned) and UPI (User Provisioned Infrastructure). The difference is the degree of automation and customization. IPI will not only deploy OpenShift but also all infrastructure components and configurations. IPI is supported in various environments including AWS, Azure, GCE, VMware Vsphere and even Baremetal. IPI is tightly coupled with the infrastructure layer whereas UPI is not and will allow the most customization and work anywhere.      Ultimately IPI vs UPI is usually dictated by the requirements. My view is unless you have special requirements (like a stretch cluster or specific integrations with infrastructure that require UPI) always default to IPI. It is far better to have the vendor, in this case Red Hat own more of the share of responsibility and ensure proper, tested infrastructure configurations are being deployed as well as maintained throughout the cluster lifecycle.            Create Hosted Zone      Before a cluster can be deployed we need to access AWS console and add a hosted zone to the route53 service. In the diagram below, we are adding the domain rh-southwest.com.           Download and Install CLI Tools      Red Hat provides a CLI tools for deploying and managing OpenShift clusters. The CLI tools wrap kubectl, podman (docker image manipulation) and other tools to provide a single easy-to-use interface. You can of course also use kubectl and individual tooling for manipulating images (docker or podman).      Download CLI Tools      Access&nbsp;https://cloud.redhat.com/openshift/install using your Red Hat account. Select AWS as infrastructure provider and the installer provisioned option. Download both the CLI and OpenShift install.      Copy Pull Secret      The pull secret is used to automatically authenticate to Red Hat content repositories for OpenShift during the install. You will need to provide the pull secret as part of a later step.         Install CLI Tools  The CLI tools can be simply extracted into /usr/bin.  $ sudo tar xvf openshift-client-linux.tar.gz -C /usr/bin  $ sudo tar xvf openshift-install-linux.tar.gz -C /usr/bin Check CLI Version  $ openshift-install version openshift-install 4.6.9 Install Configuration  Create Install Config  When creating an install config you will need to provide the platform, in this case AWS, the domain configured in route53, a name for the OpenShift cluster and of course the pull secret.  $ openshift-install create install-config \\ --dir=ocp_install ? SSH Public Key /root/.ssh/id_rsa.pub ? Platform aws ? Cloud aws ? Base Domain rh-southwest.com ? Cluster Name ocp4 ? Pull Secret [? for help] ************************************************ Edit Install Config  $ vi ocp_install/install-config.yaml  ...  compute:    architecture: amd64    hyperthreading: Enabled    name: worker    platform:      aws:        type: m5.large        userTags:          Contact: myemail@mydomain.com          AlwaysUp: True          DeleteBy: NEVER    replicas: 3  controlPlane:    architecture: amd64    hyperthreading: Enabled    name: master    platform:      aws:        type: m5.xlarge        userTags:          Contact: myemail@mydomain.com          AlwaysUp: True          DeleteBy: NEVER    replicas: 3  ... User tags allow you to label or tag the AWS VMs. In this case we have some automation that will shutdown any clusters that are not supposed to be always available or delete any clusters that have an expiration date set.  Deploy OpenShift Cluster  Now that we have adjusted the configuration we can deploy the cluster and grab a coffee.  $ [ktenzer@localhost ~]$ time openshift-install create cluster --dir=ocp_install --log-level=debug Verify Cluster  Once the deployment has completed successfully we can verify cluster. In order to gain access to cluster we can use the default system account created during deployment by exporting the kubeconfig.  $ export KUBECONFIG=ocp_install/auth/kubeconfig  $ oc get nodes  NAME                                       STATUS ROLES  AGE VERSION  ip-10-0-147-102.us-east-2.compute.internal Ready  worker 27m v1.19.0+7070803  ip-10-0-159-222.us-east-2.compute.internal Ready  master 33m v1.19.0+7070803  ip-10-0-161-231.us-east-2.compute.internal Ready  worker 24m v1.19.0+7070803  ip-10-0-178-131.us-east-2.compute.internal Ready  master 33m v1.19.0+7070803  ip-10-0-194-232.us-east-2.compute.internal Ready  worker 24m v1.19.0+7070803  ip-10-0-218-84.us-east-2.compute.internal Ready   master 33m v1.19.0+7070803 Show Cluster Version  $ oc get clusterversion  NAME    VERSION AVAILABLE PROGRESSING SINCE STATUS  version 4.6.9   True      False       7m44s Cluster version is 4.6.9 Configure OAUTH  OpenShift supports oauth2, there is a lot of choice with the identification provider. Usually you would integrate with your enterprise LDAP provider. In this case or for test environments you can use the htpasswd provider.  Create Htpasswd file  $ htpasswd -c -B -b ocp_install/ocp.htpasswd admin pasword123 Create Secret for Htpasswd  $ oc create secret generic htpass-secret --from-file=ocp.htpasswd -n openshift-config Configure Htpasswd in Oauth  $ vi ocp_install/htpasswd-cr.yaml  apiVersion: config.openshift.io/v1 kind: OAuth metadata:   name: cluster spec:   identityProviders:   - name: my_htpasswd_provider     mappingMethod: claim     type: HTPasswd     htpasswd:       fileData:         name: htpass-secret $ oc apply -f ocp_install/htpasswd-cr.yaml Add Cluster Role  Once we have added htpasswd as a oauth provider we need to add cluster-admin role to the admin user we already created.  $ oc adm policy add-cluster-role-to-user cluster-admin admin Summary    In this article we discussed the IPI and UPI deployment methods provided by OpenShift. Most organizations will want to choose IPI where possible. It not only simplifies the deployment of OpenShift but also improves the supportability likely leading to a higher SLA or quicker resolution of any potential issues. If not possible, UPI provides all the flexibility needed to deploy OpenShift into any environment albeit with a little more oversight and ownership required. Finally we walked through the IPI deployment of OpenShift on AWS including showing how to configure OAUTH htpasswd provider.      (c) 2021 Keith Tenzer     ","categories": ["OpenShift"],
        "tags": ["AWS","OpenShift","Kubernetes"],
        "url": "/openshift/openshift-4-aws-ipi-installation-getting-started-guide/",
        "teaser": null
      },{
        "title": "OpenShift Service Mesh Getting Started Guide",
        "excerpt":"             Overview  In this article we will explore the OpenShift Service Mesh and deploy a demo application to better understand the various concepts. First you might be asking yourself, why do I need a Service Mesh? If you have a microservice architecture then you likely have many services that interact with one another in various ways. If a downstream service breaks, how will you know? How can you trace an error through all services to find out where it originated? How will you manage exposing new APIs/capabilities to users? Service Mesh provides the answer to those questions providing 1) Visibility across all microservices 2) Traceability through all of the microservice interactions and 3) Ruleset governing service versions and capabilities that are introduced into the environment. OpenShift Service Mesh uses Istio as the mesh, Kiali for the dashboard and Jaeger for traceability.     Install OpenShift Service Mesh  OpenShift Service Mesh enables visibility of traffic as is traverses all services within the mesh. In case of an error you can not only see what service is actually the source for the error but also are able to trace the API communications between services. Finally it allows us to implement rules, like balancing traffic between multiple ratings API endpoints or even sending specific users to an API version. You can even create a chaos monkey using the ruleset that will inject various errors so you can observe how failures are handled. A Service Mesh is critical for any complex microservice application and without it you are literally flying blind while adding technical debt unable to manage or monitor service interactions properly.  Create a new project  First we will create a project for hosting the service mesh control plane.  $ oc new-project bookinfo-mesh Install OpenShift Service Mesh  Under the project booking-mesh go to Operator Hub and install Red Hat OpenShift Service Mesh, Red Hat OpenShift Jaeger and Kiali Operator.       Configure OpenShift Service Mesh  Create a service mesh control plane using defaults.  Create a service mesh member. Update the yaml and change namespace under controlPlaneRef to bookinfo-mesh. apiVersion: maistra.io/v1 kind: ServiceMeshMember metadata:   namespace: bookinfo-mesh   name: default spec:   controlPlaneRef:     name: basic     namespace: bookinfo-mesh Finally create a service member role adding name of the project that will access the service mesh..  apiVersion: maistra.io/v1 kind: ServiceMeshMemberRoll metadata:   namespace: bookinfo-mesh   name: default spec:   members:     - bookinfo   Deploy Demo Application  In order to look into the capabilities provided by the OpenShift Service Mesh we will deploy a simple book review app. The application is static but shows reviews for a book and has several microservices. The product page is the entry point service. It provides information on books and their reviews. It accesses book reviews through the reviews service which is also accesses the ratings service downstream to allow users to give a book a rating. It also gets details about a book via the details service. Like a true polyglot all services are written in a different programming language. The ratings service provides three different API versions, v1 doesn't display ratings, v2 shows ratings in black and v3 shows ratings in red. The idea is all about innovating quickly and getting real user feedback to take the app in the right direction.    Create a project for the book app.  Create a project to host the book app.  $ oc new-project bookinfo Deploy book app.  $ oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-2.0/samples/bookinfo/platform/kube/bookinfo.yaml Create a service mesh gateway.  Once the app is deployed we need to create a gateway and setup the URI matches.  $ oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-2.0/samples/bookinfo/networking/bookinfo-gateway.yaml Create service mesh rule set.  Since the ratings service has 3 API versions we need some rules to govern the traffic.  $ oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-2.0/samples/bookinfo/networking/destination-rule-all.yaml Access the application.  Get the route to the application and add the /productpage to access via web browser.  $ export GATEWAY_URL=$(oc -n istio-system get route istio-ingressgateway -o jsonpath='{.spec.host}') $ echo $GATEWAY_URL http://istio-ingressgateway-bookinfo-mesh.apps.ocp4.rh-southwest.com/productpage         If you refresh the web page you should see the ratings change between no ratings, black and red. This is because the ruleset is sending traffic to all three API endpoints (v1, v2 and v3).  Update Service Mesh Ruleset  As mentioned we can create rules that enable us to send certain traffic to a different API endpoint. This could be for a canary deployment where we want to test how users interact with different capabilities (red or black ratings stars), to perform a blue/green deployment upgrading to a newer API version or even sending different users to different APIs.  In this case we will simply apply a new rule that only sends traffic to v2 (black) and v3 (red) ratings API.  Apply a new ruleset.  $ oc replace -f https://raw.githubusercontent.com/istio/istio/master/samples/bookinfo/networking/virtual-service-reviews-v2-v3.yaml Now when you access the book app and refresh you should see it switched between red and black ratings.  Looking at Kiali versioned app graph we can see that traffic is only going to v2 and v3 as we would expect.               Troubleshooting Errors  As mentioned one of the key values of the OpenShift Service Mesh is the visualization and tracing. In order to generate an error we will scale down the ratings v1 deployment to 0.  $ oc scale deployment/ratings-v1 -n bookinfo --replicas 0   You should now see that the ratings service is currently unavailable when you refresh the app in a browser.  Check Kiali dashboard.  You can access Kiali via route under networking in the bookinfo-mesh project. In Kiali we clearly see the issue is the reviews service as we would expect.        Open Jaeger to trace the calls that are failing.  Next we can dig into the requests by opening distributed tracing (Jaeger) from the Kiali dashboard. We can see the flow of all calls grouped and identify all request to the ratings service that are throwing an error. We see a 503 is returned which means service in unavailable.         Kiali dashboard also shows request response (latency in ms) for slowest 50%, 95% and 99% of requests. This allows us to not only see the average latency but compare it to the slowest requests which indicates if we have spikes that could cause user slowness that we might not easily see when looking at just the average.      Summary  In this article we discussed the capabilities OpenShift Service Mesh provides for microservice applications. We deployed a service mesh and a book demo polyglot application that leverages the OpenShift Service Mesh. Using Kiali we saw how to gain visibility into our microservice application and easily identify problems as well as trends. Through Jaeger and distributed tracing we were able to identify the exact API causing error conditions. Microservice architectures provide a lot of value but they are harder to manage, control and troubleshoot in certain aspects than their monolithic peers which is why OpenShift Service Mesh is so critical.  (c) 2021 Keith Tenzer    ","categories": ["OpenShift"],
        "tags": ["Containers","istio","jaeger","kiali","microservice","OpenShift","polyglot","service mesh"],
        "url": "/openshift/openshift-service-mesh-getting-started-guide/",
        "teaser": null
      },{
        "title": "Building Ansible Operators 1-2-3",
        "excerpt":"Overview  In this article we will go step by step to build a Kubernetes Operator using Ansible and the Operator Framework. Operators provide the ability to not only deploy an application but also manage the entire lifecycle while also baking in higher degrees of intelligence as well as resilience. Before we get to the Operator lets briefly revisit the ways we can deploy and manage applications in k8s.     Deployment   Deployment Template   Helm   Operator   A k8s deployment is the simplest method but there is no way to parameterize unless you are doing it through Ansible k8s module or something else that handles that. A deployment template does provide parameterization but is only available on OpenShift and also doesn’t handle packaging or management of the application itself. Helm provides parameterization and also packaging but doesn’t provide application lifecycle management or allow for building extended intelligence for added resilience. Operators provide the full package and provide the pattern to improve operability over time. Some suggest just using the simplest tool for the job. If you just need to deploy an app for example and don’t need parameterization, use a k8s deployment. Personally my view is that with Ansible, Operators are just about as easy as a k8s deployment but with so much added benefit that the Operator approach always makes sense. My hope and goal with this article is to maybe influence a few more Operators and show that it isn’t really any additional work.   Pre-requisites  Your starting point should be an application you can deploy using a k8s deployment or deployment config. From there the next thing is to setup a development environment for building Operators using the Operator Framework and Ansible. Operator Pre-requisites Once you have operator-sdk and openshift or kubectl client you need some Ansible dependencies.  $ sudo dnf install -y ansible $ sudo dnf install -y python-ansible-runner $ sudo dnf install -y python-ansible-runner $ sudo pip install ansible-runner-http $ sudo pip install openshift $ ansible-galaxy collection install kubernetes.core  $ ansible-galaxy collection install operator_sdk.util   Create Operator Scaffolding  Before we can begin coding the Operator we need boiler plate code and thankfully the operator sdk does all that.  $ operator-sdk init --plugins=ansible --domain=kubecraft.com $ operator-sdk create api     --group cache     --version v1     --kind Kubecraft     --generate-role  Customizing Operator  At this point we need an application. My approach is to first create a k8s deployment and test deploying my application before building the Operator. In this example we will use an app called Kubekraft. It is a fun app that connects k8s world to minecraft through a minecraft websocket server written in Go. Browse to the yaml folder and you will see the k8s deployment.yaml. This is what we will use to build out our Operator. Under roles/kubecraft/tasks directory we create tasks in Ansible to deploy what we had in the k8s deployment yaml which is a deployment, service and route. In addition we added a task to get the application domain dynamically so we can build our route properly. This also demonstrates how to query and use other k8s resources in our Ansible code.   In addition, if your operator creates k8s resources you need to ensure proper permissions. Under the config/rbac directory you can add permissions to the role.yaml. For this operator I added services and routes so that those resources  can be created by the Operator.   Testing Operator  The operator sdk provides a simple way to test the operator locally. Once we have our tasks complete under the role/project/tasks directory, we simply need to create a new project, run the operator locally and create a custom resource (CR) which executes our role and tasks we defined.   Create project  $ oc new-project kubecraft  Run operator locally Run these commands from the root directory of your operator project where the Makefile resides.  $ make install $ make run  Create custom resource In a new terminal create CR. Also note that our operator expects as user input, a comma separated list of namespaces to monitor. User input is parameterized via the custom resource so if you look inside you will see the namespaces parameter set and being consumed within the Ansible role.  $ oc create -f config/samples/cache_v1_kubecraft.yaml  Building Operator  Once we have tested the operator locally we can build and publish our operator to a registry. Authenticate to registry  $ sudo docker login https://registry.redhat.io  Build operator image and push to registry  $ sudo make docker-build docker-push IMG=quay.io/ktenzer/kubecraft-operator:latest If you are using quay.io as your registry make sure to login and make the image is public so it can be accessed.   Running Operator  Now that we have the operator tested and the image built we can simply deploy it, create a CR and rule the world!  The operator sdk makes this really easy and streamlines everything into a single command.  $ make deploy IMG=quay.io/ktenzer/kubecraft-operator:latest  BY default the operator will be installed into a project operatorName-system however you can change that by updating the project name in the PROJECT file under the root of the operator project. In this case we changed it to kubecraft-operator.   We can remove the operator also using make.  $ make undeploy  Create Operator Bundle  The operator bundle allows integration with operator lifecycle manager (olm) which provides a facility for upgrading operator seamlessly as well as integrating with operator hub. First we will generate the bundle boiler plate.  $ make bundle If you want to change anything, like add image you can update bundle/manifests clusterserviceversion. When your ready you will build bundle and then push it to your repository. Remember if using quay.io to make the image public.  $ sudo make bundle-build BUNDLE_IMG=quay.io/ktenzer/kubecraft-operator-bundle:latest Push bundle to registry.  $ sudo docker push quay.io/ktenzer/kubecraft-operator-bundle:latest Run operator bundle  Now that we have the bundle built and pushed to registry we can deploy it to a cluster.  $ operator-sdk run bundle -n kubecraft-operator quay.io/ktenzer/kubecraft-operator-bundle:latest Once our operator bundle is running simply create a new project, a CR and watch the magic happen.  $ oc project kubecraft $ oc create -f config/samples/cache_v1_kubecraft.yaml $ oc get deployment NAME        READY   UP-TO-DATE   AVAILABLE   AGE kubecraft   1/1     1            1           5s   Summary  In this article we created an operator using the operator sdk and Ansible. We saw how to take a simple k8s deployment and turn it into a fully managed operator. Finally we created a bundle showing the integration with operator lifecycle manager and operator hub. Hopefully this article helps you get started with Ansible operators and next time you deploy an application in k8s you would consider the operator approach.   (c) 2021 Keith Tenzer   ","categories": ["OpenShift"],
        "tags": ["Containers","OpenShift","Operators","Operator Framework","Ansible"],
        "url": "/openshift/building-ansible-operators-1-2-3/",
        "teaser": null
      },{
        "title": "The Fedora Workstation Experience",
        "excerpt":"  Overview  A lot of people always ask me what is the best way to contribute to opensource? Of course contributing code, documentation, spreading the gospel or even talking about opensource are great. However the easiest and seemingly least obvious method is to actually use opensource. The best way to do that is by running Linux on your workstation. There are a lot of great Linux distro’s to choose from and me being a RHatter, well let’s just say I have an impartial view.   Being an ex-macbooker, I love the simplicity and minimalist approach of Apple’s UI design and experience but I dislike their proprietary, closed system approach. As such I aim to find the best of both worlds: great UI experience without compromising on freedom, choice and promoting the power of opensource!   Hardware  Currently I have a Lenovo Thinkpad X1 but great thing here is you have choice. I get that Apple’s Macbook hardware has a great look and feel (touchpad works great) but those are pretty minor things and your going to cover your laptop in stickers anyway, right?   Installing Fedora Latest  Fedora has a media writer and you can download this to create a USB Image to install Fedora. You can provide your own ISO or just let it create an image using the latest Fedora. Once you have USB image simply reboot your laptop/desktop press F12 or F8 to enter boot menu and boot from your USB. The rest is just accepting defaults. If your more advanced user you may want to also setup your own filesystem partitions and I always recommend turning on disk encryption with LUKS.   Customizing Fedora Workstation  Once you get booted into Fedora it’s time to customize. Unlike MacOS or Windows you can truly customize the desktop environment and while that is powerful and rewarding it also can be time consuming as well as turn most people off. The point here is to get something great with limited effort.   Update Fedora  Just like any OS clicking the update button is usually the first step. You can of course click on Applications-&gt;System Tools-&gt;Software which launches the UI software package manager but this is Linux right?   $ sudo dnf update -y     Install GNOME Extentions  GNOME is the UI operating environment and has a modular plugin framework for extending it. There have been huge advances in GNOME performance and while developers do all the hard work, users make those contributions meaningful which in turns leads to more development resources.  $ sudo dnf install gnome-extensions-app     Install Dock from Dash Extensions  This extension will add a docking bar in center for favorite applications just like MacOS. Navigate to https://extensions.gnome.org/ and in search filled enter “dock from dash”. Click on the extension and in upper right there is a slider to enable the extension.      Install GNOME Tweaks  Using GNOME tweaks you can configure many aspects of the UI we will be using later   $ sudo dnf install -y gnome-tweaks  Install Google Chrome   $ sudo dnf install google-chrome  Disable Wayland  Overall wayland works but one area there is still issues is in screen sharing. If you care about that I would recommend disabling it.    $ sudo vi /etc/gdm/custom.conf WaylandEnable=false   Configure MacOS Theme  These steps will enable MacOS icons and desktop theme.   $ sudo dnf install la-capitaine-icon-theme  $ git clone https://github.com/paullinuxthemer/Mc-OS-themes.git  $ mkdir ~/.themes  $ cp -r Mc-OS-themes/McOS-MJV ~/.themes  Open GNOME Tweak tool Applications-&gt;Utilities-&gt;Tweak and navigate to appearance section. Set application theme to McOS-MJV and icons to La-Capitane. Navigate to window titlebars section and enable maximize/minimize under titlebar buttons. This adds buttons to all windows that let you maximize or minimize them.            Install RPM Fusion  This is a tool that gives you access to a lot of community developed tools and is very useful for workstations.   $ sudo dnf install https://download1.rpmfusion.org/free/fedora/rpmfusion-free-release-$(rpm -E %fedora).noarch.rpm  Optimize Battery Usage  There are additional drivers needed to ensure your battery is used efficiently. Not installing these leads to quicker battery drain.   $ sudo dnf install tlp tlp-rdw  For thinkpads add additional driver from RPM fusion.  $ dnf install https://download1.rpmfusion.org/free/fedora/rpmfusion-free-release-$(rpm -E %fedora).noarch.rpm   Install Multimedia Codecs  $ sudo dnf groupupdate multimedia --setop=\"install_weak_deps=False\" --exclude=PackageKit-gstreamer-plugin $ sudo dnf groupupdate sound-and-video  Setup Solarized  Solarized is a color scheme for terminal sessions. Let’s face it staring at a black screen with green text is bad and tiring for your eyes. These color patters are soothing and will let you enjoy starring at CLI terminals.  $ sudo dnf install -y p7zip $ mkdir ~/solarized $ cd ~/solarized $ git clone https://github.com/sigurdga/gnome-terminal-colors-solarized.git $ gnome-terminal-colors-solarized/install.sh  Setup vim and install solarized colors for vim.  $ sudo dnf install -y vim $ git clone https://github.com/altercation/vim-colors-solarized.git $ mkdir -p ~/.vim/colors $ cp vim-colors-solarized/colors/solarized.vim ~/.vim/colors/  Install VSCODE  Pretty much the standard IDE for software development, infrastructure-as-code, blogging or anything that requires editing text.  $ sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc  $ cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/vscode.repo [code] name=Visual Studio Code baseurl=https://packages.microsoft.com/yumrepos/vscode enabled=1 gpgcheck=1 gpgkey=https://packages.microsoft.com/keys/microsoft.asc EOF   $ sudo dnf check-update $ sudo dnf install -y code  Install Ansible (Optional)  If you aren’t automating stuff with Ansible it is never to late to change your life.  $ sudo dnf install -y ansible $ sudo dnf install -y python-ansible-runner $ pip install ansible-runner-http $ pip install openshift $ ansible-galaxy collection install kubernetes.core  Install Go (Optional)  Golang is by far my language of choice for it’s simplicity and elegance.  $ mkdir -p ~/go/src/github.com $ sudo dnf install -y go $ vi ~/.bash_profile export GOBIN=/home/username export GOPATH=/home/username/src export EDITOR=vim   Install OpenShift and Kubectl (Optional)  Likely there are newer releases so grab latest. OpenShift and Kubectl 4.9   Install Operator SDK (Optional)  If you want to put all those Ansible skills to use in cloud-native world start writing Operators.   Likely there are newer releases so grab latest or release for your OCP release. Operator Framework 4.9   Summary  In this article we discussed the importance of choice and at least why you might consider Linux for your next workstation operating system. We also go through a step-by-step guide in configuring your Fedora workstation and give it similar look and feel to MacOS. If I only convinced one person to give Linux a try then it was all worth it!   (c) 2022 Keith Tenzer   ","categories": ["Linux"],
        "tags": ["Linux","Fedora","Opensource"],
        "url": "/linux/The-Fedora-Workstation-Experience/",
        "teaser": null
      },{
        "title": "Blog with Gitops Practices and GitHub",
        "excerpt":"Overview  Want to build your brand, while living the gitops revolution and not paying anything for it? That is exactly what this article will walk you through. I will show you how to setup your customized blog hosted in Github and create blog posts, doing it the way developers do it, one commit at a time.   Now most people go to wordpress or some blog platform and pay money for it. I think that is a shame because it is a missed opportunity to live gitops which is at the center of automating everything. Github has long provided the ability to create a basic blogs using gh-pages. Github uses a framework written in ruby called Jekyll which is available outside of Github, it’s opensource. Jekyll provides a templating framework and handles all the html, css stuff, allowing you to just write your blog in markdown (which anyone can do). The Jekyll themes Github provides are seriously limited and no fun. Most people don’t know this but with a little extra effort you can actually use any Jekyll themes and there are 100’s if not 1000’s so you can create your brand just the way you like it.   Setup Github Repository  First if you don’t have a github account get one. Next once you have an account create a new repository. This should be the name of the blog. In my case my blog is keithtenzer.com so that is also the name of the repository. Next decide if you are okay with a github domain name or if you want your own custom domain. I strongly recommend you pay the $10 a year and get your name. It is your brand after all and using your name makes it that way and also allows you to blog about anything.   Create Your Domain (optional)  If you aren’t interested in your own domain you can skip this part. Decide where you want your domain to be hosted. I strongly recommend cloudflare. Purchase your domain by selecting buy domain.  Once you have a domain you need to point it at Github. Under websites, click DNS and configure DNS as follows. Simply replace keithtenzer.com with your domain. Also make sure you add a CNAME for www to &lt;pre&gt;username.github.io&lt;/pre&gt;.  Once DNS changes are done it can take 24 hours to propagate so I recommend waiting. You can check by querying DNS using nslookup or dig and seeing when your domain shows the IPs you entered for Github.   Initialize Blog in Github  Log in to Github and under repositories and create a new one.  Clone your repository by copying the code link and running it in a CLI terminal.  $ git clone https://github.com/ktenzer/helloblog.git You may need to install git.   Enable Blog  Once the repository is initialized we can enable the blog by going to settings. Under pages you simply need to select the branch (main) and fill out your custom domain if you have one.    Select a Jekyll Themes  As I mentioned there are many Jekyll themes. You can search for curated ones here. For my blog I use minimal-mistakes. Once you have chosen a theme simply clone it from Github in a separate directory from where you cloned your blog following same steps as above.  $ git clone https://github.com/mmistakes/minimal-mistakes.git Next copy the contents into your blog repository.  cp -r minimal-mistakes/* helloblog/  Customize Blog  Now that you have a theme it is time to customize. Under your blog directory edit the _config.yml. You need to uncomment the remote_theme. In addition you can add title, author information and whatever else.  $ vi _config.yml remote_theme           : \"mmistakes/minimal-mistakes\"   Setup Jekyll  Now that you have a blog, you are dying to publish it right? Not so fast, now it is time to test it locally. After all you wouldn’t want to publish anything you didn’t review first, right? In order to test locally you need to install and setup jekyll. This part is much easier if you are running Linux like me. If not maybe you should also consider switching to Linux, your Macbook is holding you back from greatness (trust me). You could also run Fedora as a virtual machine, well as long as you don’t have one of those shiny M1 Macbook’s (maybe someone will make that work in future though). Either way below are the steps for Fedora Linux.  $ cd helloblog $ sudo dnf install ruby $ sudo dnf install -y ruby-devel $ sudo dnf install -y g++ $ bundler install $ gem install eventmachine -v '1.2.7' -- --with-cppflags=-I/usr/local/opt/openssl/include $ gem install jekyll bundler $ sudo gem update --system 3.2.30 --install-dir=/usr/share/gems --bindir /usr/local/bin $ sudo gem pristine ffi --version 1.15.4 $ sudo gem pristine http_parser.rb --version 0.8.0 $ sudo gem pristine sassc --version 2.4.0 $ bundle install $ bundle add webrick   It may want you to pristine different version of gems. If that is the case just change the version to what it wants.   Test Blog  Now that Jekyll is setup we can test our new blog.  $ bundle exec jekyll serve  Configuration file: /home/ktenzer/helloblog/_config.yml             Source: /home/ktenzer/helloblog        Destination: /home/ktenzer/helloblog/_site  Incremental build: disabled. Enable with --incremental       Generating...         Jekyll Feed: Generating feed for posts                     done in 0.192 seconds.  Auto-regeneration: enabled for '/home/ktenzer/helloblog'     Server address: http://127.0.0.1:4000   Server running... press ctrl-c to stop.  Once Jekyll is running simply open your web browser of choice and go to the server address link.    Push Blog to Github  Now that we have tested our blog and are happy lets commit the code! If you want to brush up on Github I recommend the tutorials. First lets add our files locally from within our blog directory.  $ git add . To see the files that will be commited (optional).  $ git status -s Commit files locally.  $ git commit -a -m \"first blog entry\" Push changes into Github.  $ git push origin In order to push changes you will need a token. You can create a token under account settings-&gt;developer settings-&gt;personal access tokens in Github. Once changes are pushed it goes through CI/CD (Github Actions) and then is published. It usually takes just few minutes. You can check the status by looking at workflows under actions. Once the action which does CI/CD is complete check out your blog in production hello blog.      Add Post to Blog  Now that the blog is up and running let’s create a blog entry. Create the _posts directory and then create a new file with the format YYYY-MM-DD-post-name.md. Add the header information that defines title, layout, categories and tags. Layouts are located in the _layouts directory and provided by the Jekyll theme.  $ mkdir _posts $ vi _posts/2022-02-10-hello-world.md ---  layout: single title:  \"Hello World\" categories: - Linux tags: - Linux - Fedora - Opensource --- ![Hello World Image](/assets/2022-02-10/hello_world_md.png) ## Overview This is a hellow world blog ## Hello ## World ## Summary This was a hello world blog (c) 2022 Keith Tenzer  Add Image to Post  Images, videos and other artifacts go in the assets directory. It is best to organize under the date of your post so you can find stuff. First create a directory.  $ mkdir assets/2022-02-10 Next copy image to that directory and then you can link it in your post using the /assets/2022-02-10 path.  $ cp /home/ktenzer/Pictures/hello_world_md.png assets/2022-02-10/   Test Post  Simply run Jekyll again to test locally.  $ bundle exec jekyll serve Push Blog Post to Github  Repeat same steps above when we originally pushed our first commit.  First lets add our files locally from without our blog directory.  $ git add . To see the files that will be commited (optional).  $ git status -s Commit files locally.  $ git commit -a -m \"first blog entry\" Push changes into Github.  $ git push origin  Summary  Blogging is a great way to build your own brand and be an ambassador for what is important to you. It allows you to share your experiences and knowledge with the rest of the world. In this article we went through the steps to create your own unique blog platform using Jekyll and hosting it in Github (for free). One thing I learned that I will always take with me, sharing is caring.   (c) 2022 Keith Tenzer   ","categories": ["Linux"],
        "tags": ["Linux","Fedora","Opensource"],
        "url": "/linux/blog-with-gitops-practices-and-github/",
        "teaser": null
      },{
        "title": "My First Day at Temporal",
        "excerpt":"  Overview  Today is my first day at temporal and with that I wanted to share some thoughts around my decision, why Temporal and my experience thus far. As you can imagine, making any career change is always a very careful thought process and as we get older, have more responsibilities, the gravity around those decisions becomes stronger.   Why Temporal?  After IBM acquired Red Hat in 2018, I had the opportunity to visit the IBM research and development lab in Boeblingen, Germany. The idea was so RHatters could learn about some of the IBM history and why mainframes even today are great, at least with Linux. As you can imagine, I was more than skeptical. We spent years modernizing monolithic applications and moving workloads to Linux. Mainframe technology was a dinosaur that predated anything I ever worked on and as such I dismissed it, what could I possibly learn? Even once we arrived at the facility; the gray, lifeless, government looking buildings exhumed anything but innovation or creativity.   Of course as the cliche goes, Looks can and are deceiving. Our minds judge everything and blind us if we let them. So what did I learn on that day? Reliability, you can shoot an IBM mainframe with a high-powered weapon and it will continue to run. That is why still today, well into the era of cloud some of the most critical computing workflows (government, banks, aviation, insurance and healthcare) are still done on mainframes. One of the challenges with cloud-native is of course, reliability across many disparate services. Monolithic environments are simply much easier to control and anticipate. Developers are left to solve reliability on their own with 10s or even 100s of microservices potentially involved in a workflow. Of course, this ends up being a lot of code, technical debt and reliability is never quite certain, certainly not to the level of a mainframe.   Opportunity  Temporal is solving this problem through a workflow-as-code approach. Handling workflow state, execution, retries and allowing developers to orchestrate microservices into a workflow that is reliable as running water. It is a simple purpose with enormous potential. What could we accomplish if we further increased reliability of cloud-native applications and even started to modernize legacy applications shackled in their moorings? That has me extremely excited!   Execution  Still it is one thing to have great potential but it isn’t worth that much if you can’t execute. It is far better to have great execution of a poor plan, than poor execution of a great plan. At the end of the day, execution comes down to people and the will to overcome adversity or whatever else gets thrown in the way. My interview process at Temporal was not an interview at all. It was a conversation involving sharing ideas and thoughts bi-directionally. Everyone I met was passionate, focused, professional and curious. All seemed united, being part of the same cause but each with their own role to play, led by their passion.   Opensource  While I needed to understand Temporal’s purpose, believe the vision could be executed, there is still something more. For me, especially coming from Red Hat, the technology needed to be opensource. I saw a great interview with Maxim Fateev (CEO of Temporal) where he explains the reasoning why Temporal needs to be opensource? He says that anything developer orientated today, needs to be opensource or it simply won’t be seriously considered. This was not only reassuring but also gave me great satisfaction in just how far opensource adoption has come. The problems of today are simply bigger than any one company and opensource is the only way to collaborate together across organizations, to solve them.   The Right Fit  Finally the role also had to be the right fit. I had a desire to get closer to developers after being mostly on the platform side the last 7+ years. I am in fact a closet software hacker, it is one of the things I do for fun. The solution architect role at Temporal required someone of course with pre-sales skills but also software development skills (a less common combination). Temporal is consumed through an SDK by developers and as such being able to provide recommended practice or guidance to help customers also means understanding the code. I have long waited for a pre-sales role where my software development skills and passion were not just an asset but a requirement.   My Experience So Far  It might seem odd to talk about experience when this is literally day one but there is actually a lot to talk about. I have been on Temporal’s community Slack for several weeks. Following some of the discussions and after I accepted an offer folks started to reach out. Even asking for my feedback or ideas about certain things. I attended a pre-screen of a 101 training session that will be presented at Replay, Temporal’s first user conference August 25-26 in Seattle. I even got to provide feedback all before my actual start date. I have had my laptop now for about two weeks which I am proud to say runs Fedora 36 (Gnome 42) and the Friday before my start date I had all my accounts setup. It is day 1 and I am ready to rock!   Summary  In this article I discussed thoughts around my decision to make a career change and pursue a new opportunity at Temporal. I am extremely excited about taking my first steps on this new journey and cannot wait to discover what is around the corner. I hope that my perspective may be of help, in your career or life journey.   (c) 2022 Keith Tenzer   ","categories": ["Temporal"],
        "tags": ["Temporal","Opensource","Workflows","Activities"],
        "url": "/temporal/my-first-day-at-temporal/",
        "teaser": null
      },{
        "title": "Temporal Getting Started Guide",
        "excerpt":"  Overview  In this article we will walk through setup of a development environment for Temporal. There are of course, several ways you can run the Temporal server locally. You can use Docker compose, helm or an operator on minikube and temporalite. In addition you can also consume Temporal namespaces as-a-service via the Temporal cloud. I would recommend temporalite if you want to run the Temporal server disconnected on your laptop otherwise the Temporal cloud is definitely the best option. I will cover getting started with the Temporal cloud in a future post.   Temporalite Install  Temporalite is a simple packaging of the Temporal server. It serializes data to local files, instead of a database for simplicity and less resource consumption. It is intended only for testing and not production use. In order to install it you need to have a Go environment to build the binary.   Configure Go  Create directory structure for Go in your home directory.  $ mkdir -p ~/go/src/github.com  Install Go  $ sudo dnf install -y go  Update Profile with Go Environment   $ vi ~/.bash_profile export GOBIN=/home/username export GOPATH=/home/username/src export EDITOR=vim   Build Temporalite  Create directory for Temporal  $ mkdir ~/go/src/github.com/temporalio  Change directory to temporalio  $ cd temporalio  Clone temporalite GitHub repository  $ git clone https://github.com/temporalio/temporalite.git  Build temporalite binary  $ go install github.com/temporalio/temporalite/cmd/temporalite@latest  Copy temporalite binary to bin Directory  $ sudo cp /home/ktenzer/go/bin/temporalite /usr/local/bin  Create Directory for local Temporal Database  mkdir -p /home/ktenzer/.config/temporalite/db  Start temporalite  $ temporalite start --namespace default  Build CLI  Now that we have the Temporal server running via temporalite we need to build the CLI.   Change directory to temporalio  $ cd ~/go/src/github.com/temporalio  Clone Temporal GitHub repository  $ git clone https://github.com/temporalio/temporal.git  Run tctl Makefile  $ make update-tctl  Copy tctl binary to bin directory  $ sudo cp ~/go/bin/tctl /usr/local/bin  Start Temporal Workflow  In this case we will just run the helloworld sample from the many samples that Temporal provides.   Change directory to temporalio  $ cd ~/go/src/github.com/temporalio  Clone samples GitHub  repository  $ git clone https://github.com/temporalio/samples-go.git  Change directory to samples  $ cd samples-go  Start workflow   $ go run helloworld/starter/main.go 2022/08/24 11:46:11 INFO  No logger configured for temporal client. Created default one. 2022/08/24 11:46:11 Started workflow WorkflowID hello_world_workflowID RunID b93ef614-e6df-4578-a31b-7ebd3f59df55   Notice that the workflow starts but is waiting to execute. This is because there is no worker. The starter program will tell the Temporal server to execute a workflow however, since no worker is attached the Temporal server namespace, the workflow will be in running state until a worker becomes available.   The Temporal UI will also show that we are waiting for a worker.    Start Temporal Worker  Now that we have our Temporal server and a workflow running all that is needed is a worker. We will start the corresponding helloworld worker.   $ go run helloworld/worker/main.go  The worker should immediately run and complete the workflow.   2022/08/24 11:55:26 INFO  HelloWorld workflow completed. Namespace default TaskQueue hello-world WorkerID 7576@fedora@ WorkflowType Workflow WorkflowID hello_world_workflowID RunID b93ef614-e6df-4578-a31b-7ebd3f59df55 Attempt 1 result Hello Temporal!   If you look at your starter it also has now returned and displays the workflow result returned from the workflow.   2022/08/24 11:55:26 Workflow result: Hello Temporal!   In the UI we can now also see the workflow has completed.    View Workflow in CLI  Temporal provides the tctl CLI for interacting with the Temporal server.   List workflows   $ tctl workflow list   WORKFLOW TYPE |      WORKFLOW ID       |                RUN ID                | TASK QUEUE  | START TIME | EXECUTION TIME | END TIME     Workflow      | hello_world_workflowID | b93ef614-e6df-4578-a31b-7ebd3f59df55 | hello-world | 18:46:11   | 18:46:11       | 18:55:26     Show workflow details   $ tctl workflow show --workflow_id hello_world_workflowID --run_id b93ef614-e6df-4578-a31b-7ebd3f59df55    1  WorkflowExecutionStarted    {WorkflowType:{Name:Workflow},                                                                    ParentInitiatedEventId:0, TaskQueue:{Name:hello-world,                                            Kind:Normal}, Input:[\"Temporal\"],                                                                 WorkflowExecutionTimeout:0s, WorkflowRunTimeout:0s,                                               WorkflowTaskTimeout:10s, Initiator:Unspecified,                                                   OriginalExecutionRunId:b93ef614-e6df-4578-a31b-7ebd3f59df55,                                      Identity:5846@fedora@,                                                                            FirstExecutionRunId:b93ef614-e6df-4578-a31b-7ebd3f59df55,                                         Attempt:1, FirstWorkflowTaskBackoff:0s,                                                           ParentInitiatedEventVersion:0}                                     2  WorkflowTaskScheduled       {TaskQueue:{Name:hello-world,                                                                     Kind:Normal},                                                                                     StartToCloseTimeout:10s,                                                                          Attempt:1}                                                         3  WorkflowTaskStarted         {ScheduledEventId:2, Identity:7576@fedora@,                                                       RequestId:3c597463-65e4-4d61-b014-484634f32c37,                                                   SuggestContinueAsNew:false, HistorySizeBytes:0}                    4  WorkflowTaskCompleted       {ScheduledEventId:2, StartedEventId:3,                                                            Identity:7576@fedora@,                                                                            BinaryChecksum:0990e4a32efe7cd9c6c127b9cb51ecfc}                   5  ActivityTaskScheduled       {ActivityId:5,                                                                                    ActivityType:{Name:Activity},                                                                     TaskQueue:{Name:hello-world,                                                                      Kind:Normal},                                                                                     Input:[\"Temporal\"],                                                                               ScheduleToCloseTimeout:0s,                                                                        ScheduleToStartTimeout:0s,                                                                        StartToCloseTimeout:10s,                                                                          HeartbeatTimeout:0s,                                                                              WorkflowTaskCompletedEventId:4,                                                                   RetryPolicy:{InitialInterval:1s,                                                                  BackoffCoefficient:2,                                                                             MaximumInterval:1m40s,                                                                            MaximumAttempts:0,                                                                                NonRetryableErrorTypes:[]}}                                        6  ActivityTaskStarted         {ScheduledEventId:5, Identity:7576@fedora@,                                                       RequestId:9a1da09d-2b00-4c46-a945-9e24adc10c04,                                                   Attempt:1}                                                         7  ActivityTaskCompleted       {Result:[\"Hello                                                                                   Temporal!\"],                                                                                      ScheduledEventId:5,                                                                               StartedEventId:6,                                                                                 Identity:7576@fedora@}                                             8  WorkflowTaskScheduled       {TaskQueue:{Name:fedora:e78b4021-eed7-4cb9-880e-9a7354471838,                                     Kind:Sticky}, StartToCloseTimeout:10s, Attempt:1}                  9  WorkflowTaskStarted         {ScheduledEventId:8, Identity:7576@fedora@,                                                       RequestId:8cc430d3-3d73-4261-961e-7dacd1b13005,                                                   SuggestContinueAsNew:false, HistorySizeBytes:0}                   10  WorkflowTaskCompleted       {ScheduledEventId:8, StartedEventId:9,                                                            Identity:7576@fedora@,                                                                            BinaryChecksum:0990e4a32efe7cd9c6c127b9cb51ecfc}                  11  WorkflowExecutionCompleted  {Result:[\"Hello                                                                                   Temporal!\"],                                                                                      WorkflowTaskCompletedEventId:10}    Summary  In this article we discussed several options for running the Temporal server, including using the Temporal cloud. We walked through the steps; deploying a quick local development environment using temporalite. Finally we showed how to execute and get workflow details using both the Temporal CLI and UI. Best of luck in your journey to more reliable applications with Temporal! If you have any questions reach out on Temporal community slack.   (c) 2022 Keith Tenzer   ","categories": ["Temporal"],
        "tags": ["Temporal","Opensource","Workflows","Activities"],
        "url": "/temporal/temporal_getting_started_guide/",
        "teaser": null
      }]
