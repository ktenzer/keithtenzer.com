---
layout: feedback
title: alex - 2018-05-04 14:45:29
date: 2018-05-04 14:45:29.000000000 -07:00
type: feedback
parent_id: '43'
published: true
password: ''
status: publish
categories: []
tags: []
meta:
  _feedback_extra_fields: a:0:{}
  _feedback_email: "a:2:{s:2:\"to\";a:1:{i:0;s:22:\"keith.tenzer@gmail.com\";}s:7:\"message\";s:1161:\"<b>Name:</b>
    alex<br /><br />\n<b>Email:</b> alessandro.parodi@softeco.it<br /><br />\n<b>Comment:</b>
    Hi, I have a problem with a cluster Pacemaker (0.9.158) Corosync (Corosync Cluster
    Engine 2.4.0), composed by two servers (Oracle Cloud) with Oracle Linux Server
    7.4.<br />\r\nOn one of the two node (for example node1), a service seems to fail
    a great number of times, until exhaust the counter of attempts.<br />\r\nAt this
    point, correctly, the service  is activated on the other node (node2).<br />\r\nIf
    appens a new change of server (for example in case of shutdown of the node2),
    on the node1 Pacemeker doesn't try to restart the service. It doesn't apparently
    reset the number of failed attempts. The situation is restored only following
    the cleanup (pcs resource cleanup).<br />\r\nThere is any solution? Is possible
    to tell to pacemaker that need to reset the number of failed attempts when, for
    example, the resource is activated on the other node?<br />\r\n<br />\r\nThanks,
    alex<br /><br />\n\nTime: May 4, 2018 at 2:45 pm<br />\nIP Address: 93.147.164.133<br
    />\nContact Form URL: https://keithtenzer.com/contact/<br />\nSent by an unverified
    visitor to your site.\";}"
author: 
permalink: "/"
---
<p>Hi, I have a problem with a cluster Pacemaker (0.9.158) Corosync (Corosync Cluster Engine 2.4.0), composed by two servers (Oracle Cloud) with Oracle Linux Server 7.4.<br />
On one of the two node (for example node1), a service seems to fail a great number of times, until exhaust the counter of attempts.<br />
At this point, correctly, the service  is activated on the other node (node2).<br />
If appens a new change of server (for example in case of shutdown of the node2), on the node1 Pacemeker doesn't try to restart the service. It doesn't apparently reset the number of failed attempts. The situation is restored only following the cleanup (pcs resource cleanup).<br />
There is any solution? Is possible to tell to pacemaker that need to reset the number of failed attempts when, for example, the resource is activated on the other node?</p>
<p>Thanks, alex<br />
<!--more--><br />
AUTHOR: alex<br />
AUTHOR EMAIL: alessandro.parodi@softeco.it<br />
AUTHOR URL:<br />
SUBJECT: Midnight Code Junkie<br />
IP: 93.147.164.133<br />
Array<br />
(<br />
    [1_Name] =&gt; alex<br />
    [2_Email] =&gt; alessandro.parodi@softeco.it<br />
    [3_Comment] =&gt;  Hi, I have a problem with a cluster Pacemaker (0.9.158) Corosync (Corosync Cluster Engine 2.4.0), composed by two servers (Oracle Cloud) with Oracle Linux Server 7.4.<br />
On one of the two node (for example node1), a service seems to fail a great number of times, until exhaust the counter of attempts.<br />
At this point, correctly, the service  is activated on the other node (node2).<br />
If appens a new change of server (for example in case of shutdown of the node2), on the node1 Pacemeker doesn't try to restart the service. It doesn't apparently reset the number of failed attempts. The situation is restored only following the cleanup (pcs resource cleanup).<br />
There is any solution? Is possible to tell to pacemaker that need to reset the number of failed attempts when, for example, the resource is activated on the other node?</p>
<p>Thanks, alex<br />
)</p>
